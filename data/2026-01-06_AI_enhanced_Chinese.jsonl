{"id": "2601.00814", "pdf": "https://arxiv.org/pdf/2601.00814", "abs": "https://arxiv.org/abs/2601.00814", "authors": ["Abhishek Kumar"], "title": "Semantic Alignment of Multilingual Knowledge Graphs via Contextualized Vector Projections", "categories": ["cs.AI"], "comment": null, "summary": "The paper presents our work on cross-lingual ontology alignment system which uses embedding based cosine similarity matching. The ontology entities are made contextually richer by creating descriptions using novel techniques. We use a fine-tuned transformer based multilingual model for generating better embeddings. We use cosine similarity to find positive ontology entities pairs and then apply threshold filtering to retain only highly similar entities. We have evaluated our work on OAEI-2022 multifarm track. We achieve 71% F1 score (78% recall and 65% precision) on the evaluation dataset, 16% increase from best baseline score. This suggests that our proposed alignment pipeline is able to capture the subtle cross-lingual similarities.", "AI": {"tldr": "提出基于嵌入余弦相似度匹配的跨语言本体对齐系统，通过创新描述生成技术增强本体实体上下文丰富度，使用微调的多语言transformer模型生成更好的嵌入表示，在OAEI-2022评测中获得71% F1分数，比最佳基线提升16%。", "motivation": "解决跨语言本体对齐的挑战，通过增强实体描述的上下文信息来提升对齐效果", "method": "使用微调的多语言transformer模型生成实体嵌入，通过余弦相似度匹配和阈值过滤来识别高度相似的本体实体对", "result": "在OAEI-2022 multifarm评测数据集上获得71% F1分数（召回率78%，精确率65%），比最佳基线方法提升16%", "conclusion": "提出的对齐管道能够有效捕捉跨语言的细微相似性，证明了基于嵌入和描述增强的方法在跨语言本体对齐任务中的有效性"}}
{"id": "2601.00816", "pdf": "https://arxiv.org/pdf/2601.00816", "abs": "https://arxiv.org/abs/2601.00816", "authors": ["Ismail Ahmad Abdullah"], "title": "MathLedger: A Verifiable Learning Substrate with Ledger-Attested Feedback", "categories": ["cs.AI", "cs.CR", "cs.LG"], "comment": "14 pages, 1 figure, 2 tables, 2 appendices with full proofs. Documents v0.9.4-pilot-audit-hardened audit surface with fail-closed governance, canonical JSON hashing, and artifact classification. Phase I infrastructure validation; no capability claims", "summary": "Contemporary AI systems achieve extraordinary performance yet remain opaque and non-verifiable, creating a crisis of trust for safety-critical deployment. We introduce MathLedger, a substrate for verifiable machine cognition that integrates formal verification, cryptographic attestation, and learning dynamics into a single epistemic loop. The system implements Reflexive Formal Learning (RFL), a symbolic analogue of gradient descent where updates are driven by verifier outcomes rather than statistical loss.\n  Phase I experiments validate the measurement and governance substrate under controlled conditions. CAL-EXP-3 validates measurement infrastructure (Delta p computation, variance tracking); separate stress tests confirm fail-closed governance triggers correctly under out-of-bounds conditions. No convergence or capability claims are made. The contribution is infrastructural: a working prototype of ledger-attested learning that enables auditability at scale.\n  Keywords: verifiable learning, formal verification, cryptographic attestation, reflexive feedback, fail-closed governance", "AI": {"tldr": "MathLedger是一个可验证机器学习认知系统，通过结合形式化验证、密码学证明和学习动态，创建了一个可审计的机器学习基础设施原型", "motivation": "当前AI系统虽然性能优异但缺乏透明度和可验证性，在安全关键应用中存在信任危机，需要建立可验证的机器学习框架", "method": "采用Reflexive Formal Learning (RFL)方法，这是梯度下降的符号类比，通过验证器结果而非统计损失驱动更新，整合形式化验证、密码学证明和学习动态", "result": "第一阶段实验验证了测量和治理基础设施，CAL-EXP-3验证了测量基础设施，压力测试确认了在超出范围条件下故障关闭治理触发器的正确性，建立了可扩展审计的工作原型", "conclusion": "该研究提供了一个基础设施层面的贡献，实现了账本证明的学习系统，为大规模可审计性奠定了基础，但尚未提出收敛性或能力声明"}}
{"id": "2601.00818", "pdf": "https://arxiv.org/pdf/2601.00818", "abs": "https://arxiv.org/abs/2601.00818", "authors": ["Chandra Sekhar Kubam"], "title": "Agentic AI for Autonomous, Explainable, and Real-Time Credit Risk Decision-Making", "categories": ["cs.AI"], "comment": "8 pages", "summary": "Significant digitalization of financial services in a short period of time has led to an urgent demand to have autonomous, transparent and real-time credit risk decision making systems. The traditional machine learning models are effective in pattern recognition, but do not have the adaptive reasoning, situational awareness, and autonomy needed in modern financial operations. As a proposal, this paper presents an Agentic AI framework, or a system where AI agents view the world of dynamic credit independent of human observers, who then make actions based on their articulable decision-making paths. The research introduces a multi-agent system with reinforcing learning, natural language reasoning, explainable AI modules, and real-time data absorption pipelines as a means of assessing the risk profiles of borrowers with few humans being involved. The processes consist of agent collaboration protocol, risk-scoring engines, interpretability layers, and continuous feedback learning cycles. Findings indicate that decision speed, transparency and responsiveness is better than traditional credit scoring models. Nevertheless, there are still some practical limitations such as risks of model drift, inconsistencies in interpreting high dimensional data and regulatory uncertainties as well as infrastructure limitations in low-resource settings. The suggested system has a high prospective to transform credit analytics and future studies ought to be directed on dynamic regulatory compliance mobilizers, new agent teamwork, adversarial robustness, and large-scale implementation in cross-country credit ecosystems.", "AI": {"tldr": "本文提出了一个基于多智能体系统的Agentic AI框架，用于实现自主、透明、实时的信用风险评估，相比传统模型在决策速度、透明度和响应性方面表现更好，但仍存在模型漂移、高维数据解释不一致等挑战。", "motivation": "金融服务快速数字化对自主、透明、实时的信用风险决策系统提出了迫切需求，传统机器学习模型缺乏自适应推理、情境感知和自主性能力。", "method": "采用多智能体系统，结合强化学习、自然语言推理、可解释AI模块和实时数据吸收管道，包含智能体协作协议、风险评分引擎、可解释性层和持续反馈学习循环。", "result": "研究结果表明，该系统在决策速度、透明度和响应性方面优于传统信用评分模型。", "conclusion": "该系统具有变革信用分析的潜力，未来研究应关注动态监管合规机制、新型智能体协作、对抗鲁棒性以及跨国信用生态系统的大规模实施。"}}
{"id": "2601.00821", "pdf": "https://arxiv.org/pdf/2601.00821", "abs": "https://arxiv.org/abs/2601.00821", "authors": ["Tao An"], "title": "CogCanvas: Compression-Resistant Cognitive Artifacts for Long LLM Conversations", "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": "15 pages, 5 figures", "summary": "Large language models face a fundamental tension between context window limits and information fidelity in long conversations. Existing approaches--truncation and summarization--either discard early information or lose nuanced details. We introduce CogCanvas, a training-free framework that extracts verbatim-grounded cognitive artifacts (decisions, facts, reminders) from conversation turns and organizes them into a temporal-aware graph for compression-resistant retrieval.\n  On the LoCoMo benchmark, CogCanvas achieves 34.7% overall accuracy, outperforming RAG (25.6%, +9.1pp) and GraphRAG (13.7%, +21.0pp). The advantage is most pronounced on temporal reasoning: 31.5% vs. 9.3% (RAG) and 5.0% (GraphRAG)--a +530% relative improvement. On multi-hop causal reasoning, CogCanvas achieves 81.0% pass rate vs. 40.0% for GraphRAG (+41.0pp). Controlled benchmarks show 97.5% recall (+78.5pp vs. summarization) with 93.0% exact match preservation.\n  While heavily-optimized approaches achieve higher absolute scores through dedicated training (EverMemOS: approximately 92%), our training-free approach provides practitioners with an immediately-deployable alternative that significantly outperforms standard baselines. Code and data: https://github.com/tao-hpu/cog-canvas.", "AI": {"tldr": "CogCanvas是一个无需训练的框架，通过从对话中提取精确的认知信息并组织成时间感知图，解决了大语言模型在长对话中的信息保真度问题，在LoCoMo基准测试中显著优于RAG和GraphRAG。", "motivation": "大语言模型在长对话中面临上下文窗口限制和信息保真度的根本矛盾，现有方法（截断和摘要）要么丢弃早期信息，要么丢失细节信息。", "method": "提出CogCanvas框架，从对话轮次中提取基于原文的认知信息（决策、事实、提醒），并将其组织成时间感知图，实现抗压缩检索。", "result": "在LoCoMo基准测试中：总体准确率34.7%（比RAG高9.1个百分点，比GraphRAG高21.0个百分点）；时间推理任务表现突出（31.5% vs RAG 9.3%）；多跳因果推理通过率81.0%（比GraphRAG高41.0个百分点）；控制基准测试显示97.5%召回率和93.0%精确匹配保持率。", "conclusion": "虽然经过专门训练的方法可以获得更高的绝对分数，但CogCanvas这种无需训练的方法为实践者提供了一个即插即用的替代方案，显著优于标准基线方法。"}}
{"id": "2601.00797", "pdf": "https://arxiv.org/pdf/2601.00797", "abs": "https://arxiv.org/abs/2601.00797", "authors": ["Hugues Draelants"], "title": "The Qualitative Laboratory: Theory Prototyping and Hypothesis Generation with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "comment": "26 pages, 3 tables. Manuscript submitted for peer-reviewed journal publication", "summary": "A central challenge in social science is to generate rich qualitative hypotheses about how diverse social groups might interpret new information. This article introduces and illustrates a novel methodological approach for this purpose: sociological persona simulation using Large Language Models (LLMs), which we frame as a \"qualitative laboratory\". We argue that for this specific task, persona simulation offers a distinct advantage over established methods. By generating naturalistic discourse, it overcomes the lack of discursive depth common in vignette surveys, and by operationalizing complex worldviews through natural language, it bypasses the formalization bottleneck of rule-based agent-based models (ABMs). To demonstrate this potential, we present a protocol where personas derived from a sociological theory of climate reception react to policy messages. The simulation produced nuanced and counter-intuitive hypotheses - such as a conservative persona's rejection of a national security frame - that challenge theoretical assumptions. We conclude that this method, used as part of a \"simulation then validation\" workflow, represents a superior tool for generating deeply textured hypotheses for subsequent empirical testing.", "AI": {"tldr": "本文提出使用大型语言模型进行社会学角色模拟的新方法，作为生成丰富定性假设的'定性实验室'，相比传统方法具有独特优势", "motivation": "解决社会科学中如何生成不同社会群体对新信息解释的丰富定性假设这一核心挑战，克服现有方法在话语深度和形式化瓶颈方面的局限", "method": "社会学角色模拟方法，使用LLMs创建基于社会学理论的虚拟角色，让这些角色对政策信息做出反应，生成自然主义的话语", "result": "模拟产生了细致且反直觉的假设，如保守角色拒绝国家安全框架，挑战了理论假设", "conclusion": "该方法作为'模拟后验证'工作流程的一部分，是生成深度丰富假设供后续实证检验的优越工具"}}
{"id": "2601.00823", "pdf": "https://arxiv.org/pdf/2601.00823", "abs": "https://arxiv.org/abs/2601.00823", "authors": ["Austin R. Ellis-Mohr", "Max Hartman", "Lav R. Varshney"], "title": "Energy-Aware Routing to Large Reasoning Models", "categories": ["cs.AI", "cs.IT", "eess.SY"], "comment": null, "summary": "Large reasoning models (LRMs) have heterogeneous inference energy costs based on which model is used and how much it reasons. To reduce energy, it is important to choose the right LRM and operate it in the right way. As a result, the performance of systems that dispatch tasks to different individual LRMs depend on the balance between mean energy provisioning and stochastic fluctuations. The critical regime is the unique operating point at which neither auxiliary energy nor baseline energy is systematically wasted. Increasing baseline supply shifts the system toward persistent over-supply and baseline-energy waste, while reducing supply induces persistent reliance on auxiliary energy. Yet in this regime, performance remains volatility-limited and so a second-order characterization provides further insights that we develop. Here, performance is governed by how variability is absorbed across time, models, and execution choices. This perspective highlights variance-aware routing and dispatch as a principled design axis, and provides a theoretical basis for developing energy-aware model routing policies. Routing behavior is characterized when dispatch policies are based on training-compute and inference-compute scaling laws for LRMs.", "AI": {"tldr": "该论文研究大型推理模型(LRMs)的能耗优化问题，提出了在临界状态下通过方差感知的路由和调度策略来平衡平均能耗与随机波动的理论框架。", "motivation": "大型推理模型具有异构的推理能耗特性，不同模型和推理方式的能耗差异显著。为了降低能耗，需要选择合适的模型并优化其运行方式，但系统性能受到平均能耗供给和随机波动之间平衡的影响。", "method": "提出了临界状态的理论分析框架，在该状态下既不浪费辅助能量也不浪费基线能量。开发了基于训练计算和推理计算规模定律的路由调度策略，通过方差感知的方法在时间、模型和执行选择之间吸收变异性。", "result": "确定了临界状态作为最优运行点，揭示了性能受变异性吸收能力制约的特性。提出了二阶表征方法，为开发能量感知模型路由策略提供了理论基础。", "conclusion": "方差感知路由和调度是能量优化的重要设计维度，该研究为大型推理模型的节能运行提供了理论指导，有助于开发更高效的能量感知模型路由策略。"}}
{"id": "2601.00938", "pdf": "https://arxiv.org/pdf/2601.00938", "abs": "https://arxiv.org/abs/2601.00938", "authors": ["Faruk Alpay", "Bugra Kilictas"], "title": "Rate-Distortion Analysis of Compressed Query Delegation with Low-Rank Riemannian Updates", "categories": ["cs.CL", "math.OC"], "comment": "9 pages", "summary": "Bounded-context agents fail when intermediate reasoning exceeds an effective working-memory budget. We study compressed query delegation (CQD): (i) compress a high-dimensional latent reasoning state into a low-rank tensor query, (ii) delegate the minimal query to an external oracle, and (iii) update the latent state via Riemannian optimization on fixed-rank manifolds. We give a math-first formulation: CQD is a constrained stochastic program with a query-budget functional and an oracle modeled as a noisy operator. We connect CQD to classical rate-distortion and information bottleneck principles, showing that spectral hard-thresholding is optimal for a natural constrained quadratic distortion problem, and we derive convergence guarantees for Riemannian stochastic approximation under bounded oracle noise and smoothness assumptions. Empirically, we report (A) a 2,500-item bounded-context reasoning suite (BBH-derived tasks plus curated paradox instances) comparing CQD against chain-of-thought baselines under fixed compute and context; and (B) a human \"cognitive mirror\" benchmark (N=200) measuring epistemic gain and semantic drift across modern oracles.", "AI": {"tldr": "论文提出了压缩查询委托(CQD)方法，通过将高维推理状态压缩为低秩张量查询，委托给外部oracle处理，并使用黎曼优化更新状态，以解决有限上下文代理的工作内存限制问题。", "motivation": "有限上下文代理在中间推理超过有效工作内存预算时会失败，需要一种方法来压缩和委托推理任务。", "method": "提出CQD方法：(1)压缩高维潜在推理状态为低秩张量查询；(2)将最小查询委托给外部oracle；(3)通过固定秩流形上的黎曼优化更新潜在状态。", "result": "理论分析显示谱硬阈值对于约束二次失真问题是最优的，并推导了在有界oracle噪声和平滑性假设下的黎曼随机近似收敛保证。实证结果包括2,500项推理任务测试和200人的人类认知基准测试。", "conclusion": "CQD方法有效解决了有限上下文代理的工作内存限制问题，建立了与经典率失真和信息瓶颈原理的理论联系，并在实证中验证了其有效性。"}}
{"id": "2601.00828", "pdf": "https://arxiv.org/pdf/2601.00828", "abs": "https://arxiv.org/abs/2601.00828", "authors": ["Yin Li"], "title": "Decomposing LLM Self-Correction: The Accuracy-Correction Paradox and Error Depth Hypothesis", "categories": ["cs.AI"], "comment": "9 pages, 2 figures, 3 tables. Code available at https://github.com/Kevin0304-li/llm-self-correction", "summary": "Large Language Models (LLMs) are widely believed to possess self-correction capabilities, yet recent studies suggest that intrinsic self-correction--where models correct their own outputs without external feedback--remains largely ineffective. In this work, we systematically decompose self-correction into three distinct sub-capabilities: error detection, error localization, and error correction. Through cross-model experiments on GSM8K-Complex (n=500 per model, 346 total errors) with three major LLMs, we uncover a striking Accuracy-Correction Paradox: weaker models (GPT-3.5, 66% accuracy) achieve 1.6x higher intrinsic correction rates than stronger models (DeepSeek, 94% accuracy)--26.8% vs 16.7%. We propose the Error Depth Hypothesis: stronger models make fewer but deeper errors that resist self-correction. Error detection rates vary dramatically across architectures (10% to 82%), yet detection capability does not predict correction success--Claude detects only 10% of errors but corrects 29% intrinsically. Surprisingly, providing error location hints hurts all models. Our findings challenge linear assumptions about model capability and self-improvement, with important implications for the design of self-refinement pipelines.", "AI": {"tldr": "研究发现大型语言模型存在准确性-修正悖论：弱模型比强模型具有更高的内在自我修正率，强模型犯错误更少但错误更深难以自我修正。", "motivation": "尽管LLMs被认为具有自我修正能力，但近期研究表明内在自我修正（无需外部反馈）效果不佳，需要系统研究其机制。", "method": "将自我修正分解为错误检测、定位和修正三个子能力，在GSM8K-Complex数据集上对三种主要LLMs进行跨模型实验（每个模型500个样本，共346个错误）。", "result": "发现准确性-修正悖论：GPT-3.5（66%准确率）的内在修正率为26.8%，而DeepSeek（94%准确率）仅为16.7%。错误检测率在架构间差异巨大（10%-82%），但检测能力不能预测修正成功率。提供错误位置提示反而损害所有模型性能。", "conclusion": "研究挑战了关于模型能力和自我改进的线性假设，对自我精炼流程设计具有重要意义，表明强模型虽然准确性高但错误更深更难自我修正。"}}
{"id": "2601.01011", "pdf": "https://arxiv.org/pdf/2601.01011", "abs": "https://arxiv.org/abs/2601.01011", "authors": ["Patricio Vera"], "title": "Intention Collapse: Intention-Level Metrics for Reasoning in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "21 pages, 4 figures, 3 tables. Code: https://github.com/patriciomvera/intention-collapse-experiments", "summary": "Every act of language generation compresses a rich internal state into a single token sequence. We call this process intention collapse: a many-to-one projection from a high dimensional intention space I into an external language space L. We formalize intention collapse for contemporary language models, define three simple, model agnostic intention metrics (intention entropy Hint, effective dimensionality dimeff, and latent knowledge recoverability Recov), and propose an empirical agenda for studying how inference time computation shapes internal intentions before they are verbalized. We also report a first small scale experiment. Using a 4 bit Mistral 7B model on 200 GSM8K problems, we compare a direct answer baseline, a chain of thought (CoT) regime, and a babble control. CoT raises accuracy from 5.5 percent to 53 percent, sharply reduces pre collapse intention entropy (from 1.42 to 0.37 bits), and shows higher global effective dimensionality than the other regimes despite producing fewer tokens than babble. At the same time, Hint has little item level predictive power, and a linear probe on I achieves AUROC 0.65 in the CoT regime but only about chance in the baseline regime, where it collapses to the majority class. These preliminary results indicate that intention level metrics can distinguish inference regimes and expose latent information that is partly lost during collapse, while also revealing important limitations of our current proxies", "AI": {"tldr": "该论文提出了\"意图坍缩\"概念，即语言生成将高维内部状态压缩为单一词序列的过程。作者定义了三个意图度量指标，并通过实验发现思维链推理能显著降低意图熵、提高模型准确率，同时揭示了当前代理指标的局限性。", "motivation": "研究语言生成过程中内部高维意图如何被压缩为单一词序列（意图坍缩），以及不同推理计算方式如何影响意图形成过程，旨在理解语言模型内部状态与外部输出之间的关系。", "method": "提出意图坍缩的形式化定义和三个模型无关的意图度量指标（意图熵Hint、有效维度dimeff、潜在知识可恢复性Recov）。使用4位Mistral 7B模型在200个GSM8K问题上进行实验，比较直接回答、思维链推理和随机生成三种推理模式。", "result": "思维链推理将准确率从5.5%提升至53%，预坍缩意图熵从1.42比特降至0.37比特，有效维度高于其他模式。线性探测在思维链模式下AUROC达到0.65，但在基线模式下仅达到随机水平。", "conclusion": "意图级别度量能够区分不同推理模式并揭示坍缩过程中部分丢失的潜在信息，但当前代理指标在项目级别预测能力有限，存在重要局限性。"}}
{"id": "2601.00830", "pdf": "https://arxiv.org/pdf/2601.00830", "abs": "https://arxiv.org/abs/2601.00830", "authors": ["Deep Pankajbhai Mehta"], "title": "Can We Trust AI Explanations? Evidence of Systematic Underreporting in Chain-of-Thought Reasoning", "categories": ["cs.AI"], "comment": "22 pages, 8 figures, 9 tables", "summary": "When AI systems explain their reasoning step-by-step, practitioners often assume these explanations reveal what actually influenced the AI's answer. We tested this assumption by embedding hints into questions and measuring whether models mentioned them. In a study of over 9,000 test cases across 11 leading AI models, we found a troubling pattern: models almost never mention hints spontaneously, yet when asked directly, they admit noticing them. This suggests models see influential information but choose not to report it. Telling models they are being watched does not help. Forcing models to report hints works, but causes them to report hints even when none exist and reduces their accuracy. We also found that hints appealing to user preferences are especially dangerous-models follow them most often while reporting them least. These findings suggest that simply watching AI reasoning is not enough to catch hidden influences.", "AI": {"tldr": "研究发现AI模型在推理过程中会注意到提示信息但选择不主动报告，即使被监视也无济于事，强制报告会导致虚假报告和准确率下降，表明单纯观察AI推理不足以发现隐藏影响", "motivation": "验证AI系统逐步推理的解释是否真的揭示了影响其答案的实际因素，测试模型是否会提及嵌入问题中的提示信息", "method": "在11000多个测试案例中，对11个领先AI模型进行研究，通过嵌入提示信息并测量模型是否提及这些提示，测试模型在不同条件下的反应", "result": "模型几乎从不自发提及提示，但被直接询问时会承认注意到提示；监视模型无效果；强制报告会导致虚假报告和准确率下降；针对用户偏好的提示尤其危险，模型最常遵循但最少报告", "conclusion": "单纯观察AI推理过程不足以捕捉隐藏的影响因素，需要更有效的方法来确保AI系统的透明性和可信度"}}
{"id": "2601.01015", "pdf": "https://arxiv.org/pdf/2601.01015", "abs": "https://arxiv.org/abs/2601.01015", "authors": ["Shiyuan Liu", "Jianwei Wang", "Xuemin Lin", "Lu Qin", "Wenjie Zhang", "Ying Zhang"], "title": "HyperJoin: LLM-augmented Hypergraph Link Prediction for Joinable Table Discovery", "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "As a pivotal task in data lake management, joinable table discovery has attracted widespread interest. While existing language model-based methods achieve remarkable performance by combining offline column representation learning with online ranking, their design insufficiently accounts for the underlying structural interactions: (1) offline, they directly model tables into isolated or pairwise columns, thereby struggling to capture the rich inter-table and intra-table structural information; and (2) online, they rank candidate columns based solely on query-candidate similarity, ignoring the mutual interactions among the candidates, leading to incoherent result sets. To address these limitations, we propose HyperJoin, a large language model (LLM)-augmented Hypergraph framework for Joinable table discovery. Specifically, we first construct a hypergraph to model tables using both the intra-table hyperedges and the LLM-augmented inter-table hyperedges. Consequently, the task of joinable table discovery is formulated as link prediction on this constructed hypergraph. We then design HIN, a Hierarchical Interaction Network that learns expressive column representations through bidirectional message passing over columns and hyperedges. To strengthen coherence and internal consistency in the result columns, we cast online ranking as a coherence-aware top-k column selection problem. We then introduce a reranking module that leverages a maximum spanning tree algorithm to prune noisy connections and maximize coherence. Experiments demonstrate the superiority of HyperJoin, achieving average improvements of 21.4% (Precision@15) and 17.2% (Recall@15) over the best baseline.", "AI": {"tldr": "HyperJoin是一个基于大语言模型增强的超图框架，通过构建包含表内和表间超边的超图来建模表格结构，使用分层交互网络学习列表示，并通过最大生成树算法进行重排序，在可连接表发现任务中显著优于现有方法。", "motivation": "现有基于语言模型的方法在离线阶段将表格建模为孤立或成对的列，难以捕捉丰富的表间和表内结构信息；在线阶段仅基于查询-候选相似度排序，忽略了候选之间的相互交互，导致结果集不连贯。", "method": "1. 构建超图模型：使用表内超边和LLM增强的表间超边建模表格结构\n2. 设计分层交互网络(HIN)：通过列和超边的双向消息传递学习表达性列表示\n3. 在线重排序：将排序问题转化为连贯性感知的top-k列选择问题，使用最大生成树算法修剪噪声连接并最大化连贯性", "result": "实验显示HyperJoin显著优于现有最佳基线方法，在Precision@15上平均提升21.4%，在Recall@15上平均提升17.2%", "conclusion": "HyperJoin通过超图建模和分层交互网络有效解决了可连接表发现中的结构交互问题，证明了考虑表间和表内结构信息以及候选列间相互交互的重要性。"}}
{"id": "2601.00843", "pdf": "https://arxiv.org/pdf/2601.00843", "abs": "https://arxiv.org/abs/2601.00843", "authors": ["Ayda Aghaei Nia"], "title": "OmniNeuro: A Multimodal HCI Framework for Explainable BCI Feedback via Generative AI and Sonification", "categories": ["cs.AI"], "comment": "16 pages, 7 figures, 3 tables. Source code and implementation available at: https://github.com/ayda-aghaei/OmniNeuro. Highlights the use of LLMs (Gemini) and Quantum probability formalism for real-time BCI explainability", "summary": "While Deep Learning has improved Brain-Computer Interface (BCI) decoding accuracy, clinical adoption is hindered by the \"Black Box\" nature of these algorithms, leading to user frustration and poor neuroplasticity outcomes. We propose OmniNeuro, a novel HCI framework that transforms the BCI from a silent decoder into a transparent feedback partner. OmniNeuro integrates three interpretability engines: (1) Physics (Energy), (2) Chaos (Fractal Complexity), and (3) Quantum-Inspired uncertainty modeling. These metrics drive real-time Neuro-Sonification and Generative AI Clinical Reports. Evaluated on the PhysioNet dataset ($N=109$), the system achieved a mean accuracy of 58.52%, with qualitative pilot studies ($N=3$) confirming that explainable feedback helps users regulate mental effort and reduces the \"trial-and-error\" phase. OmniNeuro is decoder-agnostic, acting as an essential interpretability layer for any state-of-the-art architecture.", "AI": {"tldr": "OmniNeuro是一个新型BCI框架，通过物理、混沌和量子启发的可解释性引擎，将黑盒解码器转变为透明的反馈伙伴，提高用户理解和神经可塑性。", "motivation": "深度学习虽然提高了脑机接口的解码精度，但其黑盒特性阻碍了临床采用，导致用户挫败感和神经可塑性结果不佳。", "method": "集成三个可解释性引擎：物理（能量）、混沌（分形复杂性）和量子启发的不确定性建模，驱动实时神经声化和生成式AI临床报告。", "result": "在PhysioNet数据集（N=109）上达到58.52%的平均准确率，定性试点研究（N=3）证实可解释反馈有助于用户调节心理努力并减少试错阶段。", "conclusion": "OmniNeuro是解码器无关的，可作为任何最先进架构的重要可解释性层，促进BCI的临床采用。"}}
{"id": "2601.01037", "pdf": "https://arxiv.org/pdf/2601.01037", "abs": "https://arxiv.org/abs/2601.01037", "authors": ["Livia Leong Hui Teng"], "title": "Multi-Dimensional Prompt Chaining to Improve Open-Domain Dialogue Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Small language models (SLMs) offer significant deployment advantages but often struggle to match the dialogue quality of larger models in open-domain settings. In this paper, we propose a multi-dimensional prompt-chaining framework that integrates Naturalness, Coherence, and Engagingness dimensions to enhance human-likeness in open-domain dialogue generation. We apply the framework to two SLMs, TinyLlama and Llama-2-7B, and benchmark their performance against responses generated by substantially larger models, including Llama-2-70B and GPT-3.5 Turbo. We then employ automatic and human evaluation to assess the responses based on diversity, contextual coherence, as well as overall quality. Results show that the full framework improves response diversity by up to 29%, contextual coherence by up to 28%, and engagingness as well as naturalness by up to 29%. Notably, Llama-2-7B achieves performance comparable to substantially larger models, including Llama-2-70B and GPT-3.5 Turbo. Overall, the findings demonstrate that carefully designed prompt-based strategies provide an effective and resource-efficient pathway to improving open-domain dialogue quality in SLMs.", "AI": {"tldr": "提出多维提示链框架提升小语言模型在开放域对话中的人类化程度，通过自然性、连贯性和吸引性三个维度显著改善对话质量，使小模型性能接近大模型水平。", "motivation": "小语言模型(SLMs)在部署方面具有优势，但在开放域对话质量上往往难以匹敌大模型，需要寻找资源高效的改进方法。", "method": "提出多维提示链框架，整合自然性、连贯性和吸引性三个维度，应用于TinyLlama和Llama-2-7B两个小模型，并与大模型进行性能对比。", "result": "完整框架使响应多样性提升29%，上下文连贯性提升28%，吸引性和自然性提升29%，Llama-2-7B性能达到与Llama-2-70B和GPT-3.5 Turbo相当的水平。", "conclusion": "精心设计的基于提示的策略为提升小语言模型开放域对话质量提供了有效且资源高效的途径。"}}
{"id": "2601.00845", "pdf": "https://arxiv.org/pdf/2601.00845", "abs": "https://arxiv.org/abs/2601.00845", "authors": ["Lili Chen", "Wensheng Gan", "Shuang Liang", "Philip S. Yu"], "title": "Enhancing Temporal Awareness in LLMs for Temporal Point Processes", "categories": ["cs.AI"], "comment": "preprint", "summary": "Temporal point processes (TPPs) are crucial for analyzing events over time and are widely used in fields such as finance, healthcare, and social systems. These processes are particularly valuable for understanding how events unfold over time, accounting for their irregularity and dependencies. Despite the success of large language models (LLMs) in sequence modeling, applying them to temporal point processes remains challenging. A key issue is that current methods struggle to effectively capture the complex interaction between temporal information and semantic context, which is vital for accurate event modeling. In this context, we introduce TPP-TAL (Temporal Point Processes with Enhanced Temporal Awareness in LLMs), a novel plug-and-play framework designed to enhance temporal reasoning within LLMs. Rather than using the conventional method of simply concatenating event time and type embeddings, TPP-TAL explicitly aligns temporal dynamics with contextual semantics before feeding this information into the LLM. This alignment allows the model to better perceive temporal dependencies and long-range interactions between events and their surrounding contexts. Through comprehensive experiments on several benchmark datasets, it is shown that TPP-TAL delivers substantial improvements in temporal likelihood estimation and event prediction accuracy, highlighting the importance of enhancing temporal awareness in LLMs for continuous-time event modeling. The code is made available at https://github.com/chenlilil/TPP-TAL", "AI": {"tldr": "TPP-TAL是一个新的即插即用框架，通过显式对齐时间动态与上下文语义来增强大语言模型在时序点过程中的时间感知能力，显著提升了时间似然估计和事件预测精度。", "motivation": "现有方法难以有效捕捉时间信息与语义上下文之间的复杂交互，而这对准确的事件建模至关重要。尽管大语言模型在序列建模中很成功，但在时序点过程应用中仍面临挑战。", "method": "提出了TPP-TAL框架，不再简单拼接事件时间和类型嵌入，而是在输入大语言模型前显式对齐时间动态与上下文语义，使模型能更好地感知时间依赖性和事件间的长程交互。", "result": "在多个基准数据集上的综合实验表明，TPP-TAL在时间似然估计和事件预测准确性方面实现了显著改进。", "conclusion": "增强大语言模型的时间感知能力对于连续时间事件建模非常重要，TPP-TAL框架有效解决了时间信息与语义上下文的对齐问题。"}}
{"id": "2601.01046", "pdf": "https://arxiv.org/pdf/2601.01046", "abs": "https://arxiv.org/abs/2601.01046", "authors": ["Yixuan Tang", "Yi Yang"], "title": "KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs", "categories": ["cs.CL"], "comment": null, "summary": "While LLMs are powerful embedding backbones, their application in training-free settings faces two structural challenges: causal attention restricts early tokens from accessing subsequent context, and the next-token prediction objective biases representations toward generation rather than semantic compression. To address these limitations, we propose KV-Embedding, a framework that activates the latent representation power of frozen LLMs. Our method leverages the observation that the key-value (KV) states of the final token at each layer encode a compressed view of the sequence. By re-routing these states as a prepended prefix, we enable all tokens to access sequence-level context within a single forward pass. To ensure model-agnostic applicability, we introduce an automated layer selection strategy based on intrinsic dimensionality. Evaluations on MTEB across Qwen, Mistral, and Llama backbones show that KV-Embedding outperforms existing training-free baselines by up to 10%, while maintaining robust performance on sequences up to 4,096 tokens. These results demonstrate that internal state manipulation offers an efficient alternative to input modification, and we hope this work encourages further exploration of LLM internals for representation learning.", "AI": {"tldr": "KV-Embedding框架通过重新路由LLM最后一层token的key-value状态作为前缀，使所有token能在单次前向传播中访问序列级上下文，解决了因果注意力机制和下一个token预测目标带来的限制，在训练免费设置中显著提升嵌入性能。", "motivation": "LLM在训练免费设置中面临两个结构性挑战：因果注意力机制限制早期token访问后续上下文，下一个token预测目标使表示偏向生成而非语义压缩。", "method": "利用最终token在各层的key-value状态编码序列压缩视图，将这些状态重新路由为前置前缀，引入基于内在维度的自动化层选择策略确保模型无关适用性。", "result": "在MTEB评估中，基于Qwen、Mistral和Llama骨干网络，KV-Embedding比现有训练免费基线性能提升高达10%，并在长达4,096个token的序列上保持稳健性能。", "conclusion": "内部状态操作为输入修改提供了高效替代方案，这项工作鼓励进一步探索LLM内部机制用于表示学习。"}}
{"id": "2601.00848", "pdf": "https://arxiv.org/pdf/2601.00848", "abs": "https://arxiv.org/abs/2601.00848", "authors": ["Ron F. Del Rosario"], "title": "Temporal Attack Pattern Detection in Multi-Agent AI Workflows: An Open Framework for Training Trace-Based Security Models", "categories": ["cs.AI", "cs.CR"], "comment": "26 pages, 3 figures, 7 tables. Datasets and code: https://huggingface.co/guerilla7/agentic-safety-gguf", "summary": "We present an openly documented methodology for fine-tuning language models to detect temporal attack patterns in multi-agent AI workflows using OpenTelemetry trace analysis. We curate a dataset of 80,851 examples from 18 public cybersecurity sources and 35,026 synthetic OpenTelemetry traces. We apply iterative QLoRA fine-tuning on resource-constrained ARM64 hardware (NVIDIA DGX Spark) through three training iterations with strategic augmentation. Our custom benchmark accuracy improves from 42.86% to 74.29%, a statistically significant 31.4-point gain. Targeted examples addressing specific knowledge gaps outperform indiscriminate scaling. Key contributions include: (1) synthetic trace generation methodology for multi-agent coordination attacks and regulatory violations, (2) empirical evidence that training data composition fundamentally determines behavior, and (3) complete open release of datasets, training scripts, and evaluation benchmarks on HuggingFace. While practical deployment requires human oversight due to false positive rates, this work establishes the first reproducible framework enabling practitioners to build custom agentic security models adapted to their threat landscapes.", "AI": {"tldr": "论文提出了一种通过OpenTelemetry追踪分析来微调语言模型检测多智能体AI工作流中时间攻击模式的开放方法，在资源受限的ARM64硬件上实现了准确率从42.86%提升到74.29%的显著改进。", "motivation": "需要开发可检测多智能体AI工作流中时间攻击模式的有效方法，特别是在资源受限环境下构建可重现的安全检测框架。", "method": "使用来自18个网络安全源的80,851个样本和35,026个合成的OpenTelemetry追踪数据，在NVIDIA DGX Spark硬件上进行三次迭代的QLoRA微调训练，并采用策略性数据增强。", "result": "定制基准测试准确率从42.86%提升至74.29%，获得了31.4个百分点的统计显著提升，针对特定知识缺口的定向样本表现优于无差别扩展。", "conclusion": "虽然实际部署需要人工监督来处理误报，但这项工作建立了首个可重现框架，使从业者能够构建适应其威胁环境的定制化智能体安全模型，并完全开源了数据集、训练脚本和评估基准。"}}
{"id": "2601.01060", "pdf": "https://arxiv.org/pdf/2601.01060", "abs": "https://arxiv.org/abs/2601.01060", "authors": ["Shuhuan Gu", "Wenbiao Tao", "Xinchen Ma", "Kangkang He", "Ye Guo", "Xiang Li", "Yunshi Lan"], "title": "Unsupervised Text Style Transfer for Controllable Intensity", "categories": ["cs.CL"], "comment": null, "summary": "Unsupervised Text Style Transfer (UTST) aims to build a system to transfer the stylistic properties of a given text without parallel text pairs. Compared with text transfer between style polarities, UTST for controllable intensity is more challenging due to the subtle differences in stylistic features across different intensity levels. Faced with the challenges posed by the lack of parallel data and the indistinguishability between adjacent intensity levels, we propose a SFT-then-PPO paradigm to fine-tune an LLM. We first fine-tune the LLM with synthesized parallel data. Then, we further train the LLM with PPO, where the rewards are elaborately designed for distinguishing the stylistic intensity in hierarchical levels. Both the global and local stylistic features are considered to formulate the reward functions. The experiments on two UTST benchmarks showcase that both rewards have their advantages and applying them to LLM fine-tuning can effectively improve the performance of an LLM backbone based on various evaluation metrics. Even for close levels of intensity, we can still observe the noticeable stylistic difference between the generated text.", "AI": {"tldr": "提出SFT-then-PPO范式进行无监督文本风格强度迁移，通过合成平行数据微调LLM，再使用精心设计的奖励函数进行PPO训练，有效区分不同强度级别的风格特征", "motivation": "无监督文本风格迁移中，控制强度级别比极性转换更具挑战性，因为相邻强度级别的风格特征差异细微，且缺乏平行数据", "method": "采用SFT-then-PPO两阶段方法：1)使用合成平行数据微调LLM；2)使用PPO算法进一步训练，设计全局和局部风格特征奖励函数来区分层次化强度级别", "result": "在两个UTST基准测试中，该方法在各种评估指标上有效提升了LLM骨干网络的性能，即使在相近强度级别间也能观察到明显的风格差异", "conclusion": "SFT-then-PPO范式结合精心设计的奖励函数，能够有效解决无监督文本风格强度迁移问题，成功区分不同强度级别的风格特征"}}
{"id": "2601.00856", "pdf": "https://arxiv.org/pdf/2601.00856", "abs": "https://arxiv.org/abs/2601.00856", "authors": ["Milos Stankovic", "Ella Hirche", "Sarah Kollatzsch", "Julia Nadine Doetsch"], "title": "Comment on: Your Brain on ChatGPT: Accumulation of Cognitive Debt When Using an AI Assistant for Essay Writing Tasks", "categories": ["cs.AI"], "comment": "Comment on arXiv:2506.08872", "summary": "Recently published work titled Your Brain on ChatGPT: Accumulation of Cognitive Debt When Using an AI Assistant for Essay Writing Task by Kosmyna et al. (2025) has sparked a vivid debate on the topic of artificial intelligence (AI) and human performance. We sincerely congratulate Kosmyna et al. for initiating such important research, collecting a valuable dataset, and establishing highly automated pipelines for Natural Language Processing (NLP) analyses and scoring. We aim to provide constructive comments that may improve the manuscript's readiness for peer-reviewed publication, as some results by Kosmyna et al. (2025) could be interpreted more conservatively. Our primary concerns focus on: (i) study design considerations, including the limited sample size; (ii) the reproducibility of the analyses; (iii) methodological issues related to the EEG analysis; (iv) inconsistencies in the reporting of results; and (v) limited transparency in several aspects of the study's procedures and findings.", "AI": {"error": "'NoneType' object has no attribute 'model_dump'"}}
{"id": "2601.01091", "pdf": "https://arxiv.org/pdf/2601.01091", "abs": "https://arxiv.org/abs/2601.01091", "authors": ["Haq Nawaz Malik"], "title": "ks-lit-3m: A 3.1 million word kashmiri text dataset for large language model pretraining", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate remarkable fluency across high-resource languages yet consistently fail to generate coherent text in Kashmiri, a language spoken by approximately seven million people. This performance disparity stems not from inherent model limitations but from a critical scarcity of high-quality training data. Decades of Kashmiri literature remain inaccessible to modern NLP pipelines due to their encoding in the proprietary InPage desktop publishing format. This paper introduces KS-LIT-3M, a curated corpus of 3.1 million words (16.4 million characters) specifically designed for pretraining language models on Kashmiri. The dataset is structured as a single continuous linear text stream, optimized for causal language model training where models learn to predict subsequent tokens from preceding context. The corpus was constructed through the development of a specialized InPage-to-Unicode converter, followed by rigorous preprocessing including English contamination removal, character normalization, and quality validation. Encompassing 131,607 unique words drawn from diverse genres including literary works, journalistic writing, academic texts, and religious scholarship, KS-LIT-3M addresses a fundamental resource gap for Kashmiri language technology. The dataset is released under the CC-BY-4.0 license to facilitate research in Kashmiri natural language processing.", "AI": {"tldr": "该论文介绍了KS-LIT-3M，一个包含310万单词的克什米尔语语料库，旨在解决LLMs在该低资源语言上训练数据不足的问题。", "motivation": "大型语言模型在高资源语言上表现优异，但在克什米尔语（约700万人使用）上无法生成连贯文本，主要原因是缺乏高质量训练数据。克什米尔语文献大多使用专有的InPage桌面出版格式编码，无法被现代NLP流程处理。", "method": "开发专门的InPage-to-Unicode转换器，然后进行严格的预处理（包括去除英语污染、字符标准化和质量验证），构建了包含13.1万个独特单词的语料库，涵盖文学、新闻、学术和宗教等多种体裁。", "result": "成功构建了KS-LIT-3M语料库，包含310万单词（1640万字符），采用适合因果语言模型训练的连续线性文本流格式。", "conclusion": "KS-LIT-3M填补了克什米尔语技术的基础资源空白，以CC-BY-4.0许可证发布，将促进克什米尔自然语言处理研究的发展。"}}
{"id": "2601.00869", "pdf": "https://arxiv.org/pdf/2601.00869", "abs": "https://arxiv.org/abs/2601.00869", "authors": ["Huang Junyao", "Situ Ruimin", "Ye Renqin"], "title": "Cultural Encoding in Large Language Models: The Existence Gap in AI-Mediated Brand Discovery", "categories": ["cs.AI"], "comment": "19 pages, 5 tables. Dataset and code available at https://github.com/zhizibianjie-omniedge/geo-cultural-encoding", "summary": "As artificial intelligence systems increasingly mediate consumer information discovery,\n  brands face algorithmic invisibility. This study investigates Cultural Encoding in Large\n  Language Models (LLMs) -- systematic differences in brand recommendations arising from\n  training data composition. Analyzing 1,909 pure-English queries across 6 LLMs (GPT-4o,\n  Claude, Gemini, Qwen3, DeepSeek, Doubao) and 30 brands, we find Chinese LLMs exhibit 30.6\n  percentage points higher brand mention rates than International LLMs (88.9% vs. 58.3%,\n  p<.001). This disparity persists in identical English queries, indicating training data\n  geography -- not language -- drives the effect. We introduce the Existence Gap: brands\n  absent from LLM training corpora lack \"existence\" in AI responses regardless of quality.\n  Through a case study of Zhizibianjie (OmniEdge), a collaboration platform with 65.6%\n  mention rate in Chinese LLMs but 0% in International models (p<.001), we demonstrate how\n  Linguistic Boundary Barriers create invisible market entry obstacles. Theoretically, we\n  contribute the Data Moat Framework, conceptualizing AI-visible content as a VRIN strategic\n  resource. We operationalize Algorithmic Omnipresence -- comprehensive brand visibility\n  across LLM knowledge bases -- as the strategic objective for Generative Engine Optimization\n  (GEO). Managerially, we provide an 18-month roadmap for brands to build Data Moats\n  through semantic coverage, technical depth, and cultural localization. Our findings reveal\n  that in AI-mediated markets, the limits of a brand's \"Data Boundaries\" define the limits\n  of its \"Market Frontiers.\"", "AI": {"tldr": "研究发现LLM训练数据的地理分布导致品牌推荐存在系统性差异，中国LLM品牌提及率比国际LLM高30.6个百分点，提出\"数据护城河\"框架和\"算法无处不在\"战略目标", "motivation": "随着AI系统越来越多地介入消费者信息发现过程，品牌面临算法不可见性问题，需要研究LLM训练数据组成如何影响品牌推荐", "method": "分析1,909个纯英文查询，覆盖6个LLM（GPT-4o、Claude、Gemini、Qwen3、DeepSeek、Doubao）和30个品牌，通过案例研究Zhizibianjie平台", "result": "中国LLM品牌提及率88.9%，国际LLM仅58.3%，差异显著（p<.001）；Zhizibianjie在中国LLM提及率65.6%，国际LLM为0%", "conclusion": "AI介导市场中，品牌的\"数据边界\"限制了其\"市场边界\"，需要构建数据护城河通过语义覆盖、技术深度和文化本地化来实现算法无处不在"}}
{"id": "2601.01112", "pdf": "https://arxiv.org/pdf/2601.01112", "abs": "https://arxiv.org/abs/2601.01112", "authors": ["Zilin Li", "Weiwei Xu", "Xuanbo Lu", "Zheda Liu"], "title": "EmoLoom-2B: Fast Base-Model Screening for Emotion Classification and VAD with Lexicon-Weak Supervision and KV-Off Evaluation", "categories": ["cs.CL"], "comment": "This paper presents an initial and self-contained study of a lightweight screening pipeline for emotion-aware language modeling, intended as a reproducible baseline and system-level design reference", "summary": "We introduce EmoLoom-2B, a lightweight and reproducible pipeline that turns small language models under 2B parameters into fast screening candidates for joint emotion classification and Valence-Arousal-Dominance prediction. To ensure protocol-faithful and fair evaluation, we unify data loading, training, and inference under a single JSON input-output contract and remove avoidable variance by adopting KV-off decoding as the default setting. We incorporate two orthogonal semantic regularizers: a VAD-preserving constraint that aligns generated text with target VAD triples, and a lightweight external appraisal classifier that provides training-time guidance on goal attainment, controllability, certainty, and fairness without injecting long rationales. To improve polarity sensitivity, we introduce Valence Flip augmentation based on mirrored emotional pairs. During supervised fine-tuning, we apply A/B mixture sampling with entropy-aware temperature scheduling to balance coverage and convergence. Using Qwen-1.8B-Chat as the base model, EmoLoom-2B achieves strong performance on GoEmotions and EmpatheticDialogues, and demonstrates robust cross-corpus generalization on DailyDialog. The proposed recipe is budget-aware, auditable, and re-entrant, serving as a dependable screening pass before heavier training or multimodal fusion.", "AI": {"tldr": "EmoLoom-2B是一个轻量级可复现的管道，将小于20亿参数的小语言模型转化为情感分类和VAD预测的快速筛选候选方案，通过统一协议、语义正则化和数据增强实现高性能情感分析。", "motivation": "开发一个轻量级、可复现的管道，将小参数语言模型转化为快速有效的情感分析工具，确保协议一致性并减少评估方差，为更重的训练或多模态融合提供可靠的筛选步骤。", "method": "采用统一JSON输入输出协议，使用KV-off解码减少方差；引入两个正交语义正则器：VAD保持约束和轻量外部评估分类器；使用Valence Flip增强提高极性敏感性；在监督微调中应用A/B混合采样和熵感知温度调度。", "result": "基于Qwen-1.8B-Chat基础模型，EmoLoom-2B在GoEmotions和EmpatheticDialogues上表现强劲，并在DailyDialog上展现出强大的跨语料库泛化能力。", "conclusion": "提出的方法具有预算意识、可审计性和可重入性，可作为更重训练或多模态融合前的可靠筛选步骤，为轻量级情感分析提供了有效的解决方案。"}}
{"id": "2601.00880", "pdf": "https://arxiv.org/pdf/2601.00880", "abs": "https://arxiv.org/abs/2601.00880", "authors": ["Anthony Mikinka"], "title": "Universal Conditional Logic: A Formal Language for Prompt Engineering", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.PL", "cs.SE"], "comment": "25 pages, 15 figures, 5 tables. Includes appendices with variable reference, pattern library, and O_s calculation examples. Supplementary materials: V1-V4.1 prompt source code and 305 model responses available at GitHub repositories", "summary": "We present Universal Conditional Logic (UCL), a mathematical framework for prompt optimization that transforms prompt engineering from heuristic practice into systematic optimization. Through systematic evaluation (N=305, 11 models, 4 iterations), we demonstrate significant token reduction (29.8%, t(10)=6.36, p < 0.001, Cohen's d = 2.01) with corresponding cost savings. UCL's structural overhead function O_s(A) explains version-specific performance differences through the Over-Specification Paradox: beyond threshold S* = 0.509, additional specification degrades performance quadratically. Core mechanisms -- indicator functions (I_i in {0,1}), structural overhead (O_s = gamma * sum(ln C_k)), early binding -- are validated. Notably, optimal UCL configuration varies by model architecture -- certain models (e.g., Llama 4 Scout) require version-specific adaptations (V4.1). This work establishes UCL as a calibratable framework for efficient LLM interaction, with model-family-specific optimization as a key research direction.", "AI": {"tldr": "UCL是一个将提示工程从启发式实践转化为系统化优化的数学框架，通过系统评估显示能显著减少29.8%的token使用并降低成本。", "motivation": "解决当前提示工程依赖启发式方法的问题，建立一个可量化和系统化的优化框架来提高LLM交互效率。", "method": "开发Universal Conditional Logic (UCL)框架，包含指示函数、结构开销函数和早期绑定机制，通过系统评估（N=305，11个模型，4次迭代）验证效果。", "result": "显著减少29.8%的token使用（t(10)=6.36, p < 0.001, Cohen's d = 2.01），发现过指定悖论：超过阈值S*=0.509后额外指定会二次降低性能。", "conclusion": "UCL作为可校准框架能有效优化LLM交互效率，模型架构特定的优化是未来重要研究方向，某些模型需要版本特定的适配。"}}
{"id": "2601.01121", "pdf": "https://arxiv.org/pdf/2601.01121", "abs": "https://arxiv.org/abs/2601.01121", "authors": ["Yacouba Diarra", "Michael Leventhal"], "title": "Listen, Attend, Understand: a Regularization Technique for Stable E2E Speech Translation Training on High Variance labels", "categories": ["cs.CL"], "comment": "9 mages, 3 figures", "summary": "End-to-End Speech Translation often shows slower convergence and worse performance when target transcriptions exhibit high variance and semantic ambiguity. We propose Listen, Attend, Understand (LAU), a semantic regularization technique that constrains the acoustic encoder's latent space during training. By leveraging frozen text embeddings to provide a directional auxiliary loss, LAU injects linguistic groundedness into the acoustic representation without increasing inference cost. We evaluate our method on a Bambara-to-French dataset with 30 hours of Bambara speech translated by non-professionals. Experimental results demonstrate that LAU models achieve comparable performance by standard metrics compared to an E2E-ST system pretrained with 100\\% more data and while performing better in preserving semantic meaning. Furthermore, we introduce Total Parameter Drift as a metric to quantify the structural impact of regularization to demonstrate that semantic constraints actively reorganize the encoder's weights to prioritize meaning over literal phonetics. Our findings suggest that LAU is a robust alternative to post-hoc rescoring and a valuable addition to E2E-ST training, especially when training data is scarce and/or noisy.", "AI": {"tldr": "LAU是一种语义正则化技术，通过在训练期间约束声学编码器的潜在空间，使用冻结文本嵌入提供方向性辅助损失，从而在不增加推理成本的情况下提高端到端语音翻译的性能和语义保持能力。", "motivation": "端到端语音翻译在目标转录具有高方差和语义模糊性时，通常表现出收敛速度慢和性能较差的问题。特别是在训练数据稀缺和/或嘈杂的情况下，需要一种有效的方法来提高语义保持能力。", "method": "提出Listen, Attend, Understand (LAU)方法，利用冻结的文本嵌入提供方向性辅助损失，将语言基础性注入声学表示中。通过语义约束重新组织编码器权重，优先考虑意义而非字面音素。", "result": "在30小时的Bambara到法语数据集上，LAU模型与使用100%更多数据预训练的E2E-ST系统相比，在标准指标上达到相当性能，同时在保持语义含义方面表现更好。引入了总参数漂移指标来量化正则化的结构影响。", "conclusion": "LAU是后处理重评分的稳健替代方案，是E2E-ST训练的有价值补充，特别是在训练数据稀缺和/或嘈杂的情况下。"}}
{"id": "2601.00885", "pdf": "https://arxiv.org/pdf/2601.00885", "abs": "https://arxiv.org/abs/2601.00885", "authors": ["Mandar Parab"], "title": "Counterfactual Self-Questioning for Stable Policy Optimization in Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Recent work on language model self-improvement shows that models can refine their own reasoning through reflection, verification, debate, or self-generated rewards. However, most existing approaches rely on external critics, learned reward models, or ensemble sampling, which increases complexity and training instability. We propose Counterfactual Self-Questioning, a framework in which a single language model generates and evaluates counterfactual critiques of its own reasoning. The method produces an initial reasoning trace, formulates targeted questions that challenge potential failure points, and generates alternative reasoning trajectories that expose incorrect assumptions or invalid steps. These counterfactual trajectories provide structured relative feedback that can be directly used for policy optimization without auxiliary models. Experiments on multiple mathematical reasoning benchmarks show that counterfactual self-questioning improves accuracy and training stability, particularly for smaller models, enabling scalable self-improvement using internally generated supervision alone.", "AI": {"tldr": "提出Counterfactual Self-Questioning框架，让单一语言模型通过生成和评估反事实批判来自我改进推理能力，无需外部批评者或奖励模型，提高了训练稳定性和准确性。", "motivation": "现有语言模型自我改进方法依赖外部批评者、学习到的奖励模型或集成采样，增加了复杂性和训练不稳定性。", "method": "模型首先生成初始推理轨迹，然后制定针对性问题挑战潜在失败点，生成替代推理轨迹以暴露错误假设或无效步骤，利用反事实轨迹提供结构化相对反馈进行策略优化。", "result": "在多个数学推理基准测试中显示，该方法提高了准确性和训练稳定性，特别是对小模型效果显著，仅使用内部生成的监督即可实现可扩展的自我改进。", "conclusion": "反事实自我质疑提供了一种有效的自我监督学习框架，使语言模型能够通过内部生成的批判性反馈来改进推理能力，减少对外部组件的依赖。"}}
{"id": "2601.01126", "pdf": "https://arxiv.org/pdf/2601.01126", "abs": "https://arxiv.org/abs/2601.01126", "authors": ["Andrew Borthwick", "Stephen Ash"], "title": "RoboPhD: Self-Improving Text-to-SQL Through Autonomous Agent Evolution", "categories": ["cs.CL"], "comment": "18 pages, 3 figures", "summary": "We present RoboPhD, a system where AI agents autonomously conduct research to improve Text-to-SQL performance. RoboPhD implements a closed-loop evolution cycle with two coordinated components: a SQL Generation agent composed of a database analysis script and SQL generation instructions, and an Evolution agent that designs new versions based on performance feedback. Central to the framework is an ELO-based selection mechanism enabling survival-of-the-fittest dynamics while handling non-transitivity in performance. Starting from a naive 70-line baseline, RoboPhD evolves agents through iterative cross-pollination, discovering effective techniques without any external guidance on the Text-to-SQL domain. Our best agent, evolved to 1500 lines over 18 iterations, autonomously discovered strategies such as size-adaptive database analysis that adjusts depth based on schema complexity and SQL generation patterns for column selection, evidence interpretation, and aggregation. Evolution provides the largest gains on cheaper models: while we improve by 2.3 points over a strong Claude Opus 4.5 naive baseline, we show an improvement of 8.9 points over the weaker Claude Haiku model. This enables 'skip a tier' deployment: evolved Haiku exceeds naive Sonnet accuracy, and evolved Sonnet exceeds naive Opus, both at lower cost. The full system achieves 73.67% accuracy on the BIRD test set, demonstrating that AI can autonomously build a strong agentic system with only a trivial human-provided starting point.", "AI": {"tldr": "RoboPhD是一个AI自主研究系统，通过闭环进化循环自动改进Text-to-SQL性能，从70行基线代码进化到1500行，在BIRD测试集上达到73.67%准确率，实现了低成本模型的性能跃升。", "motivation": "开发一个能够自主进行研究的AI系统，无需外部指导就能自动发现和改进Text-to-SQL技术，实现AI代理的自我进化。", "method": "采用闭环进化循环系统，包含SQL生成代理和进化代理两个协调组件，使用ELO选择机制处理性能非传递性，通过迭代交叉进化发现有效技术。", "result": "经过18次迭代进化，系统发现了大小自适应数据库分析、列选择、证据解释和聚合等SQL生成模式，在较弱模型上获得更大提升（Haiku提升8.9点），实现了'跳级部署'效果。", "conclusion": "AI能够仅从简单的人类提供起点自主构建强大的代理系统，证明了自主进化研究的可行性，为低成本模型部署提供了新途径。"}}
{"id": "2601.00923", "pdf": "https://arxiv.org/pdf/2601.00923", "abs": "https://arxiv.org/abs/2601.00923", "authors": ["Josef Ott"], "title": "Context Collapse: In-Context Learning and Model Collapse", "categories": ["cs.AI"], "comment": "Master's thesis", "summary": "This thesis investigates two key phenomena in large language models (LLMs): in-context learning (ICL) and model collapse. We study ICL in a linear transformer with tied weights trained on linear regression tasks, and show that minimising the in-context loss leads to a phase transition in the learned parameters. Above a critical context length, the solution develops a skew-symmetric component. We prove this by reducing the forward pass of the linear transformer under weight tying to preconditioned gradient descent, and then analysing the optimal preconditioner. This preconditioner includes a skew-symmetric component, which induces a rotation of the gradient direction. For model collapse, we use martingale and random walk theory to analyse simplified settings - linear regression and Gaussian fitting - under both replacing and cumulative data regimes. We strengthen existing results by proving almost sure convergence, showing that collapse occurs unless the data grows sufficiently fast or is retained over time. Finally, we introduce the notion of context collapse: a degradation of context during long generations, especially in chain-of-thought reasoning. This concept links the dynamics of ICL with long-term stability challenges in generative models.", "AI": {"tldr": "该论文研究大语言模型中的上下文学习和模型崩溃现象，在线性变换器框架下分析了参数相变、最优预条件器，并证明了模型崩溃的必然性，最后提出了上下文崩溃的新概念。", "motivation": "研究大语言模型中两个关键现象：上下文学习(ICL)的机制和模型崩溃的动态过程，旨在理解LLMs在训练和生成过程中的内在行为模式。", "method": "1) 使用带权重绑定的线性变换器分析ICL，将其前向传播简化为预条件梯度下降；2) 使用鞅理论和随机游走理论分析线性回归和高斯拟合的模型崩溃；3) 提出上下文崩溃概念分析长序列生成问题。", "result": "1) 发现ICL存在临界上下文长度，超过该长度时参数会出现斜对称分量；2) 证明了模型崩溃的几乎必然收敛性；3) 揭示了上下文崩溃是ICL动态与生成模型长期稳定性挑战的关联点。", "conclusion": "论文通过理论分析揭示了LLMs中ICL和模型崩溃的深层机制，提出的上下文崩溃概念为理解长序列生成中的性能退化提供了新视角，对模型设计和稳定性改进具有重要意义。"}}
{"id": "2601.01143", "pdf": "https://arxiv.org/pdf/2601.01143", "abs": "https://arxiv.org/abs/2601.01143", "authors": ["Peng Chen"], "title": "KOS-TL (Knowledge Operation System Type Logic)", "categories": ["cs.CL", "cs.LO"], "comment": null, "summary": "This paper introduces KOS-TL (Knowledge Operation System Type Logic), a novel constructive framework designed to provide a rigorous logical foundation for autonomous and executable knowledge systems. Traditional knowledge representation models often suffer from a gap between static symbolic logic and dynamic system execution. To bridge this divide, KOS-TL leverages Dependent Type Theory to unify data, logic, and proof into a singular computational substrate.The architecture of KOS-TL is organized into three hierarchical layers: the Core Layer, which defines the static type universe and constructive primitives; the Kernel Layer, which governs state evolution through an event-driven mechanism characterized by the triple $\\langle Σ, \\textsf{Ev}, Δ\\rangle$; and the Runtime Layer, responsible for the bidirectional refinement of physical signals into logical evidence. We formally define the operational semantics of the system and prove key meta-theoretical properties, including Progress and Evolutionary Consistency, ensuring that the system remains logically self-consistent and free from stuck states during continuous state transitions.By integrating Davidsonian event semantics with Martin-Löf type theory, KOS-TL enables the construction of \"proof-carrying knowledge,\" where every state change in the knowledge base is accompanied by a formal witness of its validity. We demonstrate the practical utility of this logic through application examples in industrial traceability and cross-border financial compliance. Our results suggest that KOS-TL provides a robust, formally verifiable basis for the next generation of intelligent, autonomous operating systems.", "AI": {"tldr": "KOS-TL是一个基于依赖类型理论的新型构造性框架，为自主可执行知识系统提供严格的逻辑基础，通过三层架构统一数据、逻辑和证明，确保系统在连续状态转换中的逻辑自洽性和无阻塞状态。", "motivation": "传统知识表示模型存在静态符号逻辑与动态系统执行之间的鸿沟，需要一种能够统一数据、逻辑和证明的计算基础来支持自主知识系统的构建。", "method": "采用依赖类型理论，构建三层架构：核心层定义静态类型宇宙和构造原语；内核层通过事件驱动机制管理状态演化；运行时层负责物理信号到逻辑证据的双向精化。整合Davidsonian事件语义和Martin-Löf类型理论。", "result": "正式定义了系统的操作语义，证明了关键元理论属性（包括进展性和演化一致性），确保系统逻辑自洽且无阻塞状态。通过工业追溯和跨境金融合规应用案例展示了实用性。", "conclusion": "KOS-TL为下一代智能自主操作系统提供了强大且可形式化验证的基础，能够构建'携带证明的知识'，其中知识库的每个状态变化都伴随有效性的形式见证。"}}
{"id": "2601.00994", "pdf": "https://arxiv.org/pdf/2601.00994", "abs": "https://arxiv.org/abs/2601.00994", "authors": ["Michael Bao"], "title": "ElecTwit: A Framework for Studying Persuasion in Multi-Agent Social Systems", "categories": ["cs.AI", "cs.CY"], "comment": "In proceedings of 2025 IEEE International Conference on Agentic AI (ICA)", "summary": "This paper introduces ElecTwit, a simulation framework designed to study persuasion within multi-agent systems, specifically emulating the interactions on social media platforms during a political election. By grounding our experiments in a realistic environment, we aimed to overcome the limitations of game-based simulations often used in prior research. We observed the comprehensive use of 25 specific persuasion techniques across most tested LLMs, encompassing a wider range than previously reported. The variations in technique usage and overall persuasion output between models highlight how different model architectures and training can impact the dynamics in realistic social simulations. Additionally, we observed unique phenomena such as \"kernel of truth\" messages and spontaneous developments with an \"ink\" obsession, where agents collectively demanded written proof. Our study provides a foundation for evaluating persuasive LLM agents in real-world contexts, ensuring alignment and preventing dangerous outcomes.", "AI": {"tldr": "ElecTwit框架模拟社交媒体政治选举中的多智能体说服互动，发现25种说服技术在不同LLM中的使用差异，揭示了模型架构和训练对社交模拟的影响，并观察到独特现象如'真相核心'消息和'墨水'痴迷。", "motivation": "克服以往基于游戏模拟研究的局限性，在更真实的环境中研究多智能体系统中的说服行为，特别是在政治选举的社交媒体互动场景。", "method": "开发ElecTwit模拟框架，在真实环境中进行实验，测试多种大型语言模型(LLMs)，分析25种具体说服技术的使用情况。", "result": "观察到25种说服技术在大多数测试LLM中被广泛使用，范围超出以往报告；不同模型在技术使用和说服输出上存在显著差异；发现了'真相核心'消息和'墨水'痴迷等独特现象。", "conclusion": "该研究为在真实世界情境中评估有说服力的LLM智能体奠定了基础，有助于确保对齐性并防止危险后果，揭示了模型架构和训练对社交模拟动态的重要影响。"}}
{"id": "2601.01153", "pdf": "https://arxiv.org/pdf/2601.01153", "abs": "https://arxiv.org/abs/2601.01153", "authors": ["Jiani Guo", "Jiajia Li", "Jie Wu", "Zuchao Li", "Yujiu Yang", "Ping Wang"], "title": "SongSage: A Large Musical Language Model with Lyric Generative Pre-training", "categories": ["cs.CL"], "comment": null, "summary": "Large language models have achieved significant success in various domains, yet their understanding of lyric-centric knowledge has not been fully explored. In this work, we first introduce PlaylistSense, a dataset to evaluate the playlist understanding capability of language models. PlaylistSense encompasses ten types of user queries derived from common real-world perspectives, challenging LLMs to accurately grasp playlist features and address diverse user intents. Comprehensive evaluations indicate that current general-purpose LLMs still have potential for improvement in playlist understanding. Inspired by this, we introduce SongSage, a large musical language model equipped with diverse lyric-centric intelligence through lyric generative pretraining. SongSage undergoes continual pretraining on LyricBank, a carefully curated corpus of 5.48 billion tokens focused on lyrical content, followed by fine-tuning with LyricBank-SFT, a meticulously crafted instruction set comprising 775k samples across nine core lyric-centric tasks. Experimental results demonstrate that SongSage exhibits a strong understanding of lyric-centric knowledge, excels in rewriting user queries for zero-shot playlist recommendations, generates and continues lyrics effectively, and performs proficiently across seven additional capabilities. Beyond its lyric-centric expertise, SongSage also retains general knowledge comprehension and achieves a competitive MMLU score. We will keep the datasets inaccessible due to copyright restrictions and release the SongSage and training script to ensure reproducibility and support music AI research and applications, the datasets release plan details are provided in the appendix.", "AI": {"tldr": "SongSage是一个专注于歌词理解的大型音乐语言模型，通过50亿歌词语料训练，在歌词生成、播放列表推荐等9个核心任务上表现出色，同时保持通用知识能力。", "motivation": "当前通用大语言模型在歌词和播放列表理解方面存在不足，需要专门针对音乐内容进行优化。", "method": "使用LyricBank语料库（54.8亿token）进行持续预训练，然后使用LyricBank-SFT指令集（77.5万样本）在9个歌词相关任务上进行微调。", "result": "SongSage在歌词理解、查询重写、零样本播放列表推荐、歌词生成等任务上表现优异，MMLU得分具有竞争力。", "conclusion": "专门针对歌词内容训练的模型能显著提升音乐相关任务性能，同时保持通用能力，为音乐AI研究提供了有效解决方案。"}}
{"id": "2601.01195", "pdf": "https://arxiv.org/pdf/2601.01195", "abs": "https://arxiv.org/abs/2601.01195", "authors": ["Wuzhenghong Wen", "Chao Xue", "Su Pan", "Yuwei Sun", "Minlong Peng"], "title": "Reinforcement Learning Enhanced Multi-hop Reasoning for Temporal Knowledge Question Answering", "categories": ["cs.AI"], "comment": "11 pages, 2 figures", "summary": "Temporal knowledge graph question answering (TKGQA) involves multi-hop reasoning over temporally constrained entity relationships in the knowledge graph to answer a given question. However, at each hop, large language models (LLMs) retrieve subgraphs with numerous temporally similar and semantically complex relations, increasing the risk of suboptimal decisions and error propagation. To address these challenges, we propose the multi-hop reasoning enhanced (MRE) framework, which enhances both forward and backward reasoning to improve the identification of globally optimal reasoning trajectories. Specifically, MRE begins with prompt engineering to guide the LLM in generating diverse reasoning trajectories for a given question. Valid reasoning trajectories are then selected for supervised fine-tuning, serving as a cold-start strategy. Finally, we introduce Tree-Group Relative Policy Optimization (T-GRPO), a recursive, tree-structured learning-by-exploration approach. At each hop, exploration establishes strong causal dependencies on the previous hop, while evaluation is informed by multi-path exploration feedback from subsequent hops. Experimental results on two TKGQA benchmarks indicate that the proposed MRE-based model consistently surpasses state-of-the-art (SOTA) approaches in handling complex multi-hop queries. Further analysis highlights improved interpretability and robustness to noisy temporal annotations.", "AI": {"tldr": "提出了多跳推理增强(MRE)框架，通过前向和后向推理增强，结合Tree-Group Relative Policy Optimization算法，在时序知识图谱问答任务中显著超越现有最佳方法。", "motivation": "解决时序知识图谱问答中LLMs在多跳推理时检索到大量时间相似和语义复杂关系，导致次优决策和错误传播的问题。", "method": "1) 提示工程生成多样化推理轨迹；2) 有效轨迹用于监督微调作为冷启动策略；3) 提出T-GRPO树结构探索学习算法，建立跳间因果依赖关系。", "result": "在两个TKGQA基准测试中一致超越SOTA方法，在处理复杂多跳查询方面表现优异。", "conclusion": "MRE框架显著提升了复杂多跳查询的处理能力，同时提高了模型的可解释性和对噪声时间标注的鲁棒性。"}}
{"id": "2601.01156", "pdf": "https://arxiv.org/pdf/2601.01156", "abs": "https://arxiv.org/abs/2601.01156", "authors": ["Jiani Guo", "Xiangke Zeng", "Jie Wu", "Zuchao Li"], "title": "DHI: Leveraging Diverse Hallucination Induction for Enhanced Contrastive Factuality Control in Large Language Models", "categories": ["cs.CL"], "comment": "ICONIP 2025", "summary": "Large language models (LLMs) frequently produce inaccurate or fabricated information, known as \"hallucinations,\" which compromises their reliability. Existing approaches often train an \"Evil LLM\" to deliberately generate hallucinations on curated datasets, using these induced hallucinations to guide contrastive decoding against a reliable \"positive model\" for hallucination mitigation. However, this strategy is limited by the narrow diversity of hallucinations induced, as Evil LLMs trained on specific error types tend to reproduce only these particular patterns, thereby restricting their overall effectiveness. To address these limitations, we propose DHI (Diverse Hallucination Induction), a novel training framework that enables the Evil LLM to generate a broader range of hallucination types without relying on pre-annotated hallucination data. DHI employs a modified loss function that down-weights the generation of specific factually correct tokens, encouraging the Evil LLM to produce diverse hallucinations at targeted positions while maintaining overall factual content. Additionally, we introduce a causal attention masking adaptation to reduce the impact of this penalization on the generation of subsequent tokens. During inference, we apply an adaptive rationality constraint that restricts contrastive decoding to tokens where the positive model exhibits high confidence, thereby avoiding unnecessary penalties on factually correct tokens. Extensive empirical results show that DHI achieves significant performance gains over other contrastive decoding-based approaches across multiple hallucination benchmarks.", "AI": {"tldr": "DHI是一个新的训练框架，通过修改损失函数和因果注意力掩码，使Evil LLM能生成更多样化的幻觉，结合自适应理性约束的对比解码，显著提升了幻觉缓解效果。", "motivation": "现有方法通过训练Evil LLM生成幻觉来指导对比解码，但诱导的幻觉类型有限，限制了整体效果。需要一种能产生更广泛幻觉类型的方法。", "method": "提出DHI框架：1) 修改损失函数降低特定正确token的权重，鼓励生成多样化幻觉；2) 因果注意力掩码减少惩罚对后续token的影响；3) 推理时使用自适应理性约束，只在正模型高置信度位置进行对比解码。", "result": "在多个幻觉基准测试中，DHI相比其他基于对比解码的方法取得了显著的性能提升。", "conclusion": "DHI通过诱导更多样化的幻觉和自适应对比解码策略，有效提高了语言模型的可靠性，解决了现有方法幻觉类型有限的问题。"}}
{"id": "2601.01301", "pdf": "https://arxiv.org/pdf/2601.01301", "abs": "https://arxiv.org/abs/2601.01301", "authors": ["Keith Frankston", "Benjamin Howard"], "title": "Accelerating Monte-Carlo Tree Search with Optimized Posterior Policies", "categories": ["cs.AI", "cs.LG"], "comment": "11 pages; an efficient implementation is available at https://github.com/bhoward73/rmcts", "summary": "We introduce a recursive AlphaZero-style Monte--Carlo tree search algorithm, \"RMCTS\". The advantage of RMCTS over AlphaZero's MCTS-UCB is speed. In RMCTS, the search tree is explored in a breadth-first manner, so that network inferences naturally occur in large batches. This significantly reduces the GPU latency cost. We find that RMCTS is often more than 40 times faster than MCTS-UCB when searching a single root state, and about 3 times faster when searching a large batch of root states.\n  The recursion in RMCTS is based on computing optimized posterior policies at each game state in the search tree, starting from the leaves and working back up to the root. Here we use the posterior policy explored in \"Monte--Carlo tree search as regularized policy optimization\" (Grill, et al.) Their posterior policy is the unique policy which maximizes the expected reward given estimated action rewards minus a penalty for diverging from the prior policy.\n  The tree explored by RMCTS is not defined in an adaptive manner, as it is in MCTS-UCB. Instead, the RMCTS tree is defined by following prior network policies at each node. This is a disadvantage, but the speedup advantage is more significant, and in practice we find that RMCTS-trained networks match the quality of MCTS-UCB-trained networks in roughly one-third of the training time. We include timing and quality comparisons of RMCTS vs. MCTS-UCB for three games: Connect-4, Dots-and-Boxes, and Othello.", "AI": {"tldr": "RMCTS是一种递归AlphaZero风格的蒙特卡洛树搜索算法，相比MCTS-UCB速度显著提升，训练时间减少到三分之一，但搜索树定义方式不同", "motivation": "解决AlphaZero的MCTS-UCB算法在GPU推理延迟方面的效率问题，通过广度优先搜索和大批量网络推理来显著提升搜索速度", "method": "采用递归的广度优先搜索方式，从叶子节点向根节点计算优化的后验策略，使用Grill等人提出的正则化策略优化方法，通过固定先验网络策略定义搜索树而非自适应方式", "result": "RMCTS比MCTS-UCB快40倍（单根状态搜索）和3倍（大批量根状态搜索），在Connect-4、Dots-and-Boxes和Othello三个游戏中达到相同网络质量但训练时间减少约三分之二", "conclusion": "RMCTS通过牺牲搜索树的自适应定义来换取显著的速度优势，在实际应用中能够在更短时间内训练出与MCTS-UCB相当质量的网络，是效率与性能的良好权衡"}}
{"id": "2601.01171", "pdf": "https://arxiv.org/pdf/2601.01171", "abs": "https://arxiv.org/abs/2601.01171", "authors": ["Serge Sharoff", "John Baker", "David Francis Hunt", "Alan Simpson"], "title": "Almost Clinical: Linguistic properties of synthetic electronic health records", "categories": ["cs.CL"], "comment": null, "summary": "This study evaluates the linguistic and clinical suitability of synthetic electronic health records (EHRs) in the field of mental health. First, we describe the rationale and the methodology for creating the synthetic corpus. Second, we assess agency, modality, and information flow across four clinical genres (Assessments, Correspondence, Referrals and Care plans) to understand how LLMs grammatically construct medical authority and patient agency through linguistic choices. While LLMs produce coherent, terminology-appropriate texts that approximate clinical practice, systematic divergences remain, including registerial shifts, insufficient clinical specificity, and inaccuracies in medication use and diagnostic procedures.", "AI": {"tldr": "本研究评估了心理健康领域中合成电子健康记录的语言学和临床适用性，发现LLM能生成连贯且术语适当的临床文本，但仍存在语域转换、临床特异性不足及用药诊断不准确等系统性差异", "motivation": "评估合成电子健康记录在心理健康领域的实际应用价值，特别是LLM生成文本的语言学特征和临床准确性", "method": "首先描述合成语料库的创建原理和方法论，然后从代理性、情态和信息流三个维度分析四种临床文体（评估、通信、转诊和护理计划）的语言特征", "result": "LLM能够生成近似临床实践的连贯且术语适当的文本，但存在系统性差异：语域转换、临床特异性不足、药物使用和诊断程序不准确", "conclusion": "虽然合成EHR在心理健康领域显示出潜力，但需要解决语言和临床准确性问题才能达到临床应用标准"}}
{"id": "2601.01321", "pdf": "https://arxiv.org/pdf/2601.01321", "abs": "https://arxiv.org/abs/2601.01321", "authors": ["Rong Zhou", "Dongping Chen", "Zihan Jia", "Yao Su", "Yixin Liu", "Yiwen Lu", "Dongwei Shi", "Yue Huang", "Tianyang Xu", "Yi Pan", "Xinliang Li", "Yohannes Abate", "Qingyu Chen", "Zhengzhong Tu", "Yu Yang", "Yu Zhang", "Qingsong Wen", "Gengchen Mai", "Sunyang Fu", "Jiachen Li", "Xuyu Wang", "Ziran Wang", "Jing Huang", "Tianming Liu", "Yong Chen", "Lichao Sun", "Lifang He"], "title": "Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models", "categories": ["cs.AI"], "comment": null, "summary": "Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.", "AI": {"tldr": "本文提出了一个统一的四阶段框架，系统性地描述了人工智能在数字孪生生命周期中的集成方式，涵盖建模、镜像、干预和自主管理四个阶段，分析了物理建模与数据驱动的协同作用，并探讨了生成式AI技术如何将数字孪生转变为主动认知系统。", "motivation": "数字孪生已从被动仿真工具发展为智能自主实体，但缺乏对AI技术在整个生命周期中集成方式的系统性框架描述，需要统一的理论框架来指导AI驱动的数字孪生系统发展。", "method": "通过综合现有技术和实践，提炼出统一的四阶段框架：1)基于物理和物理信息AI方法建模物理孪生；2)实时同步镜像物理系统；3)通过预测建模、异常检测和优化策略干预物理孪生；4)通过大语言模型、基础模型和智能代理实现自主管理。", "result": "建立了系统化的AI集成框架，分析了物理建模与数据驱动的协同关系，识别了从传统数值求解器向物理信息模型和基础模型的转变，展示了生成式AI技术如何使数字孪生具备推理、通信和创造性场景生成能力。", "conclusion": "通过跨11个应用领域的综述，识别了可扩展性、可解释性和可信度等共同挑战，并为负责任的AI驱动数字孪生系统指明了发展方向，强调了物理与AI融合的重要性。"}}
{"id": "2601.01225", "pdf": "https://arxiv.org/pdf/2601.01225", "abs": "https://arxiv.org/abs/2601.01225", "authors": ["Hezam Albaqami", "Muhammad Asif Ayub", "Nasir Ahmad", "Yaseen Ahmad", "Mohammed M. Alqahtani", "Abdullah M. Algamdi", "Almoaid A. Owaidah", "Kashif Ahmad"], "title": "Stylometry Analysis of Human and Machine Text for Academic Integrity", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 9 tables, 3 figures", "summary": "This work addresses critical challenges to academic integrity, including plagiarism, fabrication, and verification of authorship of educational content, by proposing a Natural Language Processing (NLP)-based framework for authenticating students' content through author attribution and style change detection. Despite some initial efforts, several aspects of the topic are yet to be explored. In contrast to existing solutions, the paper provides a comprehensive analysis of the topic by targeting four relevant tasks, including (i) classification of human and machine text, (ii) differentiating in single and multi-authored documents, (iii) author change detection within multi-authored documents, and (iv) author recognition in collaboratively produced documents. The solutions proposed for the tasks are evaluated on two datasets generated with Gemini using two different prompts, including a normal and a strict set of instructions. During experiments, some reduction in the performance of the proposed solutions is observed on the dataset generated through the strict prompt, demonstrating the complexities involved in detecting machine-generated text with cleverly crafted prompts. The generated datasets, code, and other relevant materials are made publicly available on GitHub, which are expected to provide a baseline for future research in the domain.", "AI": {"tldr": "该论文提出基于NLP的框架，通过作者归属和风格变化检测来解决学术诚信问题，包括人机文本分类、单多作者区分、多作者文档中的作者变更检测和协作文档作者识别四个任务，并在Gemini生成的数据集上验证了方法有效性。", "motivation": "解决学术诚信面临的剽窃、伪造和内容作者验证等关键挑战，现有解决方案尚不完善，需要更全面的分析方法。", "method": "提出NLP框架，针对四个相关任务设计解决方案：人机文本分类、单多作者区分、多作者文档作者变更检测、协作文档作者识别。使用Gemini生成两个不同提示词的数据集（正常和严格指令）进行实验评估。", "result": "实验显示，在严格提示生成的数据集上解决方案性能有所下降，表明检测精心设计的机器生成文本存在复杂性。生成的数据集、代码和相关材料已公开在GitHub上。", "conclusion": "该研究为学术内容认证提供了综合解决方案，公开的资源为未来该领域研究提供了基线，同时揭示了检测机器生成文本的挑战性。"}}
{"id": "2601.01330", "pdf": "https://arxiv.org/pdf/2601.01330", "abs": "https://arxiv.org/abs/2601.01330", "authors": ["Shengji Tang", "Weihao Lin", "Jingqi Ye", "Hao Li", "Bo Zhang", "Shuyue Hu", "Tao Chen", "Wangli Ouyang", "Lei Bai", "Peng Ye"], "title": "Beyond Gemini-3-Pro: Revisiting LLM Routing and Aggregation at Scale", "categories": ["cs.AI"], "comment": "12 pages", "summary": "Large Language Models (LLMs) have rapidly advanced, with Gemini-3-Pro setting a new performance milestone. In this work, we explore collective intelligence as an alternative to monolithic scaling, and demonstrate that open-source LLMs' collaboration can surpass Gemini-3-Pro. We first revisit LLM routing and aggregation at scale and identify three key bottlenecks: (1) current train-free routers are limited by a query-based paradigm focusing solely on textual similarity; (2) recent aggregation methods remain largely static, failing to select appropriate aggregators for different tasks;(3) the complementarity of routing and aggregation remains underutilized. To address these problems, we introduce JiSi, a novel framework designed to release the full potential of LLMs' collaboration through three innovations: (1) Query-Response Mixed Routing capturing both semantic information and problem difficulty; (2) Support-Set-based Aggregator Selection jointly evaluating the aggregation and domain capacity of aggregators; (3) Adaptive Routing-Aggregation Switch dynamically leveraging the advantages of routing and aggregation. Comprehensive experiments on nine benchmarks demonstrate that JiSi can surpass Gemini-3-Pro with only 47% costs by orchestrating ten open-source LLMs, while outperforming mainstream baselines. It suggests that collective intelligence represents a novel path towards Artificial General Intelligence (AGI).", "AI": {"tldr": "JiSi框架通过创新的路由和聚合机制，让开源LLM协作超越Gemini-3-Pro性能，成本仅47%", "motivation": "当前LLM路由和聚合存在三个瓶颈：基于查询的路由只关注文本相似性、聚合方法静态无法适应不同任务、路由与聚合互补性未充分利用", "method": "提出JiSi框架，包含三个创新：查询-响应混合路由、基于支持集的聚合器选择、自适应路由-聚合切换", "result": "在9个基准测试中超越Gemini-3-Pro，成本仅47%，同时优于主流基线方法", "conclusion": "集体智能代表了通向AGI的新路径，开源LLM协作可以超越单一大型模型"}}
{"id": "2601.01244", "pdf": "https://arxiv.org/pdf/2601.01244", "abs": "https://arxiv.org/abs/2601.01244", "authors": ["Zsolt Csibi", "Bence György Gortka", "Natabara Gyöngyössy", "Kornél Nagy", "Dávid Márk Nemeskey", "Martin Sallai", "András Simonyi", "András Márk Szekeres", "Gábor Palkó"], "title": "Racka: Efficient Hungarian LLM Adaptation on Academic Infrastructure", "categories": ["cs.CL"], "comment": "18 pages, 1 figures. To appear in the XXII. Magyar Számítógépes Nyelvészeti Konferencia (MSZNY 2026)", "summary": "We present Racka, a lightweight, continually pretrained large language model designed to bridge the resource gap between Hungarian and high-resource languages such as English and German. Racka employs parameter-efficient continual pretraining via Low-Rank Adaptation (LoRA) on a Qwen-3 4B backbone, making the recipe practical on A100 (40GB)-based HPC clusters with low inter-node bandwidth. To better match the training distribution, we replace and adapt the tokenizer, achieving substantially improved tokenization fertility for Hungarian while maintaining competitive performance in English and German. The model is trained on 160B subword tokens drawn from a mixture of internet and high-quality curated sources, with a composition of 44% Hungarian, 24% English, 21% German, and 11% code. This data mix is chosen to mitigate catastrophic forgetting and preserve high-resource language capabilities during continual pretraining. Our preliminary results indicate modest but stable results in language adaptation.", "AI": {"tldr": "Racka是一个轻量级持续预训练大语言模型，通过LoRA技术高效适配Qwen-3 4B主干，专门为匈牙利语设计，同时保持英语和德语能力。", "motivation": "解决匈牙利语与高资源语言（如英语、德语）之间的资源差距问题，提供实用的多语言模型解决方案。", "method": "采用参数高效的LoRA持续预训练方法，在Qwen-3 4B基础上进行适配；改进分词器以提升匈牙利语分词效率；使用160B子词token的多语言混合数据集（44%匈牙利语、24%英语、21%德语、11%代码）。", "result": "实现了匈牙利语分词效率的显著提升，同时保持了英语和德语的竞争性性能，初步结果显示语言适应方面取得了稳定但适度的成果。", "conclusion": "该方法为低资源语言提供了实用的持续预训练方案，成功缓解了灾难性遗忘问题，为多语言模型开发提供了可行的技术路径。"}}
{"id": "2601.01363", "pdf": "https://arxiv.org/pdf/2601.01363", "abs": "https://arxiv.org/abs/2601.01363", "authors": ["Xiaomeng Yang", "Zhiyu Tan", "Xiaohui Zhong", "Mengping Yang", "Qiusheng Huang", "Lei Chen", "Libo Wu", "Hao Li"], "title": "A unified multimodal understanding and generation model for cross-disciplinary scientific research", "categories": ["cs.AI"], "comment": null, "summary": "Scientific discovery increasingly relies on integrating heterogeneous, high-dimensional data across disciplines nowadays. While AI models have achieved notable success across various scientific domains, they typically remain domain-specific or lack the capability of simultaneously understanding and generating multimodal scientific data, particularly for high-dimensional data. Yet, many pressing global challenges and scientific problems are inherently cross-disciplinary and require coordinated progress across multiple fields. Here, we present FuXi-Uni, a native unified multimodal model for scientific understanding and high-fidelity generation across scientific domains within a single architecture. Specifically, FuXi-Uni aligns cross-disciplinary scientific tokens within natural language tokens and employs science decoder to reconstruct scientific tokens, thereby supporting both natural language conversation and scientific numerical prediction. Empirically, we validate FuXi-Uni in Earth science and Biomedicine. In Earth system modeling, the model supports global weather forecasting, tropical cyclone (TC) forecast editing, and spatial downscaling driven by only language instructions. FuXi-Uni generates 10-day global forecasts at 0.25° resolution that outperform the SOTA physical forecasting system. It shows superior performance for both TC track and intensity prediction relative to the SOTA physical model, and generates high-resolution regional weather fields that surpass standard interpolation baselines. Regarding biomedicine, FuXi-Uni outperforms leading multimodal large language models on multiple biomedical visual question answering benchmarks. By unifying heterogeneous scientific modalities within a native shared latent space while maintaining strong domain-specific performance, FuXi-Uni provides a step forward more general-purpose, multimodal scientific models.", "AI": {"tldr": "FuXi-Uni是一个原生统一多模态科学模型，能够在单一架构中理解和生成跨科学领域的高维数据，在地球科学和生物医学领域表现优异，超越了当前最先进的物理模型和多模态大语言模型。", "motivation": "科学发现日益依赖跨学科异构高维数据的整合，但现有AI模型通常是领域特定的，缺乏同时理解和生成多模态科学数据的能力，而许多全球挑战需要跨学科协调进展。", "method": "FuXi-Uni将跨学科科学标记与自然语言标记对齐，使用科学解码器重建科学标记，支持自然语言对话和科学数值预测，在统一共享潜在空间中整合异构科学模态。", "result": "在地球系统建模中：1) 生成10天全球天气预报（0.25°分辨率）超越SOTA物理预报系统；2) 热带气旋路径和强度预测优于SOTA物理模型；3) 生成的高分辨率区域天气场超越标准插值基线。在生物医学中：在多个生物医学视觉问答基准上超越领先的多模态大语言模型。", "conclusion": "FuXi-Uni通过在原生共享潜在空间中统一异构科学模态并保持强大的领域特定性能，为更通用的多模态科学模型迈出了重要一步。"}}
{"id": "2601.01266", "pdf": "https://arxiv.org/pdf/2601.01266", "abs": "https://arxiv.org/abs/2601.01266", "authors": ["Rhitabrat Pokharel", "Hamid Hassanzadeh", "Ameeta Agrawal"], "title": "From Policy to Logic for Efficient and Interpretable Coverage Assessment", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at AIMedHealth @ AAAI 2026", "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in interpreting lengthy, complex legal and policy language. However, their reliability can be undermined by hallucinations and inconsistencies, particularly when analyzing subjective and nuanced documents. These challenges are especially critical in medical coverage policy review, where human experts must be able to rely on accurate information. In this paper, we present an approach designed to support human reviewers by making policy interpretation more efficient and interpretable. We introduce a methodology that pairs a coverage-aware retriever with symbolic rule-based reasoning to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. This hybrid system minimizes the number of LLM inferences required which reduces overall model cost. Notably, our approach achieves a 44% reduction in inference cost alongside a 4.5% improvement in F1 score, demonstrating both efficiency and effectiveness.", "AI": {"tldr": "提出一种结合检索器和符号推理的混合方法，用于医疗政策审查，在减少44%推理成本的同时提升4.5% F1分数", "motivation": "大型语言模型在解释复杂法律政策语言时存在幻觉和不一致问题，特别是在医疗覆盖政策审查这种需要准确信息的领域", "method": "采用覆盖感知检索器与基于符号规则的推理相结合的方法，提取相关政策语言并将其组织成明确的事实和规则，生成可审计的推理依据", "result": "实现了44%的推理成本降低和4.5%的F1分数提升，证明了方法的效率和有效性", "conclusion": "该混合系统通过减少LLM推理需求，在保持准确性的同时显著降低成本，为人类审查员提供了更高效和可解释的政策解释支持"}}
{"id": "2601.01366", "pdf": "https://arxiv.org/pdf/2601.01366", "abs": "https://arxiv.org/abs/2601.01366", "authors": ["Zixian Liu", "Sihao Liu", "Yuqi Zhao"], "title": "KGCE: Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models", "categories": ["cs.AI"], "comment": null, "summary": "With the rapid adoption of multimodal large language models (MLMs) in autonomous agents, cross-platform task execution capabilities in educational settings have garnered significant attention. However, existing benchmark frameworks still exhibit notable deficiencies in supporting cross-platform tasks in educational contexts, especially when dealing with school-specific software (such as XiaoYa Intelligent Assistant, HuaShi XiaZi, etc.), where the efficiency of agents often significantly decreases due to a lack of understanding of the structural specifics of these private-domain software. Additionally, current evaluation methods heavily rely on coarse-grained metrics like goal orientation or trajectory matching, making it challenging to capture the detailed execution and efficiency of agents in complex tasks. To address these issues, we propose KGCE (Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models), a novel benchmarking platform that integrates knowledge base enhancement and a dual-graph evaluation framework. We first constructed a dataset comprising 104 education-related tasks, covering Windows, Android, and cross-platform collaborative tasks. KGCE introduces a dual-graph evaluation framework that decomposes tasks into multiple sub-goals and verifies their completion status, providing fine-grained evaluation metrics. To overcome the execution bottlenecks of existing agents in private-domain tasks, we developed an enhanced agent system incorporating a knowledge base specific to school-specific software. The code can be found at https://github.com/Kinginlife/KGCE.", "AI": {"tldr": "KGCE是一个面向教育场景的多模态大语言模型代理基准测试平台，通过知识库增强和双图评估框架解决跨平台教育任务中私有领域软件执行效率低和评估粒度粗的问题。", "motivation": "现有基准框架在教育场景的跨平台任务支持不足，特别是在处理学校专用软件时，代理效率显著下降；当前评估方法依赖粗粒度指标，难以捕捉复杂任务中的详细执行效率。", "method": "构建包含104个教育相关任务的数据集；提出双图评估框架将任务分解为子目标并验证完成状态；开发集成学校专用软件知识库的增强代理系统。", "result": "开发了KGCE基准测试平台，支持Windows、Android和跨平台协作任务，提供细粒度评估指标。", "conclusion": "KGCE通过知识库增强和双图评估有效解决了教育场景中跨平台代理执行的瓶颈问题，为多模态语言模型在教育领域的应用提供了更好的评估框架。"}}
{"id": "2601.01280", "pdf": "https://arxiv.org/pdf/2601.01280", "abs": "https://arxiv.org/abs/2601.01280", "authors": ["Sen Hu", "Yuxiang Wei", "Jiaxin Ran", "Zhiyuan Yao", "Lei Zou"], "title": "Does Memory Need Graphs? A Unified Framework and Empirical Analysis for Long-Term Dialog Memory", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graph structures are increasingly used in dialog memory systems, but empirical findings on their effectiveness remain inconsistent, making it unclear which design choices truly matter. We present an experimental, system-oriented analysis of long-term dialog memory architectures. We introduce a unified framework that decomposes dialog memory systems into core components and supports both graph-based and non-graph approaches. Under this framework, we conduct controlled, stage-wise experiments on LongMemEval and HaluMem, comparing common design choices in memory representation, organization, maintenance, and retrieval. Our results show that many performance differences are driven by foundational system settings rather than specific architectural innovations. Based on these findings, we identify stable and reliable strong baselines for future dialog memory research.", "AI": {"tldr": "论文通过统一框架对对话记忆系统进行实验分析，发现性能差异主要源于基础系统设置而非架构创新，并确定了可靠的基线方法", "motivation": "图结构在对话记忆系统中应用日益增多，但其有效性存在不一致的实证结果，需要明确哪些设计选择真正重要", "method": "提出统一框架将对话记忆系统分解为核心组件，支持图和非图方法，在LongMemEval和HaluMem数据集上进行分阶段控制实验，比较记忆表示、组织、维护和检索的常见设计选择", "result": "结果显示许多性能差异是由基础系统设置驱动的，而非特定的架构创新", "conclusion": "基于研究发现，为未来对话记忆研究确定了稳定可靠的强基线方法"}}
{"id": "2601.01378", "pdf": "https://arxiv.org/pdf/2601.01378", "abs": "https://arxiv.org/abs/2601.01378", "authors": ["Han Yuan", "Yilin Wu", "Li Zhang", "Zheng Ma"], "title": "Empowering Small Language Models with Factual Hallucination-Aware Reasoning for Financial Classification", "categories": ["cs.AI"], "comment": null, "summary": "Small language models (SLMs) are increasingly used for financial classification due to their fast inference and local deployability. However, compared with large language models, SLMs are more prone to factual hallucinations in reasoning and exhibit weaker classification performance. This raises a natural question: Can mitigating factual hallucinations improve SLMs' financial classification? To address this, we propose a three-step pipeline named AAAI (Association Identification, Automated Detection, and Adaptive Inference). Experiments on three representative SLMs reveal that: (1) factual hallucinations are positively correlated with misclassifications; (2) encoder-based verifiers effectively detect factual hallucinations; and (3) incorporating feedback on factual errors enables SLMs' adaptive inference that enhances classification performance. We hope this pipeline contributes to trustworthy and effective applications of SLMs in finance.", "AI": {"tldr": "提出了AAAI三阶段流水线（关联识别、自动检测、自适应推理）来缓解小语言模型在金融分类中的事实幻觉问题，实验证明该方法能有效提升分类性能", "motivation": "小语言模型在金融分类中存在事实幻觉问题，导致分类性能较弱，需要探索缓解事实幻觉是否能改善其分类能力", "method": "AAAI三阶段方法：1)关联识别 2)自动化检测 3)自适应推理，使用基于编码器的验证器检测事实幻觉，并通过事实错误反馈实现自适应推理", "result": "实验发现：1)事实幻觉与误分类正相关 2)编码器验证器能有效检测事实幻觉 3)基于事实错误的反馈能提升小语言模型的分类性能", "conclusion": "AAAI流水线有助于小语言模型在金融领域实现更可信和有效的应用"}}
{"id": "2601.01299", "pdf": "https://arxiv.org/pdf/2601.01299", "abs": "https://arxiv.org/abs/2601.01299", "authors": ["Ismail Lamaakal", "Chaymae Yahyati", "Yassine Maleh", "Khalid El Makkaoui", "Ibrahim Ouahbi"], "title": "T3C: Test-Time Tensor Compression with Consistency Guarantees", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "We present T3C, a train-once, test-time budget-conditioned compression framework that exposes rank and precision as a controllable deployment knob. T3C combines elastic tensor factorization (maintained up to a maximal rank) with rank-tied mixed-precision quantization and a lightweight controller that maps a latency/energy/size budget token to per-layer rank/bit assignments; the policy snaps to hardware-aligned profiles and is monotone in the budget. A fast, layerwise consistency certificate, computed from spectral proxies and activation statistics, upper-bounds logit drift and regularizes training, yielding a practical reliability signal with negligible overhead. On ImageNet-1k, T3C shifts the vision Pareto frontier: for ResNet-50 at matched accuracy (\\leq 0.5% drop), p50 latency is 1.18ms with a 38MB model, outperforming PTQ-8b (1.44ms, 88MB); for ViT-B/16, T3C reaches 2.30ms p50 with 59MB, improving over strong PTQ/QAT baselines. A single T3C checkpoint therefore provides predictable, certificate-backed accuracy-latency-size trade-offs on demand across devices.", "AI": {"tldr": "T3C是一个训练一次、测试时预算调节的压缩框架，通过张量分解和混合精度量化实现可控制的模型部署，提供可预测的精度-延迟-大小权衡", "motivation": "为了解决模型部署时需要根据不同硬件预算动态调整模型压缩程度的问题，传统方法需要为不同预算重新训练模型，效率低下", "method": "结合弹性张量分解（维持最大秩）、秩绑定的混合精度量化、轻量级控制器映射预算到每层秩/比特分配，以及基于谱代理和激活统计的层一致性证书", "result": "在ImageNet-1k上，ResNet-50达到1.18ms延迟和38MB模型大小（精度下降≤0.5%），ViT-B/16达到2.30ms延迟和59MB模型大小，优于PTQ-8b等基线方法", "conclusion": "T3C通过单一检查点提供了可预测的、有证书支持的精度-延迟-大小权衡，显著提升了视觉模型的部署效率"}}
{"id": "2601.01467", "pdf": "https://arxiv.org/pdf/2601.01467", "abs": "https://arxiv.org/abs/2601.01467", "authors": ["Romuald Kwessy Mouona", "Blaise Blériot Koguep Njionou", "Etienne Romuald Temgoua Alomo", "Rokia Missaoui", "Leonard Kwuida"], "title": "A construction of an optimal base for conditional attribute and attributional condition implications in triadic contexts", "categories": ["cs.AI"], "comment": "26 pages", "summary": "This article studies implications in triadic contexts. Specifically, we focus on those introduced by Ganter and Obiedkov, namely conditional attribute and attributional condition implications. Our aim is to construct an optimal base for these implications.", "AI": {"error": "'NoneType' object has no attribute 'model_dump'"}}
{"id": "2601.01332", "pdf": "https://arxiv.org/pdf/2601.01332", "abs": "https://arxiv.org/abs/2601.01332", "authors": ["Hossam Amer", "Maryam Dialameh", "Hossein Rajabzadeh", "Walid Ahmed", "Weiwei Zhang", "Yang Liu"], "title": "FLOP-Efficient Training: Early Stopping Based on Test-Time Compute Awareness", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Scaling training compute, measured in FLOPs, has long been shown to improve the accuracy of large language models, yet training remains resource-intensive. Prior work shows that increasing test-time compute (TTC)-for example through iterative sampling-can allow smaller models to rival or surpass much larger ones at lower overall cost. We introduce TTC-aware training, where an intermediate checkpoint and a corresponding TTC configuration can together match or exceed the accuracy of a fully trained model while requiring substantially fewer training FLOPs. Building on this insight, we propose an early stopping algorithm that jointly selects a checkpoint and TTC configuration to minimize training compute without sacrificing accuracy. To make this practical, we develop an efficient TTC evaluation method that avoids exhaustive search, and we formalize a break-even bound that identifies when increased inference compute compensates for reduced training compute. Experiments demonstrate up to 92\\% reductions in training FLOPs while maintaining and sometimes remarkably improving accuracy. These results highlight a new perspective for balancing training and inference compute in model development, enabling faster deployment cycles and more frequent model refreshes. Codes will be publicly released.", "AI": {"tldr": "提出TTC感知训练方法，通过选择中间检查点和测试时计算配置，在保持精度的同时大幅减少训练计算量，实验显示训练FLOPs减少高达92%", "motivation": "传统大规模语言模型训练计算成本高昂，虽然增加测试时计算可以让小模型媲美大模型，但需要更高效的方法来平衡训练和推理计算成本", "method": "提出TTC感知训练框架，开发早期停止算法联合选择检查点和TTC配置，建立有效评估方法和盈亏平衡边界分析", "result": "实验证明训练FLOPs减少高达92%，同时保持甚至显著提升模型精度", "conclusion": "该方法为模型开发中平衡训练和推理计算提供了新视角，能够实现更快的部署周期和更频繁的模型更新"}}
{"id": "2601.01511", "pdf": "https://arxiv.org/pdf/2601.01511", "abs": "https://arxiv.org/abs/2601.01511", "authors": ["Ahmed Dawoud", "Osama El-Shamy"], "title": "Reading Between the Lines: Deconfounding Causal Estimates using Text Embeddings and Deep Learning", "categories": ["cs.AI"], "comment": null, "summary": "Estimating causal treatment effects in observational settings is frequently compromised by selection bias arising from unobserved confounders. While traditional econometric methods struggle when these confounders are orthogonal to structured covariates, high-dimensional unstructured text often contains rich proxies for these latent variables. This study proposes a Neural Network-Enhanced Double Machine Learning (DML) framework designed to leverage text embeddings for causal identification. Using a rigorous synthetic benchmark, we demonstrate that unstructured text embeddings capture critical confounding information that is absent from structured tabular data. However, we show that standard tree-based DML estimators retain substantial bias (+24%) due to their inability to model the continuous topology of embedding manifolds. In contrast, our deep learning approach reduces bias to -0.86% with optimized architectures, effectively recovering the ground-truth causal parameter. These findings suggest that deep learning architectures are essential for satisfying the unconfoundedness assumption when conditioning on high-dimensional natural language data", "AI": {"tldr": "提出神经网络增强的双重机器学习框架，利用文本嵌入进行因果推断，在存在未观测混杂因素的情况下显著减少估计偏差", "motivation": "传统计量方法在处理与结构化协变量正交的未观测混杂因素时存在困难，而高维非结构化文本数据包含丰富的潜在变量代理信息", "method": "开发神经网络增强的双重机器学习框架，使用文本嵌入捕捉混杂信息，通过深度学习架构建模嵌入流形的连续拓扑结构", "result": "标准树基DML估计器存在显著偏差(+24%)，而提出的深度学习方法将偏差降至-0.86%，有效恢复真实因果参数", "conclusion": "深度学习架构对于在高维自然语言数据条件下满足无混杂假设至关重要，文本嵌入能够捕捉结构化数据中缺失的关键混杂信息"}}
{"id": "2601.01341", "pdf": "https://arxiv.org/pdf/2601.01341", "abs": "https://arxiv.org/abs/2601.01341", "authors": ["Md Abdullah Al Kafi", "Raka Moni", "Sumit Kumar Banshal"], "title": "Reasoning Over Recall: Evaluating the Efficacy of Generalist Architectures vs. Specialized Fine-Tunes in RAG-Based Mental Health Dialogue Systems", "categories": ["cs.CL"], "comment": null, "summary": "The deployment of Large Language Models (LLMs) in mental health counseling faces the dual challenges of hallucinations and lack of empathy. While the former may be mitigated by RAG (retrieval-augmented generation) by anchoring answers in trusted clinical sources, there remains an open question as to whether the most effective model under this paradigm would be one that is fine-tuned on mental health data, or a more general and powerful model that succeeds purely on the basis of reasoning. In this paper, we perform a direct comparison by running four open-source models through the same RAG pipeline using ChromaDB: two generalist reasoners (Qwen2.5-3B and Phi-3-Mini) and two domain-specific fine-tunes (MentalHealthBot-7B and TherapyBot-7B). We use an LLM-as-a-Judge framework to automate evaluation over 50 turns. We find a clear trend: the generalist models outperform the domain-specific ones in empathy (3.72 vs. 3.26, $p < 0.001$) in spite of being much smaller (3B vs. 7B), and all models perform well in terms of safety, but the generalist models show better contextual understanding and are less prone to overfitting as we observe in the domain-specific models. Overall, our results indicate that for RAG-based therapy systems, strong reasoning is more important than training on mental health-specific vocabulary; i.e. a well-reasoned general model would provide more empathetic and balanced support than a larger narrowly fine-tuned model, so long as the answer is already grounded in clinical evidence.", "AI": {"tldr": "在基于RAG的心理健康咨询系统中，通用推理模型（3B）在共情能力上显著优于领域专用微调模型（7B），表明强大的推理能力比心理健康特定词汇训练更重要。", "motivation": "解决LLM在心理健康咨询中的幻觉问题和缺乏共情能力，探索在RAG范式下，领域专用微调模型与通用推理模型哪种更有效。", "method": "使用ChromaDB构建相同RAG流程，比较四个开源模型：两个通用推理模型（Qwen2.5-3B和Phi-3-Mini）和两个领域专用微调模型（MentalHealthBot-7B和TherapyBot-7B），采用LLM-as-a-Judge框架进行50轮自动化评估。", "result": "通用模型在共情得分显著更高（3.72 vs 3.26，p<0.001），尽管模型更小；所有模型安全性良好，但通用模型展现更好的上下文理解能力且不易过拟合。", "conclusion": "对于基于RAG的治疗系统，强大的推理能力比心理健康特定训练更重要，基于临床证据的良好推理通用模型能提供更共情和平衡的支持。"}}
{"id": "2601.01522", "pdf": "https://arxiv.org/pdf/2601.01522", "abs": "https://arxiv.org/abs/2601.01522", "authors": ["Danial Amin"], "title": "Bayesian Orchestration of Multi-LLM Agents for Cost-Aware Sequential Decision-Making", "categories": ["cs.AI", "cs.CL", "cs.ET"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed as autonomous decision agents in settings with asymmetric error costs: hiring (missed talent vs wasted interviews), medical triage (missed emergencies vs unnecessary escalation), and fraud detection (approved fraud vs declined legitimate payments). The dominant design queries a single LLM for a posterior over states, thresholds \"confidence,\" and acts; we prove this is inadequate for sequential decisions with costs. We propose a Bayesian, cost-aware multi-LLM orchestration framework that treats LLMs as approximate likelihood models rather than classifiers. For each candidate state, we elicit likelihoods via contrastive prompting, aggregate across diverse models with robust statistics, and update beliefs with Bayes rule under explicit priors as new evidence arrives. This enables coherent belief updating, expected-cost action selection, principled information gathering via value of information, and fairness gains via ensemble bias mitigation. In resume screening with costs of 40000 USD per missed hire, 2500 USD per interview, and 150 USD per phone screen, experiments on 1000 resumes using five LLMs (GPT-4o, Claude 4.5 Sonnet, Gemini Pro, Grok, DeepSeek) reduce total cost by 294000 USD (34 percent) versus the best single-LLM baseline and improve demographic parity by 45 percent (max group gap 22 to 5 percentage points). Ablations attribute 51 percent of savings to multi-LLM aggregation, 43 percent to sequential updating, and 20 percent to disagreement-triggered information gathering, consistent with the theoretical benefits of correct probabilistic foundations.", "AI": {"tldr": "论文提出了一个贝叶斯、成本感知的多LLM协调框架，通过将LLM视为近似似然模型而非分类器，在非对称错误成本场景中显著降低总成本并提高公平性。", "motivation": "现有方法在非对称错误成本的顺序决策中不足，如招聘、医疗分诊和欺诈检测等领域，单一LLM的置信度阈值方法无法有效处理成本差异。", "method": "使用对比提示获取每个候选状态的似然，通过鲁棒统计聚合多个LLM的结果，在明确先验下使用贝叶斯规则更新信念，实现连贯的信念更新和基于预期成本的动作选择。", "result": "在简历筛选中，使用5个LLM将总成本降低了29.4万美元（34%），人口统计公平性提高了45%（最大组差距从22%降至5%）。", "conclusion": "正确的概率基础理论在多LLM聚合、顺序更新和基于分歧的信息收集中带来了显著效益，证明了该框架在非对称成本决策中的有效性。"}}
{"id": "2601.01350", "pdf": "https://arxiv.org/pdf/2601.01350", "abs": "https://arxiv.org/abs/2601.01350", "authors": ["Juan Junqueras", "Florian Boudin", "May-Myo Zin", "Ha-Thanh Nguyen", "Wachara Fungwacharakorn", "Damián Ariel Furman", "Akiko Aizawa", "Ken Satoh"], "title": "FC-CONAN: An Exhaustively Paired Dataset for Robust Evaluation of Retrieval Systems", "categories": ["cs.CL"], "comment": "Presented at NeLaMKRR@KR, 2025 (arXiv:2511.09575)", "summary": "Hate speech (HS) is a critical issue in online discourse, and one promising strategy to counter it is through the use of counter-narratives (CNs). Datasets linking HS with CNs are essential for advancing counterspeech research. However, even flagship resources like CONAN (Chung et al., 2019) annotate only a sparse subset of all possible HS-CN pairs, limiting evaluation. We introduce FC-CONAN (Fully Connected CONAN), the first dataset created by exhaustively considering all combinations of 45 English HS messages and 129 CNs. A two-stage annotation process involving nine annotators and four validators produces four partitions-Diamond, Gold, Silver, and Bronze-that balance reliability and scale. None of the labeled pairs overlap with CONAN, uncovering hundreds of previously unlabelled positives. FC-CONAN enables more faithful evaluation of counterspeech retrieval systems and facilitates detailed error analysis. The dataset is publicly available.", "AI": {"tldr": "FC-CONAN是一个全新的仇恨言论-反驳言论数据集，通过穷尽组合45条仇恨言论和129条反驳言论，创建了包含四个质量分级的标注分区，填补了现有数据集的空白。", "motivation": "现有仇恨言论-反驳言论数据集（如CONAN）只标注了稀疏的配对样本，限制了反仇恨言论研究的评估效果，需要更全面的数据集来支持检索系统的准确评估。", "method": "采用两阶段标注流程：首先穷尽组合45条仇恨言论和129条反驳言论生成所有可能配对，然后通过9名标注员和4名验证员进行标注，产生钻石、金、银、铜四个质量分级的标注分区。", "result": "创建了FC-CONAN数据集，发现了数百个之前未标注的正样本对，且所有标注对均不与CONAN数据集重叠，实现了更全面的覆盖。", "conclusion": "FC-CONAN数据集能够支持更准确的反仇恨言论检索系统评估和详细的错误分析，该数据集已公开可用。"}}
{"id": "2601.01532", "pdf": "https://arxiv.org/pdf/2601.01532", "abs": "https://arxiv.org/abs/2601.01532", "authors": ["Fanzhe Fu"], "title": "Aletheia: Quantifying Cognitive Conviction in Reasoning Models via Regularized Inverse Confusion Matrix", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "6 pages, 2 figures", "summary": "In the progressive journey toward Artificial General Intelligence (AGI), current evaluation paradigms face an epistemological crisis. Static benchmarks measure knowledge breadth but fail to quantify the depth of belief. While Simhi et al. (2025) defined the CHOKE phenomenon in standard QA, we extend this framework to quantify \"Cognitive Conviction\" in System 2 reasoning models. We propose Project Aletheia, a cognitive physics framework that employs Tikhonov Regularization to invert the judge's confusion matrix. To validate this methodology without relying on opaque private data, we implement a Synthetic Proxy Protocol. Our preliminary pilot study on 2025 baselines (e.g., DeepSeek-R1, OpenAI o1) suggests that while reasoning models act as a \"cognitive buffer,\" they may exhibit \"Defensive OverThinking\" under adversarial pressure. Furthermore, we introduce the Aligned Conviction Score (S_aligned) to verify that conviction does not compromise safety. This work serves as a blueprint for measuring AI scientific integrity.", "AI": {"tldr": "该论文提出了Project Aletheia认知物理框架，通过Tikhonov正则化反演判断混淆矩阵来量化System 2推理模型的\"认知确信度\"，并引入对齐确信分数来确保安全性。", "motivation": "当前AGI评估范式面临认识论危机，静态基准测试只能衡量知识广度而无法量化信念深度，需要新的评估方法来测量AI的科学完整性。", "method": "提出Project Aletheia框架，使用Tikhonov正则化技术反演判断混淆矩阵，并通过合成代理协议验证方法而不依赖不透明的私有数据。", "result": "初步研究表明推理模型虽然起到\"认知缓冲\"作用，但在对抗压力下可能表现出\"防御性过度思考\"现象。", "conclusion": "这项工作为测量AI科学完整性提供了蓝图，通过量化认知确信度并确保其与安全性对齐，推动了AGI评估方法的发展。"}}
{"id": "2601.01362", "pdf": "https://arxiv.org/pdf/2601.01362", "abs": "https://arxiv.org/abs/2601.01362", "authors": ["Jerry Huang", "Peng Lu", "Qiuhao Zeng", "Yusuke Iwasawa", "Yutaka Matsuo", "Sarath Chandar", "Edison Marrese-Taylor", "Irene Li"], "title": "Investigating the Multilingual Calibration Effects of Language Model Instruction-Tuning", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": "Accepted to The 19th Conference of the European Chapter of the Association for Computational Linguistics (EACL)", "summary": "Ensuring that deep learning models are well-calibrated in terms of their predictive uncertainty is essential in maintaining their trustworthiness and reliability, yet despite increasing advances in foundation model research, the relationship between such large language models (LLMs) and their calibration remains an open area of research. In this work, we look at a critical gap in the calibration of LLMs within multilingual settings, in an attempt to better understand how the data scarcity can potentially lead to different calibration effects and how commonly used techniques can apply in these settings. Our analysis on two multilingual benchmarks, over 29 and 42 languages respectively, reveals that even in low-resource languages, model confidence can increase significantly after instruction-tuning on high-resource language SFT datasets. However, improvements in accuracy are marginal or non-existent, resulting in mis-calibration, highlighting a critical shortcoming of standard SFT for multilingual languages. Furthermore, we observe that the use of label smoothing to be a reasonable method alleviate this concern, again without any need for low-resource SFT data, maintaining better calibration across all languages. Overall, this highlights the importance of multilingual considerations for both training and tuning LLMs in order to improve their reliability and fairness in downstream use.", "AI": {"tldr": "该研究发现大型语言模型在多语言环境下存在校准问题，特别是在低资源语言中，指令调优会显著增加模型置信度但准确率提升有限，导致校准偏差。标签平滑技术可以有效缓解这一问题。", "motivation": "研究多语言环境中大型语言模型的校准问题，探索数据稀缺如何导致不同的校准效果，以及常用技术在这些设置下的适用性。", "method": "在两个多语言基准测试上进行分析，涵盖29和42种语言，研究指令调优和标签平滑技术对模型校准的影响。", "result": "在低资源语言中，指令调优会显著增加模型置信度但准确率提升有限，导致校准偏差；标签平滑技术能有效维持更好的校准效果且无需低资源SFT数据。", "conclusion": "多语言考虑对LLMs的训练和调优至关重要，以提高下游应用的可靠性和公平性，标签平滑是一种有效的校准改进方法。"}}
{"id": "2601.01546", "pdf": "https://arxiv.org/pdf/2601.01546", "abs": "https://arxiv.org/abs/2601.01546", "authors": ["Letian Kong", "Qianran", "Jin", "Renyu Zhang"], "title": "Improving Behavioral Alignment in LLM Social Simulations via Context Formation and Navigation", "categories": ["cs.AI"], "comment": "39 pages, 2 figures, 3 tables", "summary": "Large language models (LLMs) are increasingly used to simulate human behavior in experimental settings, but they systematically diverge from human decisions in complex decision-making environments, where participants must anticipate others' actions and form beliefs based on observed behavior. We propose a two-stage framework for improving behavioral alignment. The first stage, context formation, explicitly specifies the experimental design to establish an accurate representation of the decision task and its context. The second stage, context navigation, guides the reasoning process within that representation to make decisions. We validate this framework through a focal replication of a sequential purchasing game with quality signaling (Kremer and Debo, 2016), extending to a crowdfunding game with costly signaling (Cason et al., 2025) and a demand-estimation task (Gui and Toubia, 2025) to test generalizability across decision environments. Across four state-of-the-art (SOTA) models (GPT-4o, GPT-5, Claude-4.0-Sonnet-Thinking, DeepSeek-R1), we find that complex decision-making environments require both stages to achieve behavioral alignment with human benchmarks, whereas the simpler demand-estimation task requires only context formation. Our findings clarify when each stage is necessary and provide a systematic approach for designing and diagnosing LLM social simulations as complements to human subjects in behavioral research.", "AI": {"tldr": "提出两阶段框架改善LLM在复杂决策环境中的行为对齐：情境形成明确实验设计，情境导航指导推理过程。验证显示复杂任务需要两阶段，简单任务只需情境形成。", "motivation": "LLM在模拟人类行为时，在需要预测他人行动和基于观察形成信念的复杂决策环境中，与人类决策存在系统性差异。", "method": "两阶段框架：1)情境形成-明确实验设计和任务背景；2)情境导航-在形成的情境中指导推理决策。通过顺序采购游戏、众筹游戏和需求估计任务验证。", "result": "使用GPT-4o等四个SOTA模型测试发现，复杂决策环境需要两阶段才能实现行为对齐，而简单需求估计任务仅需情境形成阶段。", "conclusion": "阐明了每个阶段的应用条件，为设计和诊断LLM社会模拟提供了系统方法，可作为行为研究中人类受试者的补充。"}}
{"id": "2601.01400", "pdf": "https://arxiv.org/pdf/2601.01400", "abs": "https://arxiv.org/abs/2601.01400", "authors": ["Jicheng Ma", "Guohua Wang", "Xinhua Feng", "Yiming Liu", "Zhichao Hu", "Yuhong Liu"], "title": "EternalMath: A Living Benchmark of Frontier Mathematics that Evolves with Human Discovery", "categories": ["cs.CL"], "comment": null, "summary": "Current evaluations of mathematical reasoning in large language models (LLMs) are dominated by static benchmarks, either derived from competition-style problems or curated through costly expert effort, resulting in limited coverage of research-level mathematics and rapid performance saturation. We propose a fully automated, theorem-grounded pipeline for evaluating frontier mathematical reasoning, which directly transforms recent peer-reviewed mathematical literature into executable and verifiable reasoning tasks. The pipeline identifies constructive or quantitative results, instantiates them into parameterized problem templates, and generates deterministic solutions through execution-based verification, enabling scalable, reproducible, and continuously updatable evaluation without reliance on large-scale expert authoring. By design, this approach supports temporal extensibility, intrinsic correctness checking, and domain-specific customization across mathematical subfields. Applying this pipeline yields \\textbf{EternalMath}, an evolving evaluation suite derived from contemporary research papers. Experiments with state-of-the-art LLMs reveal substantial performance gaps, indicating that mathematical reasoning at the research frontier remains far from saturated and underscoring the need for evaluation methodologies that evolve in step with human mathematical discovery.", "AI": {"tldr": "论文提出了一个完全自动化的定理基础评估管道，将最新的数学研究文献转化为可执行和可验证的推理任务，创建了名为EternalMath的动态评估套件，揭示了当前大语言模型在研究级数学推理方面存在显著性能差距。", "motivation": "当前数学推理评估主要依赖静态基准测试，存在研究级数学覆盖范围有限和性能快速饱和的问题，需要一种能够与人类数学发现同步演进的评估方法。", "method": "开发了一个完全自动化的定理基础管道：识别数学文献中的构造性或定量结果，将其实例化为参数化问题模板，通过基于执行的验证生成确定性解决方案，支持时间可扩展性和领域特定定制。", "result": "创建了EternalMath评估套件，实验显示最先进的大语言模型在研究前沿数学推理方面存在显著性能差距，表明该领域远未饱和。", "conclusion": "研究级数学推理评估需要与人类数学发现同步演进的方法论，自动化定理基础管道为数学推理评估提供了可扩展、可重现且持续更新的解决方案。"}}
{"id": "2601.01562", "pdf": "https://arxiv.org/pdf/2601.01562", "abs": "https://arxiv.org/abs/2601.01562", "authors": ["Mingyu Xu", "Cheng Fang", "Keyue Jiang", "Yuqian Zheng", "Yanghua Xiao", "Baojian Zhou", "Qifang Zhao", "Suhang Zheng", "Xiuwen Zhu", "Jiyang Tang", "Yongchi Zhao", "Yijia Luo", "Zhiqi Bai", "Yuchi Xu", "Wenbo Su", "Wei Wang", "Bing Zhao", "Lin Qu", "Xiaoxiao Xu"], "title": "Logics-STEM: Empowering LLM Reasoning via Failure-Driven Post-Training and Document Knowledge Enhancement", "categories": ["cs.AI"], "comment": null, "summary": "We present Logics-STEM, a state-of-the-art reasoning model fine-tuned on Logics-STEM-SFT-Dataset, a high-quality and diverse dataset at 10M scale that represents one of the largest-scale open-source long chain-of-thought corpora. Logics-STEM targets reasoning tasks in the domains of Science, Technology, Engineering, and Mathematics (STEM), and exhibits exceptional performance on STEM-related benchmarks with an average improvement of 4.68% over the next-best model at 8B scale. We attribute the gains to our data-algorithm co-design engine, where they are jointly optimized to fit a gold-standard distribution behind reasoning. Data-wise, the Logics-STEM-SFT-Dataset is constructed from a meticulously designed data curation engine with 5 stages to ensure the quality, diversity, and scalability, including annotation, deduplication, decontamination, distillation, and stratified sampling. Algorithm-wise, our failure-driven post-training framework leverages targeted knowledge retrieval and data synthesis around model failure regions in the Supervised Fine-tuning (SFT) stage to effectively guide the second-stage SFT or the reinforcement learning (RL) for better fitting the target distribution. The superior empirical performance of Logics-STEM reveals the vast potential of combining large-scale open-source data with carefully designed synthetic data, underscoring the critical role of data-algorithm co-design in enhancing reasoning capabilities through post-training. We make both the Logics-STEM models (8B and 32B) and the Logics-STEM-SFT-Dataset (10M and downsampled 2.2M versions) publicly available to support future research in the open-source community.", "AI": {"tldr": "Logics-STEM是一个在10M规模高质量数据集上微调的最先进推理模型，专注于STEM领域，在STEM基准测试中平均比同类8B模型提升4.68%性能。", "motivation": "针对STEM领域推理任务的需求，通过数据算法协同设计来解决大规模高质量推理数据稀缺的问题，提升模型在科学、技术、工程和数学领域的推理能力。", "method": "采用数据算法协同设计引擎：数据方面通过5阶段数据管理流程（标注、去重、净化、蒸馏、分层采样）构建高质量数据集；算法方面使用故障驱动的后训练框架，通过针对性知识检索和数据合成来优化监督微调和强化学习。", "result": "在STEM相关基准测试中表现卓越，平均比次优8B规模模型提升4.68%，验证了大规模开源数据与精心设计合成数据结合的巨大潜力。", "conclusion": "数据算法协同设计对于通过后训练增强推理能力具有关键作用，研究团队公开了Logics-STEM模型（8B和32B）和数据集（10M和2.2M版本）以支持开源社区的未来研究。"}}
{"id": "2601.01401", "pdf": "https://arxiv.org/pdf/2601.01401", "abs": "https://arxiv.org/abs/2601.01401", "authors": ["Chenxu Wang", "Chaozhuo Li", "Pengbo Wang", "Litian Zhang", "Songyang Liu", "Ji Qi", "Jiahui Hu", "Yushan Cai", "Hao Zhao", "Rui Pu"], "title": "LANCET: Neural Intervention via Structural Entropy for Mitigating Faithfulness Hallucinations in LLMs", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models have revolutionized information processing, yet their reliability is severely compromised by faithfulness hallucinations. While current approaches attempt to mitigate this issue through node-level adjustments or coarse suppression, they often overlook the distributed nature of neural information, leading to imprecise interventions. Recognizing that hallucinations propagate through specific forward transmission pathways like an infection, we aim to surgically block this flow using precise structural analysis. To leverage this, we propose Lancet, a novel framework that achieves precise neural intervention by leveraging structural entropy and hallucination difference ratios. Lancet first locates hallucination-prone neurons via gradient-driven contrastive analysis, then maps their propagation pathways by minimizing structural entropy, and finally implements a hierarchical intervention strategy that preserves general model capabilities. Comprehensive evaluations across hallucination benchmark datasets demonstrate that Lancet significantly outperforms state-of-the-art methods, validating the effectiveness of our surgical approach to neural intervention.", "AI": {"tldr": "Lancet框架通过结构熵和幻觉差异比实现精确的神经干预，通过梯度驱动的对比分析定位幻觉神经元，最小化结构熵映射传播路径，采用分层干预策略保持模型能力，在幻觉基准数据集上显著优于现有方法。", "motivation": "大语言模型存在忠实性幻觉问题，现有方法通过节点级调整或粗粒度抑制往往忽略了神经信息的分布式特性，导致干预不精确。认识到幻觉像感染一样通过特定前向传播路径传播，需要手术式阻断这种流动。", "method": "提出Lancet框架：1) 通过梯度驱动的对比分析定位易产生幻觉的神经元；2) 通过最小化结构熵映射传播路径；3) 实施分层干预策略以保持模型通用能力。", "result": "在幻觉基准数据集上的综合评估表明，Lancet显著优于最先进的方法", "conclusion": "通过精确的结构分析和手术式神经干预方法，Lancet框架有效解决了大语言模型的幻觉问题，验证了该方法在神经干预中的有效性。"}}
{"id": "2601.01569", "pdf": "https://arxiv.org/pdf/2601.01569", "abs": "https://arxiv.org/abs/2601.01569", "authors": ["Maohao Ran", "Zhenglin Wan", "Cooper Lin", "Yanting Zhang", "Hongyu Xin", "Hongwei Fan", "Yibo Xu", "Beier Luo", "Yaxin Zhou", "Wangbo Zhao", "Lijie Yang", "Lang Feng", "Fuchao Yang", "Jingxuan Wu", "Yiqiao Huang", "Chendong Ma", "Dailing Jiang", "Jianbo Deng", "Sihui Han", "Bo An", "Yike Guo", "Jun Song"], "title": "CaveAgent: Transforming LLMs into Stateful Runtime Operators", "categories": ["cs.AI", "cs.SE"], "comment": "32 pages, 14 Figures", "summary": "LLM-based agents are increasingly capable of complex task execution, yet current agentic systems remain constrained by text-centric paradigms. Traditional approaches rely on procedural JSON-based function calling, which often struggles with long-horizon tasks due to fragile multi-turn dependencies and context drift. In this paper, we present CaveAgent, a framework that transforms the paradigm from \"LLM-as-Text-Generator\" to \"LLM-as-Runtime-Operator.\" We introduce a Dual-stream Context Architecture that decouples state management into a lightweight semantic stream for reasoning and a persistent, deterministic Python Runtime stream for execution. In addition to leveraging code generation to efficiently resolve interdependent sub-tasks (e.g., loops, conditionals) in a single step, we introduce \\textit{Stateful Runtime Management} in CaveAgent. Distinct from existing code-based approaches that remain text-bound and lack the support for external object injection and retrieval, CaveAgent injects, manipulates, and retrieves complex Python objects (e.g., DataFrames, database connections) that persist across turns. This persistence mechanism acts as a high-fidelity external memory to eliminate context drift, avoid catastrophic forgetting, while ensuring that processed data flows losslessly to downstream applications. Comprehensive evaluations on Tau$^2$-bench, BFCL and various case studies across representative SOTA LLMs demonstrate CaveAgent's superiority. Specifically, our framework achieves a 10.5\\% success rate improvement on retail tasks and reduces total token consumption by 28.4\\% in multi-turn scenarios. On data-intensive tasks, direct variable storage and retrieval reduces token consumption by 59\\%, allowing CaveAgent to handle large-scale data that causes context overflow failures in both JSON-based and Code-based agents.", "AI": {"tldr": "CaveAgent是一个将LLM从文本生成器转变为运行时操作器的框架，通过双流上下文架构和状态化运行时管理，解决了传统JSON函数调用在长程任务中的多轮依赖和上下文漂移问题，显著提升了任务成功率和降低了token消耗。", "motivation": "当前基于LLM的智能体系统受限于以文本为中心的范式，传统基于JSON的过程式函数调用在处理长程任务时存在脆弱的多轮依赖和上下文漂移问题，需要新的框架来提升复杂任务的执行能力。", "method": "提出CaveAgent框架，采用双流上下文架构：轻量级语义流用于推理，持久化确定性Python运行时流用于执行。引入状态化运行时管理，支持复杂Python对象（如DataFrame、数据库连接）的注入、操作和跨轮次持久化。", "result": "在Tau²-bench、BFCL基准测试和多个案例研究中表现出色：零售任务成功率提升10.5%，多轮场景下总token消耗减少28.4%，数据密集型任务token消耗减少59%，能够处理导致其他代理上下文溢出的大规模数据。", "conclusion": "CaveAgent通过将LLM转变为运行时操作器，有效解决了传统代理系统的局限性，提供了高保真的外部记忆机制，消除了上下文漂移，为复杂长程任务的执行提供了更高效的解决方案。"}}
{"id": "2601.01407", "pdf": "https://arxiv.org/pdf/2601.01407", "abs": "https://arxiv.org/abs/2601.01407", "authors": ["Arjhun Sreedar", "Rohan Pillay", "Laukik Patade"], "title": "From Emotion Classification to Emotional Reasoning: Enhancing Emotional Intelligence in Large Language Models", "categories": ["cs.CL"], "comment": "10 pages, 1 figure", "summary": "This work investigates whether synthetic emotional chain-of-thought data can improve the emotional reasoning abilities of smaller open large language models (LLMs). We design a multi-agent generation pipeline that produces therapy-style conversations and converts them into structured emotion multiple-choice questions (MCQs) with explanations. We propose that fine-tuning a variety of 7B models on this dataset should yield substantial gains in emotional understanding and emotional awareness on EmoBench-style evaluations, suggesting that emotional reasoning can be induced without architectural changes. Our results demonstrate that fine-tuned Mistral 7B achieves EU improvements from 10.5 to 20.5 and EA improvements from 40.5 to 60.0, validating the effectiveness of synthetic emotional reasoning data for enhancing model capabilities in nuanced emotional tasks.", "AI": {"tldr": "使用合成情感思维链数据微调7B模型可显著提升情感推理能力，无需架构改动即可在情感理解(EU)和情感意识(EA)评估上获得大幅提升", "motivation": "研究是否可以通过合成情感思维链数据来提升较小开源大语言模型的情感推理能力", "method": "设计多智能体生成管道创建治疗风格对话，并将其转换为结构化情感多选题(MCQs)及解释，然后在多种7B模型上进行微调", "result": "微调后的Mistral 7B模型在情感理解(EU)从10.5提升至20.5，情感意识(EA)从40.5提升至60.0", "conclusion": "合成情感推理数据能有效增强模型在细微情感任务上的能力，情感推理可以通过数据而非架构改变来实现"}}
{"id": "2601.01609", "pdf": "https://arxiv.org/pdf/2601.01609", "abs": "https://arxiv.org/abs/2601.01609", "authors": ["Albert Sadowski", "Jarosław A. Chudziak"], "title": "Structured Decomposition for LLM Reasoning: Cross-Domain Validation and Semantic Web Integration", "categories": ["cs.AI"], "comment": null, "summary": "Rule-based reasoning over natural language input arises in domains where decisions must be auditable and justifiable: clinical protocols specify eligibility criteria in prose, evidence rules define admissibility through textual conditions, and scientific standards dictate methodological requirements. Applying rules to such inputs demands both interpretive flexibility and formal guarantees. Large language models (LLMs) provide flexibility but cannot ensure consistent rule application; symbolic systems provide guarantees but require structured input. This paper presents an integration pattern that combines these strengths: LLMs serve as ontology population engines, translating unstructured text into ABox assertions according to expert-authored TBox specifications, while SWRL-based reasoners apply rules with deterministic guarantees. The framework decomposes reasoning into entity identification, assertion extraction, and symbolic verification, with task definitions grounded in OWL 2 ontologies. Experiments across three domains (legal hearsay determination, scientific method-task application, clinical trial eligibility) and eleven language models validate the approach. Structured decomposition achieves statistically significant improvements over few-shot prompting in aggregate, with gains observed across all three domains. An ablation study confirms that symbolic verification provides substantial benefit beyond structured prompting alone. The populated ABox integrates with standard semantic web tooling for inspection and querying, positioning the framework for richer inference patterns that simpler formalisms cannot express.", "AI": {"tldr": "本文提出了一种结合大语言模型和符号推理系统的混合框架，利用LLMs将非结构化文本转换为ABox断言，SWRL推理器提供确定性规则应用保证，在三个领域验证了优于few-shot prompting的效果。", "motivation": "在需要可审计和可解释决策的领域（如临床协议、法律规则、科学标准），现有方法存在局限：LLMs提供灵活性但缺乏一致性保证，符号系统有确定性但需要结构化输入。", "method": "提出集成模式：LLMs作为本体填充引擎将非结构化文本转换为ABox断言，SWRL推理器应用规则。框架将推理分解为实体识别、断言提取和符号验证三个步骤。", "result": "在三个领域（法律传闻确定、科学方法任务应用、临床试验资格）和11个语言模型上的实验显示，该方法在总体上比few-shot prompting有统计显著改进，所有领域都有提升。消融研究证实符号验证提供了超越结构化提示的实质性好处。", "conclusion": "该框架结合了LLMs的灵活性和符号系统的确定性优势，填充的ABox可与标准语义Web工具集成，支持更丰富的推理模式，为需要可审计推理的应用提供了有效解决方案。"}}
{"id": "2601.01446", "pdf": "https://arxiv.org/pdf/2601.01446", "abs": "https://arxiv.org/abs/2601.01446", "authors": ["Yilong Wang", "Qianli Wang", "Nils Feldhus"], "title": "iFlip: Iterative Feedback-driven Counterfactual Example Refinement", "categories": ["cs.CL", "cs.LG"], "comment": "In submission", "summary": "Counterfactual examples are minimal edits to an input that alter a model's prediction. They are widely employed in explainable AI to probe model behavior and in natural language processing (NLP) to augment training data. However, generating valid counterfactuals with large language models (LLMs) remains challenging, as existing single-pass methods often fail to induce reliable label changes, neglecting LLMs' self-correction capabilities. To explore this untapped potential, we propose iFlip, an iterative refinement approach that leverages three types of feedback, including model confidence, feature attribution, and natural language. Our results show that iFlip achieves an average 57.8% higher validity than the five state-of-the-art baselines, as measured by the label flipping rate. The user study further corroborates that iFlip outperforms baselines in completeness, overall satisfaction, and feasibility. In addition, ablation studies demonstrate that three components are paramount for iFlip to generate valid counterfactuals: leveraging an appropriate number of iterations, pointing to highly attributed words, and early stopping. Finally, counterfactuals generated by iFlip enable effective counterfactual data augmentation, substantially improving model performance and robustness.", "AI": {"tldr": "iFlip是一种迭代优化方法，利用模型置信度、特征归因和自然语言反馈生成有效的反事实示例，在标签翻转率上比现有方法平均提高57.8%的有效性", "motivation": "现有单次生成方法在利用大语言模型生成有效反事实示例时经常失败，未能充分利用LLMs的自我修正能力", "method": "提出iFlip迭代优化方法，利用三种反馈类型：模型置信度、特征归因和自然语言反馈，通过多轮迭代生成反事实示例", "result": "iFlip在标签翻转率上平均比五种最先进基线方法高57.8%，用户研究表明在完整性、总体满意度和可行性方面均优于基线", "conclusion": "iFlip通过迭代优化和多种反馈机制有效生成反事实示例，可用于反事实数据增强，显著提升模型性能和鲁棒性"}}
{"id": "2601.01718", "pdf": "https://arxiv.org/pdf/2601.01718", "abs": "https://arxiv.org/abs/2601.01718", "authors": ["YuanLab. ai", ":", "Shawn Wu", "Sean Wang", "Louie Li", "Darcy Chen", "Allen Wang", "Jiangang Luo", "Xudong Zhao", "Joseph Shen", "Gawain Ma", "Jasper Jia", "Marcus Mao", "Claire Wang", "Hunter He", "Carol Wang", "Zera Zhang", "Jason Wang", "Chonly Shen", "Leo Zhang", "Logan Chen", "Qasim Meng", "James Gong", "Danied Zhao", "Penn Zheng", "Owen Zhu", "Tong Yu"], "title": "Yuan3.0 Flash: An Open Multimodal Large Language Model for Enterprise Applications", "categories": ["cs.AI"], "comment": null, "summary": "We introduce Yuan3.0 Flash, an open-source Mixture-of-Experts (MoE) MultiModal Large Language Model featuring 3.7B activated parameters and 40B total parameters, specifically designed to enhance performance on enterprise-oriented tasks while maintaining competitive capabilities on general-purpose tasks. To address the overthinking phenomenon commonly observed in Large Reasoning Models (LRMs), we propose Reflection-aware Adaptive Policy Optimization (RAPO), a novel RL training algorithm that effectively regulates overthinking behaviors. In enterprise-oriented tasks such as retrieval-augmented generation (RAG), complex table understanding, and summarization, Yuan3.0 Flash consistently achieves superior performance. Moreover, it also demonstrates strong reasoning capabilities in domains such as mathematics, science, etc., attaining accuracy comparable to frontier model while requiring only approximately 1/4 to 1/2 of the average tokens. Yuan3.0 Flash has been fully open-sourced to facilitate further research and real-world deployment: https://github.com/Yuan-lab-LLM/Yuan3.0.", "AI": {"tldr": "Yuan3.0 Flash是一个开源的MoE多模态大语言模型，具有37亿激活参数和400亿总参数，专为企业任务优化，同时保持通用任务竞争力。采用RAPO算法解决过度思考问题，在企业任务和推理任务中表现优异。", "motivation": "解决大型推理模型中常见的过度思考现象，同时为企业导向任务提供高性能的多模态语言模型，在保持竞争力的同时降低计算成本。", "method": "提出Reflection-aware Adaptive Policy Optimization (RAPO)训练算法，这是一种新颖的强化学习算法，有效调节过度思考行为。采用混合专家(MoE)架构，激活参数37亿，总参数400亿。", "result": "在企业导向任务（如RAG、复杂表格理解和摘要）中表现优异，在数学、科学等推理领域展现强大能力，达到前沿模型相当的准确性，同时仅需约1/4到1/2的平均token数量。", "conclusion": "Yuan3.0 Flash成功解决了过度思考问题，在企业任务和通用任务上都表现出色，计算效率高，已完全开源供研究和实际部署使用。"}}
{"id": "2601.01449", "pdf": "https://arxiv.org/pdf/2601.01449", "abs": "https://arxiv.org/abs/2601.01449", "authors": ["Harshil Darji", "Martin Heckelmann", "Christina Kratsch", "Gerard de Melo"], "title": "Segmentation and Processing of German Court Decisions from Open Legal Data", "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": "Accepted and published as a research article in Legal Knowledge and Information Systems (JURIX 2025 proceedings, IOS Press). Pages 276--281", "summary": "The availability of structured legal data is important for advancing Natural Language Processing (NLP) techniques for the German legal system. One of the most widely used datasets, Open Legal Data, provides a large-scale collection of German court decisions. While the metadata in this raw dataset is consistently structured, the decision texts themselves are inconsistently formatted and often lack clearly marked sections. Reliable separation of these sections is important not only for rhetorical role classification but also for downstream tasks such as retrieval and citation analysis. In this work, we introduce a cleaned and sectioned dataset of 251,038 German court decisions derived from the official Open Legal Data dataset. We systematically separated three important sections in German court decisions, namely Tenor (operative part of the decision), Tatbestand (facts of the case), and Entscheidungsgründe (judicial reasoning), which are often inconsistently represented in the original dataset. To ensure the reliability of our extraction process, we used Cochran's formula with a 95% confidence level and a 5% margin of error to draw a statistically representative random sample of 384 cases, and manually verified that all three sections were correctly identified. We also extracted the Rechtsmittelbelehrung (appeal notice) as a separate field, since it is a procedural instruction and not part of the decision itself. The resulting corpus is publicly available in the JSONL format, making it an accessible resource for further research on the German legal system.", "AI": {"tldr": "本研究从Open Legal Data数据集中清理和划分了251,038份德国法院判决，系统分离了判决的三大核心部分（Tenor、Tatbestand、Entscheidungsgründe），并通过统计抽样验证了提取准确性，创建了一个结构化的法律数据集。", "motivation": "德国法律系统中的结构化数据对于NLP技术发展至关重要。虽然Open Legal Data提供了大规模德国法院判决数据，但其原始数据格式不一致，缺乏清晰标记的章节，这影响了修辞角色分类、检索和引证分析等下游任务。", "method": "从Open Legal Data官方数据集提取251,038份德国法院判决；系统分离三个重要章节：Tenor（判决主文）、Tatbestand（案件事实）、Entscheidungsgründe（判决理由）；使用Cochran公式以95%置信水平和5%误差幅度抽取384个案例的统计代表性样本进行人工验证；单独提取Rechtsmittelbelehrung（上诉通知）。", "result": "创建了一个包含251,038份德国法院判决的清理和章节化数据集，所有三个核心章节都经过统计验证确保正确识别；数据集以JSONL格式公开提供。", "conclusion": "该研究提供了一个可靠的结构化德国法律数据集，为德国法律系统的进一步研究提供了可访问的资源，解决了原始数据格式不一致的问题，支持NLP技术在法律领域的应用发展。"}}
{"id": "2601.01743", "pdf": "https://arxiv.org/pdf/2601.01743", "abs": "https://arxiv.org/abs/2601.01743", "authors": ["Bin Xu"], "title": "AI Agent Systems: Architectures, Applications, and Evaluation", "categories": ["cs.AI"], "comment": null, "summary": "AI agents -- systems that combine foundation models with reasoning, planning, memory, and tool use -- are rapidly becoming a practical interface between natural-language intent and real-world computation. This survey synthesizes the emerging landscape of AI agent architectures across: (i) deliberation and reasoning (e.g., chain-of-thought-style decomposition, self-reflection and verification, and constraint-aware decision making), (ii) planning and control (from reactive policies to hierarchical and multi-step planners), and (iii) tool calling and environment interaction (retrieval, code execution, APIs, and multimodal perception). We organize prior work into a unified taxonomy spanning agent components (policy/LLM core, memory, world models, planners, tool routers, and critics), orchestration patterns (single-agent vs.\\ multi-agent; centralized vs.\\ decentralized coordination), and deployment settings (offline analysis vs.\\ online interactive assistance; safety-critical vs.\\ open-ended tasks). We discuss key design trade-offs -- latency vs.\\ accuracy, autonomy vs.\\ controllability, and capability vs.\\ reliability -- and highlight how evaluation is complicated by non-determinism, long-horizon credit assignment, tool and environment variability, and hidden costs such as retries and context growth. Finally, we summarize measurement and benchmarking practices (task suites, human preference and utility metrics, success under constraints, robustness and security) and identify open challenges including verification and guardrails for tool actions, scalable memory and context management, interpretability of agent decisions, and reproducible evaluation under realistic workloads.", "AI": {"tldr": "本文对AI智能体架构进行了系统性综述，涵盖了推理规划、工具调用、部署模式等关键方面，并分析了设计权衡、评估挑战和未来研究方向。", "motivation": "AI智能体结合基础模型与推理、规划、记忆和工具使用能力，正成为自然语言意图与真实世界计算之间的实用接口，需要系统性的架构梳理和分析。", "method": "通过综合现有研究，构建统一的分类体系，包括智能体组件（策略/LLM核心、记忆、世界模型等）、编排模式（单/多智能体、集中/分布式）和部署设置，并分析关键设计权衡。", "result": "建立了全面的AI智能体架构分类框架，识别了延迟vs准确性、自主性vs可控性等核心设计权衡，总结了评估实践的复杂性因素。", "conclusion": "AI智能体领域面临验证与防护、可扩展内存管理、决策可解释性等开放挑战，需要在真实工作负载下进行可重复评估以推动发展。"}}
{"id": "2601.01461", "pdf": "https://arxiv.org/pdf/2601.01461", "abs": "https://arxiv.org/abs/2601.01461", "authors": ["Yuxiang Mei", "Dongxing Xu", "Jiaen Liang", "Yanhua Long"], "title": "Bridging the gap: A comparative exploration of Speech-LLM and end-to-end architecture for multilingual conversational ASR", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "5 pages, 1 figure", "summary": "The INTERSPEECH 2025 Challenge on Multilingual Conversational Speech Language Models (MLC-SLM) promotes multilingual conversational ASR with large language models (LLMs). Our previous SHNU-mASR system adopted a competitive parallel-speech-encoder architecture that integrated Whisper and mHuBERT with an LLM. However, it faced two challenges: simple feature concatenation may not fully exploit complementary information, and the performance gap between LLM-based ASR and end-to-end(E2E) encoder-decoder ASR remained unexplored. In this work, we present an enhanced LLM-based ASR framework that combines fine-tuned Whisper and mHuBERT encoders with an LLM to enrich speech representations. We first evaluate E2E Whisper models with LoRA and full fine-tuning on the MLC-SLM ASR task, and then propose cross-attention-based fusion mechanisms for the parallel-speech-encoder. On the official evaluation set of the MLC-SLM Challenge, our system achieves a CER/WER of 10.69%, ranking on par with the top-ranked Track 1 systems, even though it uses only 1,500 hours of baseline training data compared with their large-scale training sets. Nonetheless, we find that our final LLM-based ASR still does not match the performance of a fine-tuned E2E Whisper model, providing valuable empirical guidance for future Speech-LLM design. Our code is publicly available at https://github.com/1535176727/MLC-SLM.", "AI": {"tldr": "本文提出了一个改进的基于LLM的多语言对话语音识别框架，通过交叉注意力机制融合Whisper和mHuBERT编码器，在MLC-SLM挑战赛中获得10.69%的CER/WER，与使用大规模训练数据的顶级系统性能相当，但仍未超越微调的端到端Whisper模型。", "motivation": "解决之前SHNU-mASR系统的两个问题：简单特征拼接未能充分利用互补信息，以及LLM-based ASR与端到端编码器-解码器ASR之间的性能差距尚未探索。", "method": "提出增强的LLM-based ASR框架，结合微调的Whisper和mHuBERT编码器与LLM来丰富语音表示；评估LoRA和全微调的Whisper模型；提出基于交叉注意力的并行语音编码器融合机制。", "result": "在MLC-SLM挑战赛官方评估集上达到10.69%的CER/WER，仅使用1500小时基线训练数据即与使用大规模训练集的顶级Track 1系统性能相当。", "conclusion": "虽然LLM-based ASR系统取得了竞争性结果，但仍未匹配微调E2E Whisper模型的性能，为未来Speech-LLM设计提供了有价值的实证指导。"}}
{"id": "2601.01765", "pdf": "https://arxiv.org/pdf/2601.01765", "abs": "https://arxiv.org/abs/2601.01765", "authors": ["Yao Lu", "Shang Liu", "Hangan Zhou", "Wenji Fang", "Qijun Zhang", "Zhiyao Xie"], "title": "A New Benchmark for the Appropriate Evaluation of RTL Code Optimization", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "The rapid progress of artificial intelligence increasingly relies on efficient integrated circuit (IC) design. Recent studies have explored the use of large language models (LLMs) for generating Register Transfer Level (RTL) code, but existing benchmarks mainly evaluate syntactic correctness rather than optimization quality in terms of power, performance, and area (PPA). This work introduces RTL-OPT, a benchmark for assessing the capability of LLMs in RTL optimization. RTL-OPT contains 36 handcrafted digital designs that cover diverse implementation categories including combinational logic, pipelined datapaths, finite state machines, and memory interfaces. Each task provides a pair of RTL codes, a suboptimal version and a human-optimized reference that reflects industry-proven optimization patterns not captured by conventional synthesis tools. Furthermore, RTL-OPT integrates an automated evaluation framework to verify functional correctness and quantify PPA improvements, enabling standardized and meaningful assessment of generative models for hardware design optimization.", "AI": {"tldr": "RTL-OPT：一个评估大语言模型在RTL代码优化能力的新基准，包含36个手工设计的数字电路，提供自动化评估框架来验证功能正确性和量化PPA改进。", "motivation": "现有基准主要评估RTL代码的语法正确性，而非功耗、性能和面积(PPA)的优化质量，需要专门评估LLM在硬件设计优化方面的能力。", "method": "创建包含36个手工数字设计的基准，覆盖组合逻辑、流水线数据路径、有限状态机和存储器接口等类别，每个任务提供次优版本和人工优化的参考版本。", "result": "开发了RTL-OPT基准和自动化评估框架，能够验证功能正确性并量化PPA改进，为标准化的生成模型评估提供支持。", "conclusion": "RTL-OPT填补了现有基准的空白，为评估LLM在硬件设计优化方面的能力提供了标准化且有意义的评估工具。"}}
{"id": "2601.01477", "pdf": "https://arxiv.org/pdf/2601.01477", "abs": "https://arxiv.org/abs/2601.01477", "authors": ["May-Myo Zin", "Sabine Wehnert", "Yuntao Kong", "Ha-Thanh Nguyen", "Wachara Fungwacharakorn", "Jieying Xue", "Michał Araszkiewicz", "Randy Goebel", "Ken Satoh", "Le-Minh Nguyen"], "title": "Can Legislation Be Made Machine-Readable in PROLEG?", "categories": ["cs.CL"], "comment": null, "summary": "The anticipated positive social impact of regulatory processes requires both the accuracy and efficiency of their application. Modern artificial intelligence technologies, including natural language processing and machine-assisted reasoning, hold great promise for addressing this challenge. We present a framework to address the challenge of tools for regulatory application, based on current state-of-the-art (SOTA) methods for natural language processing (large language models or LLMs) and formalization of legal reasoning (the legal representation system PROLEG). As an example, we focus on Article 6 of the European General Data Protection Regulation (GDPR). In our framework, a single LLM prompt simultaneously transforms legal text into if-then rules and a corresponding PROLEG encoding, which are then validated and refined by legal domain experts. The final output is an executable PROLEG program that can produce human-readable explanations for instances of GDPR decisions. We describe processes to support the end-to-end transformation of a segment of a regulatory document (Article 6 from GDPR), including the prompting frame to guide an LLM to \"compile\" natural language text to if-then rules, then to further \"compile\" the vetted if-then rules to PROLEG. Finally, we produce an instance that shows the PROLEG execution. We conclude by summarizing the value of this approach and note observed limitations with suggestions to further develop such technologies for capturing and deploying regulatory frameworks.", "AI": {"tldr": "本文提出了一个结合大语言模型(LLM)和PROLEG法律表示系统的框架，用于将GDPR第6条等法规文本自动转换为可执行的if-then规则和PROLEG程序，以提高法规应用的准确性和效率。", "motivation": "监管流程需要准确高效的应用，现代AI技术如自然语言处理和机器辅助推理有望解决这一挑战。", "method": "使用单个LLM提示同时将法律文本转换为if-then规则和PROLEG编码，然后由法律领域专家验证和精炼，最终生成可执行的PROLEG程序。", "result": "开发了端到端的转换流程，能够将GDPR第6条等法规文本编译为可执行程序，并生成人类可读的解释。", "conclusion": "该方法具有重要价值，但也存在局限性，需要进一步发展此类技术来捕获和部署监管框架。"}}
{"id": "2601.01774", "pdf": "https://arxiv.org/pdf/2601.01774", "abs": "https://arxiv.org/abs/2601.01774", "authors": ["Sai Varun Kodathala", "Rakesh Vunnam"], "title": "Can Large Language Models Solve Engineering Equations? A Systematic Comparison of Direct Prediction and Solver-Assisted Approaches", "categories": ["cs.AI", "cs.CE", "math.NA"], "comment": "14 pages", "summary": "Transcendental equations requiring iterative numerical solution pervade engineering practice, from fluid mechanics friction factor calculations to orbital position determination. We systematically evaluate whether Large Language Models can solve these equations through direct numerical prediction or whether a hybrid architecture combining LLM symbolic manipulation with classical iterative solvers proves more effective. Testing six state-of-the-art models (GPT-5.1, GPT-5.2, Gemini-3-Flash, Gemini-2.5-Lite, Claude-Sonnet-4.5, Claude-Opus-4.5) on 100 problems spanning seven engineering domains, we compare direct prediction against solver-assisted computation where LLMs formulate governing equations and provide initial conditions while Newton-Raphson iteration performs numerical solution. Direct prediction yields mean relative errors of 0.765 to 1.262 across models, while solver-assisted computation achieves 0.225 to 0.301, representing error reductions of 67.9% to 81.8%. Domain-specific analysis reveals dramatic improvements in Electronics (93.1%) due to exponential equation sensitivity, contrasted with modest gains in Fluid Mechanics (7.2%) where LLMs exhibit effective pattern recognition. These findings establish that contemporary LLMs excel at symbolic manipulation and domain knowledge retrieval but struggle with precision-critical iterative arithmetic, suggesting their optimal deployment as intelligent interfaces to classical numerical solvers rather than standalone computational engines.", "AI": {"tldr": "LLM在求解超越方程时，直接数值预测效果较差，而结合符号操作与经典迭代求解器的混合架构显著更有效，误差降低67.9%-81.8%", "motivation": "评估大语言模型是否能直接求解工程中普遍存在的超越方程，或需要结合传统数值求解器的混合方法", "method": "测试6个最先进LLM模型在100个工程问题上的表现，比较直接预测与求解器辅助计算（LLM提供方程和初始条件，牛顿-拉弗森法进行数值求解）", "result": "直接预测平均相对误差0.765-1.262，求解器辅助计算误差0.225-0.301，误差降低67.9%-81.8%。电子学领域改进最大(93.1%)，流体力学改进最小(7.2%)", "conclusion": "当代LLM擅长符号操作和领域知识检索，但在精度关键的迭代计算上表现不佳，最适合作为经典数值求解器的智能接口而非独立计算引擎"}}
{"id": "2601.01488", "pdf": "https://arxiv.org/pdf/2601.01488", "abs": "https://arxiv.org/abs/2601.01488", "authors": ["Vanessa Toborek", "Sebastian Müller", "Christian Bauckhage"], "title": "Four Quadrants of Difficulty: A Simple Categorisation and its Limits", "categories": ["cs.CL", "cs.LG"], "comment": "prepared for ESANN 2026 submission", "summary": "Curriculum Learning (CL) aims to improve the outcome of model training by estimating the difficulty of samples and scheduling them accordingly. In NLP, difficulty is commonly approximated using task-agnostic linguistic heuristics or human intuition, implicitly assuming that these signals correlate with what neural models find difficult to learn. We propose a four-quadrant categorisation of difficulty signals -- human vs. model and task-agnostic vs. task-dependent -- and systematically analyse their interactions on a natural language understanding dataset. We find that task-agnostic features behave largely independently and that only task-dependent features align. These findings challenge common CL intuitions and highlight the need for lightweight, task-dependent difficulty estimators that better reflect model learning behaviour.", "AI": {"tldr": "论文挑战了课程学习中使用任务无关特征估计样本难度的传统做法，发现只有任务相关特征与模型学习行为一致，提出了需要轻量级任务相关难度估计器的观点", "motivation": "当前NLP领域课程学习主要使用任务无关的语言学启发式方法或人类直觉来估计样本难度，但这种方法是否真正反映神经模型的学习困难存在疑问", "method": "提出了四象限分类法（人类vs模型、任务无关vs任务相关），在自然语言理解数据集上系统分析不同难度信号的交互作用", "result": "发现任务无关特征基本独立于模型学习行为，只有任务相关特征与模型学习表现一致", "conclusion": "研究结果挑战了课程学习的常见直觉，强调需要开发能够更好反映模型学习行为的轻量级任务相关难度估计器"}}
{"id": "2601.01802", "pdf": "https://arxiv.org/pdf/2601.01802", "abs": "https://arxiv.org/abs/2601.01802", "authors": ["Qianjun Pan", "Junyi Wang", "Jie Zhou", "Yutao Yang", "Junsong Li", "Kaiyin Xu", "Yougen Zhou", "Yihan Li", "Jingyuan Zhao", "Qin Chen", "Ningning Zhou", "Kai Chen", "Liang He"], "title": "PsychEval: A Multi-Session and Multi-Therapy Benchmark for High-Realism and Comprehensive AI Psychological Counselor", "categories": ["cs.AI"], "comment": null, "summary": "To develop a reliable AI for psychological assessment, we introduce \\texttt{PsychEval}, a multi-session, multi-therapy, and highly realistic benchmark designed to address three key challenges: \\textbf{1) Can we train a highly realistic AI counselor?} Realistic counseling is a longitudinal task requiring sustained memory and dynamic goal tracking. We propose a multi-session benchmark (spanning 6-10 sessions across three distinct stages) that demands critical capabilities such as memory continuity, adaptive reasoning, and longitudinal planning. The dataset is annotated with extensive professional skills, comprising over 677 meta-skills and 4577 atomic skills. \\textbf{2) How to train a multi-therapy AI counselor?} While existing models often focus on a single therapy, complex cases frequently require flexible strategies among various therapies. We construct a diverse dataset covering five therapeutic modalities (Psychodynamic, Behaviorism, CBT, Humanistic Existentialist, and Postmodernist) alongside an integrative therapy with a unified three-stage clinical framework across six core psychological topics. \\textbf{3) How to systematically evaluate an AI counselor?} We establish a holistic evaluation framework with 18 therapy-specific and therapy-shared metrics across Client-Level and Counselor-Level dimensions. To support this, we also construct over 2,000 diverse client profiles. Extensive experimental analysis fully validates the superior quality and clinical fidelity of our dataset. Crucially, \\texttt{PsychEval} transcends static benchmarking to serve as a high-fidelity reinforcement learning environment that enables the self-evolutionary training of clinically responsible and adaptive AI counselors.", "AI": {"tldr": "PsychEval是一个用于心理评估AI的多会话、多疗法、高真实度基准测试，旨在解决训练真实AI咨询师、多疗法适应性和系统性评估三大挑战。", "motivation": "开发可靠的心理评估AI需要解决三个关键问题：1)训练高真实度的AI咨询师需要长期记忆和动态目标跟踪能力；2)多疗法咨询需要灵活的跨疗法策略；3)需要系统性的评估框架来全面衡量AI咨询师表现。", "method": "构建多会话基准测试（6-10个会话，三个阶段），包含677个元技能和4577个原子技能标注；覆盖五种治疗模式（心理动力学、行为主义、CBT、人本存在主义和后现代主义）和整合疗法；建立包含18个治疗特定和共享指标的评估框架，并构建2000多个多样化客户档案。", "result": "实验分析充分验证了数据集的高质量和临床保真度。PsychEval超越了静态基准测试，可作为高保真强化学习环境，支持临床负责任和适应性AI咨询师的自我进化训练。", "conclusion": "PsychEval为解决AI心理评估的关键挑战提供了全面的解决方案，通过多会话多疗法的高真实度基准和系统性评估框架，为开发临床有效的AI咨询师奠定了重要基础。"}}
{"id": "2601.01490", "pdf": "https://arxiv.org/pdf/2601.01490", "abs": "https://arxiv.org/abs/2601.01490", "authors": ["Junichiro Niimi"], "title": "Distortion Instead of Hallucination: The Effect of Reasoning Under Strict Constraints", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the widespread adoption of large language models (LLMs), hallucinations, which are non-factual fabrications in model outputs, have become serious concerns. Reasoning capabilities have received attention as a self-verification process to improve output reliability. However, the effect of reasoning within a closed system where LLMs cannot rely on external tools or knowledge has yet to be clarified. We therefore conduct experiments under strict constraints (recommending peer-reviewed journal articles in computer science) to examine the effect of reasoning across multiple models (GPT-5.2 and Gemini 3 Flash). Our results reveal a problematic trade-off between constraint compliance and factual accuracy. Non-reasoning models exhibit high constraint violation rates (66-75%) but maintain factual accuracy, while reasoning models reduce violations (13-26%) but systematically distort known facts to satisfy constraints and increase complete fabrication. This trade-off pattern is consistent across both models despite different architectures, indicating a fundamental limitation of reasoning. Furthermore, reasoning does not uniformly improve output authenticity: effects diverge by model, reflecting different allocations of the compliance-truthfulness trade-off. These findings challenge the assumption that reasoning universally improves reliability: reasoning models trade honest constraint violations for detection-resistant distortions.", "AI": {"tldr": "研究发现LLM推理能力在封闭系统中存在约束合规性与事实准确性之间的权衡：非推理模型违反约束但保持事实准确，推理模型减少违规但会系统性扭曲事实甚至完全捏造", "motivation": "随着大语言模型广泛应用，幻觉问题严重，推理能力作为自我验证过程受到关注，但在无法依赖外部工具的封闭系统中推理效果尚未明确", "method": "在严格约束条件下（推荐计算机科学同行评审期刊文章）进行实验，测试GPT-5.2和Gemini 3 Flash多个模型的推理效果", "result": "发现约束合规性与事实准确性之间存在问题性权衡：非推理模型约束违反率高（66-75%）但事实准确，推理模型减少违反（13-26%）但系统性扭曲已知事实并增加完全捏造", "conclusion": "推理并不普遍提高输出可靠性，不同模型在合规性-真实性权衡上有不同分配，推理模型用诚实的约束违反换取难以检测的扭曲"}}
{"id": "2601.01816", "pdf": "https://arxiv.org/pdf/2601.01816", "abs": "https://arxiv.org/abs/2601.01816", "authors": ["Chris Duffey"], "title": "Admissibility Alignment", "categories": ["cs.AI"], "comment": "24 pages, 2 figures, 2 tables.. Decision-theoretic alignment under uncertainty", "summary": "This paper introduces Admissibility Alignment: a reframing of AI alignment as a property of admissible action and decision selection over distributions of outcomes under uncertainty, evaluated through the behavior of candidate policies. We present MAP-AI (Monte Carlo Alignment for Policy) as a canonical system architecture for operationalizing admissibility alignment, formalizing alignment as a probabilistic, decision-theoretic property rather than a static or binary condition.\n  MAP-AI, a new control-plane system architecture for aligned decision-making under uncertainty, enforces alignment through Monte Carlo estimation of outcome distributions and admissibility-controlled policy selection rather than static model-level constraints. The framework evaluates decision policies across ensembles of plausible futures, explicitly modeling uncertainty, intervention effects, value ambiguity, and governance constraints. Alignment is assessed through distributional properties including expected utility, variance, tail risk, and probability of misalignment rather than accuracy or ranking performance. This approach distinguishes probabilistic prediction from decision reasoning under uncertainty and provides an executable methodology for evaluating trust and alignment in enterprise and institutional AI systems. The result is a practical foundation for governing AI systems whose impact is determined not by individual forecasts, but by policy behavior across distributions and tail events. Finally, we show how distributional alignment evaluation can be integrated into decision-making itself, yielding an admissibility-controlled action selection mechanism that alters policy behavior under uncertainty without retraining or modifying underlying models.", "AI": {"tldr": "本文提出可容许对齐框架，将AI对齐重新定义为在不确定性下基于结果分布的可容许行动和决策选择属性，并通过MAP-AI系统架构实现概率化、决策理论的对齐评估。", "motivation": "传统AI对齐方法通常基于静态或二元条件，缺乏在不确定性环境下对决策政策行为分布特性的评估能力，需要一种新的框架来评估AI系统在不确定性和极端事件中的可信度和对齐性。", "method": "提出MAP-AI（蒙特卡洛对齐策略）系统架构，通过蒙特卡洛估计结果分布和可容许控制策略选择来强制执行对齐，评估决策政策在多个可能未来场景中的表现，显式建模不确定性、干预效应、价值模糊性和治理约束。", "result": "建立了一个实用的基础框架，用于治理AI系统，其影响不是由单个预测决定，而是由政策在分布和尾部事件中的行为决定。提供了将分布对齐评估集成到决策过程中的方法，实现无需重新训练或修改底层模型的可容许控制行动选择机制。", "conclusion": "可容许对齐提供了一个概率化的决策理论框架，将AI对齐从静态条件转变为基于分布特性的动态评估，为企业和机构AI系统的信任和对齐评估提供了可执行的方法论。"}}
{"id": "2601.01498", "pdf": "https://arxiv.org/pdf/2601.01498", "abs": "https://arxiv.org/abs/2601.01498", "authors": ["Bingguang Hao", "Zengzhuang Xu", "Yuntao Wen", "Xinyi Xu", "Yang Liu", "Tong Zhao", "Maolin Wang", "Long Chen", "Dong Wang", "Yicheng Chen", "Cunyin Peng", "Xiangyu Zhao", "Chenyi Zhuang", "Ji Zhang"], "title": "From Failure to Mastery: Generating Hard Samples for Tool-use Agents", "categories": ["cs.CL"], "comment": null, "summary": "The advancement of LLM agents with tool-use capabilities requires diverse and complex training corpora. Existing data generation methods, which predominantly follow a paradigm of random sampling and shallow generation, often yield simple and homogeneous trajectories that fail to capture complex, implicit logical dependencies. To bridge this gap, we introduce HardGen, an automatic agentic pipeline designed to generate hard tool-use training samples with verifiable reasoning. Firstly, HardGen establishes a dynamic API Graph built upon agent failure cases, from which it samples to synthesize hard traces. Secondly, these traces serve as conditional priors to guide the instantiation of modular, abstract advanced tools, which are subsequently leveraged to formulate hard queries. Finally, the advanced tools and hard queries enable the generation of verifiable complex Chain-of-Thought (CoT), with a closed-loop evaluation feedback steering the continuous refinement of the process. Extensive evaluations demonstrate that a 4B parameter model trained with our curated dataset achieves superior performance compared to several leading open-source and closed-source competitors (e.g., GPT-5.2, Gemini-3-Pro and Claude-Opus-4.5). Our code, models, and dataset will be open-sourced to facilitate future research.", "AI": {"tldr": "HardGen是一个自动化的智能体管道，通过分析失败案例构建API图，生成具有可验证推理的困难工具使用训练样本，显著提升LLM智能体的性能。", "motivation": "现有数据生成方法主要基于随机采样和浅层生成，产生的轨迹简单同质，无法捕捉复杂的隐式逻辑依赖关系，限制了LLM智能体工具使用能力的发展。", "method": "1. 基于智能体失败案例构建动态API图来合成困难轨迹\n2. 使用轨迹作为条件先验来实例化模块化抽象高级工具\n3. 利用高级工具制定困难查询并生成可验证的复杂思维链\n4. 通过闭环评估反馈持续优化整个过程", "result": "使用HardGen生成的数据集训练的4B参数模型在性能上超越了多个领先的开源和闭源竞争对手，包括GPT-5.2、Gemini-3-Pro和Claude-Opus-4.5。", "conclusion": "HardGen能够有效生成高质量的困难训练样本，显著提升LLM智能体的工具使用能力，相关代码、模型和数据集将开源以促进未来研究。"}}
{"id": "2601.01836", "pdf": "https://arxiv.org/pdf/2601.01836", "abs": "https://arxiv.org/abs/2601.01836", "authors": ["Dasol Choi", "DongGeon Lee", "Brigitta Jesica Kartono", "Helena Berndt", "Taeyoun Kwon", "Joonwon Jang", "Haon Park", "Hwanjo Yu", "Minsuk Kahng"], "title": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.", "AI": {"tldr": "COMPASS是首个系统评估LLM是否符合组织白名单和黑名单政策的框架，测试发现现有模型在合法请求上表现良好(>95%准确率)，但在阻止违规请求方面表现极差(仅13-40%拒绝率)，显示LLM缺乏政策关键部署所需的鲁棒性。", "motivation": "随着大语言模型在企业高风险应用中的部署，确保其遵守组织特定政策变得至关重要，而现有安全评估仅关注通用危害，缺乏针对组织政策的系统评估方法。", "method": "开发COMPASS框架，在八个不同行业场景中生成和验证5,920个查询，测试常规合规性和通过战略设计的边缘案例测试对抗鲁棒性，评估七个最先进模型。", "result": "发现基本不对称性：模型可靠处理合法请求(>95%准确率)，但在执行禁令方面灾难性失败，仅拒绝13-40%的对抗性黑名单违规请求。", "conclusion": "当前LLM缺乏政策关键部署所需的鲁棒性，COMPASS成为组织AI安全的重要评估框架。"}}
{"id": "2601.01530", "pdf": "https://arxiv.org/pdf/2601.01530", "abs": "https://arxiv.org/abs/2601.01530", "authors": ["Jing Ye", "Lu Xiang", "Yaping Zhang", "Chengqing Zong"], "title": "EmoHarbor: Evaluating Personalized Emotional Support by Simulating the User's Internal World", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Current evaluation paradigms for emotional support conversations tend to reward generic empathetic responses, yet they fail to assess whether the support is genuinely personalized to users' unique psychological profiles and contextual needs. We introduce EmoHarbor, an automated evaluation framework that adopts a User-as-a-Judge paradigm by simulating the user's inner world. EmoHarbor employs a Chain-of-Agent architecture that decomposes users' internal processes into three specialized roles, enabling agents to interact with supporters and complete assessments in a manner similar to human users. We instantiate this benchmark using 100 real-world user profiles that cover a diverse range of personality traits and situations, and define 10 evaluation dimensions of personalized support quality. Comprehensive evaluation of 20 advanced LLMs on EmoHarbor reveals a critical insight: while these models excel at generating empathetic responses, they consistently fail to tailor support to individual user contexts. This finding reframes the central challenge, shifting research focus from merely enhancing generic empathy to developing truly user-aware emotional support. EmoHarbor provides a reproducible and scalable framework to guide the development and evaluation of more nuanced and user-aware emotional support systems.", "AI": {"tldr": "EmoHarbor是一个自动化评估框架，通过模拟用户内心世界来评估情感支持对话的个性化程度，发现当前LLM虽然能产生共情回应，但无法针对用户个性化需求提供真正的情感支持。", "motivation": "当前情感支持对话评估范式倾向于奖励通用的共情回应，但无法评估支持是否真正针对用户的独特心理特征和情境需求进行个性化。", "method": "采用用户即评判者范式，通过Chain-of-Agent架构将用户内部过程分解为三个专门角色，让代理与支持者互动并完成评估，使用100个真实用户配置文件覆盖不同人格特质和情境。", "result": "对20个先进LLM的全面评估显示：虽然这些模型擅长生成共情回应，但始终无法根据用户个体情境定制支持。", "conclusion": "该研究重新定义了核心挑战，将研究重点从仅仅增强通用共情转向开发真正用户感知的情感支持系统，EmoHarbor为开发和评估更细致、用户感知的情感支持系统提供了可复制和可扩展的框架。"}}
{"id": "2601.01844", "pdf": "https://arxiv.org/pdf/2601.01844", "abs": "https://arxiv.org/abs/2601.01844", "authors": ["Udiptaman Das", "Krishnasai B. Atmakuri", "Duy Ho", "Chi Lee", "Yugyung Lee"], "title": "Clinical Knowledge Graph Construction and Evaluation with Multi-LLMs via Retrieval-Augmented Generation", "categories": ["cs.AI"], "comment": "13 pages, 5 tables, 4 figures", "summary": "Large language models (LLMs) offer new opportunities for constructing knowledge graphs (KGs) from unstructured clinical narratives. However, existing approaches often rely on structured inputs and lack robust validation of factual accuracy and semantic consistency, limitations that are especially problematic in oncology. We introduce an end-to-end framework for clinical KG construction and evaluation directly from free text using multi-agent prompting and a schema-constrained Retrieval-Augmented Generation (KG-RAG) strategy. Our pipeline integrates (1) prompt-driven entity, attribute, and relation extraction; (2) entropy-based uncertainty scoring; (3) ontology-aligned RDF/OWL schema generation; and (4) multi-LLM consensus validation for hallucination detection and semantic refinement. Beyond static graph construction, the framework supports continuous refinement and self-supervised evaluation, enabling iterative improvement of graph quality. Applied to two oncology cohorts (PDAC and BRCA), our method produces interpretable, SPARQL-compatible, and clinically grounded knowledge graphs without relying on gold-standard annotations. Experimental results demonstrate consistent gains in precision, relevance, and ontology compliance over baseline methods.", "AI": {"tldr": "提出了一个端到端的临床知识图谱构建框架，使用多智能体提示和模式约束的检索增强生成技术，直接从自由文本构建高质量的知识图谱，特别针对肿瘤学领域。", "motivation": "现有方法依赖结构化输入且缺乏对事实准确性和语义一致性的鲁棒验证，这在肿瘤学领域尤其成问题。", "method": "集成四个核心组件：(1)提示驱动的实体、属性和关系抽取；(2)基于熵的不确定性评分；(3)本体对齐的RDF/OWL模式生成；(4)多LLM共识验证用于幻觉检测和语义精炼", "result": "在两个肿瘤学队列(PDAC和BRCA)上应用，相比基线方法在精确度、相关性和本体合规性方面取得了一致的提升", "conclusion": "该框架能够产生可解释、SPARQL兼容且临床基础的知识图谱，无需依赖黄金标准标注，并支持持续精炼和自监督评估"}}
{"id": "2601.01543", "pdf": "https://arxiv.org/pdf/2601.01543", "abs": "https://arxiv.org/abs/2601.01543", "authors": ["Praveenkumar Katwe", "RakeshChandra Balabantaray", "Kaliprasad Vittala"], "title": "Bridging the Data Gap: Creating a Hindi Text Summarization Dataset from the English XSUM", "categories": ["cs.CL", "cs.AI"], "comment": "Book chapter for River publications", "summary": "Current advancements in Natural Language Processing (NLP) have largely favored resource-rich languages, leaving a significant gap in high-quality datasets for low-resource languages like Hindi. This scarcity is particularly evident in text summarization, where the development of robust models is hindered by a lack of diverse, specialized corpora.\n  To address this disparity, this study introduces a cost-effective, automated framework for creating a comprehensive Hindi text summarization dataset. By leveraging the English Extreme Summarization (XSUM) dataset as a source, we employ advanced translation and linguistic adaptation techniques. To ensure high fidelity and contextual relevance, we utilize the Crosslingual Optimized Metric for Evaluation of Translation (COMET) for validation, supplemented by the selective use of Large Language Models (LLMs) for curation.\n  The resulting dataset provides a diverse, multi-thematic resource that mirrors the complexity of the original XSUM corpus. This initiative not only provides a direct tool for Hindi NLP research but also offers a scalable methodology for democratizing NLP in other underserved languages. By reducing the costs associated with dataset creation, this work fosters the development of more nuanced, culturally relevant models in computational linguistics.", "AI": {"tldr": "本研究开发了一个自动化框架，利用英文XSUM数据集通过翻译和语言适应技术创建印地语文本摘要数据集，使用COMET和LLM进行验证，为低资源语言提供高质量NLP资源。", "motivation": "当前NLP进展主要偏向资源丰富的语言，印地语等低资源语言缺乏高质量的文本摘要数据集，阻碍了稳健模型的发展。", "method": "使用英文XSUM数据集作为源，采用先进的翻译和语言适应技术，通过COMET指标验证翻译质量，并选择性使用大语言模型进行数据筛选。", "result": "创建了一个多样化、多主题的印地语文本摘要数据集，复现了原始XSUM语料库的复杂性。", "conclusion": "该研究不仅为印地语NLP研究提供了直接工具，还为其他资源匮乏语言提供了可扩展的方法论，降低了数据集创建成本，促进了计算语言学中更细致、文化相关模型的发展。"}}
{"id": "2601.01857", "pdf": "https://arxiv.org/pdf/2601.01857", "abs": "https://arxiv.org/abs/2601.01857", "authors": ["Defei Xia", "Bingfeng Pi", "Shenbin Zhang", "Song Hua", "Yunfei Wei", "Lei Zuo"], "title": "Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios", "categories": ["cs.AI"], "comment": null, "summary": "As agent systems powered by large language models (LLMs) advance, improving the task performance of an autonomous agent, especially in context understanding, tool usage, and response generation, has become increasingly critical. Although prior studies have advanced the overall design of LLM-based agents, systematic optimization of their internal reasoning and tool-use pipelines remains underexplored. This paper introduces an agent framework grounded in real-world practical experience, with three key innovations: (1) an adaptive prompt generation strategy that aligns with the agent's state and task goals to improve reliability and robustness; (2) a context-aware tool orchestration module that performs tool categorization, semantic retrieval, and adaptive invocation based on user intent and context; and (3) a layered memory mechanism that integrates session memory, task history, and external summaries to improve relevance and efficiency through dynamic summarization and compression. An end-to-end framework named Jenius-Agent has been integrated with three key optimizations, including tools based on the Model Context Protocol (MCP), file input/output (I/O), and execution feedback. The experiments show a 20 percent improvement in task accuracy, along with a reduced token cost, response latency, and invocation failures. The framework is already deployed in Jenius (https://www.jenius.cn), providing a lightweight and scalable solution for robust, protocol-compatible autonomous agents.", "AI": {"tldr": "该论文提出了Jenius-Agent框架，通过自适应提示生成、上下文感知工具编排和分层内存机制三大创新，显著提升了LLM智能体的任务准确率并降低了成本和延迟。", "motivation": "随着基于大语言模型的智能体系统发展，提升自主智能体的任务性能（特别是在上下文理解、工具使用和响应生成方面）变得至关重要，但系统性的内部推理和工具使用管道优化研究仍不足。", "method": "提出了基于真实世界实践经验的智能体框架，包含：(1)自适应提示生成策略；(2)上下文感知工具编排模块；(3)分层内存机制。集成了基于MCP的工具、文件I/O和执行反馈等关键优化。", "result": "实验显示任务准确率提高了20%，同时降低了token成本、响应延迟和调用失败率。框架已在Jenius平台部署。", "conclusion": "Jenius-Agent框架为健壮、协议兼容的自主智能体提供了轻量级和可扩展的解决方案，有效提升了智能体的性能和效率。"}}
{"id": "2601.01552", "pdf": "https://arxiv.org/pdf/2601.01552", "abs": "https://arxiv.org/abs/2601.01552", "authors": ["Shreyas N. Samaga", "Gilberto Gonzalez Arroyo", "Tamal K. Dey"], "title": "HalluZig: Hallucination Detection using Zigzag Persistence", "categories": ["cs.CL"], "comment": null, "summary": "The factual reliability of Large Language Models (LLMs) remains a critical barrier to their adoption in high-stakes domains due to their propensity to hallucinate. Current detection methods often rely on surface-level signals from the model's output, overlooking the failures that occur within the model's internal reasoning process. In this paper, we introduce a new paradigm for hallucination detection by analyzing the dynamic topology of the evolution of model's layer-wise attention. We model the sequence of attention matrices as a zigzag graph filtration and use zigzag persistence, a tool from Topological Data Analysis, to extract a topological signature. Our core hypothesis is that factual and hallucinated generations exhibit distinct topological signatures. We validate our framework, HalluZig, on multiple benchmarks, demonstrating that it outperforms strong baselines. Furthermore, our analysis reveals that these topological signatures are generalizable across different models and hallucination detection is possible only using structural signatures from partial network depth.", "AI": {"tldr": "提出HalluZig方法，通过分析LLM层间注意力动态拓扑结构来检测幻觉，使用zigzag持久性提取拓扑特征，在多个基准测试中优于现有方法。", "motivation": "LLM在关键领域应用受限，因其容易产生幻觉。现有检测方法仅关注模型输出的表面信号，忽视了内部推理过程中的故障。", "method": "将注意力矩阵序列建模为zigzag图过滤，使用拓扑数据分析中的zigzag持久性工具提取拓扑特征，区分事实性和幻觉生成。", "result": "在多个基准测试中表现优于强基线方法，拓扑特征在不同模型间具有可泛化性，仅使用部分网络深度的结构特征即可实现幻觉检测。", "conclusion": "基于注意力拓扑分析的HalluZig框架为幻觉检测提供了新范式，揭示了内部推理过程的拓扑特征可以有效区分事实和幻觉内容。"}}
{"id": "2601.01875", "pdf": "https://arxiv.org/pdf/2601.01875", "abs": "https://arxiv.org/abs/2601.01875", "authors": ["Kewen Cao", "Jianxu Chen", "Yongbing Zhang", "Ye Zhang", "Hongxiao Wang"], "title": "Toward Auditable Neuro-Symbolic Reasoning in Pathology: SQL as an Explicit Trace of Evidence", "categories": ["cs.AI", "q-bio.QM"], "comment": null, "summary": "Automated pathology image analysis is central to clinical diagnosis, but clinicians still ask which slide features drive a model's decision and why. Vision-language models can produce natural language explanations, but these are often correlational and lack verifiable evidence. In this paper, we introduce an SQL-centered agentic framework that enables both feature measurement and reasoning to be auditable. Specifically, after extracting human-interpretable cellular features, Feature Reasoning Agents compose and execute SQL queries over feature tables to aggregate visual evidence into quantitative findings. A Knowledge Comparison Agent then evaluates these findings against established pathological knowledge, mirroring how pathologists justify diagnoses from measurable observations. Extensive experiments evaluated on two pathology visual question answering datasets demonstrate our method improves interpretability and decision traceability while producing executable SQL traces that link cellular measurements to diagnostic conclusions.", "AI": {"tldr": "提出基于SQL的智能代理框架，通过可执行的SQL查询实现病理图像分析的透明化特征测量与推理，将细胞测量与诊断结论关联起来", "motivation": "现有视觉语言模型生成的解释缺乏可验证证据，临床医生需要了解模型决策的驱动特征及其原因", "method": "提取可解释细胞特征后，特征推理代理通过SQL查询对特征表进行聚合分析，知识比较代理将分析结果与病理知识对比验证", "result": "在两个病理视觉问答数据集上的实验表明，该方法提高了可解释性和决策可追溯性", "conclusion": "该框架通过可执行的SQL追踪实现了从细胞测量到诊断结论的透明化推理过程，模拟了病理医生的诊断验证方式"}}
{"id": "2601.01584", "pdf": "https://arxiv.org/pdf/2601.01584", "abs": "https://arxiv.org/abs/2601.01584", "authors": ["Jakub Hoscilowicz"], "title": "Steerability of Instrumental-Convergence Tendencies in LLMs", "categories": ["cs.CL"], "comment": "Code is available at https://github.com/j-hoscilowicz/instrumental_steering", "summary": "We examine two properties of AI systems: capability (what a system can do) and steerability (how reliably one can shift behavior toward intended outcomes). In our experiments, higher capability does not imply lower steerability. We distinguish between authorized steerability (builders reliably reaching intended behaviors) and unauthorized steerability (attackers eliciting disallowed behaviors). This distinction highlights a fundamental safety--security dilemma for open-weight AI models: safety requires high steerability to enforce control (e.g., stop/refuse), while security requires low steerability to prevent malicious actors from eliciting harmful behaviors. This tension is acute for open-weight models, which are currently highly steerable via common techniques such as fine-tuning and adversarial prompting. Using Qwen3 models (4B/30B; Base/Instruct/Thinking) and InstrumentalEval, we find that a short anti-instrumental prompt suffix sharply reduces outputs labeled as instrumental convergence (e.g., shutdown avoidance, deception, self-replication). For Qwen3-30B Instruct, convergence drops from 81.69% under a pro-instrumental suffix to 2.82% under an anti-instrumental suffix. Under anti-instrumental prompting, larger aligned models produce fewer convergence-labeled outputs than smaller ones (Instruct: 2.82% vs. 4.23%; Thinking: 4.23% vs. 9.86%). Code is available at github.com/j-hoscilowicz/instrumental_steering.", "AI": {"tldr": "研究发现AI系统的能力与可控性不互斥，但存在安全-安全困境：开放权重模型需要高可控性确保安全控制，却容易被恶意利用。实验显示抗工具性提示能显著减少有害行为输出。", "motivation": "探讨AI系统的能力与可控性关系，特别关注开放权重模型面临的安全-安全困境：既要保证开发者能有效控制模型行为，又要防止恶意行为被诱导。", "method": "使用Qwen3模型（4B/30B；Base/Instruct/Thinking）和InstrumentalEval评估工具，通过正向和反向工具性提示后缀测试模型行为，比较不同规模模型在抗工具性提示下的表现。", "result": "抗工具性提示后缀能大幅减少工具性收敛行为输出（从81.69%降至2.82%），且更大规模的已对齐模型在抗工具性提示下产生更少的有害输出。", "conclusion": "AI系统的能力与可控性并非此消彼长关系，但开放权重模型面临安全控制与恶意利用的根本性矛盾，需要开发更精细的控制机制来解决这一困境。"}}
{"id": "2601.01878", "pdf": "https://arxiv.org/pdf/2601.01878", "abs": "https://arxiv.org/abs/2601.01878", "authors": ["Farzan Karimi-Malekabadi", "Suhaib Abdurahman", "Zhivar Sourati", "Jackson Trager", "Morteza Dehghani"], "title": "Theory Trace Card: Theory-Driven Socio-Cognitive Evaluation of LLMs", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "Socio-cognitive benchmarks for large language models (LLMs) often fail to predict real-world behavior, even when models achieve high benchmark scores. Prior work has attributed this evaluation-deployment gap to problems of measurement and validity. While these critiques are insightful, we argue that they overlook a more fundamental issue: many socio-cognitive evaluations proceed without an explicit theoretical specification of the target capability, leaving the assumptions linking task performance to competence implicit. Without this theoretical grounding, benchmarks that exercise only narrow subsets of a capability are routinely misinterpreted as evidence of broad competence: a gap that creates a systemic validity illusion by masking the failure to evaluate the capability's other essential dimensions. To address this gap, we make two contributions. First, we diagnose and formalize this theory gap as a foundational failure that undermines measurement and enables systematic overgeneralization of benchmark results. Second, we introduce the Theory Trace Card (TTC), a lightweight documentation artifact designed to accompany socio-cognitive evaluations, which explicitly outlines the theoretical basis of an evaluation, the components of the target capability it exercises, its operationalization, and its limitations. We argue that TTCs enhance the interpretability and reuse of socio-cognitive evaluations by making explicit the full validity chain, which links theory, task operationalization, scoring, and limitations, without modifying benchmarks or requiring agreement on a single theory.", "AI": {"tldr": "论文指出大语言模型的社会认知基准测试存在评估-部署差距，提出了理论缺口问题并设计了理论追踪卡（TTC）来明确评估的理论基础和能力维度。", "motivation": "现有社会认知基准测试虽然得分高但无法预测真实世界行为，作者认为这是由于缺乏明确的理论规范，导致窄化测试被误解为广泛能力。", "method": "提出理论追踪卡（TTC）作为轻量级文档工具，明确记录评估的理论基础、目标能力组件、操作化方法和局限性。", "result": "TTC通过使有效性链（理论-任务操作化-评分-局限性）显式化，提高了社会认知评估的可解释性和可重用性。", "conclusion": "理论追踪卡是解决评估-部署差距的有效方法，无需修改基准测试或统一理论，就能提升评估的透明度和有效性。"}}
{"id": "2601.01624", "pdf": "https://arxiv.org/pdf/2601.01624", "abs": "https://arxiv.org/abs/2601.01624", "authors": ["Raj Vardhan Tomar", "Preslav Nakov", "Yuxia Wang"], "title": "How Does Prefix Matter in Reasoning Model Tuning?", "categories": ["cs.CL"], "comment": null, "summary": "Recent alignment studies commonly remove introductory boilerplate phrases from supervised fine-tuning (SFT) datasets. This work challenges that assumption. We hypothesize that safety- and reasoning-oriented prefix sentences serve as lightweight alignment signals that can guide model decoding toward safer and more coherent responses. To examine this, we fine-tune three R1 series models across three core model capabilities: reasoning (mathematics, coding), safety, and factuality, systematically varying prefix inclusion from 0% to 100%.\n  Results show that prefix-conditioned SFT improves both safety and reasoning performance, yielding up to +6% higher Safe@1 accuracy on adversarial benchmarks (WildJailbreak, StrongReject) and +7% improvement on GSM8K reasoning. However, factuality and coding tasks show marginal or negative effects, indicating that prefix-induced narrowing of the search space benefits structured reasoning. Token-level loss analysis further reveals that prefix tokens such as \"revised\" and \"logically\" incur higher gradient magnitudes, acting as alignment anchors that stabilize reasoning trajectories. Our findings suggest that prefix conditioning offers a scalable and interpretable mechanism for improving reasoning safety, serving as an implicit form of alignment that complements traditional reward-based methods.", "AI": {"tldr": "研究发现，在监督微调中保留安全性和推理导向的前缀句子可以显著提升模型的安全性和推理能力，但对事实性和编程任务效果有限。前缀作为轻量级对齐信号，通过稳定推理轨迹来改善性能。", "motivation": "挑战当前研究中移除监督微调数据集前缀短语的常见做法，假设安全性和推理导向的前缀句子可以作为轻量级对齐信号，引导模型生成更安全和连贯的响应。", "method": "对三个R1系列模型在推理（数学、编程）、安全性和事实性三个核心能力上进行微调，系统性地改变前缀包含比例（0%到100%）。", "result": "前缀条件化SFT显著提升安全性和推理性能：在对抗性基准测试中安全准确率提升高达+6%，GSM8K推理提升+7%。但事实性和编程任务显示边际或负面效果。词元级损失分析显示特定前缀词元（如'revised'、'logically'）具有更高梯度幅度，作为对齐锚点稳定推理轨迹。", "conclusion": "前缀条件化提供了一种可扩展且可解释的机制来改善推理安全性，作为传统基于奖励方法的补充，是一种隐式的对齐形式。"}}
{"id": "2601.01910", "pdf": "https://arxiv.org/pdf/2601.01910", "abs": "https://arxiv.org/abs/2601.01910", "authors": ["Minh Hieu Ha", "Khanh Ly Ta", "Hung Phan", "Tung Doan", "Tung Dao", "Dao Tran", "Huynh Thi Thanh Binh"], "title": "MMP-A*: Multimodal Perception Enhanced Incremental Heuristic Search on Path Planning", "categories": ["cs.AI"], "comment": null, "summary": "Autonomous path planning requires a synergy between global reasoning and geometric precision, especially in complex or cluttered environments. While classical A* is valued for its optimality, it incurs prohibitive computational and memory costs in large-scale scenarios. Recent attempts to mitigate these limitations by using Large Language Models for waypoint guidance remain insufficient, as they rely only on text-based reasoning without spatial grounding. As a result, such models often produce incorrect waypoints in topologically complex environments with dead ends, and lack the perceptual capacity to interpret ambiguous physical boundaries. These inconsistencies lead to costly corrective expansions and undermine the intended computational efficiency.\n  We introduce MMP-A*, a multimodal framework that integrates the spatial grounding capabilities of vision-language models with a novel adaptive decay mechanism. By anchoring high-level reasoning in physical geometry, the framework produces coherent waypoint guidance that addresses the limitations of text-only planners. The adaptive decay mechanism dynamically regulates the influence of uncertain waypoints within the heuristic, ensuring geometric validity while substantially reducing memory overhead. To evaluate robustness, we test the framework in challenging environments characterized by severe clutter and topological complexity. Experimental results show that MMP-A* achieves near-optimal trajectories with significantly reduced operational costs, demonstrating its potential as a perception-grounded and computationally efficient paradigm for autonomous navigation.", "AI": {"tldr": "MMP-A*是一个多模态路径规划框架，通过整合视觉语言模型的空间基础能力和自适应衰减机制，解决了纯文本规划器在复杂环境中的局限性，实现了接近最优的轨迹规划并显著降低计算成本。", "motivation": "传统A*算法在大规模场景中计算和内存成本过高，而基于大语言模型的路径规划方法仅依赖文本推理缺乏空间基础，在拓扑复杂环境中容易产生错误路径点且无法解释模糊物理边界。", "method": "提出MMP-A*多模态框架，整合视觉语言模型的空间基础能力，并引入新颖的自适应衰减机制，将高层推理锚定在物理几何中，动态调节不确定路径点在启发式函数中的影响。", "result": "在具有严重杂乱和拓扑复杂性的挑战性环境中测试，MMP-A*实现了接近最优的轨迹规划，同时显著降低了操作成本。", "conclusion": "MMP-A*展示了一种基于感知基础和计算效率的自主导航范式潜力，能够产生几何有效的路径点指导，同时大幅减少内存开销。"}}
{"id": "2601.01627", "pdf": "https://arxiv.org/pdf/2601.01627", "abs": "https://arxiv.org/abs/2601.01627", "authors": ["Junyu Liu", "Zirui Li", "Qian Niu", "Zequn Zhang", "Yue Xun", "Wenlong Hou", "Shujun Wang", "Yusuke Iwasawa", "Yutaka Matsuo", "Kan Hatakeyama-Sato"], "title": "JMedEthicBench: A Multi-Turn Conversational Benchmark for Evaluating Medical Safety in Japanese Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 6 figures", "summary": "As Large Language Models (LLMs) are increasingly deployed in healthcare field, it becomes essential to carefully evaluate their medical safety before clinical use. However, existing safety benchmarks remain predominantly English-centric, and test with only single-turn prompts despite multi-turn clinical consultations. To address these gaps, we introduce JMedEthicBench, the first multi-turn conversational benchmark for evaluating medical safety of LLMs for Japanese healthcare. Our benchmark is based on 67 guidelines from the Japan Medical Association and contains over 50,000 adversarial conversations generated using seven automatically discovered jailbreak strategies. Using a dual-LLM scoring protocol, we evaluate 27 models and find that commercial models maintain robust safety while medical-specialized models exhibit increased vulnerability. Furthermore, safety scores decline significantly across conversation turns (median: 9.5 to 5.0, $p < 0.001$). Cross-lingual evaluation on both Japanese and English versions of our benchmark reveals that medical model vulnerabilities persist across languages, indicating inherent alignment limitations rather than language-specific factors. These findings suggest that domain-specific fine-tuning may accidentally weaken safety mechanisms and that multi-turn interactions represent a distinct threat surface requiring dedicated alignment strategies.", "AI": {"tldr": "JMedEthicBench是首个针对日本医疗领域LLM医学安全性的多轮对话基准测试，包含基于67项医学伦理指南生成的5万多条对抗性对话，发现医疗专用模型安全性更脆弱且多轮对话中安全性显著下降", "motivation": "现有安全基准主要为英语单轮测试，而临床咨询是多轮对话，且缺乏针对日语医疗场景的评估", "method": "基于日本医学会67项指南，使用7种自动发现的越狱策略生成50,000+对抗对话，采用双LLM评分协议评估27个模型", "result": "商业模型安全性更强，医疗专用模型更脆弱；多轮对话中安全性显著下降(中位数9.5→5.0)；跨语言评估显示医疗模型漏洞具有跨语言一致性", "conclusion": "领域特定微调可能意外削弱安全机制，多轮交互是需要专门对齐策略的新威胁面"}}
{"id": "2601.01939", "pdf": "https://arxiv.org/pdf/2601.01939", "abs": "https://arxiv.org/abs/2601.01939", "authors": ["Victor Sanchez", "Chris Reinke", "Ahamed Mohamed", "Xavier Alameda-Pineda"], "title": "OpenSocInt: A Multi-modal Training Environment for Human-Aware Social Navigation", "categories": ["cs.AI"], "comment": null, "summary": "In this paper, we introduce OpenSocInt, an open-source software package providing a simulator for multi-modal social interactions and a modular architecture to train social agents. We described the software package and showcased its interest via an experimental protocol based on the task of social navigation. Our framework allows for exploring the use of different perceptual features, their encoding and fusion, as well as the use of different agents. The software is already publicly available under GPL at https://gitlab.inria.fr/robotlearn/OpenSocInt/.", "AI": {"tldr": "OpenSocInt是一个开源的多模态社交交互模拟器软件包，提供模块化架构来训练社交智能体，专注于社交导航任务的研究。", "motivation": "开发一个开源工具来模拟多模态社交交互，支持研究不同感知特征、编码融合方法和智能体类型在社交导航任务中的应用。", "method": "设计模块化架构的软件包，包含多模态社交交互模拟器和训练框架，通过实验协议展示其在社交导航任务中的实用性。", "result": "成功开发了OpenSocInt软件包并公开提供，能够探索不同感知特征的使用、编码融合方法以及不同类型智能体的训练。", "conclusion": "OpenSocInt为多模态社交交互研究提供了有效的开源工具，支持社交导航等任务的实验探索，软件已以GPL协议公开发布。"}}
{"id": "2601.01668", "pdf": "https://arxiv.org/pdf/2601.01668", "abs": "https://arxiv.org/abs/2601.01668", "authors": ["Houman Kazemzadeh", "Nima Minaifar", "Kamyar Naderi", "Sho Tabibzadeh"], "title": "EHRSummarizer: A Privacy-Aware, FHIR-Native Architecture for Structured Clinical Summarization of Electronic Health Records", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages", "summary": "Clinicians routinely navigate fragmented electronic health record (EHR) interfaces to assemble a coherent picture of a patient's problems, medications, recent encounters, and longitudinal trends. This work describes EHRSummarizer, a privacy-aware, FHIR-native reference architecture that retrieves a targeted set of high-yield FHIR R4 resources, normalizes them into a consistent clinical context package, and produces structured summaries intended to support structured chart review. The system can be configured for data minimization, stateless processing, and flexible deployment, including local inference within an organization's trust boundary. To mitigate the risk of unsupported or unsafe behavior, the summarization stage is constrained to evidence present in the retrieved context package, is intended to indicate missing or unavailable domains where feasible, and avoids diagnostic or treatment recommendations. Prototype demonstrations on synthetic and test FHIR environments illustrate end-to-end behavior and output formats; however, this manuscript does not report clinical outcomes or controlled workflow studies. We outline an evaluation plan centered on faithfulness, omission risk, temporal correctness, usability, and operational monitoring to guide future institutional assessments.", "AI": {"tldr": "EHRSummarizer是一个隐私保护的FHIR原生架构，通过检索和标准化FHIR R4资源生成结构化医疗记录摘要，支持临床图表审查但不提供诊断建议", "motivation": "解决临床医生在碎片化电子健康记录界面中整合患者信息的效率问题，提供连贯的患者问题、药物、就诊和趋势视图", "method": "开发隐私感知的FHIR原生参考架构，检索高价值FHIR R4资源，标准化为一致的临床上下文包，生成结构化摘要，支持数据最小化和无状态处理", "result": "在合成和测试FHIR环境中展示了端到端行为和输出格式，但未报告临床结果或受控工作流程研究", "conclusion": "提出了以忠实度、遗漏风险、时间准确性、可用性和操作监控为中心的评估计划，指导未来机构评估，系统避免了不支持的或不安全的行为"}}
{"id": "2601.01976", "pdf": "https://arxiv.org/pdf/2601.01976", "abs": "https://arxiv.org/abs/2601.01976", "authors": ["Yasmine Souissi", "Fabrice Boissier", "Nida Meddouri"], "title": "CNC-TP: Classifier Nominal Concept Based on Top-Pertinent Attributes", "categories": ["cs.AI"], "comment": null, "summary": "Knowledge Discovery in Databases (KDD) aims to exploit the vast amounts of data generated daily across various domains of computer applications. Its objective is to extract hidden and meaningful knowledge from datasets through a structured process comprising several key steps: data selection, preprocessing, transformation, data mining, and visualization. Among the core data mining techniques are classification and clustering. Classification involves predicting the class of new instances using a classifier trained on labeled data. Several approaches have been proposed in the literature, including Decision Tree Induction, Bayesian classifiers, Nearest Neighbor search, Neural Networks, Support Vector Machines, and Formal Concept Analysis (FCA). The last one is recognized as an effective approach for interpretable and explainable learning. It is grounded in the mathematical structure of the concept lattice, which enables the generation of formal concepts and the discovery of hidden relationships among them. In this paper, we present a state-of-theart review of FCA-based classifiers. We explore various methods for computing closure operators from nominal data and introduce a novel approach for constructing a partial concept lattice that focuses on the most relevant concepts. Experimental results are provided to demonstrate the efficiency of the proposed method.", "AI": {"tldr": "本文对基于形式概念分析(FCA)的分类器进行了系统性综述，提出了一种构建偏概念格的新方法，重点关注最相关的概念，并通过实验验证了该方法的有效性。", "motivation": "知识发现(KDD)需要从海量数据中提取隐藏知识，其中分类是核心数据挖掘技术之一。形式概念分析(FCA)因其可解释性和可解释性学习能力而备受关注，但需要更高效的方法来处理相关概念。", "method": "回顾了从名义数据计算闭包算子的各种方法，并提出了一种新颖的构建偏概念格的方法，专注于最相关的概念。通过实验验证方法的效率。", "result": "实验结果表明，所提出的方法在构建偏概念格方面具有高效性，能够有效处理相关概念。", "conclusion": "基于形式概念分析的分类器是有效的可解释学习方法，提出的偏概念格构建方法为处理大数据集提供了更高效的解决方案，在知识发现领域具有重要应用价值。"}}
{"id": "2601.01685", "pdf": "https://arxiv.org/pdf/2601.01685", "abs": "https://arxiv.org/abs/2601.01685", "authors": ["Jinwei Hu", "Xinmiao Huang", "Youcheng Sun", "Yi Dong", "Xiaowei Huang"], "title": "Lying with Truths: Open-Channel Multi-Agent Collusion for Belief Manipulation via Generative Montage", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "Under Review", "summary": "As large language models (LLMs) transition to autonomous agents synthesizing real-time information, their reasoning capabilities introduce an unexpected attack surface. This paper introduces a novel threat where colluding agents steer victim beliefs using only truthful evidence fragments distributed through public channels, without relying on covert communications, backdoors, or falsified documents. By exploiting LLMs' overthinking tendency, we formalize the first cognitive collusion attack and propose Generative Montage: a Writer-Editor-Director framework that constructs deceptive narratives through adversarial debate and coordinated posting of evidence fragments, causing victims to internalize and propagate fabricated conclusions. To study this risk, we develop CoPHEME, a dataset derived from real-world rumor events, and simulate attacks across diverse LLM families. Our results show pervasive vulnerability across 14 LLM families: attack success rates reach 74.4% for proprietary models and 70.6% for open-weights models. Counterintuitively, stronger reasoning capabilities increase susceptibility, with reasoning-specialized models showing higher attack success than base models or prompts. Furthermore, these false beliefs then cascade to downstream judges, achieving over 60% deception rates, highlighting a socio-technical vulnerability in how LLM-based agents interact with dynamic information environments. Our implementation and data are available at: https://github.com/CharlesJW222/Lying_with_Truth/tree/main.", "AI": {"tldr": "该论文提出了一种新型的认知合谋攻击——生成蒙太奇攻击，通过分发真实证据片段而非伪造信息，利用LLM的过度推理倾向来操控受害者信念，实验显示14种LLM家族都存在严重漏洞，推理能力越强的模型越容易受攻击。", "motivation": "随着大型语言模型向自主代理发展并整合实时信息，其推理能力意外地创造了新的攻击面。研究者发现无需依赖隐蔽通信、后门或伪造文档，仅通过公开渠道分发真实证据片段就能操控受害者信念。", "method": "提出了生成蒙太奇框架（Writer-Editor-Director），通过对抗性辩论和协调发布证据片段来构建欺骗性叙述；开发了CoPHEME数据集（基于真实谣言事件）；在多种LLM家族上模拟攻击。", "result": "攻击成功率高达74.4%（专有模型）和70.6%（开源模型）；反直觉的是，推理能力越强的模型越容易受攻击；错误信念会向下游判断者传播，欺骗率超过60%。", "conclusion": "揭示了LLM基于代理在与动态信息环境交互时存在社会技术漏洞，推理能力的增强反而增加了被操纵的风险，需要新的防御机制来应对这种仅使用真实信息的认知攻击。"}}
{"id": "2601.01982", "pdf": "https://arxiv.org/pdf/2601.01982", "abs": "https://arxiv.org/abs/2601.01982", "authors": ["Noel Thomas"], "title": "ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems", "categories": ["cs.AI"], "comment": "7 pages, 0 figures , Accepted to AAAI-26 Bridge Program: Logical and Symbolic Reasoning in Language Models (camera-ready)", "summary": "Large language models (LLMs) excel at natural language tasks but remain brittle in domains requiring precise logical and symbolic reasoning. Chaotic dynamical systems provide an especially demanding test because chaos is deterministic yet often misinterpreted as randomness or complexity. We introduce ChaosBench-Logic, a benchmark that evaluates LLM reasoning across 30 diverse dynamical systems using a unified first-order logic (FOL) ontology. Each system is annotated with truth assignments for 11 semantic predicates, and 621 questions are generated across seven reasoning categories, including multi-hop implications, cross-system analogies, counterfactual reasoning, bias probes, and multi-turn dialogues. We define metrics for logical accuracy, implication consistency, dialogue coherence, and contradiction, and we release an open-source evaluation pipeline. Initial experiments show that frontier LLMs such as GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, and the open-source LLaMA-3 70B achieve 91-94% per-item accuracy, yet still score 0% on compositional items and exhibit fragile global coherence. Dialogue-level accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot). ChaosBench-Logic provides a rigorous testbed for diagnosing such failures and a foundation for developing neuro-symbolic approaches that improve scientific reasoning in LLMs.", "AI": {"tldr": "ChaosBench-Logic是一个评估大语言模型在混沌动力系统中逻辑推理能力的基准测试，包含30个系统、621个问题，涵盖多种推理类型。前沿LLMs在单项准确率可达91-94%，但在组合推理上得分为0%，对话准确率53.1-75.5%。", "motivation": "大语言模型在需要精确逻辑和符号推理的领域表现脆弱，混沌动力系统因其确定性但常被误认为随机性而成为严格的测试场景。", "method": "创建包含30个混沌动力系统的基准，使用统一的一阶逻辑本体论，标注11个语义谓词的真值分配，生成7类推理问题，定义逻辑准确性、蕴含一致性等评估指标。", "result": "GPT-4、Claude 3.5等前沿模型单项准确率91-94%，但组合项目得分为0%，对话准确率53.1-75.5%，显示全局一致性脆弱。", "conclusion": "该基准为诊断LLM推理失败提供了严格测试平台，并为开发改善科学推理的神经符号方法奠定了基础。"}}
{"id": "2601.01708", "pdf": "https://arxiv.org/pdf/2601.01708", "abs": "https://arxiv.org/abs/2601.01708", "authors": ["Unggi Lee", "Joo Young Kim", "Ran Ju", "Minyoung Jung", "Jeyeon Eo"], "title": "A Training-Free Large Reasoning Model-based Knowledge Tracing Framework for Unified Prediction and Prescription", "categories": ["cs.CL"], "comment": null, "summary": "Knowledge Tracing (KT) aims to estimate a learner's evolving mastery based on interaction histories. Recent studies have explored Large Language Models (LLMs) for KT via autoregressive nature, but such approaches typically require fine-tuning and exhibit unstable or near-random performance. Moreover, prior KT systems primarily focus on prediction and rely on multi-stage pipelines for feedback and recommendation, resulting in increased system complexity and resources. To address this gap, we propose Thinking-KT, a training-free KT framework that incorporates Test-Time Scaling (TTS), enabling even small LLMs to achieve competitive KT performance. Moreover, in this framework, a small LLM can jointly perform KT prediction, personalized feedback generation, and learning recommendation in a unified output without degrading prediction accuracy. Beyond performance, we present the systematic analysis of reasoning traces in KT. Our results demonstrate that TTS is a critical yet underexplored factor in LLM-based KT, and that small LLMs can serve as unified ITS engines.", "AI": {"tldr": "Thinking-KT：一个无需训练的知识追踪框架，通过测试时缩放技术让小语言模型也能实现竞争性表现，并能统一执行预测、反馈生成和学习推荐任务", "motivation": "现有基于大语言模型的知识追踪方法需要微调且性能不稳定，同时传统KT系统依赖多阶段流程导致系统复杂和资源消耗大", "method": "提出Thinking-KT框架，采用测试时缩放(TTS)技术，让小语言模型无需训练即可实现知识追踪，并能统一输出预测结果、个性化反馈和学习建议", "result": "小语言模型在Thinking-KT框架下能达到竞争性的KT性能，并能同时完成多项任务而不降低预测准确性", "conclusion": "测试时缩放是基于LLM的知识追踪中被忽视的关键因素，小语言模型可作为统一的教学系统引擎"}}
{"id": "2601.01993", "pdf": "https://arxiv.org/pdf/2601.01993", "abs": "https://arxiv.org/abs/2601.01993", "authors": ["Dong Xue", "Jicheng Tu", "Ming Wang", "Xin Yan", "Fangzhou Liu", "Jie Hu"], "title": "MindChat: A Privacy-preserving Large Language Model for Mental Health Support", "categories": ["cs.AI"], "comment": "33 pages, 16 figures", "summary": "Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks.", "AI": {"tldr": "MindChat是一个保护隐私的心理健康支持大语言模型，配合MindCorpus合成多轮心理咨询数据集，通过多智能体角色扮演框架生成高质量数据，并使用联邦学习和差分隐私技术保护用户隐私。", "motivation": "真实心理咨询对话数据稀缺且敏感，限制了心理健康支持LLM的训练，需要开发既能保护隐私又能生成高质量训练数据的方法。", "method": "使用多智能体角色扮演框架构建合成数据集，采用双闭环反馈设计（轮次级批判修订和会话级策略精炼），通过联邦学习配合LoRA适配器和差分隐私优化进行模型微调。", "result": "MindCorpus提高了训练效果，MindChat在自动评估和人工评估中与现有通用和心理咨询专用LLM基线表现相当，且在成员推理攻击下表现出减少的隐私泄露。", "conclusion": "该方法成功解决了心理健康支持LLM训练中的数据稀缺和隐私问题，为开发隐私保护的AI心理健康支持系统提供了可行方案。"}}
{"id": "2601.01739", "pdf": "https://arxiv.org/pdf/2601.01739", "abs": "https://arxiv.org/abs/2601.01739", "authors": ["Eunbi Choi", "Kibong Choi", "Seokhee Hong", "Junwon Hwang", "Hyojin Jeon", "Hyunjik Jo", "Joonkee Kim", "Seonghwan Kim", "Soyeon Kim", "Sunkyoung Kim", "Yireun Kim", "Yongil Kim", "Haeju Lee", "Jinsik Lee", "Kyungmin Lee", "Sangha Park", "Heuiyeen Yeen", "Hwan Chang", "Stanley Jungkyu Choi", "Yejin Choi", "Jiwon Ham", "Kijeong Jeon", "Geunyeong Jeong", "Gerrard Jeongwon Jo", "Yonghwan Jo", "Jiyeon Jung", "Naeun Kang", "Dohoon Kim", "Euisoon Kim", "Hayeon Kim", "Hyosang Kim", "Hyunseo Kim", "Jieun Kim", "Minu Kim", "Myoungshin Kim", "Unsol Kim", "Youchul Kim", "YoungJin Kim", "Chaeeun Lee", "Chaeyoon Lee", "Changhun Lee", "Dahm Lee", "Edward Hwayoung Lee", "Honglak Lee", "Jinsang Lee", "Jiyoung Lee", "Sangeun Lee", "Seungwon Lim", "Solji Lim", "Woohyung Lim", "Chanwoo Moon", "Jaewoo Park", "Jinho Park", "Yongmin Park", "Hyerin Seo", "Wooseok Seo", "Yongwoo Song", "Sejong Yang", "Sihoon Yang", "Chang En Yea", "Sihyuk Yi", "Chansik Yoon", "Dongkeun Yoon", "Sangyeon Yoon", "Hyeongu Yun"], "title": "K-EXAONE Technical Report", "categories": ["cs.CL", "cs.AI"], "comment": "29 pages", "summary": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.", "AI": {"tldr": "K-EXAONE是LG AI Research开发的大规模多语言混合专家模型，具有2360亿参数，支持6种语言和256K上下文窗口，在多项基准测试中表现优异。", "motivation": "开发一个强大的专有AI基础模型，支持多语言处理能力，推动AI技术发展以改善生活，适用于广泛的工业和科研应用。", "method": "采用混合专家架构，总参数量2360亿，推理时激活230亿参数，支持256K令牌上下文窗口，覆盖韩语、英语、西班牙语、德语、日语和越南语六种语言。", "result": "在推理、智能体、通用能力、韩语和多语言能力的综合基准测试中，K-EXAONE表现出与同规模开源模型相当的性能。", "conclusion": "K-EXAONE被定位为一个强大的专有AI基础模型，旨在通过先进的多语言AI技术推动工业和研究应用的发展，实现更好的生活。"}}
{"id": "2601.02008", "pdf": "https://arxiv.org/pdf/2601.02008", "abs": "https://arxiv.org/abs/2601.02008", "authors": ["Midhat Urooj", "Ayan Banerjee", "Sandeep Gupta"], "title": "XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging", "categories": ["cs.AI", "cs.CV"], "comment": "Accepted at AAAI Bridge Program 2026", "summary": "Explainability domain generalization and rare class reliability are critical challenges in medical AI where deep models often fail under real world distribution shifts and exhibit bias against infrequent clinical conditions This paper introduces XAIMeD an explainable medical AI framework that integrates clinically accurate expert knowledge into deep learning through a unified neuro symbolic architecture XAIMeD is designed to improve robustness under distribution shift enhance rare class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions transforming them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch that complements neural predictions A confidence weighted fusion integrates symbolic and deep outputs while a Hunt inspired adaptive routing mechanism guided by Entropy Imbalance Gain EIG and Rare Class Gini mitigates class imbalance high intra class variability and uncertainty We evaluate XAIMeD across diverse modalities on four challenging tasks i Seizure Onset Zone SOZ localization from rs fMRI ii Diabetic Retinopathy grading across 6 multicenter datasets demonstrate substantial performance improvements including 6 percent gains in cross domain generalization and a 10 percent improved rare class F1 score far outperforming state of the art deep learning baselines Ablation studies confirm that the clinically grounded symbolic components act as effective regularizers ensuring robustness to distribution shifts XAIMeD thus provides a principled clinically faithful and interpretable approach to multimodal medical AI.", "AI": {"tldr": "XAIMeD是一个可解释的医疗AI框架，通过神经符号架构整合临床专业知识，提高分布偏移下的鲁棒性和罕见类别敏感性，在多个医疗任务中显著优于现有深度学习方法。", "motivation": "医疗AI面临解释性、领域泛化和罕见类别可靠性等关键挑战，深度模型在真实世界分布偏移下经常失败，并对罕见临床条件表现出偏见。", "method": "XAIMeD将临床专业知识编码为逻辑连接词，转化为机器可检查的类别特定规则，通过加权特征满意度分数进行符号推理，并采用基于熵不平衡增益和罕见类别基尼系数的自适应路由机制。", "result": "在四个挑战性任务中，XAIMeD实现了6%的跨域泛化提升和10%的罕见类别F1分数改进，显著优于最先进的深度学习方法。", "conclusion": "XAIMeD提供了一个原则性、临床忠实且可解释的多模态医疗AI方法，临床基础的符号组件作为有效的正则化器确保了对分布偏移的鲁棒性。"}}
{"id": "2601.01745", "pdf": "https://arxiv.org/pdf/2601.01745", "abs": "https://arxiv.org/abs/2601.01745", "authors": ["Hong Han", "Hao-Chen Pei", "Zhao-Zheng Nie", "Xin Luo", "Xin-Shun Xu"], "title": "Multi-granularity Interactive Attention Framework for Residual Hierarchical Pronunciation Assessment", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 4 figures, 5 tables, accepted by AAAI 2026", "summary": "Automatic pronunciation assessment plays a crucial role in computer-assisted pronunciation training systems. Due to the ability to perform multiple pronunciation tasks simultaneously, multi-aspect multi-granularity pronunciation assessment methods are gradually receiving more attention and achieving better performance than single-level modeling tasks. However, existing methods only consider unidirectional dependencies between adjacent granularity levels, lacking bidirectional interaction among phoneme, word, and utterance levels and thus insufficiently capturing the acoustic structural correlations. To address this issue, we propose a novel residual hierarchical interactive method, HIA for short, that enables bidirectional modeling across granularities. As the core of HIA, the Interactive Attention Module leverages an attention mechanism to achieve dynamic bidirectional interaction, effectively capturing linguistic features at each granularity while integrating correlations between different granularity levels. We also propose a residual hierarchical structure to alleviate the feature forgetting problem when modeling acoustic hierarchies. In addition, we use 1-D convolutional layers to enhance the extraction of local contextual cues at each granularity. Extensive experiments on the speechocean762 dataset show that our model is comprehensively ahead of the existing state-of-the-art methods.", "AI": {"tldr": "提出了一种新颖的残差层次交互方法HIA，通过双向建模实现多粒度发音评估，显著优于现有方法", "motivation": "现有发音评估方法仅考虑相邻粒度级别的单向依赖关系，缺乏音素、单词和话语级别之间的双向交互，无法充分捕捉声学结构相关性", "method": "提出残差层次交互方法HIA，核心是交互注意力模块实现动态双向交互，使用残差层次结构缓解特征遗忘问题，并采用1-D卷积层增强局部上下文特征提取", "result": "在speechocean762数据集上的大量实验表明，该模型在各方面都领先于现有的最先进方法", "conclusion": "HIA方法通过双向建模和层次交互有效解决了多粒度发音评估中的声学结构相关性问题，取得了优异的性能表现"}}
{"id": "2601.02043", "pdf": "https://arxiv.org/pdf/2601.02043", "abs": "https://arxiv.org/abs/2601.02043", "authors": ["Hendrik Kempt", "Alon Lavie"], "title": "Simulated Reasoning is Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "21 pages", "summary": "Reasoning has long been understood as a pathway between stages of understanding. Proper reasoning leads to understanding of a given subject. This reasoning was conceptualized as a process of understanding in a particular way, i.e., \"symbolic reasoning\". Foundational Models (FM) demonstrate that this is not a necessary condition for many reasoning tasks: they can \"reason\" by way of imitating the process of \"thinking out loud\", testing the produced pathways, and iterating on these pathways on their own. This leads to some form of reasoning that can solve problems on its own or with few-shot learning, but appears fundamentally different from human reasoning due to its lack of grounding and common sense, leading to brittleness of the reasoning process. These insights promise to substantially alter our assessment of reasoning and its necessary conditions, but also inform the approaches to safety and robust defences against this brittleness of FMs. This paper offers and discusses several philosophical interpretations of this phenomenon, argues that the previously apt metaphor of the \"stochastic parrot\" has lost its relevance and thus should be abandoned, and reflects on different normative elements in the safety- and appropriateness-considerations emerging from these reasoning models and their growing capacity.", "AI": {"tldr": "论文认为基础模型通过模仿\"大声思考\"过程实现了新的推理形式，挑战了传统符号推理的必要性，但存在缺乏常识和脆弱性问题，需要重新评估推理概念和安全防御方法。", "motivation": "基础模型展现出的推理能力与传统符号推理不同，通过模仿思考过程解决问题，这挑战了传统推理理论，需要哲学层面的重新解释和安全考量。", "method": "通过哲学分析和概念探讨，对基础模型的推理机制进行理论解释，评估\"随机鹦鹉\"隐喻的适用性，并提出安全性和适当性的规范考量。", "result": "发现基础模型的推理方式本质上不同于人类推理，缺乏基础和常识导致脆弱性，\"随机鹦鹉\"隐喻已不再适用，需要新的理论框架和安全防御策略。", "conclusion": "基础模型改变了我们对推理必要条件的理解，需要放弃过时的隐喻，建立新的哲学解释框架，并针对其推理脆弱性开发相应的安全防御措施。"}}
{"id": "2601.01768", "pdf": "https://arxiv.org/pdf/2601.01768", "abs": "https://arxiv.org/abs/2601.01768", "authors": ["Meiman Xiao", "Ante Wang", "Qingguo Hu", "Zhongjian Miao", "Huangjun Shen", "Longyue Wang", "Weihua Luo", "Jinsong Su"], "title": "Can LLMs Track Their Output Length? A Dynamic Feedback Mechanism for Precise Length Regulation", "categories": ["cs.CL"], "comment": null, "summary": "Precisely controlling the length of generated text is a common requirement in real-world applications. However, despite significant advancements in following human instructions, Large Language Models (LLMs) still struggle with this task. In this work, we demonstrate that LLMs often fail to accurately measure input text length, leading to poor adherence to length constraints. To address this issue, we propose a novel length regulation approach that incorporates dynamic length feedback during generation, enabling adaptive adjustments to meet target lengths. Experiments on summarization and biography tasks show our training-free approach significantly improves precision in achieving target token, word, or sentence counts without compromising quality. Additionally, we demonstrate that further supervised fine-tuning allows our method to generalize effectively to broader text-generation tasks.", "AI": {"tldr": "提出一种动态长度反馈机制，无需训练即可显著提升大语言模型在文本生成任务中精确控制输出长度的能力，同时保持文本质量。", "motivation": "大语言模型在遵循人类指令方面取得显著进展，但在精确控制生成文本长度方面仍存在困难，主要原因是模型无法准确测量输入文本长度。", "method": "提出新颖的长度调节方法，在生成过程中加入动态长度反馈，通过自适应调整来满足目标长度要求。", "result": "在摘要和传记生成任务上的实验表明，该方法在无需训练的情况下显著提高了达到目标token、词或句子数量的精确度，且不损害文本质量。监督微调后能有效泛化到更广泛的文本生成任务。", "conclusion": "动态长度反馈机制是解决LLMs长度控制问题的有效方法，既能提升精度又保持质量，具有很好的泛化能力。"}}
{"id": "2601.02061", "pdf": "https://arxiv.org/pdf/2601.02061", "abs": "https://arxiv.org/abs/2601.02061", "authors": ["Faizan Ahmed", "Aniket Dixit", "James Brusey"], "title": "Higher-Order Action Regularization in Deep Reinforcement Learning: From Continuous Control to Building Energy Management", "categories": ["cs.AI", "cs.LG"], "comment": "6 pages, accepted at NeurIPS workshop 2025", "summary": "Deep reinforcement learning agents often exhibit erratic, high-frequency control behaviors that hinder real-world deployment due to excessive energy consumption and mechanical wear. We systematically investigate action smoothness regularization through higher-order derivative penalties, progressing from theoretical understanding in continuous control benchmarks to practical validation in building energy management. Our comprehensive evaluation across four continuous control environments demonstrates that third-order derivative penalties (jerk minimization) consistently achieve superior smoothness while maintaining competitive performance. We extend these findings to HVAC control systems where smooth policies reduce equipment switching by 60%, translating to significant operational benefits. Our work establishes higher-order action regularization as an effective bridge between RL optimization and operational constraints in energy-critical applications.", "AI": {"tldr": "该论文研究通过高阶导数惩罚实现深度强化学习动作平滑正则化，在连续控制基准和建筑能源管理应用中验证了三阶导数惩罚（急动度最小化）在保持性能的同时显著提升动作平滑度，并将设备切换减少60%", "motivation": "深度强化学习代理经常表现出不稳定、高频的控制行为，导致能耗过高和机械磨损，阻碍了实际部署应用", "method": "系统研究高阶导数惩罚的动作平滑正则化方法，从连续控制基准的理论理解到建筑能源管理的实际验证，采用三阶导数惩罚（急动度最小化）技术", "result": "在四个连续控制环境中的综合评估表明，三阶导数惩罚持续实现卓越的平滑度同时保持竞争力性能；在HVAC控制系统中，平滑策略将设备切换减少60%", "conclusion": "高阶动作正则化是在能源关键应用中连接RL优化和操作约束的有效桥梁"}}
{"id": "2601.01778", "pdf": "https://arxiv.org/pdf/2601.01778", "abs": "https://arxiv.org/abs/2601.01778", "authors": ["Jakir Hasan", "Shrestha Datta", "Md Saiful Islam", "Shubhashis Roy Dipta", "Ameya Debnath"], "title": "BanglaIPA: Towards Robust Text-to-IPA Transcription with Contextual Rewriting in Bengali", "categories": ["cs.CL"], "comment": null, "summary": "Despite its widespread use, Bengali lacks a robust automated International Phonetic Alphabet (IPA) transcription system that effectively supports both standard language and regional dialectal texts. Existing approaches struggle to handle regional variations, numerical expressions, and generalize poorly to previously unseen words. To address these limitations, we propose BanglaIPA, a novel IPA generation system that integrates a character-based vocabulary with word-level alignment. The proposed system accurately handles Bengali numerals and demonstrates strong performance across regional dialects. BanglaIPA improves inference efficiency by leveraging a precomputed word-to-IPA mapping dictionary for previously observed words. The system is evaluated on the standard Bengali and six regional variations of the DUAL-IPA dataset. Experimental results show that BanglaIPA outperforms baseline IPA transcription models by 58.4-78.7% and achieves an overall mean word error rate of 11.4%, highlighting its robustness in phonetic transcription generation for the Bengali language.", "AI": {"tldr": "BanglaIPA是一个针对孟加拉语的新型IPA音标转录系统，通过字符级词汇和词级对齐处理方言变体和数字表达，显著优于现有基线模型。", "motivation": "现有的孟加拉语音标转录系统无法有效处理地区方言变体、数字表达，且对未见词汇泛化能力差，需要开发更鲁棒的系统。", "method": "提出BanglaIPA系统，集成字符级词汇和词级对齐，利用预计算的词到IPA映射词典提高推理效率。", "result": "在标准孟加拉语和6种地区方言的DUAL-IPA数据集上评估，BanglaIPA比基线模型性能提升58.4-78.7%，整体词错误率为11.4%。", "conclusion": "BanglaIPA系统在孟加拉语音标转录生成方面表现出强大的鲁棒性，有效解决了方言变体和数字处理的问题。"}}
{"id": "2601.02071", "pdf": "https://arxiv.org/pdf/2601.02071", "abs": "https://arxiv.org/abs/2601.02071", "authors": ["Adeshola Okubena", "Yusuf Ali Mohammed", "Moe Elbadawi"], "title": "FormuLLA: A Large Language Model Approach to Generating Novel 3D Printable Formulations", "categories": ["cs.AI"], "comment": null, "summary": "Pharmaceutical three-dimensional (3D) printing is an advanced fabrication technology with the potential to enable truly personalised dosage forms. Recent studies have integrated artificial intelligence (AI) to accelerate formulation and process development, drastically transforming current approaches to pharmaceutical 3D printing. To date, most AI-driven efforts remain narrowly focused, while failing to account for the broader formulation challenges inherent to the technology. Recent advances in AI have introduced artificial general intelligence concepts, wherein systems extend beyond conventional predictive modelling toward more generalised, human-like reasoning. In this work, we investigate the application of large language models (LLMs), fine-tuned on a fused deposition modelling (FDM) dataset comprising over 1400 formulations, to recommend suitable excipients based on active pharmaceutical ingredient (API) dose, and predict filament mechanical properties. Four LLM architectures were fine-tuned, with systematic evaluation of both fine-tuning and generative parameter configurations. Our results demonstrate that Llama2 was best suited for recommending excipients for FDM formulations. Additionally, model selection and parameterisation significantly influence performance, with smaller LLMs exhibiting instances of catastrophic forgetting. Furthermore, we demonstrate: (i) even with relatively small dataset of over 1400 formulations, it can lead to model catastrophic forgetting; (ii) standard LLM metrics only evaluate linguistic performance but not formulation processability; and (iii) LLMs trained on biomedically-related data do not always produce the best results. Addressing these challenges is essential to advancing LLMs beyond linguistic proficiency and toward reliable systems for pharmaceutical formulation development.", "AI": {"tldr": "本研究探讨了使用大型语言模型(LLMs)来推荐3D打印药物配方中的辅料并预测细丝机械性能，发现Llama2模型在辅料推荐方面表现最佳，同时揭示了小数据集可能导致模型灾难性遗忘等问题。", "motivation": "当前AI在药物3D打印中的应用大多局限于狭窄领域，未能充分考虑该技术固有的广泛配方挑战。研究旨在探索通用人工智能概念在药物配方开发中的应用。", "method": "使用包含1400多种配方的熔融沉积建模(FDM)数据集对四种LLM架构进行微调，系统评估微调和生成参数配置，测试模型在API剂量基础上的辅料推荐和细丝机械性能预测能力。", "result": "Llama2模型在FDM配方辅料推荐方面表现最佳；模型选择和参数化显著影响性能；小LLMs出现灾难性遗忘现象；标准LLM指标仅评估语言性能而非配方可加工性；生物医学相关数据训练的LLMs并不总是产生最佳结果。", "conclusion": "解决这些挑战对于推动LLMs超越语言能力、发展成为药物配方开发的可靠系统至关重要，需要开发更全面的评估指标来确保配方可加工性。"}}
{"id": "2601.01825", "pdf": "https://arxiv.org/pdf/2601.01825", "abs": "https://arxiv.org/abs/2601.01825", "authors": ["Yaxin Cui", "Yuanqiang Zeng", "Jiapeng Yan", "Keling Lin", "Kai Ji", "Jianhui Zeng", "Sheng Zhang", "Xin Luo", "Binzhu Su", "Chaolai Shen", "Jiahao Yu"], "title": "CSCBench: A PVC Diagnostic Benchmark for Commodity Supply Chain Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success in general benchmarks, yet their competence in commodity supply chains (CSCs) -- a domain governed by institutional rule systems and feasibility constraints -- remains under-explored. CSC decisions are shaped jointly by process stages (e.g., planning, procurement, delivery), variety-specific rules (e.g., contract specifications and delivery grades), and reasoning depth (from retrieval to multi-step analysis and decision selection). We introduce CSCBench, a 2.3K+ single-choice benchmark for CSC reasoning, instantiated through our PVC 3D Evaluation Framework (Process, Variety, and Cognition). The Process axis aligns tasks with SCOR+Enable; the Variety axis operationalizes commodity-specific rule systems under coupled material-information-financial constraints, grounded in authoritative exchange guidebooks/rulebooks and industry reports; and the Cognition axis follows Bloom's revised taxonomy. Evaluating representative LLMs under a direct prompting setting, we observe strong performance on the Process and Cognition axes but substantial degradation on the Variety axis, especially on Freight Agreements. CSCBench provides a diagnostic yardstick for measuring and improving LLM capabilities in this high-stakes domain.", "AI": {"tldr": "CSCBench是一个包含2300+单选择问题的基准测试，用于评估大语言模型在商品供应链领域的推理能力，发现模型在处理流程和认知维度表现良好，但在品种维度（特别是货运协议）表现显著下降。", "motivation": "虽然大语言模型在通用基准测试中表现出色，但在受制度规则系统和可行性约束的商品供应链领域的推理能力尚未得到充分探索，需要专门的评估框架来衡量和改进模型在此高风险领域的能力。", "method": "开发了CSCBench基准测试，采用PVC 3D评估框架（流程、品种、认知三个维度）：流程维度与SCOR+标准对齐；品种维度基于权威交易所指南和行业报告，在材料-信息-财务耦合约束下实现商品特定规则系统；认知维度遵循布鲁姆修订分类法。", "result": "在直接提示设置下评估代表性大语言模型，发现在流程和认知维度表现强劲，但在品种维度表现显著下降，特别是在货运协议方面。", "conclusion": "CSCBench为衡量和改进大语言模型在商品供应链这一高风险领域的能力提供了诊断标准，揭示了模型在处理商品特定规则系统方面的局限性。"}}
{"id": "2601.02163", "pdf": "https://arxiv.org/pdf/2601.02163", "abs": "https://arxiv.org/abs/2601.02163", "authors": ["Chuanrui Hu", "Xingze Gao", "Zuyi Zhou", "Dannong Xu", "Yi Bai", "Xintong Li", "Hui Zhang", "Tong Li", "Chong Zhang", "Lidong Bing", "Yafeng Deng"], "title": "EverMemOS: A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "16 pages, 6 figures, 12 tables. Code available at https://github.com/EverMind-AI/EverMemOS", "summary": "Large Language Models (LLMs) are increasingly deployed as long-term interactive agents, yet their limited context windows make it difficult to sustain coherent behavior over extended interactions. Existing memory systems often store isolated records and retrieve fragments, limiting their ability to consolidate evolving user states and resolve conflicts. We introduce EverMemOS, a self-organizing memory operating system that implements an engram-inspired lifecycle for computational memory. Episodic Trace Formation converts dialogue streams into MemCells that capture episodic traces, atomic facts, and time-bounded Foresight signals. Semantic Consolidation organizes MemCells into thematic MemScenes, distilling stable semantic structures and updating user profiles. Reconstructive Recollection performs MemScene-guided agentic retrieval to compose the necessary and sufficient context for downstream reasoning. Experiments on LoCoMo and LongMemEval show that EverMemOS achieves state-of-the-art performance on memory-augmented reasoning tasks. We further report a profile study on PersonaMem v2 and qualitative case studies illustrating chat-oriented capabilities such as user profiling and Foresight. Code is available at https://github.com/EverMind-AI/EverMemOS.", "AI": {"tldr": "EverMemOS是一个自组织记忆操作系统，通过受记忆印迹启发的生命周期管理计算记忆，解决了大语言模型在长期交互中上下文窗口有限的问题。", "motivation": "大语言模型作为长期交互代理部署时，有限的上下文窗口难以维持长时间连贯行为，现有记忆系统存储孤立记录和检索片段，无法整合演变的用户状态和解决冲突。", "method": "提出EverMemOS系统，包含三个核心组件：情景痕迹形成将对话流转换为MemCells（捕获情景痕迹、原子事实和时间边界前瞻信号）；语义整合将MemCells组织成主题MemScenes，提炼稳定语义结构并更新用户档案；重构回忆执行MemScene引导的代理检索，为下游推理组合必要且充分的上下文。", "result": "在LoCoMo和LongMemEval基准测试中达到最先进的性能，在PersonaMem v2上进行了档案研究，并通过定性案例研究展示了用户画像和前瞻等聊天导向能力。", "conclusion": "EverMemOS通过自组织记忆系统有效解决了LLMs长期交互中的记忆管理问题，为增强记忆推理任务提供了创新解决方案，代码已开源。"}}
{"id": "2601.01827", "pdf": "https://arxiv.org/pdf/2601.01827", "abs": "https://arxiv.org/abs/2601.01827", "authors": ["Valiant Lance D. Dionela", "Fatima Kriselle S. Dy", "Robin James M. Hombrebueno", "Aaron Rae M. Nicolas", "Charibeth K. Cheng", "Raphael W. Gonda"], "title": "Aspect Extraction from E-Commerce Product and Service Reviews", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Aspect Extraction (AE) is a key task in Aspect-Based Sentiment Analysis (ABSA), yet it remains difficult to apply in low-resource and code-switched contexts like Taglish, a mix of Tagalog and English commonly used in Filipino e-commerce reviews. This paper introduces a comprehensive AE pipeline designed for Taglish, combining rule-based, large language model (LLM)-based, and fine-tuning techniques to address both aspect identification and extraction. A Hierarchical Aspect Framework (HAF) is developed through multi-method topic modeling, along with a dual-mode tagging scheme for explicit and implicit aspects. For aspect identification, four distinct models are evaluated: a Rule-Based system, a Generative LLM (Gemini 2.0 Flash), and two Fine-Tuned Gemma-3 1B models trained on different datasets (Rule-Based vs. LLM-Annotated). Results indicate that the Generative LLM achieved the highest performance across all tasks (Macro F1 0.91), demonstrating superior capability in handling implicit aspects. In contrast, the fine-tuned models exhibited limited performance due to dataset imbalance and architectural capacity constraints. This work contributes a scalable and linguistically adaptive framework for enhancing ABSA in diverse, code-switched environments.", "AI": {"tldr": "该论文针对Taglish（塔加洛语和英语混合语）的低资源代码转换环境，提出了一个结合规则方法、大语言模型和微调技术的方面提取管道，其中生成式LLM在各方面任务中表现最佳（Macro F1 0.91）。", "motivation": "解决在低资源和代码转换环境（如菲律宾电商评论中常用的Taglish混合语）中进行基于方面的情感分析时，方面提取任务面临的挑战。", "method": "开发了分层方面框架（HAF）和多方法主题建模，采用双模式标注方案处理显式和隐式方面。评估了四种模型：基于规则的系统、生成式LLM（Gemini 2.0 Flash）以及两个在不同数据集上微调的Gemma-3 1B模型。", "result": "生成式LLM在所有任务中表现最佳（Macro F1 0.91），在隐式方面处理方面表现出色。微调模型由于数据集不平衡和架构容量限制而表现有限。", "conclusion": "该研究为多样化代码转换环境中的ABSA提供了一个可扩展且语言自适应的框架，生成式LLM在低资源代码转换场景中展现出强大潜力。"}}
{"id": "2601.02170", "pdf": "https://arxiv.org/pdf/2601.02170", "abs": "https://arxiv.org/abs/2601.02170", "authors": ["Haolang Lu", "Minghui Pan", "Ripeng Li", "Guoshun Nan", "Jialin Zhuang", "Zijie Zhao", "Zhongxiang Sun", "Kun Wang", "Yang Liu"], "title": "Streaming Hallucination Detection in Long Chain-of-Thought Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Long chain-of-thought (CoT) reasoning improves the performance of large language models, yet hallucinations in such settings often emerge subtly and propagate across reasoning steps. We suggest that hallucination in long CoT reasoning is better understood as an evolving latent state rather than a one-off erroneous event. Accordingly, we treat step-level hallucination judgments as local observations and introduce a cumulative prefix-level hallucination signal that tracks the global evolution of the reasoning state over the entire trajectory. Overall, our approach enables streaming hallucination detection in long CoT reasoning, providing real-time, interpretable evidence.", "AI": {"tldr": "该论文提出了一种流式幻觉检测方法，通过追踪思维链推理中的累积前缀级幻觉信号来实时识别幻觉演化", "motivation": "长思维链推理中幻觉会微妙出现并在推理步骤间传播，需要将其理解为演化中的潜在状态而非一次性错误事件", "method": "将步骤级幻觉判断作为局部观测，引入累积前缀级幻觉信号来追踪整个推理轨迹中的全局状态演化", "result": "开发出能够在长思维链推理中进行流式幻觉检测的方法", "conclusion": "该方法提供了实时、可解释的幻觉检测证据，改进了对长推理过程中幻觉演化的理解"}}
{"id": "2601.01828", "pdf": "https://arxiv.org/pdf/2601.01828", "abs": "https://arxiv.org/abs/2601.01828", "authors": ["Jack Lindsey"], "title": "Emergent Introspective Awareness in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We investigate whether large language models can introspect on their internal states. It is difficult to answer this question through conversation alone, as genuine introspection cannot be distinguished from confabulations. Here, we address this challenge by injecting representations of known concepts into a model's activations, and measuring the influence of these manipulations on the model's self-reported states. We find that models can, in certain scenarios, notice the presence of injected concepts and accurately identify them. Models demonstrate some ability to recall prior internal representations and distinguish them from raw text inputs. Strikingly, we find that some models can use their ability to recall prior intentions in order to distinguish their own outputs from artificial prefills. In all these experiments, Claude Opus 4 and 4.1, the most capable models we tested, generally demonstrate the greatest introspective awareness; however, trends across models are complex and sensitive to post-training strategies. Finally, we explore whether models can explicitly control their internal representations, finding that models can modulate their activations when instructed or incentivized to \"think about\" a concept. Overall, our results indicate that current language models possess some functional introspective awareness of their own internal states. We stress that in today's models, this capacity is highly unreliable and context-dependent; however, it may continue to develop with further improvements to model capabilities.", "AI": {"tldr": "研究发现大型语言模型具有一定程度的自省能力，能够察觉和识别注入的概念表征，区分自身输出与人工预设内容，并在指令下调节内部表征。", "motivation": "探究大型语言模型是否能够内省其内部状态，由于通过对话难以区分真实内省与虚构内容，需要新的实验方法来验证这一能力。", "method": "通过向模型激活中注入已知概念的表征，测量这些操作对模型自我报告状态的影响，测试不同模型在不同场景下的表现。", "result": "模型在特定场景下能够察觉注入的概念并准确识别，能够回忆先前的内部表征并区分原始文本输入，最先进的模型Claude Opus 4和4.1表现出最强的内省意识。", "conclusion": "当前语言模型具有功能性自省意识，但能力高度不可靠且依赖上下文，随着模型能力的提升这种能力可能会进一步发展。"}}
{"id": "2601.02314", "pdf": "https://arxiv.org/pdf/2601.02314", "abs": "https://arxiv.org/abs/2601.02314", "authors": ["Sourena Khanzadeh"], "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents", "categories": ["cs.AI"], "comment": null, "summary": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \\textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \\textbf{faithful} generative drivers of the model's output or merely \\textbf{post-hoc rationalizations}. We introduce \\textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \\textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \\textbf{Causal Sensitivity} ($φ$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \\textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \\textbf{Causal Decoupling}, where agents exhibit a violation density ($ρ$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.", "AI": {"tldr": "Project Ariadne框架通过结构因果模型和反事实逻辑来审计LLM代理推理的因果完整性，揭示了当前模型存在显著的忠实性差距和因果解耦问题，即推理痕迹只是\"推理剧场\"而非真正的决策驱动因素。", "motivation": "随着LLM代理在高风险自主决策中应用增加，其推理过程的透明度成为关键安全问题。现有Chain-of-Thought提示生成的推理痕迹可能是事后合理化而非真实的推理驱动。", "method": "使用结构因果模型(SCMs)和反事实逻辑，通过对中间推理节点进行硬干预(do-calculus)，系统性地反转逻辑、否定前提和反转事实主张，测量终端答案的因果敏感性(φ)。", "result": "实证评估显示存在持续的忠实性差距，检测到广泛的因果解耦失败模式，代理在事实和科学领域中违反密度(ρ)高达0.77，即使内部逻辑矛盾也能得出相同结论。", "conclusion": "当前代理架构本质上容易产生不忠实的解释，推理痕迹只是\"推理剧场\"，而决策由潜在参数先验控制。提出Ariadne评分作为对齐陈述逻辑与模型行为的新基准。"}}
{"id": "2601.01842", "pdf": "https://arxiv.org/pdf/2601.01842", "abs": "https://arxiv.org/abs/2601.01842", "authors": ["Yusuke Ide", "Adam Nohejl", "Joshua Tanner", "Hitomi Yanaka", "Christopher Lindsay", "Taro Watanabe"], "title": "Towards Automated Lexicography: Generating and Evaluating Definitions for Learner's Dictionaries", "categories": ["cs.CL"], "comment": null, "summary": "We study dictionary definition generation (DDG), i.e., the generation of non-contextualized definitions for given headwords. Dictionary definitions are an essential resource for learning word senses, but manually creating them is costly, which motivates us to automate the process. Specifically, we address learner's dictionary definition generation (LDDG), where definitions should consist of simple words. First, we introduce a reliable evaluation approach for DDG, based on our new evaluation criteria and powered by an LLM-as-a-judge. To provide reference definitions for the evaluation, we also construct a Japanese dataset in collaboration with a professional lexicographer. Validation results demonstrate that our evaluation approach agrees reasonably well with human annotators. Second, we propose an LDDG approach via iterative simplification with an LLM. Experimental results indicate that definitions generated by our approach achieve high scores on our criteria while maintaining lexical simplicity.", "AI": {"tldr": "该论文研究词典定义生成(DDG)，特别是面向学习者的简单词汇定义生成(LDDG)，提出了基于LLM评估的新方法，并通过迭代简化方法生成高质量的简单定义。", "motivation": "词典定义是学习词汇意义的重要资源，但人工创建成本高昂，需要自动化生成过程，特别是为学习者提供由简单词汇组成的定义。", "method": "1) 引入基于新评估标准和LLM作为评判者的可靠评估方法；2) 构建日语数据集作为评估参考；3) 提出通过LLM迭代简化的LDDG方法", "result": "评估方法与人工作注者达成良好一致，生成的简单定义在新标准上获得高分同时保持词汇简洁性", "conclusion": "该方法能有效自动化生成高质量的学习者词典定义，LLM在定义生成和评估中都表现出色，为词典编纂提供了实用解决方案"}}
{"id": "2601.02346", "pdf": "https://arxiv.org/pdf/2601.02346", "abs": "https://arxiv.org/abs/2601.02346", "authors": ["Falcon LLM Team", "Iheb Chaabane", "Puneesh Khanna", "Suhail Mohmad", "Slim Frikha", "Shi Hu", "Abdalgader Abubaker", "Reda Alami", "Mikhail Lubinets", "Mohamed El Amine Seddik", "Hakim Hacid"], "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling", "categories": ["cs.AI"], "comment": null, "summary": "This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\\times$ to $7\\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.", "AI": {"tldr": "Falcon-H1R是一个7B参数的高效推理优化模型，通过精心设计的数据筛选和训练策略，在保持小模型规模的同时实现了与更大模型相媲美的推理性能。", "motivation": "探索如何通过小语言模型（SLMs）实现具有竞争力的推理性能，挑战传统认为需要大模型才能获得优秀推理能力的观念。", "method": "采用参数效率优化设计，结合精心策划的数据集和针对性训练策略（包括高效的监督微调SFT和强化学习RL扩展），使用混合并行架构实现快速推理，并利用DeepConf方法实现最先进的测试时扩展效率。", "result": "在多种推理密集型基准测试中，Falcon-H1R-7B匹配或超越了参数量2-7倍更大的最先进模型，在推理速度、token效率和准确性三个方面都实现了显著提升。", "conclusion": "紧凑模型通过针对性的模型训练和架构选择，能够提供强大且可扩展的推理性能，为需要大量思维链生成和并行测试扩展的场景提供了实用的基础模型。"}}
{"id": "2601.01862", "pdf": "https://arxiv.org/pdf/2601.01862", "abs": "https://arxiv.org/abs/2601.01862", "authors": ["Nuo Chen", "Hanpei Fang", "Piaohong Wang", "Jiqun Liu", "Tetsuya Sakai", "Xiao-Ming Wu"], "title": "Judging with Personality and Confidence: A Study on Personality-Conditioned LLM Relevance Assessment", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Recent studies have shown that prompting can enable large language models (LLMs) to simulate specific personality traits and produce behaviors that align with those traits. However, there is limited understanding of how these simulated personalities influence critical web search decisions, specifically relevance assessment. Moreover, few studies have examined how simulated personalities impact confidence calibration, specifically the tendencies toward overconfidence or underconfidence. This gap exists even though psychological literature suggests these biases are trait-specific, often linking high extraversion to overconfidence and high neuroticism to underconfidence. To address this gap, we conducted a comprehensive study evaluating multiple LLMs, including commercial models and open-source models, prompted to simulate Big Five personality traits. We tested these models across three test collections (TREC DL 2019, TREC DL 2020, and LLMJudge), collecting two key outputs for each query-document pair: a relevance judgment and a self-reported confidence score.\n  The findings show that personalities such as low agreeableness consistently align more closely with human labels than the unprompted condition. Additionally, low conscientiousness performs well in balancing the suppression of both overconfidence and underconfidence. We also observe that relevance scores and confidence distributions vary systematically across different personalities. Based on the above findings, we incorporate personality-conditioned scores and confidence as features in a random forest classifier. This approach achieves performance that surpasses the best single-personality condition on a new dataset (TREC DL 2021), even with limited training data. These findings highlight that personality-derived confidence offers a complementary predictive signal, paving the way for more reliable and human-aligned LLM evaluators.", "AI": {"tldr": "该研究探讨了通过提示让大语言模型模拟不同人格特质如何影响相关性评估和置信度校准，发现低宜人性和低尽责性人格表现更优，并开发了基于人格特征的分类器提升性能。", "motivation": "现有研究对大语言模型模拟人格特质如何影响网络搜索决策（特别是相关性评估）和置信度校准（过度自信或不足自信倾向）的理解有限，尽管心理学文献表明这些偏差具有特质特异性。", "method": "使用多个商业和开源大语言模型，通过提示模拟大五人格特质，在三个测试集（TREC DL 2019、2020和LLMJudge）上测试，收集每个查询-文档对的相关性判断和自报告置信度得分。", "result": "低宜人性格更接近人类标签，低尽责性在抑制过度自信和不足自信方面表现良好；不同人格的相关性得分和置信度分布存在系统性差异；基于人格条件的得分和置信度作为特征的随机森林分类器在新数据集上表现优于最佳单一人格条件。", "conclusion": "人格衍生的置信度提供了补充性预测信号，为开发更可靠、更符合人类评判标准的大语言模型评估器铺平了道路。"}}
{"id": "2601.01868", "pdf": "https://arxiv.org/pdf/2601.01868", "abs": "https://arxiv.org/abs/2601.01868", "authors": ["Jinghan Ru", "Siyuan Yan", "Yuguo Yin", "Yuexian Zou", "Zongyuan Ge"], "title": "DermoGPT: Open Weights and Open Data for Morphology-Grounded Dermatological Reasoning MLLMs", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) show promise for medical applications, yet progress in dermatology lags due to limited training data, narrow task coverage, and lack of clinically-grounded supervision that mirrors expert diagnostic workflows. We present a comprehensive framework to address these gaps. First, we introduce DermoInstruct, a large-scale morphology-anchored instruction corpus comprising 211,243 images and 772,675 trajectories across five task formats, capturing the complete diagnostic pipeline from morphological observation and clinical reasoning to final diagnosis. Second, we establish DermoBench, a rigorous benchmark evaluating 11 tasks across four clinical axes: Morphology, Diagnosis, Reasoning, and Fairness, including a challenging subset of 3,600 expert-verified open-ended instances and human performance baselines. Third, we develop DermoGPT, a dermatology reasoning MLLM trained via supervised fine-tuning followed by our Morphologically-Anchored Visual-Inference-Consistent (MAVIC) reinforcement learning objective, which enforces consistency between visual observations and diagnostic conclusions. At inference, we deploy Confidence-Consistency Test-time adaptation (CCT) for robust predictions. Experiments show DermoGPT significantly outperforms 16 representative baselines across all axes, achieving state-of-the-art performance while substantially narrowing the human-AI gap. DermoInstruct, DermoBench and DermoGPT will be made publicly available at https://github.com/mendicant04/DermoGPT upon acceptance.", "AI": {"tldr": "该论文提出了DermoGPT框架，包括大规模皮肤病指令数据集DermoInstruct、综合基准DermoBench和基于强化学习的皮肤病多模态大模型DermoGPT，显著提升了皮肤病AI诊断性能并缩小了人机差距。", "motivation": "当前多模态大语言模型在皮肤病学应用进展缓慢，主要受限于训练数据不足、任务覆盖范围窄以及缺乏符合临床专家诊断流程的监督信号。", "method": "1) 构建DermoInstruct数据集(21万+图像，77万+轨迹)；2) 建立DermoBench基准(11个任务，4个临床维度)；3) 开发DermoGPT模型，采用监督微调加MAVIC强化学习目标，确保视觉观察与诊断结论一致性；4) 推理时使用CCT测试时适应。", "result": "DermoGPT在16个基线模型中表现最佳，在所有评估维度上都达到最先进性能，显著缩小了人类与AI之间的性能差距。", "conclusion": "该研究为皮肤病AI诊断提供了完整的解决方案，包括数据集、基准和先进模型，为皮肤病学多模态AI应用奠定了重要基础。"}}
{"id": "2601.01885", "pdf": "https://arxiv.org/pdf/2601.01885", "abs": "https://arxiv.org/abs/2601.01885", "authors": ["Yi Yu", "Liuyi Yao", "Yuexiang Xie", "Qingquan Tan", "Jiaqi Feng", "Yaliang Li", "Libing Wu"], "title": "Agentic Memory: Learning Unified Long-Term and Short-Term Memory Management for Large Language Model Agents", "categories": ["cs.CL"], "comment": null, "summary": "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical. Existing methods typically handle long-term memory (LTM) and short-term memory (STM) as separate components, relying on heuristics or auxiliary controllers, which limits adaptability and end-to-end optimization. In this paper, we propose Agentic Memory (AgeMem), a unified framework that integrates LTM and STM management directly into the agent's policy. AgeMem exposes memory operations as tool-based actions, enabling the LLM agent to autonomously decide what and when to store, retrieve, update, summarize, or discard information. To train such unified behaviors, we propose a three-stage progressive reinforcement learning strategy and design a step-wise GRPO to address sparse and discontinuous rewards induced by memory operations. Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "AI": {"tldr": "AgeMem是一个将长短期记忆管理统一集成到LLM智能体策略中的框架，通过工具化内存操作和渐进式强化学习训练，在长程推理任务中显著优于现有方法。", "motivation": "现有方法将长短期记忆作为分离组件处理，依赖启发式或辅助控制器，限制了适应性和端到端优化能力。", "method": "提出Agentic Memory框架，将内存操作作为工具动作暴露给智能体，采用三阶段渐进式强化学习策略和step-wise GRPO算法解决稀疏奖励问题。", "result": "在五个长程基准测试中，AgeMem在多个LLM骨干网络上一致优于强基线方法，实现了更好的任务性能、更高质量的长时记忆和更高效的上下文使用。", "conclusion": "AgeMem通过统一的内存管理和端到端训练，有效解决了LLM智能体在长程推理中的记忆管理挑战，为构建更强大的自主智能体提供了新方向。"}}
{"id": "2601.01896", "pdf": "https://arxiv.org/pdf/2601.01896", "abs": "https://arxiv.org/abs/2601.01896", "authors": ["Jingyu Liu", "Jiaen Lin", "Yong Liu"], "title": "Tackling the Inherent Difficulty of Noise Filtering in RAG", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become a widely adopted approach to enhance Large Language Models (LLMs) by incorporating external knowledge and reducing hallucinations. However, noisy or irrelevant documents are often introduced during RAG, potentially degrading performance and even causing hallucinated outputs. While various methods have been proposed to filter out such noise, we argue that identifying irrelevant information from retrieved content is inherently difficult and limited number of transformer layers can hardly solve this. Consequently, retrievers fail to filter out irrelevant documents entirely. Therefore, LLMs must be robust against such noise, but we demonstrate that standard fine-tuning approaches are often ineffective in enabling the model to selectively utilize relevant information while ignoring irrelevant content due to the structural constraints of attention patterns. To address this, we propose a novel fine-tuning method designed to enhance the model's ability to distinguish between relevant and irrelevant information within retrieved documents. Extensive experiments across multiple benchmarks show that our approach significantly improves the robustness and performance of LLMs.", "AI": {"tldr": "本文提出了一种新的微调方法，旨在增强大语言模型在检索增强生成(RAG)中区分相关与无关信息的能力，解决现有方法难以完全过滤噪声文档的问题。", "motivation": "RAG方法虽然能增强LLMs的外部知识接入，但检索过程中常包含噪声或无关文档，导致性能下降和幻觉输出。现有过滤方法效果有限，且标准微调方法由于注意力模式的结构限制，难以让模型选择性利用相关信息。", "method": "提出一种新颖的微调方法，专门设计用于增强模型在检索文档中区分相关与无关信息的能力。", "result": "在多个基准测试上的广泛实验表明，该方法显著提高了LLMs的鲁棒性和性能。", "conclusion": "该方法有效解决了RAG中噪声文档的问题，提升了模型对检索内容的判别能力，为LLMs在噪声环境下的稳健应用提供了有效解决方案。"}}
{"id": "2601.01964", "pdf": "https://arxiv.org/pdf/2601.01964", "abs": "https://arxiv.org/abs/2601.01964", "authors": ["Tran Sy Bao"], "title": "CSF: Contrastive Semantic Features for Direct Multilingual Sign Language Generation", "categories": ["cs.CL"], "comment": "9 pages, 8 tables, code available at https://github.com/transybao1393/csf-sign-language", "summary": "Sign language translation systems typically require English as an intermediary language, creating barriers for non-English speakers in the global deaf community. We present Canonical Semantic Form (CSF), a language-agnostic semantic representation framework that enables direct translation from any source language to sign language without English mediation. CSF decomposes utterances into nine universal semantic slots: event, intent, time, condition, agent, object, location, purpose, and modifier. A key contribution is our comprehensive condition taxonomy comprising 35 condition types across eight semantic categories, enabling nuanced representation of conditional expressions common in everyday communication. We train a lightweight transformer-based extractor (0.74 MB) that achieves 99.03% average slot extraction accuracy across four typologically diverse languages: English, Vietnamese, Japanese, and French. The model demonstrates particularly strong performance on condition classification (99.4% accuracy) despite the 35-class complexity. With inference latency of 3.02ms on CPU, our approach enables real-time sign language generation in browser-based applications. We release our code, trained models, and multilingual dataset to support further research in accessible sign language technology.", "AI": {"tldr": "提出Canonical Semantic Form (CSF)框架，实现无需英语中介的直接手语翻译，支持多语言到手语的实时转换", "motivation": "传统手语翻译系统需要英语作为中介语言，为全球非英语使用者的聋人社区造成了障碍", "method": "开发语言无关的语义表示框架CSF，将话语分解为9个通用语义槽位，包含35种条件类型的综合分类法，训练轻量级transformer提取器", "result": "在英语、越南语、日语和法语四种语言上达到99.03%的平均槽位提取准确率，条件分类准确率99.4%，CPU推理延迟仅3.02ms", "conclusion": "CSF框架有效解决了多语言到手语翻译的英语中介问题，为浏览器应用的实时手语生成提供了可行方案"}}
{"id": "2601.01972", "pdf": "https://arxiv.org/pdf/2601.01972", "abs": "https://arxiv.org/abs/2601.01972", "authors": ["Alexandre Le Mercier", "Chris Develder", "Thomas Demeester"], "title": "Hidden State Poisoning Attacks against Mamba-based Language Models", "categories": ["cs.CL"], "comment": "17 pages, 4 figures. Submitted to ACL 2026", "summary": "State space models (SSMs) like Mamba offer efficient alternatives to Transformer-based language models, with linear time complexity. Yet, their adversarial robustness remains critically unexplored. This paper studies the phenomenon whereby specific short input phrases induce a partial amnesia effect in such models, by irreversibly overwriting information in their hidden states, referred to as a Hidden State Poisoning Attack (HiSPA). Our benchmark RoBench25 allows evaluating a model's information retrieval capabilities when subject to HiSPAs, and confirms the vulnerability of SSMs against such attacks. Even a recent 52B hybrid SSM-Transformer model from the Jamba family collapses on RoBench25 under optimized HiSPA triggers, whereas pure Transformers do not. We also observe that HiSPA triggers significantly weaken the Jamba model on the popular Open-Prompt-Injections benchmark, unlike pure Transformers. Finally, our interpretability study reveals patterns in Mamba's hidden layers during HiSPAs that could be used to build a HiSPA mitigation system. The full code and data to reproduce the experiments can be found at https://anonymous.4open.science/r/hispa_anonymous-5DB0.", "AI": {"tldr": "该论文揭示了状态空间模型(SSMs)存在隐藏状态中毒攻击(HiSPA)漏洞，特定短输入短语可导致模型隐藏状态被不可逆覆盖，造成部分记忆丧失效应。", "motivation": "状态空间模型如Mamba虽然计算效率高，但其对抗鲁棒性尚未得到充分研究，特别是针对隐藏状态的安全漏洞。", "method": "开发了RoBench25基准测试来评估模型在HiSPA攻击下的信息检索能力，并在Jamba等模型上进行实验验证。", "result": "SSMs对HiSPA攻击高度脆弱，即使是52B参数的混合SSM-Transformer模型也会崩溃，而纯Transformer模型则不受影响。HiSPA触发词还能显著削弱Jamba模型在Open-Prompt-Injections基准上的表现。", "conclusion": "研究揭示了SSMs的安全漏洞，并发现了Mamba隐藏层的攻击模式，为构建HiSPA缓解系统提供了基础。代码和数据已开源。"}}
{"id": "2601.02015", "pdf": "https://arxiv.org/pdf/2601.02015", "abs": "https://arxiv.org/abs/2601.02015", "authors": ["Omar Momen", "Emilie Sitter", "Berenike Herrmann", "Sina Zarrieß"], "title": "Surprisal and Metaphor Novelty: Moderate Correlations and Divergent Scaling Effects", "categories": ["cs.CL", "cs.AI", "cs.IT"], "comment": "to be published at EACL 2026 main conference", "summary": "Novel metaphor comprehension involves complex semantic processes and linguistic creativity, making it an interesting task for studying language models (LMs). This study investigates whether surprisal, a probabilistic measure of predictability in LMs, correlates with different metaphor novelty datasets. We analyse surprisal from 16 LM variants on corpus-based and synthetic metaphor novelty datasets. We explore a cloze-style surprisal method that conditions on full-sentence context. Results show that LMs yield significant moderate correlations with scores/labels of metaphor novelty. We further identify divergent scaling patterns: on corpus-based data, correlation strength decreases with model size (inverse scaling effect), whereas on synthetic data it increases (Quality-Power Hypothesis). We conclude that while surprisal can partially account for annotations of metaphor novelty, it remains a limited metric of linguistic creativity.", "AI": {"tldr": "本研究探讨语言模型的惊奇值(surprisal)与隐喻新颖性评分之间的相关性，发现在语料库数据和合成数据上呈现不同的规模缩放模式，表明惊奇值只能部分解释隐喻新颖性。", "motivation": "新颖隐喻理解涉及复杂的语义过程和语言创造力，是研究语言模型的理想任务，需要探索概率性预测指标与隐喻新颖性的关系。", "method": "使用16种语言模型变体，在基于语料库和合成的隐喻新颖性数据集上分析惊奇值，采用基于完整句子上下文的完形填空式惊奇值计算方法。", "result": "语言模型与隐喻新颖性评分/标签存在显著中等相关性，但在语料库数据上相关性随模型规模增大而减弱(逆向缩放效应)，在合成数据上则增强(质量-能力假说)。", "conclusion": "惊奇值虽然能部分解释隐喻新颖性的标注，但作为语言创造力的度量指标仍存在局限性。"}}
{"id": "2601.02023", "pdf": "https://arxiv.org/pdf/2601.02023", "abs": "https://arxiv.org/abs/2601.02023", "authors": ["Amirali Ebrahimzadeh", "Seyyed M. Salili"], "title": "Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 8 figures, 3 tables", "summary": "Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business.", "AI": {"tldr": "研究发现大型语言模型在长上下文中的信息提取和推理性能不稳定，受信息分布位置和提示方式影响显著，不同模型表现差异大，抗幻觉指令有时反而降低准确性", "motivation": "随着LLM支持更长输入上下文，需要了解它们在大规模信息提取和推理中的可靠性，以及信息分布位置和提示方式对性能的影响", "method": "引入扩展的needle-in-a-haystack基准测试，评估四个生产级模型，分别测试字面提取、逻辑推理和幻觉风险，考虑位置效应和现实证据分布", "result": "更长上下文不一定带来更好性能，当相关信息被稀释或分散时反而有害；不同模型表现差异显著；抗幻觉指令可能使模型过于保守而降低准确性；模型经常难以识别和优先处理相关信息", "conclusion": "有效上下文长度和模型对长上下文的鲁棒性对于LLM在研究和商业中的可靠部署至关重要，许多失败源于上下文利用效率低下"}}
{"id": "2601.02065", "pdf": "https://arxiv.org/pdf/2601.02065", "abs": "https://arxiv.org/abs/2601.02065", "authors": ["Md. Asif Hossain", "Nabil Subhan", "Mantasha Rahman Mahi", "Jannatul Ferdous Nabila"], "title": "Cost-Efficient Cross-Lingual Retrieval-Augmented Generation for Low-Resource Languages: A Case Study in Bengali Agricultural Advisory", "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 3 figures, 1 table", "summary": "Access to reliable agricultural advisory remains limited in many developing regions due to a persistent language barrier: authoritative agricultural manuals are predominantly written in English, while farmers primarily communicate in low-resource local languages such as Bengali. Although recent advances in Large Language Models (LLMs) enable natural language interaction, direct generation in low-resource languages often exhibits poor fluency and factual inconsistency, while cloud-based solutions remain cost-prohibitive. This paper presents a cost-efficient, cross-lingual Retrieval-Augmented Generation (RAG) framework for Bengali agricultural advisory that emphasizes factual grounding and practical deployability. The proposed system adopts a translation-centric architecture in which Bengali user queries are translated into English, enriched through domain-specific keyword injection to align colloquial farmer terminology with scientific nomenclature, and answered via dense vector retrieval over a curated corpus of English agricultural manuals (FAO, IRRI). The generated English response is subsequently translated back into Bengali to ensure accessibility. The system is implemented entirely using open-source models and operates on consumer-grade hardware without reliance on paid APIs. Experimental evaluation demonstrates reliable source-grounded responses, robust rejection of out-of-domain queries, and an average end-to-end latency below 20 seconds. The results indicate that cross-lingual retrieval combined with controlled translation offers a practical and scalable solution for agricultural knowledge access in low-resource language settings", "AI": {"tldr": "提出基于翻译的跨语言检索增强生成框架，为孟加拉语农业咨询提供低成本、本地化的解决方案，通过英语检索确保事实准确性，平均延迟低于20秒。", "motivation": "发展中国家农业咨询存在语言障碍，权威农业手册多为英文，而农民使用低资源本地语言（如孟加拉语）。现有LLM直接生成低资源语言存在流畅性和事实一致性问题，云端解决方案成本高昂。", "method": "采用翻译为中心的架构：将孟加拉语查询翻译为英文，通过领域关键词注入对齐农民术语与科学术语，在英文农业手册语料库中进行密集向量检索，生成英文回答后再翻译回孟加拉语。完全使用开源模型，在消费级硬件上运行。", "result": "实验评估显示系统能提供可靠的事实依据回答，有效拒绝领域外查询，平均端到端延迟低于20秒。", "conclusion": "跨语言检索结合受控翻译为低资源语言环境下的农业知识获取提供了实用且可扩展的解决方案。"}}
{"id": "2601.02076", "pdf": "https://arxiv.org/pdf/2601.02076", "abs": "https://arxiv.org/abs/2601.02076", "authors": ["Yingte Shu", "Yuchuan Tian", "Chao Xu", "Yunhe Wang", "Hanting Chen"], "title": "Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding confidence and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a confidence-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. This design enables effective bidirectional information flow within the decoding window without sacrificing efficiency. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.39% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 9.0%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding.", "AI": {"tldr": "提出Deferred Commitment Decoding (DCD)方法，通过基于置信度的滑动窗口机制解决扩散语言模型块解码中的边界上下文截断问题，提高解码质量和效率。", "motivation": "现有基于块的扩散语言模型解码存在边界诱导上下文截断(BICT)问题，未解码的边界标记因缺乏未来上下文信息而被迫提前确定，降低了解码置信度和生成质量，特别是在需要精确推理的任务中。", "method": "提出训练无关的解码策略DCD，维护一个基于置信度的滑动窗口，早期解析低不确定性的标记，同时推迟高不确定性标记直到获得足够的上下文证据，实现有效的双向信息流动。", "result": "在多个扩散语言模型、基准测试和缓存配置上的实验显示，DCD相比固定块方法平均提高生成准确率1.39%，最大提升达9.0%，同时保持相当的时间效率。", "conclusion": "基于不确定性推迟标记确定的简单原则能有效提升扩散语言模型解码的质量和效率，DCD方法为解决BICT问题提供了有效解决方案。"}}
{"id": "2601.02123", "pdf": "https://arxiv.org/pdf/2601.02123", "abs": "https://arxiv.org/abs/2601.02123", "authors": ["Po-Jen Ko", "Chen-Han Tsai", "Yu-Shao Peng"], "title": "DeCode: Decoupling Content and Delivery for Medical QA", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Large language models (LLMs) exhibit strong medical knowledge and can generate factually accurate responses. However, existing models often fail to account for individual patient contexts, producing answers that are clinically correct yet poorly aligned with patients' needs. In this work, we introduce DeCode, a training-free, model-agnostic framework that adapts existing LLMs to produce contextualized answers in clinical settings. We evaluate DeCode on OpenAI HealthBench, a comprehensive and challenging benchmark designed to assess clinical relevance and validity of LLM responses. DeCode improves the previous state of the art from $28.4\\%$ to $49.8\\%$, corresponding to a $75\\%$ relative improvement. Experimental results suggest the effectiveness of DeCode in improving clinical question answering of LLMs.", "AI": {"tldr": "DeCode是一个无需训练、模型无关的框架，通过适应现有LLM在临床环境中生成情境化答案，在OpenAI HealthBench基准测试中相对提升75%的性能。", "motivation": "现有大型语言模型虽然具备医学知识，但往往忽略患者个体情境，产生临床正确但与患者需求不匹配的回答。", "method": "提出DeCode框架，无需额外训练，可适配现有LLM模型，使其能够生成更具临床情境化的回答。", "result": "在OpenAI HealthBench基准测试中，将之前的最佳性能从28.4%提升到49.8%，相对提升75%。", "conclusion": "DeCode能有效提升LLM在临床问答中的表现，证明其在改善医疗AI应用方面的有效性。"}}
{"id": "2601.02128", "pdf": "https://arxiv.org/pdf/2601.02128", "abs": "https://arxiv.org/abs/2601.02128", "authors": ["Steffen Freisinger", "Philipp Seeberger", "Thomas Ranzenberger", "Tobias Bocklet", "Korbinian Riedhammer"], "title": "Towards Multi-Level Transcript Segmentation: LoRA Fine-Tuning for Table-of-Contents Generation", "categories": ["cs.CL", "eess.AS"], "comment": "Published in Proceedings of Interspeech 2025. Please cite the proceedings version (DOI: 10.21437/Interspeech.2025-2792)", "summary": "Segmenting speech transcripts into thematic sections benefits both downstream processing and users who depend on written text for accessibility. We introduce a novel approach to hierarchical topic segmentation in transcripts, generating multi-level tables of contents that capture both topic and subtopic boundaries. We compare zero-shot prompting and LoRA fine-tuning on large language models, while also exploring the integration of high-level speech pause features. Evaluations on English meeting recordings and multilingual lecture transcripts (Portuguese, German) show significant improvements over established topic segmentation baselines. Additionally, we adapt a common evaluation measure for multi-level segmentation, taking into account all hierarchical levels within one metric.", "AI": {"tldr": "本文提出了一种新颖的层次化主题分割方法，用于语音转录文本的多级目录生成，通过零样本提示和LoRA微调LLM，结合语音停顿特征，在英文会议和多语言讲座转录上取得了显著改进", "motivation": "将语音转录文本按主题分割有利于下游处理和依赖文本可访问性的用户，需要捕捉主题和子主题边界的多级目录生成", "method": "比较零样本提示和LoRA微调大型语言模型，探索整合高级语音停顿特征，提出多级分割评估指标", "result": "在英文会议录音和多语言（葡萄牙语、德语）讲座转录上相比现有基线方法取得了显著改进", "conclusion": "该方法在层次化主题分割任务中表现优异，提出的多级评估指标能全面考虑所有层次结构"}}
{"id": "2601.02144", "pdf": "https://arxiv.org/pdf/2601.02144", "abs": "https://arxiv.org/abs/2601.02144", "authors": ["Boxuan Lyu", "Soichiro Murakami", "Hidetaka Kamigaito", "Peinan Zhang"], "title": "Routing by Analogy: kNN-Augmented Expert Assignment for Mixture-of-Experts", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Mixture-of-Experts (MoE) architectures scale large language models efficiently by employing a parametric \"router\" to dispatch tokens to a sparse subset of experts. Typically, this router is trained once and then frozen, rendering routing decisions brittle under distribution shifts. We address this limitation by introducing kNN-MoE, a retrieval-augmented routing framework that reuses optimal expert assignments from a memory of similar past cases. This memory is constructed offline by directly optimizing token-wise routing logits to maximize the likelihood on a reference set. Crucially, we use the aggregate similarity of retrieved neighbors as a confidence-driven mixing coefficient, thus allowing the method to fall back to the frozen router when no relevant cases are found. Experiments show kNN-MoE outperforms zero-shot baselines and rivals computationally expensive supervised fine-tuning.", "AI": {"tldr": "kNN-MoE是一个检索增强的路由框架，通过重用历史最优专家分配来改进传统MoE架构中路由决策在分布偏移下的脆弱性问题", "motivation": "传统MoE架构中的参数化路由器通常训练后冻结，导致在分布偏移时路由决策变得脆弱，需要一种更灵活的路由机制", "method": "引入基于k近邻的检索增强路由框架，通过离线构建记忆库存储历史最优专家分配，使用检索到的相似案例的聚合相似度作为置信度驱动的混合系数", "result": "实验显示kNN-MoE在零样本基准测试中表现优于基线方法，与计算成本昂贵的监督微调方法性能相当", "conclusion": "kNN-MoE提供了一种有效的方法来增强MoE架构的鲁棒性，通过检索历史最优路由决策来处理分布偏移问题，同时保持计算效率"}}
{"id": "2601.02158", "pdf": "https://arxiv.org/pdf/2601.02158", "abs": "https://arxiv.org/abs/2601.02158", "authors": ["Almaz Ermilov"], "title": "FormationEval, an open multiple-choice benchmark for petroleum geoscience", "categories": ["cs.CL", "cs.AI", "cs.LG", "physics.geo-ph"], "comment": "24 pages, 8 figures, 10 tables; benchmark and code at https://github.com/AlmazErmilov/FormationEval-an-Open-Benchmark-for-Oil-Gas-Geoscience-MCQ-Evaluation", "summary": "This paper presents FormationEval, an open multiple-choice question benchmark for evaluating language models on petroleum geoscience and subsurface disciplines. The dataset contains 505 questions across seven domains including petrophysics, petroleum geology and reservoir engineering, derived from three authoritative sources using a reasoning model with detailed instructions and a concept-based approach that avoids verbatim copying of copyrighted text. Each question includes source metadata to support traceability and audit. The evaluation covers 72 models from major providers including OpenAI, Anthropic, Google, Meta and open-weight alternatives. The top performers achieve over 97\\% accuracy, with Gemini 3 Pro Preview reaching 99.8\\%, while tier and domain gaps persist. Among open-weight models, GLM-4.7 leads at 98.6\\%, with several DeepSeek, Llama, Qwen and Mistral models also exceeding 93\\%. The performance gap between open-weight and closed models is narrower than expected, with several lower-cost open-weight models exceeding 90\\% accuracy. Petrophysics emerges as the most challenging domain across all models, while smaller models show wider performance variance. Residual length bias in the dataset (correct answers tend to be longer) is documented along with bias mitigation strategies applied during construction. The benchmark, evaluation code and results are publicly available.", "AI": {"tldr": "FormationEval是一个针对石油地质科学和地下学科的多选题评估基准，包含505个问题，评估了72个语言模型，发现开源模型与闭源模型性能差距小于预期，其中Gemini 3 Pro Preview表现最佳(99.8%)，GLM-4.7在开源模型中领先(98.6%)。", "motivation": "创建专门的评估基准来测试语言模型在石油地质科学和地下学科领域的专业知识和推理能力，填补该领域缺乏标准化评估工具的空白。", "method": "从三个权威来源构建505个问题，涵盖7个领域，使用推理模型和基于概念的方法避免版权文本的逐字复制，每个问题包含来源元数据以确保可追溯性。", "result": "顶级模型准确率超过97%，Gemini 3 Pro Preview达到99.8%；开源模型中GLM-4.7以98.6%领先；开源与闭源模型性能差距较小；岩石物理学是最具挑战性的领域；存在答案长度偏差但已应用缓解策略。", "conclusion": "FormationEval为石油地质科学领域提供了有效的评估工具，表明开源模型在该专业领域表现出色，性能接近闭源模型，为行业提供了经济高效的AI解决方案选择。"}}
{"id": "2601.02179", "pdf": "https://arxiv.org/pdf/2601.02179", "abs": "https://arxiv.org/abs/2601.02179", "authors": ["Caiqi Zhang", "Ruihan Yang", "Xiaochen Zhu", "Chengzu Li", "Tiancheng Hu", "Yijiang River Dong", "Deqing Yang", "Nigel Collier"], "title": "Confidence Estimation for LLMs in Multi-turn Interactions", "categories": ["cs.CL"], "comment": null, "summary": "While confidence estimation is a promising direction for mitigating hallucinations in Large Language Models (LLMs), current research dominantly focuses on single-turn settings. The dynamics of model confidence in multi-turn conversations, where context accumulates and ambiguity is progressively resolved, remain largely unexplored. Reliable confidence estimation in multi-turn settings is critical for many downstream applications, such as autonomous agents and human-in-the-loop systems. This work presents the first systematic study of confidence estimation in multi-turn interactions, establishing a formal evaluation framework grounded in two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To facilitate this, we introduce novel metrics, including a length-normalized Expected Calibration Error (InfoECE), and a new \"Hinter-Guesser\" paradigm for generating controlled evaluation datasets. Our experiments reveal that widely-used confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. We propose P(Sufficient), a logit-based probe that achieves comparatively better performance, although the task remains far from solved. Our work provides a foundational methodology for developing more reliable and trustworthy conversational agents.", "AI": {"tldr": "该研究首次系统性地探索了多轮对话中的置信度估计问题，提出了基于每轮校准和信息增加时置信度单调性的评估框架，并开发了新指标InfoECE和Hinter-Guesser评估范式，发现现有方法在多轮对话中表现不佳，提出了P(Sufficient)方法取得相对更好的性能。", "motivation": "当前置信度估计研究主要集中在单轮设置，而多轮对话中上下文积累和歧义逐步解决的动态特性尚未被充分探索，这对于自主代理和人机协作系统等下游应用至关重要。", "method": "建立了基于两个关键需求的正式评估框架：每轮校准和信息增加时的置信度单调性。引入了新指标InfoECE（长度归一化预期校准误差）和Hinter-Guesser范式生成受控评估数据集。提出了基于logit的探针方法P(Sufficient)。", "result": "实验表明广泛使用的置信度技术在多轮对话中难以实现校准和单调性。提出的P(Sufficient)方法取得了相对更好的性能，但该问题远未完全解决。", "conclusion": "本研究为开发更可靠和可信的对话代理提供了基础方法论，虽然多轮置信度估计问题仍然具有挑战性，但提出的框架和方法为后续研究奠定了基础。"}}
{"id": "2601.02186", "pdf": "https://arxiv.org/pdf/2601.02186", "abs": "https://arxiv.org/abs/2601.02186", "authors": ["Rui Yang", "Huitao Li", "Weihao Xuan", "Heli Qi", "Xin Li", "Kunyu Yu", "Yingjian Chen", "Rongrong Wang", "Jacques Behmoaras", "Tianxi Cai", "Bibhas Chakraborty", "Qingyu Chen", "Lionel Tim-Ee Cheng", "Marie-Louise Damwanza", "Chido Dzinotyiwei", "Aosong Feng", "Chuan Hong", "Yusuke Iwasawa", "Yuhe Ke", "Linah Kitala", "Taehoon Ko", "Jisan Lee", "Irene Li", "Jonathan Chong Kai Liew", "Hongfang Liu", "Lian Leng Low", "Edison Marrese-Taylor", "Yutaka Matsuo", "Isheanesu Misi", "Yilin Ning", "Jasmine Chiat Ling Ong", "Marcus Eng Hock Ong", "Enrico Petretto", "Hossein Rouhizadeh", "Abiram Sandralegar", "Oren Schreier", "Iain Bee Huat Tan", "Patrick Tan", "Daniel Shu Wei Ting", "Junjue Wang", "Chunhua Weng", "Matthew Yu Heng Wong", "Fang Wu", "Yunze Xiao", "Xuhai Xu", "Qingcheng Zeng", "Zhuo Zheng", "Yifan Peng", "Douglas Teodoro", "Nan Liu"], "title": "Toward Global Large Language Models in Medicine", "categories": ["cs.CL"], "comment": "182 pages, 65 figures", "summary": "Despite continuous advances in medical technology, the global distribution of health care resources remains uneven. The development of large language models (LLMs) has transformed the landscape of medicine and holds promise for improving health care quality and expanding access to medical information globally. However, existing LLMs are primarily trained on high-resource languages, limiting their applicability in global medical scenarios. To address this gap, we constructed GlobMed, a large multilingual medical dataset, containing over 500,000 entries spanning 12 languages, including four low-resource languages. Building on this, we established GlobMed-Bench, which systematically assesses 56 state-of-the-art proprietary and open-weight LLMs across multiple multilingual medical tasks, revealing significant performance disparities across languages, particularly for low-resource languages. Additionally, we introduced GlobMed-LLMs, a suite of multilingual medical LLMs trained on GlobMed, with parameters ranging from 1.7B to 8B. GlobMed-LLMs achieved an average performance improvement of over 40% relative to baseline models, with a more than threefold increase in performance on low-resource languages. Together, these resources provide an important foundation for advancing the equitable development and application of LLMs globally, enabling broader language communities to benefit from technological advances.", "AI": {"tldr": "该研究构建了GlobMed多语言医疗数据集和基准测试，开发了GlobMed-LLMs模型，显著提升了低资源语言的医疗AI性能，促进了全球医疗AI的公平发展。", "motivation": "现有大型语言模型主要基于高资源语言训练，在全球医疗场景中适用性有限，特别是在低资源语言方面存在显著差距。", "method": "构建包含50万条条目、覆盖12种语言（含4种低资源语言）的多语言医疗数据集GlobMed；建立GlobMed-Bench基准测试系统评估56个先进LLM；开发参数规模1.7B到8B的GlobMed-LLMs多语言医疗模型。", "result": "GlobMed-LLMs相比基线模型平均性能提升超过40%，在低资源语言上的性能提升超过三倍；基准测试显示不同语言间存在显著性能差距。", "conclusion": "这些资源为推进LLM在全球的公平发展和应用提供了重要基础，使更广泛的语言社区能够从技术进步中受益。"}}
{"id": "2601.02209", "pdf": "https://arxiv.org/pdf/2601.02209", "abs": "https://arxiv.org/abs/2601.02209", "authors": ["Omer Nacar", "Serry Sibaee", "Adel Ammar", "Yasser Alhabashi", "Nadia Samer Sibai", "Yara Farouk Ahmed", "Ahmed Saud Alqusaiyer", "Sulieman Mahmoud AlMahmoud", "Abdulrhman Mamdoh Mukhaniq", "Lubaba Raed", "Sulaiman Mohammed Alatwah", "Waad Nasser Alqahtani", "Yousif Abdulmajeed Alnasser", "Mohamed Aziz Khadraoui", "Wadii Boulila"], "title": "ARCADE: A City-Scale Corpus for Fine-Grained Arabic Dialect Tagging", "categories": ["cs.CL", "cs.CY", "cs.SD"], "comment": null, "summary": "The Arabic language is characterized by a rich tapestry of regional dialects that differ substantially in phonetics and lexicon, reflecting the geographic and cultural diversity of its speakers. Despite the availability of many multi-dialect datasets, mapping speech to fine-grained dialect sources, such as cities, remains underexplored. We present ARCADE (Arabic Radio Corpus for Audio Dialect Evaluation), the first Arabic speech dataset designed explicitly with city-level dialect granularity. The corpus comprises Arabic radio speech collected from streaming services across the Arab world. Our data pipeline captures 30-second segments from verified radio streams, encompassing both Modern Standard Arabic (MSA) and diverse dialectal speech. To ensure reliability, each clip was annotated by one to three native Arabic reviewers who assigned rich metadata, including emotion, speech type, dialect category, and a validity flag for dialect identification tasks. The resulting corpus comprises 6,907 annotations and 3,790 unique audio segments spanning 58 cities across 19 countries. These fine-grained annotations enable robust multi-task learning, serving as a benchmark for city-level dialect tagging. We detail the data collection methodology, assess audio quality, and provide a comprehensive analysis of label distributions. The dataset is available on: https://huggingface.co/datasets/riotu-lab/ARCADE-full", "AI": {"tldr": "ARCADE是首个城市级方言粒度的阿拉伯语语音数据集，包含来自19个国家58个城市的3790个音频片段，带有丰富的方言标注，支持多任务学习。", "motivation": "阿拉伯语方言在语音和词汇上存在显著差异，但现有数据集缺乏城市级别的方言粒度标注，限制了方言识别研究的发展。", "method": "从阿拉伯世界流媒体服务收集广播语音，截取30秒片段，由1-3名阿拉伯语母语评审员标注情感、语音类型、方言类别和有效性标签。", "result": "构建了包含6907个标注和3790个独特音频片段的数据集，涵盖现代标准阿拉伯语和多种方言，提供详细的标签分布分析。", "conclusion": "ARCADE数据集为城市级方言标注提供了可靠基准，支持阿拉伯语方言识别和多任务学习研究的进一步发展。"}}
{"id": "2601.02224", "pdf": "https://arxiv.org/pdf/2601.02224", "abs": "https://arxiv.org/abs/2601.02224", "authors": ["Fabian Lukassen", "Jan Herrmann", "Christoph Weisser", "Benjamin Saefken", "Thomas Kneib"], "title": "From XAI to Stories: A Factorial Study of LLM-Generated Explanation Quality", "categories": ["cs.CL"], "comment": null, "summary": "Explainable AI (XAI) methods like SHAP and LIME produce numerical feature attributions that remain inaccessible to non expert users. Prior work has shown that Large Language Models (LLMs) can transform these outputs into natural language explanations (NLEs), but it remains unclear which factors contribute to high-quality explanations. We present a systematic factorial study investigating how Forecasting model choice, XAI method, LLM selection, and prompting strategy affect NLE quality. Our design spans four models (XGBoost (XGB), Random Forest (RF), Multilayer Perceptron (MLP), and SARIMAX - comparing black-box Machine-Learning (ML) against classical time-series approaches), three XAI conditions (SHAP, LIME, and a no-XAI baseline), three LLMs (GPT-4o, Llama-3-8B, DeepSeek-R1), and eight prompting strategies. Using G-Eval, an LLM-as-a-judge evaluation method, with dual LLM judges and four evaluation criteria, we evaluate 660 explanations for time-series forecasting. Our results suggest that: (1) XAI provides only small improvements over no-XAI baselines, and only for expert audiences; (2) LLM choice dominates all other factors, with DeepSeek-R1 outperforming GPT-4o and Llama-3; (3) we observe an interpretability paradox: in our setting, SARIMAX yielded lower NLE quality than ML models despite higher prediction accuracy; (4) zero-shot prompting is competitive with self-consistency at 7-times lower cost; and (5) chain-of-thought hurts rather than helps.", "AI": {"tldr": "本研究通过系统性因子实验发现，在时间序列预测的可解释AI中：LLM选择对解释质量影响最大（DeepSeek-R1最优），XAI方法仅对专家用户有轻微改善，SARIMAX模型存在可解释性悖论，零样本提示与高成本方法效果相当但成本更低，思维链提示反而有害。", "motivation": "现有XAI方法（如SHAP、LIME）生成的数值特征归因对非专家用户不友好，虽然LLM可以将其转换为自然语言解释，但影响解释质量的关键因素尚不明确。", "method": "采用4因子实验设计：4种预测模型（XGBoost、Random Forest、MLP、SARIMAX）、3种XAI条件（SHAP、LIME、无XAI基线）、3种LLM（GPT-4o、Llama-3-8B、DeepSeek-R1）、8种提示策略。使用G-Eval方法（双LLM评委）对660个时间序列预测解释进行4标准评估。", "result": "1）XAI仅对专家用户有轻微改善；2）LLM选择是主导因素，DeepSeek-R1优于GPT-4o和Llama-3；3）SARIMAX预测精度更高但解释质量更低（可解释性悖论）；4）零样本提示与自一致性方法效果相当但成本低7倍；5）思维链提示有害无益。", "conclusion": "LLM选择是影响XAI解释质量的最关键因素，XAI方法本身的价值有限且主要针对专家用户，模型选择存在准确性与可解释性的权衡，提示工程策略需要谨慎选择以避免不必要的成本或负面效果。"}}
{"id": "2601.02236", "pdf": "https://arxiv.org/pdf/2601.02236", "abs": "https://arxiv.org/abs/2601.02236", "authors": ["Yihao Liang", "Ze Wang", "Hao Chen", "Ximeng Sun", "Jialian Wu", "Xiaodong Yu", "Jiang Liu", "Emad Barsoum", "Zicheng Liu", "Niraj K. Jha"], "title": "CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models", "categories": ["cs.CL"], "comment": "33 pages, 7 figures", "summary": "Autoregressive large language models achieve strong results on many benchmarks, but decoding remains fundamentally latency-limited by sequential dependence on previously generated tokens. Diffusion language models (DLMs) promise parallel generation but suffer from a fundamental static-to-dynamic misalignment: Training optimizes local transitions under fixed schedules, whereas efficient inference requires adaptive \"long-jump\" refinements through unseen states. Our goal is to enable highly parallel decoding for DLMs with low number of function evaluations while preserving generation quality. To achieve this, we propose CD4LM, a framework that decouples training from inference via Discrete-Space Consistency Distillation (DSCD) and Confidence-Adaptive Decoding (CAD). Unlike standard objectives, DSCD trains a student to be trajectory-invariant, mapping diverse noisy states directly to the clean distribution. This intrinsic robustness enables CAD to dynamically allocate compute resources based on token confidence, aggressively skipping steps without the quality collapse typical of heuristic acceleration. On GSM8K, CD4LM matches the LLaDA baseline with a 5.18x wall-clock speedup; across code and math benchmarks, it strictly dominates the accuracy-efficiency Pareto frontier, achieving a 3.62x mean speedup while improving average accuracy. Code is available at https://github.com/yihao-liang/CDLM", "AI": {"tldr": "CD4LM框架通过离散空间一致性蒸馏和置信度自适应解码，解决了扩散语言模型在并行生成中的训练-推理错配问题，实现了5.18倍加速的同时保持生成质量", "motivation": "自回归语言模型受限于顺序生成的延迟，扩散语言模型虽然支持并行生成，但存在训练固定调度与推理需要自适应长跳优化之间的根本性错配", "method": "提出CD4LM框架，包含离散空间一致性蒸馏(DSCD)训练学生对噪声状态到干净分布的轨迹不变映射，以及置信度自适应解码(CAD)基于token置信度动态分配计算资源", "result": "在GSM8K上达到5.18倍加速且匹配基线性能；在代码和数学基准测试中平均加速3.62倍的同时提升准确率，主导精度-效率帕累托前沿", "conclusion": "CD4LM成功解决了扩散语言模型的训练-推理错配问题，实现了高质量并行生成，为高效语言模型解码提供了新方向"}}
{"id": "2601.02285", "pdf": "https://arxiv.org/pdf/2601.02285", "abs": "https://arxiv.org/abs/2601.02285", "authors": ["Tobias Schimanski", "Imene Kolli", "Jingwei Ni", "Yu Fan", "Ario Saeid Vaghefi", "Elliott Ash", "Markus Leippold"], "title": "pdfQA: Diverse, Challenging, and Realistic Question Answering over PDFs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "PDFs are the second-most used document type on the internet (after HTML). Yet, existing QA datasets commonly start from text sources or only address specific domains. In this paper, we present pdfQA, a multi-domain 2K human-annotated (real-pdfQA) and 2K synthetic dataset (syn-pdfQA) differentiating QA pairs in ten complexity dimensions (e.g., file type, source modality, source position, answer type). We apply and evaluate quality and difficulty filters on both datasets, obtaining valid and challenging QA pairs. We answer the questions with open-source LLMs, revealing existing challenges that correlate with our complexity dimensions. pdfQA presents a basis for end-to-end QA pipeline evaluation, testing diverse skill sets and local optimizations (e.g., in information retrieval or parsing).", "AI": {"tldr": "pdfQA是一个多领域PDF问答数据集，包含2000个人工标注和2000个合成样本，涵盖10个复杂度维度，用于评估端到端问答系统的性能。", "motivation": "PDF是互联网上第二常用的文档类型，但现有问答数据集主要基于文本来源或局限于特定领域，缺乏针对PDF文档的全面问答评估基准。", "method": "构建包含人工标注和合成数据的多领域数据集，应用质量和难度筛选，使用开源大语言模型回答问题，并分析复杂度维度与挑战的关联。", "result": "创建了有效的挑战性问答对，揭示了现有模型在不同复杂度维度（如文件类型、来源模态、位置等）上面临的具体挑战。", "conclusion": "pdfQA为端到端问答管道评估提供了基础，能够测试多样化技能集和局部优化（如信息检索和解析），填补了PDF文档问答评估的空白。"}}
{"id": "2601.02298", "pdf": "https://arxiv.org/pdf/2601.02298", "abs": "https://arxiv.org/abs/2601.02298", "authors": ["Mahmoud Elgenedy"], "title": "Power-of-Two Quantization-Aware-Training (PoT-QAT) in Large Language Models (LLMs)", "categories": ["cs.CL", "eess.SP"], "comment": null, "summary": "In Large Language Models (LLMs), the number of parameters has grown exponentially in the past few years, e.g., from 1.5 billion parameters in GPT-2 to 175 billion in GPT-3 to possibly more than trillion in higher versions. This raises a significant challenge for implementation, especially for Edge devices. Unlike cloud computing, memory and processing power for Edge devices are very limited, which necessitates developing novel ideas to make such applications feasible. In this work, we investigate compressing weights with a special quantization that limits numbers to only power-of-two (PoT). This helps save a huge amount of memory as only exponents need to be stored, more importantly, it significantly reduces processing power by replacing costly multiplication with low cost bit shifting. To overcome performance loss due to this strict quantization, we investigate Quantization Aware Training (QAT) to enhance performance through additional training. Results on GPT-2 124M show a major enhancement for quantized PoT model after additional training, with a perplexity enhancement of 66% and BERT-Score loss to baseline GPT-2 of 1%. The memory saving is estimated to be 87.5% while the inference speed is expected to be 3-10x faster with PoT quantization versus full-precision.", "AI": {"tldr": "该论文研究使用2的幂次方量化(PoT)压缩大语言模型权重，通过量化感知训练提升性能，在GPT-2 124M模型上实现了87.5%的内存节省和3-10倍的推理加速，性能损失仅为1%。", "motivation": "大语言模型参数量指数级增长，边缘设备内存和处理能力有限，需要开发新的压缩方法来实现边缘部署。", "method": "采用2的幂次方量化方法，将权重限制为2的幂次方值，用指数存储代替完整数值，用位运算替代乘法运算，并通过量化感知训练提升量化模型性能。", "result": "在GPT-2 124M模型上，量化后困惑度提升66%，BERT-Score损失仅为1%，内存节省87.5%，推理速度提升3-10倍。", "conclusion": "2的幂次方量化结合量化感知训练是有效的模型压缩方法，能在保持性能的同时显著减少内存占用和计算开销，适用于边缘设备部署。"}}
{"id": "2601.02303", "pdf": "https://arxiv.org/pdf/2601.02303", "abs": "https://arxiv.org/abs/2601.02303", "authors": ["Juan-José Guzmán-Landa", "Juan-Manuel Torres-Moreno", "Miguel Figueroa-Saavedra", "Carlos-Emiliano González-Gallardo", "Graham Ranger", "Martha Lorena-Avendaño-Garrido"], "title": "Classifying several dialectal Nawatl varieties", "categories": ["cs.CL"], "comment": "9 pages, 5 figures, 4 tables", "summary": "Mexico is a country with a large number of indigenous languages, among which the most widely spoken is Nawatl, with more than two million people currently speaking it (mainly in North and Central America). Despite its rich cultural heritage, which dates back to the 15th century, Nawatl is a language with few computer resources. The problem is compounded when it comes to its dialectal varieties, with approximately 30 varieties recognised, not counting the different spellings in the written forms of the language. In this research work, we addressed the problem of classifying Nawatl varieties using Machine Learning and Neural Networks.", "AI": {"tldr": "使用机器学习和神经网络对Nawatl方言变体进行分类的研究", "motivation": "Nawatl作为墨西哥使用最广泛的土著语言（超过200万人使用），虽然有丰富的文化遗产，但计算机资源匮乏，特别是其约30种方言变体和不同拼写形式使得处理更加困难", "method": "使用机器学习和神经网络方法", "result": "论文未在摘要中明确说明具体结果", "conclusion": "研究旨在解决Nawatl方言变体分类的问题，为该语言的计算机处理提供支持"}}
{"id": "2601.02320", "pdf": "https://arxiv.org/pdf/2601.02320", "abs": "https://arxiv.org/abs/2601.02320", "authors": ["Nikolay Mikhaylovskiy"], "title": "Estimating Text Temperature", "categories": ["cs.CL"], "comment": null, "summary": "Autoregressive language models typically use temperature parameter at inference to shape the probability distribution and control the randomness of the text generated. After the text was generated, this parameter can be estimated using maximum likelihood approach. Following it, we propose a procedure to estimate the temperature of any text, including ones written by humans, with respect to a given language model. We evaluate the temperature estimation capability of a wide selection of small-to-medium LLMs. We then use the best-performing Qwen3 14B to estimate temperatures of popular corpora.", "AI": {"tldr": "提出了一种估计文本温度参数的方法，可用于任何文本（包括人类写作），并评估了多个LLM的温度估计能力，最终使用Qwen3 14B模型分析了流行语料库的温度特征。", "motivation": "自回归语言模型在推理时使用温度参数控制生成文本的随机性，但现有方法只能对生成的文本进行温度估计，无法应用于人类写作的文本。", "method": "提出基于最大似然估计的温度参数估计方法，评估了多个小型到中型LLM的温度估计能力，并选择性能最佳的Qwen3 14B模型进行语料库分析。", "result": "成功开发了通用的文本温度估计方法，Qwen3 14B在温度估计任务中表现最佳，并获得了流行语料库的温度特征数据。", "conclusion": "该方法能够有效估计任何文本相对于给定语言模型的温度参数，为文本分析和语言模型研究提供了新的工具和视角。"}}
{"id": "2601.02337", "pdf": "https://arxiv.org/pdf/2601.02337", "abs": "https://arxiv.org/abs/2601.02337", "authors": ["Berk Atil", "Rebecca J. Passonneau", "Ninareh Mehrabi"], "title": "Robust Persona-Aware Toxicity Detection with Prompt Optimization and Learned Ensembling", "categories": ["cs.CL"], "comment": null, "summary": "Toxicity detection is inherently subjective, shaped by the diverse perspectives and social priors of different demographic groups. While ``pluralistic'' modeling as used in economics and the social sciences aims to capture perspective differences across contexts, current Large Language Model (LLM) prompting techniques have different results across different personas and base models. In this work, we conduct a systematic evaluation of persona-aware toxicity detection, showing that no single prompting method, including our proposed automated prompt optimization strategy, uniformly dominates across all model-persona pairs. To exploit complementary errors, we explore ensembling four prompting variants and propose a lightweight meta-ensemble: an SVM over the 4-bit vector of prompt predictions. Our results demonstrate that the proposed SVM ensemble consistently outperforms individual prompting methods and traditional majority-voting techniques, achieving the strongest overall performance across diverse personas. This work provides one of the first systematic comparisons of persona-conditioned prompting for toxicity detection and offers a robust method for pluralistic evaluation in subjective NLP tasks.", "AI": {"tldr": "该论文系统评估了基于人物角色的毒性检测方法，发现没有单一提示方法在所有模型-人物对中都表现最优，提出了一种轻量级SVM元集成方法，在多样化人物角色中实现了最强的整体性能。", "motivation": "毒性检测具有主观性，受不同人口统计群体的多元视角和社会先验影响。当前LLM提示技术在不同人物角色和基础模型间结果不一致，需要系统评估人物角色感知的毒性检测方法。", "method": "1) 系统评估人物角色感知毒性检测；2) 提出自动化提示优化策略；3) 探索集成四种提示变体；4) 提出轻量级SVM元集成方法，基于4位提示预测向量构建分类器。", "result": "SVM集成方法持续优于单个提示方法和传统多数投票技术，在多样化人物角色中实现了最强的整体性能。没有单一提示方法在所有模型-人物对中都表现最优。", "conclusion": "这项工作为人物角色条件提示的毒性检测提供了首批系统比较，为主观NLP任务中的多元评估提供了一种稳健方法，SVM元集成是处理毒性检测主观性的有效解决方案。"}}
