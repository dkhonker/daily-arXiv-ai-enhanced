{"id": "2505.14689", "pdf": "https://arxiv.org/pdf/2505.14689", "abs": "https://arxiv.org/abs/2505.14689", "authors": ["Ashwani Anand", "Satya Prakash Nayak", "Ritam Raha", "Anne-Kathrin Schmuck"], "title": "Follow the STARs: Dynamic $ω$-Regular Shielding of Learned Policies", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "This paper presents a novel dynamic post-shielding framework that enforces\nthe full class of $\\omega$-regular correctness properties over pre-computed\nprobabilistic policies. This constitutes a paradigm shift from the predominant\nsetting of safety-shielding -- i.e., ensuring that nothing bad ever happens --\nto a shielding process that additionally enforces liveness -- i.e., ensures\nthat something good eventually happens. At the core, our method uses\nStrategy-Template-based Adaptive Runtime Shields (STARs), which leverage\npermissive strategy templates to enable post-shielding with minimal\ninterference. As its main feature, STARs introduce a mechanism to dynamically\ncontrol interference, allowing a tunable enforcement parameter to balance\nformal obligations and task-specific behavior at runtime. This allows to\ntrigger more aggressive enforcement when needed, while allowing for optimized\npolicy choices otherwise. In addition, STARs support runtime adaptation to\nchanging specifications or actuator failures, making them especially suited for\ncyber-physical applications. We evaluate STARs on a mobile robot benchmark to\ndemonstrate their controllable interference when enforcing (incrementally\nupdated) $\\omega$-regular correctness properties over learned probabilistic\npolicies."}
{"id": "2505.14738", "pdf": "https://arxiv.org/pdf/2505.14738", "abs": "https://arxiv.org/abs/2505.14738", "authors": ["Xu Yang", "Xiao Yang", "Shikai Fang", "Bowen Xian", "Yuante Li", "Jian Wang", "Minrui Xu", "Haoran Pan", "Xinpeng Hong", "Weiqing Liu", "Yelong Shen", "Weizhu Chen", "Jiang Bian"], "title": "R&D-Agent: Automating Data-Driven AI Solution Building Through LLM-Powered Automated Research, Development, and Evolution", "categories": ["cs.AI"], "comment": "7 pages, 1 figure, 1 table", "summary": "Recent advances in AI and ML have transformed data science, yet increasing\ncomplexity and expertise requirements continue to hinder progress. While\ncrowdsourcing platforms alleviate some challenges, high-level data science\ntasks remain labor-intensive and iterative. To overcome these limitations, we\nintroduce R&D-Agent, a dual-agent framework for iterative exploration. The\nResearcher agent uses performance feedback to generate ideas, while the\nDeveloper agent refines code based on error feedback. By enabling multiple\nparallel exploration traces that merge and enhance one another, R&D-Agent\nnarrows the gap between automated solutions and expert-level performance.\nEvaluated on MLE-Bench, R&D-Agent emerges as the top-performing machine\nlearning engineering agent, demonstrating its potential to accelerate\ninnovation and improve precision across diverse data science applications. We\nhave open-sourced R&D-Agent on GitHub: https://github.com/microsoft/RD-Agent."}
{"id": "2505.14932", "pdf": "https://arxiv.org/pdf/2505.14932", "abs": "https://arxiv.org/abs/2505.14932", "authors": ["Isabelle Lee", "Sarah Liaw", "Dani Yogatama"], "title": "FOL-Pretrain: A complexity annotated corpus of first-order logic", "categories": ["cs.AI"], "comment": null, "summary": "Transformer-based large language models (LLMs) have demonstrated remarkable\nreasoning capabilities such as coding and solving mathematical problems to\ncommonsense inference. While these tasks vary in complexity, they all require\nmodels to integrate and compute over structured information. Despite recent\nefforts to reverse-engineer LLM behavior through controlled experiments, our\nunderstanding of how these models internalize and execute complex algorithms\nremains limited. Progress has largely been confined to small-scale studies or\nshallow tasks such as basic arithmetic and grammatical pattern matching. One\nbarrier to deeper understanding is the nature of pretraining data -- vast,\nheterogeneous, and often poorly annotated, making it difficult to isolate\nmechanisms of reasoning. To bridge this gap, we introduce a large-scale, fully\nopen, complexity-annotated dataset of first-order logic reasoning traces,\ndesigned to probe and analyze algorithmic reasoning in LLMs. The dataset\nconsists of 3.5 billion tokens, including 8.8 million LLM-augmented,\nhuman-annotated examples and 7.5 million synthetically generated examples. Each\nsynthetic example is verifiably correct, produced by a custom automated theorem\nsolver, and accompanied by metadata tracing its algorithmic provenance. We aim\nto provide a scalable, interpretable artifact for studying how LLMs learn and\ngeneralize symbolic reasoning processes, paving the way for more transparent\nand targeted investigations into the algorithmic capabilities of modern models."}
{"id": "2505.14940", "pdf": "https://arxiv.org/pdf/2505.14940", "abs": "https://arxiv.org/abs/2505.14940", "authors": ["Kaspar Rothenfusser"], "title": "To Be or Not To Be: Vector ontologies as a truly formal ontological framework", "categories": ["cs.AI", "cs.SC"], "comment": null, "summary": "Since Edmund Husserl coined the term \"Formal Ontologies\" in the early 20th\ncentury, a field that identifies itself with this particular branch of sciences\nhas gained increasing attention. Many authors, and even Husserl himself have\ndeveloped what they claim to be formal ontologies. I argue that under close\ninspection, none of these so claimed formal ontologies are truly formal in the\nHusserlian sense. More concretely, I demonstrate that they violate the two most\nimportant notions of formal ontology as developed in Husserl's Logical\nInvestigations, namely a priori validity independent of perception and\nformalism as the total absence of content. I hence propose repositioning the\nwork previously understood as formal ontology as the foundational ontology it\nreally is. This is to recognize the potential of a truly formal ontology in the\nHusserlian sense. Specifically, I argue that formal ontology following his\nconditions, allows us to formulate ontological structures, which could capture\nwhat is more objectively without presupposing a particular framework arising\nfrom perception. I further argue that the ability to design the formal\nstructure deliberately allows us to create highly scalable and interoperable\ninformation artifacts. As concrete evidence, I showcase that a class of formal\nontology, which uses the axioms of vector spaces, is able to express most of\nthe conceptualizations found in foundational ontologies. Most importantly, I\nargue that many information systems, specifically artificial intelligence, are\nlikely already using some type of vector ontologies to represent reality in\ntheir internal worldviews and elaborate on the evidence that humans do as well.\nI hence propose a thorough investigation of the ability of vector ontologies to\nact as a human-machine interoperable ontological framework that allows us to\nunderstand highly sophisticated machines and machines to understand us."}
{"id": "2505.14808", "pdf": "https://arxiv.org/pdf/2505.14808", "abs": "https://arxiv.org/abs/2505.14808", "authors": ["Soo Min Kwon", "Alec S. Xu", "Can Yaras", "Laura Balzano", "Qing Qu"], "title": "Out-of-Distribution Generalization of In-Context Learning: A Low-Dimensional Subspace Perspective", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "This work aims to demystify the out-of-distribution (OOD) capabilities of\nin-context learning (ICL) by studying linear regression tasks parameterized\nwith low-rank covariance matrices. With such a parameterization, we can model\ndistribution shifts as a varying angle between the subspace of the training and\ntesting covariance matrices. We prove that a single-layer linear attention\nmodel incurs a test risk with a non-negligible dependence on the angle,\nillustrating that ICL is not robust to such distribution shifts. However, using\nthis framework, we also prove an interesting property of ICL: when trained on\ntask vectors drawn from a union of low-dimensional subspaces, ICL can\ngeneralize to any subspace within their span, given sufficiently long prompt\nlengths. This suggests that the OOD generalization ability of Transformers may\nactually stem from the new task lying within the span of those encountered\nduring training. We empirically show that our results also hold for models such\nas GPT-2, and conclude with (i) experiments on how our observations extend to\nnonlinear function classes and (ii) results on how LoRA has the ability to\ncapture distribution shifts."}
{"id": "2505.14700", "pdf": "https://arxiv.org/pdf/2505.14700", "abs": "https://arxiv.org/abs/2505.14700", "authors": ["Rômulo Damasclin Chaves dos Santos", "Jorge Henrique de Oliveira Sales"], "title": "Stochastic Fractional Neural Operators: A Symmetrized Approach to Modeling Turbulence in Complex Fluid Dynamics", "categories": ["cs.LG"], "comment": "17 pages", "summary": "In this work, we introduce a new class of neural network operators designed\nto handle problems where memory effects and randomness play a central role. In\nthis work, we introduce a new class of neural network operators designed to\nhandle problems where memory effects and randomness play a central role. These\noperators merge symmetrized activation functions, Caputo-type fractional\nderivatives, and stochastic perturbations introduced via It\\^o type noise. The\nresult is a powerful framework capable of approximating functions that evolve\nover time with both long-term memory and uncertain dynamics. We develop the\nmathematical foundations of these operators, proving three key theorems of\nVoronovskaya type. These results describe the asymptotic behavior of the\noperators, their convergence in the mean-square sense, and their consistency\nunder fractional regularity assumptions. All estimates explicitly account for\nthe influence of the memory parameter $\\alpha$ and the noise level $\\sigma$. As\na practical application, we apply the proposed theory to the fractional\nNavier-Stokes equations with stochastic forcing, a model often used to describe\nturbulence in fluid flows with memory. Our approach provides theoretical\nguarantees for the approximation quality and suggests that these neural\noperators can serve as effective tools in the analysis and simulation of\ncomplex systems. By blending ideas from neural networks, fractional calculus,\nand stochastic analysis, this research opens new perspectives for modeling\nturbulent phenomena and other multiscale processes where memory and randomness\nare fundamental. The results lay the groundwork for hybrid learning-based\nmethods with strong analytical backing."}
{"id": "2505.14946", "pdf": "https://arxiv.org/pdf/2505.14946", "abs": "https://arxiv.org/abs/2505.14946", "authors": ["Eric Han", "Jun Chen", "Karthik Abinav Sankararaman", "Xiaoliang Peng", "Tengyu Xu", "Eryk Helenowski", "Kaiyan Peng", "Mrinal Kumar", "Sinong Wang", "Han Fang", "Arya Talebzadeh"], "title": "Reinforcement Learning from User Feedback", "categories": ["cs.AI"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in diverse user\nfacing applications, aligning them with real user preferences becomes\nessential. Existing methods like Reinforcement Learning from Human Feedback\n(RLHF) rely on expert annotators trained on manually defined guidelines, whose\njudgments may not reflect the priorities of everyday users. We introduce\nReinforcement Learning from User Feedback (RLUF), a framework for aligning LLMs\ndirectly to implicit signals from users in production. RLUF addresses key\nchallenges of user feedback: user feedback is often binary (e.g., emoji\nreactions), sparse, and occasionally adversarial. We train a reward model,\nP[Love], to predict the likelihood that an LLM response will receive a Love\nReaction, a lightweight form of positive user feedback, and integrate P[Love]\ninto a multi-objective policy optimization framework alongside helpfulness and\nsafety objectives. In large-scale experiments, we show that P[Love] is\npredictive of increased positive feedback and serves as a reliable offline\nevaluator of future user behavior. Policy optimization using P[Love]\nsignificantly raises observed positive-feedback rates, including a 28% increase\nin Love Reactions during live A/B tests. However, optimizing for positive\nreactions introduces reward hacking challenges, requiring careful balancing of\nobjectives. By directly leveraging implicit signals from users, RLUF offers a\npath to aligning LLMs with real-world user preferences at scale."}
{"id": "2505.14867", "pdf": "https://arxiv.org/pdf/2505.14867", "abs": "https://arxiv.org/abs/2505.14867", "authors": ["So Won Jeong", "Claire Donnat"], "title": "LOBSTUR: A Local Bootstrap Framework for Tuning Unsupervised Representations in Graph Neural Networks", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) are increasingly used in conjunction with\nunsupervised learning techniques to learn powerful node representations, but\ntheir deployment is hindered by their high sensitivity to hyperparameter tuning\nand the absence of established methodologies for selecting the optimal models.\nTo address these challenges, we propose LOBSTUR-GNN ({\\bf Lo}cal {\\bf B}oot{\\bf\ns}trap for {\\bf T}uning {\\bf U}nsupervised {\\bf R}epresentations in GNNs) i), a\nnovel framework designed to adapt bootstrapping techniques for unsupervised\ngraph representation learning. LOBSTUR-GNN tackles two main challenges: (a)\nadapting the bootstrap edge and feature resampling process to account for local\ngraph dependencies in creating alternative versions of the same graph, and (b)\nestablishing robust metrics for evaluating learned representations without\nground-truth labels. Using locally bootstrapped resampling and leveraging\nCanonical Correlation Analysis (CCA) to assess embedding consistency, LOBSTUR\nprovides a principled approach for hyperparameter tuning in unsupervised GNNs.\nWe validate the effectiveness and efficiency of our proposed method through\nextensive experiments on established academic datasets, showing an 65.9\\%\nimprovement in the classification accuracy compared to an uninformed selection\nof hyperparameters. Finally, we deploy our framework on a real-world\napplication, thereby demonstrating its validity and practical utility in\nvarious settings. \\footnote{The code is available at\n\\href{https://github.com/sowonjeong/lobstur-graph-bootstrap}{github.com/sowonjeong/lobstur-graph-bootstrap}.}"}
{"id": "2505.14727", "pdf": "https://arxiv.org/pdf/2505.14727", "abs": "https://arxiv.org/abs/2505.14727", "authors": ["Mohammad Rubyet Islam"], "title": "The Evolution of Alpha in Finance Harnessing Human Insight and LLM Agents", "categories": ["cs.LG", "91G70 Statistical methods, risk measures 91B84 Economic models\n  (financial models, industrial models, growth models)", "I.2.6; I.5.1; I.2.7"], "comment": null, "summary": "The pursuit of alpha returns that exceed market benchmarks has undergone a\nprofound transformation, evolving from intuition-driven investing to\nautonomous, AI powered systems. This paper introduces a comprehensive five\nstage taxonomy that traces this progression across manual strategies,\nstatistical models, classical machine learning, deep learning, and agentic\narchitectures powered by large language models (LLMs). Unlike prior surveys\nfocused narrowly on modeling techniques, this review adopts a system level\nlens, integrating advances in representation learning, multimodal data fusion,\nand tool augmented LLM agents. The strategic shift from static predictors to\ncontextaware financial agents capable of real time reasoning, scenario\nsimulation, and cross modal decision making is emphasized. Key challenges in\ninterpretability, data fragility, governance, and regulatory compliance areas\ncritical to production deployment are examined. The proposed taxonomy offers a\nunified framework for evaluating maturity, aligning infrastructure, and guiding\nthe responsible development of next generation alpha systems."}
{"id": "2505.14970", "pdf": "https://arxiv.org/pdf/2505.14970", "abs": "https://arxiv.org/abs/2505.14970", "authors": ["Xiaoyin Chen", "Jiarui Lu", "Minsu Kim", "Dinghuai Zhang", "Jian Tang", "Alexandre Piché", "Nicolas Gontier", "Yoshua Bengio", "Ehsan Kamalloo"], "title": "Self-Evolving Curriculum for LLM Reasoning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) has proven effective for fine-tuning large\nlanguage models (LLMs), significantly enhancing their reasoning abilities in\ndomains such as mathematics and code generation. A crucial factor influencing\nRL fine-tuning success is the training curriculum: the order in which training\nproblems are presented. While random curricula serve as common baselines, they\nremain suboptimal; manually designed curricula often rely heavily on\nheuristics, and online filtering methods can be computationally prohibitive. To\naddress these limitations, we propose Self-Evolving Curriculum (SEC), an\nautomatic curriculum learning method that learns a curriculum policy\nconcurrently with the RL fine-tuning process. Our approach formulates\ncurriculum selection as a non-stationary Multi-Armed Bandit problem, treating\neach problem category (e.g., difficulty level or problem type) as an individual\narm. We leverage the absolute advantage from policy gradient methods as a proxy\nmeasure for immediate learning gain. At each training step, the curriculum\npolicy selects categories to maximize this reward signal and is updated using\nthe TD(0) method. Across three distinct reasoning domains: planning, inductive\nreasoning, and mathematics, our experiments demonstrate that SEC significantly\nimproves models' reasoning capabilities, enabling better generalization to\nharder, out-of-distribution test problems. Additionally, our approach achieves\nbetter skill balance when fine-tuning simultaneously on multiple reasoning\ndomains. These findings highlight SEC as a promising strategy for RL\nfine-tuning of LLMs."}
{"id": "2505.15013", "pdf": "https://arxiv.org/pdf/2505.15013", "abs": "https://arxiv.org/abs/2505.15013", "authors": ["Anupama Sridhar", "Alexander Johansen"], "title": "Convergence of Adam in Deep ReLU Networks via Directional Complexity and Kakeya Bounds", "categories": ["stat.ML", "cs.LG"], "comment": "9 pages main paper", "summary": "First-order adaptive optimization methods like Adam are the default choices\nfor training modern deep neural networks. Despite their empirical success, the\ntheoretical understanding of these methods in non-smooth settings, particularly\nin Deep ReLU networks, remains limited. ReLU activations create exponentially\nmany region boundaries where standard smoothness assumptions break down.\n\\textbf{We derive the first\n\\(\\tilde{O}\\!\\bigl(\\sqrt{d_{\\mathrm{eff}}/n}\\bigr)\\) generalization bound for\nAdam in Deep ReLU networks and the first global-optimal convergence for Adam in\nthe non smooth, non convex relu landscape without a global PL or convexity\nassumption.} Our analysis is based on stratified Morse theory and novel results\nin Kakeya sets. We develop a multi-layer refinement framework that\nprogressively tightens bounds on region crossings. We prove that the number of\nregion crossings collapses from exponential to near-linear in the effective\ndimension. Using a Kakeya based method, we give a tighter generalization bound\nthan PAC-Bayes approaches and showcase convergence using a mild uniform low\nbarrier assumption."}
{"id": "2505.14733", "pdf": "https://arxiv.org/pdf/2505.14733", "abs": "https://arxiv.org/abs/2505.14733", "authors": ["Yunho Jin", "Gu-Yeon Wei", "David Brooks"], "title": "The Energy Cost of Reasoning: Analyzing Energy Usage in LLMs with Test-time Compute", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Scaling large language models (LLMs) has driven significant advancements, yet\nit faces diminishing returns and escalating energy demands. This work\nintroduces test-time compute (TTC)-allocating additional computational\nresources during inference-as a compelling complement to conventional scaling\nstrategies. Specifically, we investigate whether employing TTC can achieve\nsuperior accuracy-energy trade-offs compared to simply increasing model size.\nOur empirical analysis reveals that TTC surpasses traditional model scaling in\naccuracy/energy efficiency, with notable gains in tasks demanding complex\nreasoning rather than mere factual recall. Further, we identify a critical\ninteraction between TTC performance and output sequence length, demonstrating\nthat strategically adjusting compute resources at inference time according to\nquery complexity can substantially enhance efficiency. Our findings advocate\nfor TTC as a promising direction, enabling more sustainable, accurate, and\nadaptable deployment of future language models without incurring additional\npretraining costs."}
{"id": "2505.14983", "pdf": "https://arxiv.org/pdf/2505.14983", "abs": "https://arxiv.org/abs/2505.14983", "authors": ["Zahra Zahedi", "Shashank Mehrotra", "Teruhisa Misu", "Kumar Akash"], "title": "Toward Informed AV Decision-Making: Computational Model of Well-being and Trust in Mobility", "categories": ["cs.AI", "cs.HC", "cs.RO"], "comment": null, "summary": "For future human-autonomous vehicle (AV) interactions to be effective and\nsmooth, human-aware systems that analyze and align human needs with automation\ndecisions are essential. Achieving this requires systems that account for human\ncognitive states. We present a novel computational model in the form of a\nDynamic Bayesian Network (DBN) that infers the cognitive states of both AV\nusers and other road users, integrating this information into the AV's\ndecision-making process. Specifically, our model captures the well-being of\nboth an AV user and an interacting road user as cognitive states alongside\ntrust. Our DBN models infer beliefs over the AV user's evolving well-being,\ntrust, and intention states, as well as the possible well-being of other road\nusers, based on observed interaction experiences. Using data collected from an\ninteraction study, we refine the model parameters and empirically assess its\nperformance. Finally, we extend our model into a causal inference model (CIM)\nframework for AV decision-making, enabling the AV to enhance user well-being\nand trust while balancing these factors with its own operational costs and the\nwell-being of interacting road users. Our evaluation demonstrates the model's\neffectiveness in accurately predicting user's states and guiding informed,\nhuman-centered AV decisions."}
{"id": "2505.15022", "pdf": "https://arxiv.org/pdf/2505.15022", "abs": "https://arxiv.org/abs/2505.15022", "authors": ["Ya-Yun Huang", "Joseph McClernon", "Jason A. Oliver", "Matthew M. Engelhard"], "title": "Infinite hierarchical contrastive clustering for personal digital envirotyping", "categories": ["stat.ML", "cs.LG"], "comment": "10pages, 5 figures, Machine Learning four Health(ML4H 2024)", "summary": "Daily environments have profound influence on our health and behavior. Recent\nwork has shown that digital envirotyping, where computer vision is applied to\nimages of daily environments taken during ecological momentary assessment\n(EMA), can be used to identify meaningful relationships between environmental\nfeatures and health outcomes of interest. To systematically study such effects\non an individual level, it is helpful to group images into distinct\nenvironments encountered in an individual's daily life; these may then be\nanalyzed, further grouped into related environments with similar features, and\nlinked to health outcomes. Here we introduce infinite hierarchical contrastive\nclustering to address this challenge. Building on the established contrastive\nclustering framework, our method a) allows an arbitrary number of clusters\nwithout requiring the full Dirichlet Process machinery by placing a\nstick-breaking prior on predicted cluster probabilities; and b) encourages\ndistinct environments to form well-defined sub-clusters within each cluster of\nrelated environments by incorporating a participant-specific prediction loss.\nOur experiments show that our model effectively identifies distinct personal\nenvironments and groups these environments into meaningful environment types.\nWe then illustrate how the resulting clusters can be linked to various health\noutcomes, highlighting the potential of our approach to advance the\nenvirotyping paradigm."}
{"id": "2505.14737", "pdf": "https://arxiv.org/pdf/2505.14737", "abs": "https://arxiv.org/abs/2505.14737", "authors": ["Huiliang Zhang", "Di Wu", "Arnaud Zinflou", "Stephane Dellacherie", "Mouhamadou Makhtar Dione", "Benoit Boulet"], "title": "Leveraging Multivariate Long-Term History Representation for Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multivariate Time Series (MTS) forecasting has a wide range of applications\nin both industry and academia. Recent advances in Spatial-Temporal Graph Neural\nNetwork (STGNN) have achieved great progress in modelling spatial-temporal\ncorrelations. Limited by computational complexity, most STGNNs for MTS\nforecasting focus primarily on short-term and local spatial-temporal\ndependencies. Although some recent methods attempt to incorporate univariate\nhistory into modeling, they still overlook crucial long-term spatial-temporal\nsimilarities and correlations across MTS, which are essential for accurate\nforecasting. To fill this gap, we propose a framework called the Long-term\nMultivariate History Representation (LMHR) Enhanced STGNN for MTS forecasting.\nSpecifically, a Long-term History Encoder (LHEncoder) is adopted to effectively\nencode the long-term history into segment-level contextual representations and\nreduce point-level noise. A non-parametric Hierarchical Representation\nRetriever (HRetriever) is designed to include the spatial information in the\nlong-term spatial-temporal dependency modelling and pick out the most valuable\nrepresentations with no additional training. A Transformer-based Aggregator\n(TAggregator) selectively fuses the sparsely retrieved contextual\nrepresentations based on the ranking positional embedding efficiently.\nExperimental results demonstrate that LMHR outperforms typical STGNNs by 10.72%\non the average prediction horizons and state-of-the-art methods by 4.12% on\nseveral real-world datasets. Additionally, it consistently improves prediction\naccuracy by 9.8% on the top 10% of rapidly changing patterns across the\ndatasets."}
{"id": "2505.15011", "pdf": "https://arxiv.org/pdf/2505.15011", "abs": "https://arxiv.org/abs/2505.15011", "authors": ["Kryspin Varys", "Federico Cerutti", "Adam Sobey", "Timothy J. Norman"], "title": "HAVA: Hybrid Approach to Value-Alignment through Reward Weighing for Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Our society is governed by a set of norms which together bring about the\nvalues we cherish such as safety, fairness or trustworthiness. The goal of\nvalue-alignment is to create agents that not only do their tasks but through\ntheir behaviours also promote these values. Many of the norms are written as\nlaws or rules (legal / safety norms) but even more remain unwritten (social\nnorms). Furthermore, the techniques used to represent these norms also differ.\nSafety / legal norms are often represented explicitly, for example, in some\nlogical language while social norms are typically learned and remain hidden in\nthe parameter space of a neural network. There is a lack of approaches in the\nliterature that could combine these various norm representations into a single\nalgorithm. We propose a novel method that integrates these norms into the\nreinforcement learning process. Our method monitors the agent's compliance with\nthe given norms and summarizes it in a quantity we call the agent's reputation.\nThis quantity is used to weigh the received rewards to motivate the agent to\nbecome value-aligned. We carry out a series of experiments including a\ncontinuous state space traffic problem to demonstrate the importance of the\nwritten and unwritten norms and show how our method can find the value-aligned\npolicies. Furthermore, we carry out ablations to demonstrate why it is better\nto combine these two groups of norms rather than using either separately."}
{"id": "2505.15175", "pdf": "https://arxiv.org/pdf/2505.15175", "abs": "https://arxiv.org/abs/2505.15175", "authors": ["Diego Granziol", "Donald Flynn"], "title": "A Linear Approach to Data Poisoning", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": "9 pages, 9 Figures", "summary": "We investigate the theoretical foundations of data poisoning attacks in\nmachine learning models. Our analysis reveals that the Hessian with respect to\nthe input serves as a diagnostic tool for detecting poisoning, exhibiting\nspectral signatures that characterize compromised datasets. We use random\nmatrix theory (RMT) to develop a theory for the impact of poisoning proportion\nand regularisation on attack efficacy in linear regression. Through QR stepwise\nregression, we study the spectral signatures of the Hessian in multi-output\nregression. We perform experiments on deep networks to show experimentally that\nthis theory extends to modern convolutional and transformer networks under the\ncross-entropy loss. Based on these insights we develop preliminary algorithms\nto determine if a network has been poisoned and remedies which do not require\nfurther training."}
{"id": "2505.14739", "pdf": "https://arxiv.org/pdf/2505.14739", "abs": "https://arxiv.org/abs/2505.14739", "authors": ["Heiko Oppel", "Andreas Spilz", "Michael Munz"], "title": "Time Series Similarity Score Functions to Monitor and Interact with the Training and Denoising Process of a Time Series Diffusion Model applied to a Human Activity Recognition Dataset based on IMUs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Denoising diffusion probabilistic models are able to generate synthetic\nsensor signals. The training process of such a model is controlled by a loss\nfunction which measures the difference between the noise that was added in the\nforward process and the noise that was predicted by the diffusion model. This\nenables the generation of realistic data. However, the randomness within the\nprocess and the loss function itself makes it difficult to estimate the quality\nof the data. Therefore, we examine multiple similarity metrics and adapt an\nexisting metric to overcome this issue by monitoring the training and\nsynthetisation process using those metrics. The adapted metric can even be\nfine-tuned on the input data to comply with the requirements of an underlying\nclassification task. We were able to significantly reduce the amount of\ntraining epochs without a performance reduction in the classification task. An\noptimized training process not only saves resources, but also reduces the time\nfor training generative models."}
{"id": "2505.15068", "pdf": "https://arxiv.org/pdf/2505.15068", "abs": "https://arxiv.org/abs/2505.15068", "authors": ["Cheng Qian", "Hongyi Du", "Hongru Wang", "Xiusi Chen", "Yuji Zhang", "Avirup Sil", "Chengxiang Zhai", "Kathleen McKeown", "Heng Ji"], "title": "ModelingAgent: Bridging LLMs and Mathematical Modeling for Real-World Challenges", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "36 Pages, 26 Figures, 5 Tables", "summary": "Recent progress in large language models (LLMs) has enabled substantial\nadvances in solving mathematical problems. However, existing benchmarks often\nfail to reflect the complexity of real-world problems, which demand open-ended,\ninterdisciplinary reasoning and integration of computational tools. To address\nthis gap, we introduce ModelingBench, a novel benchmark featuring\nreal-world-inspired, open-ended problems from math modeling competitions across\ndiverse domains, ranging from urban traffic optimization to ecosystem resource\nplanning. These tasks require translating natural language into formal\nmathematical formulations, applying appropriate tools, and producing\nstructured, defensible reports. ModelingBench also supports multiple valid\nsolutions, capturing the ambiguity and creativity of practical modeling. We\nalso present ModelingAgent, a multi-agent framework that coordinates tool use,\nsupports structured workflows, and enables iterative self-refinement to\ngenerate well-grounded, creative solutions. To evaluate outputs, we further\npropose ModelingJudge, an expert-in-the-loop system leveraging LLMs as\ndomain-specialized judges assessing solutions from multiple expert\nperspectives. Empirical results show that ModelingAgent substantially\noutperforms strong baselines and often produces solutions indistinguishable\nfrom those of human experts. Together, our work provides a comprehensive\nframework for evaluating and advancing real-world problem-solving in\nopen-ended, interdisciplinary modeling challenges."}
{"id": "2505.15215", "pdf": "https://arxiv.org/pdf/2505.15215", "abs": "https://arxiv.org/abs/2505.15215", "authors": ["Otto Tabell", "Santtu Tikka", "Juha Karvanen"], "title": "Clustering and Pruning in Causal Data Fusion", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Data fusion, the process of combining observational and experimental data,\ncan enable the identification of causal effects that would otherwise remain\nnon-identifiable. Although identification algorithms have been developed for\nspecific scenarios, do-calculus remains the only general-purpose tool for\ncausal data fusion, particularly when variables are present in some data\nsources but not others. However, approaches based on do-calculus may encounter\ncomputational challenges as the number of variables increases and the causal\ngraph grows in complexity. Consequently, there exists a need to reduce the size\nof such models while preserving the essential features. For this purpose, we\npropose pruning (removing unnecessary variables) and clustering (combining\nvariables) as preprocessing operations for causal data fusion. We generalize\nearlier results on a single data source and derive conditions for applying\npruning and clustering in the case of multiple data sources. We give sufficient\nconditions for inferring the identifiability or non-identifiability of a causal\neffect in a larger graph based on a smaller graph and show how to obtain the\ncorresponding identifying functional for identifiable causal effects. Examples\nfrom epidemiology and social science demonstrate the use of the results."}
{"id": "2505.14741", "pdf": "https://arxiv.org/pdf/2505.14741", "abs": "https://arxiv.org/abs/2505.14741", "authors": ["Kunyun Wang", "Bohan Li", "Kai Yu", "Minyi Guo", "Jieru Zhao"], "title": "Communication-Efficient Diffusion Denoising Parallelization via Reuse-then-Predict Mechanism", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Diffusion models have emerged as a powerful class of generative models across\nvarious modalities, including image, video, and audio synthesis. However, their\ndeployment is often limited by significant inference latency, primarily due to\nthe inherently sequential nature of the denoising process. While existing\nparallelization strategies attempt to accelerate inference by distributing\ncomputation across multiple devices, they typically incur high communication\noverhead, hindering deployment on commercial hardware. To address this\nchallenge, we propose \\textbf{ParaStep}, a novel parallelization method based\non a reuse-then-predict mechanism that parallelizes diffusion inference by\nexploiting similarity between adjacent denoising steps. Unlike prior approaches\nthat rely on layer-wise or stage-wise communication, ParaStep employs\nlightweight, step-wise communication, substantially reducing overhead. ParaStep\nachieves end-to-end speedups of up to \\textbf{3.88}$\\times$ on SVD,\n\\textbf{2.43}$\\times$ on CogVideoX-2b, and \\textbf{6.56}$\\times$ on\nAudioLDM2-large, while maintaining generation quality. These results highlight\nParaStep as a scalable and communication-efficient solution for accelerating\ndiffusion inference, particularly in bandwidth-constrained environments."}
{"id": "2505.15146", "pdf": "https://arxiv.org/pdf/2505.15146", "abs": "https://arxiv.org/abs/2505.15146", "authors": ["Lanxiang Hu", "Mingjia Huo", "Yuxuan Zhang", "Haoyang Yu", "Eric P. Xing", "Ion Stoica", "Tajana Rosing", "Haojian Jin", "Hao Zhang"], "title": "lmgame-Bench: How Good are LLMs at Playing Games?", "categories": ["cs.AI"], "comment": null, "summary": "Playing video games requires perception, memory, and planning, exactly the\nfaculties modern large language model (LLM) agents are expected to master. We\nstudy the major challenges in using popular video games to evaluate modern LLMs\nand find that directly dropping LLMs into games cannot make an effective\nevaluation, for three reasons -- brittle vision perception, prompt sensitivity,\nand potential data contamination. We introduce lmgame-Bench to turn games into\nreliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and\nnarrative games delivered through a unified Gym-style API and paired with\nlightweight perception and memory scaffolds, and is designed to stabilize\nprompt variance and remove contamination. Across 13 leading models, we show\nlmgame-Bench is challenging while still separating models well. Correlation\nanalysis shows that every game probes a unique blend of capabilities often\ntested in isolation elsewhere. More interestingly, performing reinforcement\nlearning on a single game from lmgame-Bench transfers both to unseen games and\nto external planning tasks. Our evaluation code is available at\nhttps://github.com/lmgame-org/GamingAgent/lmgame-bench."}
{"id": "2505.15342", "pdf": "https://arxiv.org/pdf/2505.15342", "abs": "https://arxiv.org/abs/2505.15342", "authors": ["Kaito Ariu", "Po-An Wang", "Alexandre Proutiere", "Kenshi Abe"], "title": "Policy Testing in Markov Decision Processes", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "We study the policy testing problem in discounted Markov decision processes\n(MDPs) under the fixed-confidence setting. The goal is to determine whether the\nvalue of a given policy exceeds a specified threshold while minimizing the\nnumber of observations. We begin by deriving an instance-specific lower bound\nthat any algorithm must satisfy. This lower bound is characterized as the\nsolution to an optimization problem with non-convex constraints. We propose a\npolicy testing algorithm inspired by this optimization problem--a common\napproach in pure exploration problems such as best-arm identification, where\nasymptotically optimal algorithms often stem from such optimization-based\ncharacterizations. As for other pure exploration tasks in MDPs, however, the\nnon-convex constraints in the lower-bound problem present significant\nchallenges, raising doubts about whether statistically optimal and\ncomputationally tractable algorithms can be designed. To address this, we\nreformulate the lower-bound problem by interchanging the roles of the objective\nand the constraints, yielding an alternative problem with a non-convex\nobjective but convex constraints. Strikingly, this reformulated problem admits\nan interpretation as a policy optimization task in a newly constructed reversed\nMDP. Leveraging recent advances in policy gradient methods, we efficiently\nsolve this problem and use it to design a policy testing algorithm that is\nstatistically optimal--matching the instance-specific lower bound on sample\ncomplexity--while remaining computationally tractable. We validate our approach\nwith numerical experiments."}
{"id": "2505.14742", "pdf": "https://arxiv.org/pdf/2505.14742", "abs": "https://arxiv.org/abs/2505.14742", "authors": ["Hong Huang", "Dapeng Wu"], "title": "Quaff: Quantized Parameter-Efficient Fine-Tuning under Outlier Spatial Stability Hypothesis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have made exciting achievements across various\ndomains, yet their deployment on resource-constrained personal devices remains\nhindered by the prohibitive computational and memory demands of task-specific\nfine-tuning. While quantization offers a pathway to efficiency, existing\nmethods struggle to balance performance and overhead, either incurring high\ncomputational/memory costs or failing to address activation outliers, a\ncritical bottleneck in quantized fine-tuning. To address these challenges, we\npropose the Outlier Spatial Stability Hypothesis (OSSH): During fine-tuning,\ncertain activation outlier channels retain stable spatial positions across\ntraining iterations. Building on OSSH, we propose Quaff, a Quantized\nparameter-efficient fine-tuning framework for LLMs, optimizing low-precision\nactivation representations through targeted momentum scaling. Quaff dynamically\nsuppresses outliers exclusively in invariant channels using lightweight\noperations, eliminating full-precision weight storage and global rescaling\nwhile reducing quantization errors. Extensive experiments across ten benchmarks\nvalidate OSSH and demonstrate Quaff's efficacy. Specifically, on the GPQA\nreasoning benchmark, Quaff achieves a 1.73x latency reduction and 30% memory\nsavings over full-precision fine-tuning while improving accuracy by 0.6% on the\nPhi-3 model, reconciling the triple trade-off between efficiency, performance,\nand deployability. By enabling consumer-grade GPU fine-tuning (e.g., RTX 2080\nSuper) without sacrificing model utility, Quaff democratizes personalized LLM\ndeployment. The code is available at https://github.com/Little0o0/Quaff.git."}
{"id": "2505.15240", "pdf": "https://arxiv.org/pdf/2505.15240", "abs": "https://arxiv.org/abs/2505.15240", "authors": ["Yassir Fathullah", "Mark J. F. Gales"], "title": "Generalised Probabilistic Modelling and Improved Uncertainty Estimation in Comparative LLM-as-a-judge", "categories": ["cs.AI", "cs.LG", "stat.ML"], "comment": "To appear in UAI 2025", "summary": "This paper explores generalised probabilistic modelling and uncertainty\nestimation in comparative LLM-as-a-judge frameworks. We show that existing\nProduct-of-Experts methods are specific cases of a broader framework, enabling\ndiverse modelling options. Furthermore, we propose improved uncertainty\nestimates for individual comparisons, enabling more efficient selection and\nachieving strong performance with fewer evaluations. We also introduce a method\nfor estimating overall ranking uncertainty. Finally, we demonstrate that\ncombining absolute and comparative scoring improves performance. Experiments\nshow that the specific expert model has a limited impact on final rankings but\nour proposed uncertainty estimates, especially the probability of reordering,\nsignificantly improve the efficiency of systems reducing the number of needed\ncomparisons by ~50%. Furthermore, ranking-level uncertainty metrics can be used\nto identify low-performing predictions, where the nature of the probabilistic\nmodel has a notable impact on the quality of the overall uncertainty."}
{"id": "2505.15417", "pdf": "https://arxiv.org/pdf/2505.15417", "abs": "https://arxiv.org/abs/2505.15417", "authors": ["Leon Chlon", "Maggie Chlon", "MarcAntonio M. Awada"], "title": "Robust Multimodal Learning via Entropy-Gated Contrastive Fusion", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Real-world multimodal systems routinely face missing-input scenarios, and in\nreality, robots lose audio in a factory or a clinical record omits lab tests at\ninference time. Standard fusion layers either preserve robustness or\ncalibration but never both. We introduce Adaptive Entropy-Gated Contrastive\nFusion (AECF), a single light-weight layer that (i) adapts its entropy\ncoefficient per instance, (ii) enforces monotone calibration across all\nmodality subsets, and (iii) drives a curriculum mask directly from\ntraining-time entropy. On AV-MNIST and MS-COCO, AECF improves masked-input mAP\nby +18 pp at a 50% drop rate while reducing ECE by up to 200%, yet adds 1%\nrun-time. All back-bones remain frozen, making AECF an easy drop-in layer for\nrobust, calibrated multimodal inference."}
{"id": "2505.14745", "pdf": "https://arxiv.org/pdf/2505.14745", "abs": "https://arxiv.org/abs/2505.14745", "authors": ["Varun Raaghav", "Dimitrios Bikos", "Antonio Rago", "Francesca Toni", "Maria Charalambides"], "title": "Explainable Prediction of the Mechanical Properties of Composites with CNNs", "categories": ["cs.LG", "cs.AI", "I.2.1"], "comment": "9 pages, 6 figures", "summary": "Composites are amongst the most important materials manufactured today, as\nevidenced by their use in countless applications. In order to establish the\nsuitability of composites in specific applications, finite element (FE)\nmodelling, a numerical method based on partial differential equations, is the\nindustry standard for assessing their mechanical properties. However, FE\nmodelling is exceptionally costly from a computational viewpoint, a limitation\nwhich has led to efforts towards applying AI models to this task. However, in\nthese approaches: the chosen model architectures were rudimentary, feed-forward\nneural networks giving limited accuracy; the studies focus on predicting\nelastic mechanical properties, without considering material strength limits;\nand the models lacked transparency, hindering trustworthiness by users. In this\npaper, we show that convolutional neural networks (CNNs) equipped with methods\nfrom explainable AI (XAI) can be successfully deployed to solve this problem.\nOur approach uses customised CNNs trained on a dataset we generate using\ntransverse tension tests in FE modelling to predict composites' mechanical\nproperties, i.e., Young's modulus and yield strength. We show empirically that\nour approach achieves high accuracy, outperforming a baseline, ResNet-34, in\nestimating the mechanical properties. We then use SHAP and Integrated\nGradients, two post-hoc XAI methods, to explain the predictions, showing that\nthe CNNs use the critical geometrical features that influence the composites'\nbehaviour, thus allowing engineers to verify that the models are trustworthy by\nrepresenting the science of composites."}
{"id": "2505.15274", "pdf": "https://arxiv.org/pdf/2505.15274", "abs": "https://arxiv.org/abs/2505.15274", "authors": ["Xin Shu", "Shuai Wang", "Ang Li"], "title": "Identification of Probabilities of Causation: A Complete Characterization", "categories": ["cs.AI"], "comment": null, "summary": "Probabilities of causation are fundamental to modern decision-making. Pearl\nfirst introduced three binary probabilities of causation, and Tian and Pearl\nlater derived tight bounds for them using Balke's linear programming. The\ntheoretical characterization of probabilities of causation with multi-valued\ntreatments and outcomes has remained unresolved for decades, limiting the scope\nof causality-based decision-making. In this paper, we resolve this foundational\ngap by proposing a complete set of representative probabilities of causation\nand proving that they are sufficient to characterize all possible probabilities\nof causation within the framework of Structural Causal Models (SCMs). We then\nformally derive tight bounds for these representative quantities using formal\nmathematical proofs. Finally, we demonstrate the practical relevance of our\nresults through illustrative toy examples."}
{"id": "2505.15429", "pdf": "https://arxiv.org/pdf/2505.15429", "abs": "https://arxiv.org/abs/2505.15429", "authors": ["Pritam Anand"], "title": "Uncertainty Quantification in SVM prediction", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper explores Uncertainty Quantification (UQ) in SVM predictions,\nparticularly for regression and forecasting tasks. Unlike the Neural Network,\nthe SVM solutions are typically more stable, sparse, optimal and interpretable.\nHowever, there are only few literature which addresses the UQ in SVM\nprediction. At first, we provide a comprehensive summary of existing Prediction\nInterval (PI) estimation and probabilistic forecasting methods developed in the\nSVM framework and evaluate them against the key properties expected from an\nideal PI model. We find that none of the existing SVM PI models achieves a\nsparse solution. To introduce sparsity in SVM model, we propose the Sparse\nSupport Vector Quantile Regression (SSVQR) model, which constructs PIs and\nprobabilistic forecasts by solving a pair of linear programs. Further, we\ndevelop a feature selection algorithm for PI estimation using SSVQR that\neffectively eliminates a significant number of features while improving PI\nquality in case of high-dimensional dataset. Finally we extend the SVM models\nin Conformal Regression setting for obtaining more stable prediction set with\nfinite test set guarantees. Extensive experiments on artificial, real-world\nbenchmark datasets compare the different characteristics of both existing and\nproposed SVM-based PI estimation methods and also highlight the advantages of\nthe feature selection in PI estimation. Furthermore, we compare both, the\nexisting and proposed SVM-based PI estimation models, with modern deep learning\nmodels for probabilistic forecasting tasks on benchmark datasets. Furthermore,\nSVM models show comparable or superior performance to modern complex deep\nlearning models for probabilistic forecasting task in our experiments."}
{"id": "2505.14748", "pdf": "https://arxiv.org/pdf/2505.14748", "abs": "https://arxiv.org/abs/2505.14748", "authors": ["Zaifa Xue", "Tao Zhang", "Tuo Xu", "Huaixin Liang", "Le Gao"], "title": "Cooperative Causal GraphSAGE", "categories": ["cs.LG", "cs.GT"], "comment": null, "summary": "GraphSAGE is a widely used graph neural network. The introduction of causal\ninference has improved its robust performance and named as Causal GraphSAGE.\nHowever, Causal GraphSAGE focuses on measuring causal weighting among\nindividual nodes, but neglecting the cooperative relationships among sampling\nnodes as a whole. To address this issue, this paper proposes Cooperative Causal\nGraphSAGE (CoCa-GraphSAGE), which combines cooperative game theory with Causal\nGraphSAGE. Initially, a cooperative causal structure model is constructed in\nthe case of cooperation based on the graph structure. Subsequently, Cooperative\nCausal sampling (CoCa-sampling) algorithm is proposed, employing the Shapley\nvalues to calculate the cooperative contribution based on causal weights of the\nnodes sets. CoCa-sampling guides the selection of nodes with significant\ncooperative causal effects during the neighborhood sampling process, thus\nintegrating the selected neighborhood features under cooperative relationships,\nwhich takes the sampled nodes as a whole and generates more stable target node\nembeddings. Experiments on publicly available datasets show that the proposed\nmethod has comparable classification performance to the compared methods and\noutperforms under perturbations, demonstrating the robustness improvement by\nCoCa-sampling."}
{"id": "2505.15276", "pdf": "https://arxiv.org/pdf/2505.15276", "abs": "https://arxiv.org/abs/2505.15276", "authors": ["Rongzhi Zhu", "Yi Liu", "Zequn Sun", "Yiwei Wang", "Wei Hu"], "title": "When Can Large Reasoning Models Save Thinking? Mechanistic Analysis of Behavioral Divergence in Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) have significantly advanced performance on\ncomplex tasks, yet their tendency to overthink introduces inefficiencies. This\nstudy investigates the internal mechanisms of reinforcement learning\n(RL)-trained LRMs when prompted to save thinking, revealing three distinct\nthinking modes: no thinking (NT), explicit thinking (ET), and implicit thinking\n(IT). Through comprehensive analysis of confidence in thinking termination,\nattention from thinking to generation, and attentional focus on input sections,\nwe uncover key factors influencing the reasoning behaviors. We further find\nthat NT reduces output length at the cost of accuracy, while ET and IT maintain\naccuracy with reduced response length. Our findings expose fundamental\ninconsistencies in RL-optimized LRMs, necessitating adaptive improvements for\nreliable efficiency."}
{"id": "2505.15437", "pdf": "https://arxiv.org/pdf/2505.15437", "abs": "https://arxiv.org/abs/2505.15437", "authors": ["Nikita Kotelevskii", "Mohsen Guizani", "Eric Moulines", "Maxim Panov"], "title": "Adaptive Temperature Scaling with Conformal Prediction", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Conformal prediction enables the construction of high-coverage prediction\nsets for any pre-trained model, guaranteeing that the true label lies within\nthe set with a specified probability. However, these sets do not provide\nprobability estimates for individual labels, limiting their practical use. In\nthis paper, we propose, to the best of our knowledge, the first method for\nassigning calibrated probabilities to elements of a conformal prediction set.\nOur approach frames this as an adaptive calibration problem, selecting an\ninput-specific temperature parameter to match the desired coverage level.\nExperiments on several challenging image classification datasets demonstrate\nthat our method maintains coverage guarantees while significantly reducing\nexpected calibration error."}
{"id": "2505.14751", "pdf": "https://arxiv.org/pdf/2505.14751", "abs": "https://arxiv.org/abs/2505.14751", "authors": ["Maheak Dave", "Aniket Kumar Singh", "Aryan Pareek", "Harshita Jha", "Debasis Chaudhuri", "Manish Pratap Singh"], "title": "Self Distillation via Iterative Constructive Perturbations", "categories": ["cs.LG", "cs.AI", "cs.ET"], "comment": null, "summary": "Deep Neural Networks have achieved remarkable achievements across various\ndomains, however balancing performance and generalization still remains a\nchallenge while training these networks. In this paper, we propose a novel\nframework that uses a cyclic optimization strategy to concurrently optimize the\nmodel and its input data for better training, rethinking the traditional\ntraining paradigm. Central to our approach is Iterative Constructive\nPerturbation (ICP), which leverages the model's loss to iteratively perturb the\ninput, progressively constructing an enhanced representation over some\nrefinement steps. This ICP input is then fed back into the model to produce\nimproved intermediate features, which serve as a target in a self-distillation\nframework against the original features. By alternately altering the model's\nparameters to the data and the data to the model, our method effectively\naddresses the gap between fitting and generalization, leading to enhanced\nperformance. Extensive experiments demonstrate that our approach not only\nmitigates common performance bottlenecks in neural networks but also\ndemonstrates significant improvements across training variations."}
{"id": "2505.15400", "pdf": "https://arxiv.org/pdf/2505.15400", "abs": "https://arxiv.org/abs/2505.15400", "authors": ["Xiaoyun Zhang", "Jingqing Ruan", "Xing Ma", "Yawen Zhu", "Haodong Zhao", "Hao Li", "Jiansong Chen", "Ke Zeng", "Xunliang Cai"], "title": "When to Continue Thinking: Adaptive Thinking Mode Switching for Efficient Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) achieve remarkable performance via long\nreasoning chains, but often incur excessive computational overhead due to\nredundant reasoning, especially on simple tasks. In this work, we\nsystematically quantify the upper bounds of LRMs under both Long-Thinking and\nNo-Thinking modes, and uncover the phenomenon of \"Internal Self-Recovery\nMechanism\" where models implicitly supplement reasoning during answer\ngeneration. Building on this insight, we propose Adaptive Self-Recovery\nReasoning (ASRR), a framework that suppresses unnecessary reasoning and enables\nimplicit recovery. By introducing accuracy-aware length reward regulation, ASRR\nadaptively allocates reasoning effort according to problem difficulty,\nachieving high efficiency with negligible performance sacrifice. Experiments\nacross multiple benchmarks and models show that, compared with GRPO, ASRR\nreduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minimal\naccuracy loss (1.2% and 0.6% pass@1), and significantly boosts harmless rates\non safety benchmarks (up to +21.7%). Our results highlight the potential of\nASRR for enabling efficient, adaptive, and safer reasoning in LRMs."}
{"id": "2505.15728", "pdf": "https://arxiv.org/pdf/2505.15728", "abs": "https://arxiv.org/abs/2505.15728", "authors": ["Luqin Gan", "Tarek M. Zikry", "Genevera I. Allen"], "title": "Are machine learning interpretations reliable? A stability study on global interpretations", "categories": ["stat.ML", "cs.LG", "stat.AP"], "comment": "17 pages main text, 5 main text figures. 57 pages in total with\n  Appendix and Bibliography", "summary": "As machine learning systems are increasingly used in high-stakes domains,\nthere is a growing emphasis placed on making them interpretable to improve\ntrust in these systems. In response, a range of interpretable machine learning\n(IML) methods have been developed to generate human-understandable insights\ninto otherwise black box models. With these methods, a fundamental question\narises: Are these interpretations reliable? Unlike with prediction accuracy or\nother evaluation metrics for supervised models, the proximity to the true\ninterpretation is difficult to define. Instead, we ask a closely related\nquestion that we argue is a prerequisite for reliability: Are these\ninterpretations stable? We define stability as findings that are consistent or\nreliable under small random perturbations to the data or algorithms. In this\nstudy, we conduct the first systematic, large-scale empirical stability study\non popular machine learning global interpretations for both supervised and\nunsupervised tasks on tabular data. Our findings reveal that popular\ninterpretation methods are frequently unstable, notably less stable than the\npredictions themselves, and that there is no association between the accuracy\nof machine learning predictions and the stability of their associated\ninterpretations. Moreover, we show that no single method consistently provides\nthe most stable interpretations across a range of benchmark datasets. Overall,\nthese results suggest that interpretability alone does not warrant trust, and\nunderscores the need for rigorous evaluation of interpretation stability in\nfuture work. To support these principles, we have developed and released an\nopen source IML dashboard and Python package to enable researchers to assess\nthe stability and reliability of their own data-driven interpretations and\ndiscoveries."}
{"id": "2505.14752", "pdf": "https://arxiv.org/pdf/2505.14752", "abs": "https://arxiv.org/abs/2505.14752", "authors": ["Yihong Tang", "Menglin Kong", "Lijun Sun"], "title": "Large Language Models for Data Synthesis", "categories": ["cs.LG"], "comment": null, "summary": "Generating synthetic data that faithfully captures the statistical structure\nof real-world distributions is a fundamental challenge in data modeling.\nClassical approaches often depend on strong parametric assumptions or manual\nstructural design and struggle in high-dimensional or heterogeneous domains.\nRecent progress in Large Language Models (LLMs) reveals their potential as\nflexible, high-dimensional priors over real-world distributions. However, when\napplied to data synthesis, standard LLM-based sampling is inefficient,\nconstrained by fixed context limits, and fails to ensure statistical alignment.\nGiven this, we introduce LLMSynthor, a general framework for data synthesis\nthat transforms LLMs into structure-aware simulators guided by distributional\nfeedback. LLMSynthor treats the LLM as a nonparametric copula simulator for\nmodeling high-order dependencies and introduces LLM Proposal Sampling to\ngenerate grounded proposal distributions that improve sampling efficiency\nwithout requiring rejection. By minimizing discrepancies in the summary\nstatistics space, the iterative synthesis loop aligns real and synthetic data\nwhile gradually uncovering and refining the latent generative structure. We\nevaluate LLMSynthor in both controlled and real-world settings using\nheterogeneous datasets in privacy-sensitive domains (e.g., e-commerce,\npopulation, and mobility) that encompass both structured and unstructured\nformats. The synthetic data produced by LLMSynthor shows high statistical\nfidelity, practical utility, and cross-data adaptability, positioning it as a\nvaluable tool across economics, social science, urban studies, and beyond."}
{"id": "2505.15410", "pdf": "https://arxiv.org/pdf/2505.15410", "abs": "https://arxiv.org/abs/2505.15410", "authors": ["Bahar Radmehr", "Ekaterina Shved", "Fatma Betül Güreş", "Adish Singla", "Tanja Käser"], "title": "ClickSight: Interpreting Student Clickstreams to Reveal Insights on Learning Strategies via LLMs", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted in Latebreaking results track in AIED 2025(26th\n  International Conference on Artificial Intelligence in Education JULY 22-26,\n  2025 PALERMO, ITALY)", "summary": "Clickstream data from digital learning environments offer valuable insights\ninto students' learning behaviors, but are challenging to interpret due to\ntheir high dimensionality and granularity. Prior approaches have relied mainly\non handcrafted features, expert labeling, clustering, or supervised models,\ntherefore often lacking generalizability and scalability. In this work, we\nintroduce ClickSight, an in-context Large Language Model (LLM)-based pipeline\nthat interprets student clickstreams to reveal their learning strategies.\nClickSight takes raw clickstreams and a list of learning strategies as input\nand generates textual interpretations of students' behaviors during\ninteraction. We evaluate four different prompting strategies and investigate\nthe impact of self-refinement on interpretation quality. Our evaluation spans\ntwo open-ended learning environments and uses a rubric-based domain-expert\nevaluation. Results show that while LLMs can reasonably interpret learning\nstrategies from clickstreams, interpretation quality varies by prompting\nstrategy, and self-refinement offers limited improvement. ClickSight\ndemonstrates the potential of LLMs to generate theory-driven insights from\neducational interaction data."}
{"id": "2505.14825", "pdf": "https://arxiv.org/pdf/2505.14825", "abs": "https://arxiv.org/abs/2505.14825", "authors": ["Marios Andreou", "Nan Chen", "Erik Bollt"], "title": "Assimilative Causal Inference", "categories": ["cs.LG", "math.ST", "physics.data-an", "stat.ME", "stat.ML", "stat.TH", "62F15, 62D20, 62M20, 93E11, 93E14, 60H10"], "comment": "Includes the Main Text and Supporting Information in a single\n  document. 39 pages (p. 1--2 Title, Contents and Abstract | p. 3--14 Main Text\n  | p. 15--39 Supporting Information), 9 figures (3 in the Main Text and 6 in\n  the Supporting Information), typeset in LaTeX. Submitted for peer-review. For\n  more info see https://mariosandreou.short.gy/ACI", "summary": "Causal inference determines cause-and-effect relationships between variables\nand has broad applications across disciplines. Traditional time-series methods\noften reveal causal links only in a time-averaged sense, while ensemble-based\ninformation transfer approaches detect the time evolution of short-term causal\nrelationships but are typically limited to low-dimensional systems. In this\npaper, a new causal inference framework, called assimilative causal inference\n(ACI), is developed. Fundamentally different from the state-of-the-art methods,\nACI uses a dynamical system and a single realization of a subset of the state\nvariables to identify instantaneous causal relationships and the dynamic\nevolution of the associated causal influence range (CIR). Instead of\nquantifying how causes influence effects as done traditionally, ACI solves an\ninverse problem via Bayesian data assimilation, thus tracing causes backward\nfrom observed effects with an implicit Bayesian hypothesis. Causality is\ndetermined by assessing whether incorporating the information of the effect\nvariables reduces the uncertainty in recovering the potential cause variables.\nACI has several desirable features. First, it captures the dynamic interplay of\nvariables, where their roles as causes and effects can shift repeatedly over\ntime. Second, a mathematically justified objective criterion determines the CIR\nwithout empirical thresholds. Third, ACI is scalable to high-dimensional\nproblems by leveraging computationally efficient Bayesian data assimilation\ntechniques. Finally, ACI applies to short time series and incomplete datasets.\nNotably, ACI does not require observations of candidate causes, which is a key\nadvantage since potential drivers are often unknown or unmeasured. The\neffectiveness of ACI is demonstrated by complex dynamical systems showcasing\nintermittency and extreme events."}
{"id": "2505.14756", "pdf": "https://arxiv.org/pdf/2505.14756", "abs": "https://arxiv.org/abs/2505.14756", "authors": ["Chih-Yu Chang", "Milad Azvar", "Chinedum Okwudire", "Raed Al Kontar"], "title": "$\\texttt{LLINBO}$: Trustworthy LLM-in-the-Loop Bayesian Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Bayesian optimization (BO) is a sequential decision-making tool widely used\nfor optimizing expensive black-box functions. Recently, Large Language Models\n(LLMs) have shown remarkable adaptability in low-data regimes, making them\npromising tools for black-box optimization by leveraging contextual knowledge\nto propose high-quality query points. However, relying solely on LLMs as\noptimization agents introduces risks due to their lack of explicit surrogate\nmodeling and calibrated uncertainty, as well as their inherently opaque\ninternal mechanisms. This structural opacity makes it difficult to characterize\nor control the exploration-exploitation trade-off, ultimately undermining\ntheoretical tractability and reliability. To address this, we propose LLINBO:\nLLM-in-the-Loop BO, a hybrid framework for BO that combines LLMs with\nstatistical surrogate experts (e.g., Gaussian Processes (GP)). The core\nphilosophy is to leverage contextual reasoning strengths of LLMs for early\nexploration, while relying on principled statistical models to guide efficient\nexploitation. Specifically, we introduce three mechanisms that enable this\ncollaboration and establish their theoretical guarantees. We end the paper with\na real-life proof-of-concept in the context of 3D printing. The code to\nreproduce the results can be found at\nhttps://github.com/UMDataScienceLab/LLM-in-the-Loop-BO."}
{"id": "2505.15693", "pdf": "https://arxiv.org/pdf/2505.15693", "abs": "https://arxiv.org/abs/2505.15693", "authors": ["Milad Kazemi", "Mateo Perez", "Fabio Somenzi", "Sadegh Soudjani", "Ashutosh Trivedi", "Alvaro Velasquez"], "title": "Average Reward Reinforcement Learning for Omega-Regular and Mean-Payoff Objectives", "categories": ["cs.AI"], "comment": "29 pages, 6 figures and 2 tables", "summary": "Recent advances in reinforcement learning (RL) have renewed focus on the\ndesign of reward functions that shape agent behavior. Manually designing reward\nfunctions is tedious and error-prone. A principled alternative is to specify\nbehaviors in a formal language that can be automatically translated into\nrewards. Omega-regular languages are a natural choice for this purpose, given\ntheir established role in formal verification and synthesis. However, existing\nmethods using omega-regular specifications typically rely on discounted reward\nRL in episodic settings, with periodic resets. This setup misaligns with the\nsemantics of omega-regular specifications, which describe properties over\ninfinite behavior traces. In such cases, the average reward criterion and the\ncontinuing setting -- where the agent interacts with the environment over a\nsingle, uninterrupted lifetime -- are more appropriate.\n  To address the challenges of infinite-horizon, continuing tasks, we focus on\nabsolute liveness specifications -- a subclass of omega-regular languages that\ncannot be violated by any finite behavior prefix, making them well-suited to\nthe continuing setting. We present the first model-free RL framework that\ntranslates absolute liveness specifications to average-reward objectives. Our\napproach enables learning in communicating MDPs without episodic resetting. We\nalso introduce a reward structure for lexicographic multi-objective\noptimization, aiming to maximize an external average-reward objective among the\npolicies that also maximize the satisfaction probability of a given\nomega-regular specification. Our method guarantees convergence in unknown\ncommunicating MDPs and supports on-the-fly reductions that do not require full\nknowledge of the environment, thus enabling model-free RL. Empirical results\nshow our average-reward approach in continuing setting outperforms\ndiscount-based methods across benchmarks."}
{"id": "2505.14826", "pdf": "https://arxiv.org/pdf/2505.14826", "abs": "https://arxiv.org/abs/2505.14826", "authors": ["Rohan Deb", "Kiran Thekumparampil", "Kousha Kalantari", "Gaurush Hiranandani", "Shoham Sabach", "Branislav Kveton"], "title": "FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain", "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": null, "summary": "Supervised fine-tuning (SFT) is a standard approach to adapting large\nlanguage models (LLMs) to new domains. In this work, we improve the statistical\nefficiency of SFT by selecting an informative subset of training examples.\nSpecifically, for a fixed budget of training examples, which determines the\ncomputational cost of fine-tuning, we determine the most informative ones. The\nkey idea in our method is to select examples that maximize information gain,\nmeasured by the Hessian of the log-likelihood of the LLM. We approximate it\nefficiently by linearizing the LLM at the last layer using multinomial logistic\nregression models. Our approach is computationally efficient, analyzable, and\nperforms well empirically. We demonstrate this on several problems, and back\nour claims with both quantitative results and an LLM evaluation."}
{"id": "2505.14765", "pdf": "https://arxiv.org/pdf/2505.14765", "abs": "https://arxiv.org/abs/2505.14765", "authors": ["Orhun Vural", "Bunyamin Ozaydin", "Khalid Y. Aram", "James Booth", "Brittany F. Lindsey", "Abdulaziz Ahmed"], "title": "Deep Learning-Based Forecasting of Boarding Patient Counts to Address ED Overcrowding", "categories": ["cs.LG", "cs.AI", "68T07", "I.2.6; J.3"], "comment": null, "summary": "This study develops deep learning models to forecast the number of patients\nin the emergency department (ED) boarding phase six hours in advance, aiming to\nsupport proactive operational decision-making using only non-clinical,\noperational, and contextual features. Data were collected from five sources: ED\ntracking systems, inpatient census records, weather reports, federal holiday\ncalendars, and local event schedules. After feature engineering, the data were\naggregated at an hourly level, cleaned, and merged into a unified dataset for\nmodel training. Several time series deep learning models, including ResNetPlus,\nTSTPlus, TSiTPlus (from the tsai library), and N-BEATSx, were trained using\nOptuna and grid search for hyperparameter tuning. The average ED boarding count\nwas 28.7, with a standard deviation of 11.2. N-BEATSx achieved the best\nperformance, with a mean absolute error of 2.10, mean squared error of 7.08,\nroot mean squared error of 2.66, and a coefficient of determination of 0.95.\nThe model maintained stable accuracy even during periods of extremely high\nboarding counts, defined as values exceeding one, two, or three standard\ndeviations above the mean. Results show that accurate six-hour-ahead forecasts\nare achievable without using patient-level clinical data. While strong\nperformance was observed even with a basic feature set, the inclusion of\nadditional features improved prediction stability under extreme conditions.\nThis framework offers a practical and generalizable approach for hospital\nsystems to anticipate boarding levels and help mitigate ED overcrowding."}
{"id": "2505.15742", "pdf": "https://arxiv.org/pdf/2505.15742", "abs": "https://arxiv.org/abs/2505.15742", "authors": ["Adam Gould", "Francesca Toni"], "title": "Neuro-Argumentative Learning with Case-Based Reasoning", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted to NeSy25", "summary": "We introduce Gradual Abstract Argumentation for Case-Based Reasoning (Gradual\nAA-CBR), a data-driven, neurosymbolic classification model in which the outcome\nis determined by an argumentation debate structure that is learned\nsimultaneously with neural-based feature extractors. Each argument in the\ndebate is an observed case from the training data, favouring their labelling.\nCases attack or support those with opposing or agreeing labellings, with the\nstrength of each argument and relationship learned through gradient-based\nmethods. This argumentation debate structure provides human-aligned reasoning,\nimproving model interpretability compared to traditional neural networks (NNs).\nUnlike the existing purely symbolic variant, Abstract Argumentation for\nCase-Based Reasoning (AA-CBR), Gradual AA-CBR is capable of multi-class\nclassification, automatic learning of feature and data point importance,\nassigning uncertainty values to outcomes, using all available data points, and\ndoes not require binary features. We show that Gradual AA-CBR performs\ncomparably to NNs whilst significantly outperforming existing AA-CBR\nformulations."}
{"id": "2505.14999", "pdf": "https://arxiv.org/pdf/2505.14999", "abs": "https://arxiv.org/abs/2505.14999", "authors": ["Eric Hanchen Jiang", "Haozheng Luo", "Shengyuan Pang", "Xiaomin Li", "Zhenting Qi", "Hengli Li", "Cheng-Fu Yang", "Zongyu Lin", "Xinfeng Li", "Hao Xu", "Kai-Wei Chang", "Ying Nian Wu"], "title": "Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Mathematical reasoning presents a significant challenge for Large Language\nModels (LLMs), often requiring robust multi step logical consistency. While\nChain of Thought (CoT) prompting elicits reasoning steps, it doesn't guarantee\ncorrectness, and improving reliability via extensive sampling is\ncomputationally costly. This paper introduces the Energy Outcome Reward Model\n(EORM), an effective, lightweight, post hoc verifier. EORM leverages Energy\nBased Models (EBMs) to simplify the training of reward models by learning to\nassign a scalar energy score to CoT solutions using only outcome labels,\nthereby avoiding detailed annotations. It achieves this by interpreting\ndiscriminator output logits as negative energies, effectively ranking\ncandidates where lower energy is assigned to solutions leading to correct final\noutcomes implicitly favoring coherent reasoning. On mathematical benchmarks\n(GSM8k, MATH), EORM significantly improves final answer accuracy (e.g., with\nLlama 3 8B, achieving 90.7% on GSM8k and 63.7% on MATH). EORM effectively\nleverages a given pool of candidate solutions to match or exceed the\nperformance of brute force sampling, thereby enhancing LLM reasoning outcome\nreliability through its streamlined post hoc verification process."}
{"id": "2505.14766", "pdf": "https://arxiv.org/pdf/2505.14766", "abs": "https://arxiv.org/abs/2505.14766", "authors": ["Ben Cohen", "Emaad Khwaja", "Youssef Doubli", "Salahidine Lemaachi", "Chris Lettieri", "Charles Masson", "Hugo Miccinilli", "Elise Ramé", "Qiqi Ren", "Afshin Rostamizadeh", "Jean Ogier du Terrail", "Anna-Monica Toon", "Kan Wang", "Stephan Xie", "David Asker", "Ameet Talwalkar", "Othmane Abou-Amal"], "title": "This Time is Different: An Observability Perspective on Time Series Foundation Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce Toto, a time series forecasting foundation model with 151\nmillion parameters. Toto uses a modern decoder-only architecture coupled with\narchitectural innovations designed to account for specific challenges found in\nmultivariate observability time series data. Toto's pre-training corpus is a\nmixture of observability data, open datasets, and synthetic data, and is\n4-10$\\times$ larger than those of leading time series foundation models.\nAdditionally, we introduce BOOM, a large-scale benchmark consisting of 350\nmillion observations across 2,807 real-world time series. For both Toto and\nBOOM, we source observability data exclusively from Datadog's own telemetry and\ninternal observability metrics. Extensive evaluations demonstrate that Toto\nachieves state-of-the-art performance on both BOOM and on established general\npurpose time series forecasting benchmarks. Toto's model weights, inference\ncode, and evaluation scripts, as well as BOOM's data and evaluation code, are\nall available as open source under the Apache 2.0 License available at\nhttps://huggingface.co/Datadog/Toto-Open-Base-1.0 and\nhttps://github.com/DataDog/toto."}
{"id": "2505.14733", "pdf": "https://arxiv.org/pdf/2505.14733", "abs": "https://arxiv.org/abs/2505.14733", "authors": ["Yunho Jin", "Gu-Yeon Wei", "David Brooks"], "title": "The Energy Cost of Reasoning: Analyzing Energy Usage in LLMs with Test-time Compute", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Scaling large language models (LLMs) has driven significant advancements, yet\nit faces diminishing returns and escalating energy demands. This work\nintroduces test-time compute (TTC)-allocating additional computational\nresources during inference-as a compelling complement to conventional scaling\nstrategies. Specifically, we investigate whether employing TTC can achieve\nsuperior accuracy-energy trade-offs compared to simply increasing model size.\nOur empirical analysis reveals that TTC surpasses traditional model scaling in\naccuracy/energy efficiency, with notable gains in tasks demanding complex\nreasoning rather than mere factual recall. Further, we identify a critical\ninteraction between TTC performance and output sequence length, demonstrating\nthat strategically adjusting compute resources at inference time according to\nquery complexity can substantially enhance efficiency. Our findings advocate\nfor TTC as a promising direction, enabling more sustainable, accurate, and\nadaptable deployment of future language models without incurring additional\npretraining costs."}
{"id": "2505.15008", "pdf": "https://arxiv.org/pdf/2505.15008", "abs": "https://arxiv.org/abs/2505.15008", "authors": ["Alvin Heng", "Harold Soh"], "title": "Know When to Abstain: Optimal Selective Classification with Likelihood Ratios", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Selective classification enhances the reliability of predictive models by\nallowing them to abstain from making uncertain predictions. In this work, we\nrevisit the design of optimal selection functions through the lens of the\nNeyman--Pearson lemma, a classical result in statistics that characterizes the\noptimal rejection rule as a likelihood ratio test. We show that this\nperspective not only unifies the behavior of several post-hoc selection\nbaselines, but also motivates new approaches to selective classification which\nwe propose here. A central focus of our work is the setting of covariate shift,\nwhere the input distribution at test time differs from that at training. This\nrealistic and challenging scenario remains relatively underexplored in the\ncontext of selective classification. We evaluate our proposed methods across a\nrange of vision and language tasks, including both supervised learning and\nvision-language models. Our experiments demonstrate that our\nNeyman--Pearson-informed methods consistently outperform existing baselines,\nindicating that likelihood ratio-based selection offers a robust mechanism for\nimproving selective classification under covariate shifts. Our code is publicly\navailable at https://github.com/clear-nus/sc-likelihood-ratios."}
{"id": "2505.14777", "pdf": "https://arxiv.org/pdf/2505.14777", "abs": "https://arxiv.org/abs/2505.14777", "authors": ["Mingquan Feng", "Yixin Huang", "Yifan Fu", "Shaobo Wang", "Junchi Yan"], "title": "KO: Kinetics-inspired Neural Optimizer with PDE Simulation Approaches", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The design of optimization algorithms for neural networks remains a critical\nchallenge, with most existing methods relying on heuristic adaptations of\ngradient-based approaches. This paper introduces KO (Kinetics-inspired\nOptimizer), a novel neural optimizer inspired by kinetic theory and partial\ndifferential equation (PDE) simulations. We reimagine the training dynamics of\nnetwork parameters as the evolution of a particle system governed by kinetic\nprinciples, where parameter updates are simulated via a numerical scheme for\nthe Boltzmann transport equation (BTE) that models stochastic particle\ncollisions. This physics-driven approach inherently promotes parameter\ndiversity during optimization, mitigating the phenomenon of parameter\ncondensation, i.e. collapse of network parameters into low-dimensional\nsubspaces, through mechanisms analogous to thermal diffusion in physical\nsystems. We analyze this property, establishing both a mathematical proof and a\nphysical interpretation. Extensive experiments on image classification\n(CIFAR-10/100, ImageNet) and text classification (IMDB, Snips) tasks\ndemonstrate that KO consistently outperforms baseline optimizers (e.g., Adam,\nSGD), achieving accuracy improvements while computation cost remains\ncomparable."}
{"id": "2505.14737", "pdf": "https://arxiv.org/pdf/2505.14737", "abs": "https://arxiv.org/abs/2505.14737", "authors": ["Huiliang Zhang", "Di Wu", "Arnaud Zinflou", "Stephane Dellacherie", "Mouhamadou Makhtar Dione", "Benoit Boulet"], "title": "Leveraging Multivariate Long-Term History Representation for Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multivariate Time Series (MTS) forecasting has a wide range of applications\nin both industry and academia. Recent advances in Spatial-Temporal Graph Neural\nNetwork (STGNN) have achieved great progress in modelling spatial-temporal\ncorrelations. Limited by computational complexity, most STGNNs for MTS\nforecasting focus primarily on short-term and local spatial-temporal\ndependencies. Although some recent methods attempt to incorporate univariate\nhistory into modeling, they still overlook crucial long-term spatial-temporal\nsimilarities and correlations across MTS, which are essential for accurate\nforecasting. To fill this gap, we propose a framework called the Long-term\nMultivariate History Representation (LMHR) Enhanced STGNN for MTS forecasting.\nSpecifically, a Long-term History Encoder (LHEncoder) is adopted to effectively\nencode the long-term history into segment-level contextual representations and\nreduce point-level noise. A non-parametric Hierarchical Representation\nRetriever (HRetriever) is designed to include the spatial information in the\nlong-term spatial-temporal dependency modelling and pick out the most valuable\nrepresentations with no additional training. A Transformer-based Aggregator\n(TAggregator) selectively fuses the sparsely retrieved contextual\nrepresentations based on the ranking positional embedding efficiently.\nExperimental results demonstrate that LMHR outperforms typical STGNNs by 10.72%\non the average prediction horizons and state-of-the-art methods by 4.12% on\nseveral real-world datasets. Additionally, it consistently improves prediction\naccuracy by 9.8% on the top 10% of rapidly changing patterns across the\ndatasets."}
{"id": "2505.15064", "pdf": "https://arxiv.org/pdf/2505.15064", "abs": "https://arxiv.org/abs/2505.15064", "authors": ["Sho Sonoda", "Yuka Hashimoto", "Isao Ishikawa", "Masahiro Ikeda"], "title": "Generalization Through Growth: Hidden Dynamics Controls Depth Dependence", "categories": ["cs.LG", "math.DS", "stat.ML"], "comment": null, "summary": "Recent theory has reduced the depth dependence of generalization bounds from\nexponential to polynomial and even depth-independent rates, yet these results\nremain tied to specific architectures and Euclidean inputs. We present a\nunified framework for arbitrary \\blue{pseudo-metric} spaces in which a\ndepth-\\(k\\) network is the composition of continuous hidden maps\n\\(f:\\mathcal{X}\\to \\mathcal{X}\\) and an output map \\(h:\\mathcal{X}\\to\n\\mathbb{R}\\). The resulting bound $O(\\sqrt{(\\alpha + \\log \\beta(k))/n})$\nisolates the sole depth contribution in \\(\\beta(k)\\), the word-ball growth of\nthe semigroup generated by the hidden layers. By Gromov's theorem polynomial\n(resp. exponential) growth corresponds to virtually nilpotent (resp. expanding)\ndynamics, revealing a geometric dichotomy behind existing $O(\\sqrt{k})$\n(sublinear depth) and $\\tilde{O}(1)$ (depth-independent) rates. We further\nprovide covering-number estimates showing that expanding dynamics yield an\nexponential parameter saving via compositional expressivity. Our results\ndecouple specification from implementation, offering architecture-agnostic and\ndynamical-systems-aware guarantees applicable to modern deep-learning paradigms\nsuch as test-time inference and diffusion models."}
{"id": "2505.14802", "pdf": "https://arxiv.org/pdf/2505.14802", "abs": "https://arxiv.org/abs/2505.14802", "authors": ["Iman Kazemian", "Paritosh Ramanan", "Murat Yildirim"], "title": "Text embedding models can be great data engineers", "categories": ["cs.LG"], "comment": null, "summary": "Data engineering pipelines are essential - albeit costly - components of\npredictive analytics frameworks requiring significant engineering time and\ndomain expertise for carrying out tasks such as data ingestion, preprocessing,\nfeature extraction, and feature engineering. In this paper, we propose ADEPT,\nan automated data engineering pipeline via text embeddings. At the core of the\nADEPT framework is a simple yet powerful idea that the entropy of embeddings\ncorresponding to textually dense raw format representation of time series can\nbe intuitively viewed as equivalent (or in many cases superior) to that of\nnumerically dense vector representations obtained by data engineering\npipelines. Consequently, ADEPT uses a two step approach that (i) leverages text\nembeddings to represent the diverse data sources, and (ii) constructs a\nvariational information bottleneck criteria to mitigate entropy variance in\ntext embeddings of time series data. ADEPT provides an end-to-end automated\nimplementation of predictive models that offers superior predictive performance\ndespite issues such as missing data, ill-formed records, improper or corrupted\ndata formats and irregular timestamps. Through exhaustive experiments, we show\nthat the ADEPT outperforms the best existing benchmarks in a diverse set of\ndatasets from large-scale applications across healthcare, finance, science and\nindustrial internet of things. Our results show that ADEPT can potentially\nleapfrog many conventional data pipeline steps thereby paving the way for\nefficient and scalable automation pathways for diverse data science\napplications."}
{"id": "2505.14739", "pdf": "https://arxiv.org/pdf/2505.14739", "abs": "https://arxiv.org/abs/2505.14739", "authors": ["Heiko Oppel", "Andreas Spilz", "Michael Munz"], "title": "Time Series Similarity Score Functions to Monitor and Interact with the Training and Denoising Process of a Time Series Diffusion Model applied to a Human Activity Recognition Dataset based on IMUs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Denoising diffusion probabilistic models are able to generate synthetic\nsensor signals. The training process of such a model is controlled by a loss\nfunction which measures the difference between the noise that was added in the\nforward process and the noise that was predicted by the diffusion model. This\nenables the generation of realistic data. However, the randomness within the\nprocess and the loss function itself makes it difficult to estimate the quality\nof the data. Therefore, we examine multiple similarity metrics and adapt an\nexisting metric to overcome this issue by monitoring the training and\nsynthetisation process using those metrics. The adapted metric can even be\nfine-tuned on the input data to comply with the requirements of an underlying\nclassification task. We were able to significantly reduce the amount of\ntraining epochs without a performance reduction in the classification task. An\noptimized training process not only saves resources, but also reduces the time\nfor training generative models."}
{"id": "2505.15141", "pdf": "https://arxiv.org/pdf/2505.15141", "abs": "https://arxiv.org/abs/2505.15141", "authors": ["Yunlong Hou", "Fengzhuo Zhang", "Cunxiao Du", "Xuan Zhang", "Jiachun Pan", "Tianyu Pang", "Chao Du", "Vincent Y. F. Tan", "Zhuoran Yang"], "title": "BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "35 pages, 4 figures", "summary": "Speculative decoding has emerged as a popular method to accelerate the\ninference of Large Language Models (LLMs) while retaining their superior text\ngeneration performance. Previous methods either adopt a fixed speculative\ndecoding configuration regardless of the prefix tokens, or train draft models\nin an offline or online manner to align them with the context. This paper\nproposes a training-free online learning framework to adaptively choose the\nconfiguration of the hyperparameters for speculative decoding as text is being\ngenerated. We first formulate this hyperparameter selection problem as a\nMulti-Armed Bandit problem and provide a general speculative decoding framework\nBanditSpec. Furthermore, two bandit-based hyperparameter selection algorithms,\nUCBSpec and EXP3Spec, are designed and analyzed in terms of a novel quantity,\nthe stopping time regret. We upper bound this regret under both stochastic and\nadversarial reward settings. By deriving an information-theoretic impossibility\nresult, it is shown that the regret performance of UCBSpec is optimal up to\nuniversal constants. Finally, extensive empirical experiments with LLaMA3 and\nQwen2 demonstrate that our algorithms are effective compared to existing\nmethods, and the throughput is close to the oracle best hyperparameter in\nsimulated real-life LLM serving scenarios with diverse input prompts."}
{"id": "2505.14803", "pdf": "https://arxiv.org/pdf/2505.14803", "abs": "https://arxiv.org/abs/2505.14803", "authors": ["Yu Liu", "Weiyao Tao", "Tong Xia", "Simon Knight", "Tingting Zhu"], "title": "SurvUnc: A Meta-Model Based Uncertainty Quantification Framework for Survival Analysis", "categories": ["cs.LG", "cs.AI", "cs.ET"], "comment": "KDD 2025", "summary": "Survival analysis, which estimates the probability of event occurrence over\ntime from censored data, is fundamental in numerous real-world applications,\nparticularly in high-stakes domains such as healthcare and risk assessment.\nDespite advances in numerous survival models, quantifying the uncertainty of\npredictions from these models remains underexplored and challenging. The lack\nof reliable uncertainty quantification limits the interpretability and\ntrustworthiness of survival models, hindering their adoption in clinical\ndecision-making and other sensitive applications. To bridge this gap, in this\nwork, we introduce SurvUnc, a novel meta-model based framework for post-hoc\nuncertainty quantification for survival models. SurvUnc introduces an\nanchor-based learning strategy that integrates concordance knowledge into\nmeta-model optimization, leveraging pairwise ranking performance to estimate\nuncertainty effectively. Notably, our framework is model-agnostic, ensuring\ncompatibility with any survival model without requiring modifications to its\narchitecture or access to its internal parameters. Especially, we design a\ncomprehensive evaluation pipeline tailored to this critical yet overlooked\nproblem. Through extensive experiments on four publicly available benchmarking\ndatasets and five representative survival models, we demonstrate the\nsuperiority of SurvUnc across multiple evaluation scenarios, including\nselective prediction, misprediction detection, and out-of-domain detection. Our\nresults highlight the effectiveness of SurvUnc in enhancing model\ninterpretability and reliability, paving the way for more trustworthy survival\npredictions in real-world applications."}
{"id": "2505.14741", "pdf": "https://arxiv.org/pdf/2505.14741", "abs": "https://arxiv.org/abs/2505.14741", "authors": ["Kunyun Wang", "Bohan Li", "Kai Yu", "Minyi Guo", "Jieru Zhao"], "title": "Communication-Efficient Diffusion Denoising Parallelization via Reuse-then-Predict Mechanism", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Diffusion models have emerged as a powerful class of generative models across\nvarious modalities, including image, video, and audio synthesis. However, their\ndeployment is often limited by significant inference latency, primarily due to\nthe inherently sequential nature of the denoising process. While existing\nparallelization strategies attempt to accelerate inference by distributing\ncomputation across multiple devices, they typically incur high communication\noverhead, hindering deployment on commercial hardware. To address this\nchallenge, we propose \\textbf{ParaStep}, a novel parallelization method based\non a reuse-then-predict mechanism that parallelizes diffusion inference by\nexploiting similarity between adjacent denoising steps. Unlike prior approaches\nthat rely on layer-wise or stage-wise communication, ParaStep employs\nlightweight, step-wise communication, substantially reducing overhead. ParaStep\nachieves end-to-end speedups of up to \\textbf{3.88}$\\times$ on SVD,\n\\textbf{2.43}$\\times$ on CogVideoX-2b, and \\textbf{6.56}$\\times$ on\nAudioLDM2-large, while maintaining generation quality. These results highlight\nParaStep as a scalable and communication-efficient solution for accelerating\ndiffusion inference, particularly in bandwidth-constrained environments."}
{"id": "2505.15195", "pdf": "https://arxiv.org/pdf/2505.15195", "abs": "https://arxiv.org/abs/2505.15195", "authors": ["Adel Javanmard", "Rudrajit Das", "Alessandro Epasto", "Vahab Mirrokni"], "title": "Self-Boost via Optimal Retraining: An Analysis via Approximate Message Passing", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": "31 pages, 6 figures, 5 tables", "summary": "Retraining a model using its own predictions together with the original,\npotentially noisy labels is a well-known strategy for improving the model\nperformance. While prior works have demonstrated the benefits of specific\nheuristic retraining schemes, the question of how to optimally combine the\nmodel's predictions and the provided labels remains largely open. This paper\naddresses this fundamental question for binary classification tasks. We develop\na principled framework based on approximate message passing (AMP) to analyze\niterative retraining procedures for two ground truth settings: Gaussian mixture\nmodel (GMM) and generalized linear model (GLM). Our main contribution is the\nderivation of the Bayes optimal aggregator function to combine the current\nmodel's predictions and the given labels, which when used to retrain the same\nmodel, minimizes its prediction error. We also quantify the performance of this\noptimal retraining strategy over multiple rounds. We complement our theoretical\nresults by proposing a practically usable version of the theoretically-optimal\naggregator function for linear probing with the cross-entropy loss, and\ndemonstrate its superiority over baseline methods in the high label noise\nregime."}
{"id": "2505.14820", "pdf": "https://arxiv.org/pdf/2505.14820", "abs": "https://arxiv.org/abs/2505.14820", "authors": ["Rushit N. Shah", "Nikolaos Agadakos", "Synthia Sasulski", "Ali Farajzadeh", "Sanjiban Choudhury", "Brian Ziebart"], "title": "Imitation Learning via Focused Satisficing", "categories": ["cs.LG"], "comment": "Accepted for publication at the 34th International Joint Conference\n  on Artificial Intelligence (IJCAI 2025)", "summary": "Imitation learning often assumes that demonstrations are close to optimal\naccording to some fixed, but unknown, cost function. However, according to\nsatisficing theory, humans often choose acceptable behavior based on their\npersonal (and potentially dynamic) levels of aspiration, rather than achieving\n(near-) optimality. For example, a lunar lander demonstration that successfully\nlands without crashing might be acceptable to a novice despite being slow or\njerky. Using a margin-based objective to guide deep reinforcement learning, our\nfocused satisficing approach to imitation learning seeks a policy that\nsurpasses the demonstrator's aspiration levels -- defined over trajectories or\nportions of trajectories -- on unseen demonstrations without explicitly\nlearning those aspirations. We show experimentally that this focuses the policy\nto imitate the highest quality (portions of) demonstrations better than\nexisting imitation learning methods, providing much higher rates of guaranteed\nacceptability to the demonstrator, and competitive true returns on a range of\nenvironments."}
{"id": "2505.14742", "pdf": "https://arxiv.org/pdf/2505.14742", "abs": "https://arxiv.org/abs/2505.14742", "authors": ["Hong Huang", "Dapeng Wu"], "title": "Quaff: Quantized Parameter-Efficient Fine-Tuning under Outlier Spatial Stability Hypothesis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have made exciting achievements across various\ndomains, yet their deployment on resource-constrained personal devices remains\nhindered by the prohibitive computational and memory demands of task-specific\nfine-tuning. While quantization offers a pathway to efficiency, existing\nmethods struggle to balance performance and overhead, either incurring high\ncomputational/memory costs or failing to address activation outliers, a\ncritical bottleneck in quantized fine-tuning. To address these challenges, we\npropose the Outlier Spatial Stability Hypothesis (OSSH): During fine-tuning,\ncertain activation outlier channels retain stable spatial positions across\ntraining iterations. Building on OSSH, we propose Quaff, a Quantized\nparameter-efficient fine-tuning framework for LLMs, optimizing low-precision\nactivation representations through targeted momentum scaling. Quaff dynamically\nsuppresses outliers exclusively in invariant channels using lightweight\noperations, eliminating full-precision weight storage and global rescaling\nwhile reducing quantization errors. Extensive experiments across ten benchmarks\nvalidate OSSH and demonstrate Quaff's efficacy. Specifically, on the GPQA\nreasoning benchmark, Quaff achieves a 1.73x latency reduction and 30% memory\nsavings over full-precision fine-tuning while improving accuracy by 0.6% on the\nPhi-3 model, reconciling the triple trade-off between efficiency, performance,\nand deployability. By enabling consumer-grade GPU fine-tuning (e.g., RTX 2080\nSuper) without sacrificing model utility, Quaff democratizes personalized LLM\ndeployment. The code is available at https://github.com/Little0o0/Quaff.git."}
{"id": "2505.15201", "pdf": "https://arxiv.org/pdf/2505.15201", "abs": "https://arxiv.org/abs/2505.15201", "authors": ["Christian Walder", "Deep Karkhanis"], "title": "Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts\nfor each problem and reward them independently. This optimizes for pass@1\nperformance and prioritizes the strength of isolated samples at the expense of\nthe diversity and collective utility of sets of samples. This under-utilizes\nthe sampling capacity, limiting exploration and eventual improvement on harder\nexamples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a\ntransformation on the final rewards which leads to direct optimization of\npass@k performance, thus optimizing for sets of samples that maximize reward\nwhen considered jointly. Our contribution is to derive novel low variance\nunbiased estimators for pass@k and its gradient, in both the binary and\ncontinuous reward settings. We show optimization with our estimators reduces to\nstandard RL with rewards that have been jointly transformed by a stable and\nefficient transformation function.\n  While previous efforts are restricted to k=n, ours is the first to enable\nrobust optimization of pass@k for any arbitrary k <= n. Moreover, instead of\ntrading off pass@1 performance for pass@k gains, our method allows annealing k\nduring training, optimizing both metrics and often achieving strong pass@1\nnumbers alongside significant pass@k gains.\n  We validate our reward transformations on toy experiments, which reveal the\nvariance reducing properties of our formulations. We also include real-world\nexamples using the open-source LLM, GEMMA-2. We find that our transformation\neffectively optimizes for the target k. Furthermore, higher k values enable\nsolving more and harder problems, while annealing k boosts both the pass@1 and\npass@k . Crucially, for challenging task sets where conventional pass@1\noptimization stalls, our pass@k approach unblocks learning, likely due to\nbetter exploration by prioritizing joint utility over the utility of individual\nsamples."}
{"id": "2505.14821", "pdf": "https://arxiv.org/pdf/2505.14821", "abs": "https://arxiv.org/abs/2505.14821", "authors": ["Runze Zhao", "Yue Yu", "Adams Yiyue Zhu", "Chen Yang", "Dongruo Zhou"], "title": "Sample and Computationally Efficient Continuous-Time Reinforcement Learning with General Function Approximation", "categories": ["cs.LG", "cs.AI"], "comment": "28 pages, 4 figures, 5 tables. Accepted to UAI 2025", "summary": "Continuous-time reinforcement learning (CTRL) provides a principled framework\nfor sequential decision-making in environments where interactions evolve\ncontinuously over time. Despite its empirical success, the theoretical\nunderstanding of CTRL remains limited, especially in settings with general\nfunction approximation. In this work, we propose a model-based CTRL algorithm\nthat achieves both sample and computational efficiency. Our approach leverages\noptimism-based confidence sets to establish the first sample complexity\nguarantee for CTRL with general function approximation, showing that a\nnear-optimal policy can be learned with a suboptimality gap of\n$\\tilde{O}(\\sqrt{d_{\\mathcal{R}} + d_{\\mathcal{F}}}N^{-1/2})$ using $N$\nmeasurements, where $d_{\\mathcal{R}}$ and $d_{\\mathcal{F}}$ denote the\ndistributional Eluder dimensions of the reward and dynamic functions,\nrespectively, capturing the complexity of general function approximation in\nreinforcement learning. Moreover, we introduce structured policy updates and an\nalternative measurement strategy that significantly reduce the number of policy\nupdates and rollouts while maintaining competitive sample efficiency. We\nimplemented experiments to backup our proposed algorithms on continuous control\ntasks and diffusion model fine-tuning, demonstrating comparable performance\nwith significantly fewer policy updates and rollouts."}
{"id": "2505.14745", "pdf": "https://arxiv.org/pdf/2505.14745", "abs": "https://arxiv.org/abs/2505.14745", "authors": ["Varun Raaghav", "Dimitrios Bikos", "Antonio Rago", "Francesca Toni", "Maria Charalambides"], "title": "Explainable Prediction of the Mechanical Properties of Composites with CNNs", "categories": ["cs.LG", "cs.AI", "I.2.1"], "comment": "9 pages, 6 figures", "summary": "Composites are amongst the most important materials manufactured today, as\nevidenced by their use in countless applications. In order to establish the\nsuitability of composites in specific applications, finite element (FE)\nmodelling, a numerical method based on partial differential equations, is the\nindustry standard for assessing their mechanical properties. However, FE\nmodelling is exceptionally costly from a computational viewpoint, a limitation\nwhich has led to efforts towards applying AI models to this task. However, in\nthese approaches: the chosen model architectures were rudimentary, feed-forward\nneural networks giving limited accuracy; the studies focus on predicting\nelastic mechanical properties, without considering material strength limits;\nand the models lacked transparency, hindering trustworthiness by users. In this\npaper, we show that convolutional neural networks (CNNs) equipped with methods\nfrom explainable AI (XAI) can be successfully deployed to solve this problem.\nOur approach uses customised CNNs trained on a dataset we generate using\ntransverse tension tests in FE modelling to predict composites' mechanical\nproperties, i.e., Young's modulus and yield strength. We show empirically that\nour approach achieves high accuracy, outperforming a baseline, ResNet-34, in\nestimating the mechanical properties. We then use SHAP and Integrated\nGradients, two post-hoc XAI methods, to explain the predictions, showing that\nthe CNNs use the critical geometrical features that influence the composites'\nbehaviour, thus allowing engineers to verify that the models are trustworthy by\nrepresenting the science of composites."}
{"id": "2505.15239", "pdf": "https://arxiv.org/pdf/2505.15239", "abs": "https://arxiv.org/abs/2505.15239", "authors": ["Peter Súkeník", "Christoph H. Lampert", "Marco Mondelli"], "title": "Neural Collapse is Globally Optimal in Deep Regularized ResNets and Transformers", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "The empirical emergence of neural collapse -- a surprising symmetry in the\nfeature representations of the training data in the penultimate layer of deep\nneural networks -- has spurred a line of theoretical research aimed at its\nunderstanding. However, existing work focuses on data-agnostic models or, when\ndata structure is taken into account, it remains limited to multi-layer\nperceptrons. Our paper fills both these gaps by analyzing modern architectures\nin a data-aware regime: we prove that global optima of deep regularized\ntransformers and residual networks (ResNets) with LayerNorm trained with cross\nentropy or mean squared error loss are approximately collapsed, and the\napproximation gets tighter as the depth grows. More generally, we formally\nreduce any end-to-end large-depth ResNet or transformer training into an\nequivalent unconstrained features model, thus justifying its wide use in the\nliterature even beyond data-agnostic settings. Our theoretical results are\nsupported by experiments on computer vision and language datasets showing that,\nas the depth grows, neural collapse indeed becomes more prominent."}
{"id": "2505.14825", "pdf": "https://arxiv.org/pdf/2505.14825", "abs": "https://arxiv.org/abs/2505.14825", "authors": ["Marios Andreou", "Nan Chen", "Erik Bollt"], "title": "Assimilative Causal Inference", "categories": ["cs.LG", "math.ST", "physics.data-an", "stat.ME", "stat.ML", "stat.TH", "62F15, 62D20, 62M20, 93E11, 93E14, 60H10"], "comment": "Includes the Main Text and Supporting Information in a single\n  document. 39 pages (p. 1--2 Title, Contents and Abstract | p. 3--14 Main Text\n  | p. 15--39 Supporting Information), 9 figures (3 in the Main Text and 6 in\n  the Supporting Information), typeset in LaTeX. Submitted for peer-review. For\n  more info see https://mariosandreou.short.gy/ACI", "summary": "Causal inference determines cause-and-effect relationships between variables\nand has broad applications across disciplines. Traditional time-series methods\noften reveal causal links only in a time-averaged sense, while ensemble-based\ninformation transfer approaches detect the time evolution of short-term causal\nrelationships but are typically limited to low-dimensional systems. In this\npaper, a new causal inference framework, called assimilative causal inference\n(ACI), is developed. Fundamentally different from the state-of-the-art methods,\nACI uses a dynamical system and a single realization of a subset of the state\nvariables to identify instantaneous causal relationships and the dynamic\nevolution of the associated causal influence range (CIR). Instead of\nquantifying how causes influence effects as done traditionally, ACI solves an\ninverse problem via Bayesian data assimilation, thus tracing causes backward\nfrom observed effects with an implicit Bayesian hypothesis. Causality is\ndetermined by assessing whether incorporating the information of the effect\nvariables reduces the uncertainty in recovering the potential cause variables.\nACI has several desirable features. First, it captures the dynamic interplay of\nvariables, where their roles as causes and effects can shift repeatedly over\ntime. Second, a mathematically justified objective criterion determines the CIR\nwithout empirical thresholds. Third, ACI is scalable to high-dimensional\nproblems by leveraging computationally efficient Bayesian data assimilation\ntechniques. Finally, ACI applies to short time series and incomplete datasets.\nNotably, ACI does not require observations of candidate causes, which is a key\nadvantage since potential drivers are often unknown or unmeasured. The\neffectiveness of ACI is demonstrated by complex dynamical systems showcasing\nintermittency and extreme events."}
{"id": "2505.14751", "pdf": "https://arxiv.org/pdf/2505.14751", "abs": "https://arxiv.org/abs/2505.14751", "authors": ["Maheak Dave", "Aniket Kumar Singh", "Aryan Pareek", "Harshita Jha", "Debasis Chaudhuri", "Manish Pratap Singh"], "title": "Self Distillation via Iterative Constructive Perturbations", "categories": ["cs.LG", "cs.AI", "cs.ET"], "comment": null, "summary": "Deep Neural Networks have achieved remarkable achievements across various\ndomains, however balancing performance and generalization still remains a\nchallenge while training these networks. In this paper, we propose a novel\nframework that uses a cyclic optimization strategy to concurrently optimize the\nmodel and its input data for better training, rethinking the traditional\ntraining paradigm. Central to our approach is Iterative Constructive\nPerturbation (ICP), which leverages the model's loss to iteratively perturb the\ninput, progressively constructing an enhanced representation over some\nrefinement steps. This ICP input is then fed back into the model to produce\nimproved intermediate features, which serve as a target in a self-distillation\nframework against the original features. By alternately altering the model's\nparameters to the data and the data to the model, our method effectively\naddresses the gap between fitting and generalization, leading to enhanced\nperformance. Extensive experiments demonstrate that our approach not only\nmitigates common performance bottlenecks in neural networks but also\ndemonstrates significant improvements across training variations."}
{"id": "2505.15240", "pdf": "https://arxiv.org/pdf/2505.15240", "abs": "https://arxiv.org/abs/2505.15240", "authors": ["Yassir Fathullah", "Mark J. F. Gales"], "title": "Generalised Probabilistic Modelling and Improved Uncertainty Estimation in Comparative LLM-as-a-judge", "categories": ["cs.AI", "cs.LG", "stat.ML"], "comment": "To appear in UAI 2025", "summary": "This paper explores generalised probabilistic modelling and uncertainty\nestimation in comparative LLM-as-a-judge frameworks. We show that existing\nProduct-of-Experts methods are specific cases of a broader framework, enabling\ndiverse modelling options. Furthermore, we propose improved uncertainty\nestimates for individual comparisons, enabling more efficient selection and\nachieving strong performance with fewer evaluations. We also introduce a method\nfor estimating overall ranking uncertainty. Finally, we demonstrate that\ncombining absolute and comparative scoring improves performance. Experiments\nshow that the specific expert model has a limited impact on final rankings but\nour proposed uncertainty estimates, especially the probability of reordering,\nsignificantly improve the efficiency of systems reducing the number of needed\ncomparisons by ~50%. Furthermore, ranking-level uncertainty metrics can be used\nto identify low-performing predictions, where the nature of the probabilistic\nmodel has a notable impact on the quality of the overall uncertainty."}
{"id": "2505.14826", "pdf": "https://arxiv.org/pdf/2505.14826", "abs": "https://arxiv.org/abs/2505.14826", "authors": ["Rohan Deb", "Kiran Thekumparampil", "Kousha Kalantari", "Gaurush Hiranandani", "Shoham Sabach", "Branislav Kveton"], "title": "FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain", "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": null, "summary": "Supervised fine-tuning (SFT) is a standard approach to adapting large\nlanguage models (LLMs) to new domains. In this work, we improve the statistical\nefficiency of SFT by selecting an informative subset of training examples.\nSpecifically, for a fixed budget of training examples, which determines the\ncomputational cost of fine-tuning, we determine the most informative ones. The\nkey idea in our method is to select examples that maximize information gain,\nmeasured by the Hessian of the log-likelihood of the LLM. We approximate it\nefficiently by linearizing the LLM at the last layer using multinomial logistic\nregression models. Our approach is computationally efficient, analyzable, and\nperforms well empirically. We demonstrate this on several problems, and back\nour claims with both quantitative results and an LLM evaluation."}
{"id": "2505.14756", "pdf": "https://arxiv.org/pdf/2505.14756", "abs": "https://arxiv.org/abs/2505.14756", "authors": ["Chih-Yu Chang", "Milad Azvar", "Chinedum Okwudire", "Raed Al Kontar"], "title": "$\\texttt{LLINBO}$: Trustworthy LLM-in-the-Loop Bayesian Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Bayesian optimization (BO) is a sequential decision-making tool widely used\nfor optimizing expensive black-box functions. Recently, Large Language Models\n(LLMs) have shown remarkable adaptability in low-data regimes, making them\npromising tools for black-box optimization by leveraging contextual knowledge\nto propose high-quality query points. However, relying solely on LLMs as\noptimization agents introduces risks due to their lack of explicit surrogate\nmodeling and calibrated uncertainty, as well as their inherently opaque\ninternal mechanisms. This structural opacity makes it difficult to characterize\nor control the exploration-exploitation trade-off, ultimately undermining\ntheoretical tractability and reliability. To address this, we propose LLINBO:\nLLM-in-the-Loop BO, a hybrid framework for BO that combines LLMs with\nstatistical surrogate experts (e.g., Gaussian Processes (GP)). The core\nphilosophy is to leverage contextual reasoning strengths of LLMs for early\nexploration, while relying on principled statistical models to guide efficient\nexploitation. Specifically, we introduce three mechanisms that enable this\ncollaboration and establish their theoretical guarantees. We end the paper with\na real-life proof-of-concept in the context of 3D printing. The code to\nreproduce the results can be found at\nhttps://github.com/UMDataScienceLab/LLM-in-the-Loop-BO."}
{"id": "2505.15354", "pdf": "https://arxiv.org/pdf/2505.15354", "abs": "https://arxiv.org/abs/2505.15354", "authors": ["Malik Tiomoko", "Hamza Cherkaoui", "Giuseppe Paolo", "Zhang Yili", "Yu Meng", "Zhang Keli", "Hafiz Tiomoko Ali"], "title": "Human in the Loop Adaptive Optimization for Improved Time Series Forecasting", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Time series forecasting models often produce systematic, predictable errors\neven in critical domains such as energy, finance, and healthcare. We introduce\na novel post training adaptive optimization framework that improves forecast\naccuracy without retraining or architectural changes. Our method automatically\napplies expressive transformations optimized via reinforcement learning,\ncontextual bandits, or genetic algorithms to correct model outputs in a\nlightweight and model agnostic way. Theoretically, we prove that affine\ncorrections always reduce the mean squared error; practically, we extend this\nidea with dynamic action based optimization. The framework also supports an\noptional human in the loop component: domain experts can guide corrections\nusing natural language, which is parsed into actions by a language model.\nAcross multiple benchmarks (e.g., electricity, weather, traffic), we observe\nconsistent accuracy gains with minimal computational overhead. Our interactive\ndemo shows the framework's real time usability. By combining automated post hoc\nrefinement with interpretable and extensible mechanisms, our approach offers a\npowerful new direction for practical forecasting systems."}
{"id": "2505.14828", "pdf": "https://arxiv.org/pdf/2505.14828", "abs": "https://arxiv.org/abs/2505.14828", "authors": ["Juan Nathaniel", "Carla Roesch", "Jatan Buch", "Derek DeSantis", "Adam Rupe", "Kara Lamb", "Pierre Gentine"], "title": "Deep Koopman operator framework for causal discovery in nonlinear dynamical systems", "categories": ["cs.LG"], "comment": "10+14 pages, 10+13 figures", "summary": "We use a deep Koopman operator-theoretic formalism to develop a novel causal\ndiscovery algorithm, Kausal. Causal discovery aims to identify cause-effect\nmechanisms for better scientific understanding, explainable decision-making,\nand more accurate modeling. Standard statistical frameworks, such as Granger\ncausality, lack the ability to quantify causal relationships in nonlinear\ndynamics due to the presence of complex feedback mechanisms, timescale mixing,\nand nonstationarity. This presents a challenge in studying many real-world\nsystems, such as the Earth's climate. Meanwhile, Koopman operator methods have\nemerged as a promising tool for approximating nonlinear dynamics in a linear\nspace of observables. In Kausal, we propose to leverage this powerful idea for\ncausal analysis where optimal observables are inferred using deep learning.\nCausal estimates are then evaluated in a reproducing kernel Hilbert space, and\ndefined as the distance between the marginal dynamics of the effect and the\njoint dynamics of the cause-effect observables. Our numerical experiments\ndemonstrate Kausal's superior ability in discovering and characterizing causal\nsignals compared to existing approaches of prescribed observables. Lastly, we\nextend our analysis to observations of El Ni\\~no-Southern Oscillation\nhighlighting our algorithm's applicability to real-world phenomena. Our code is\navailable at https://github.com/juannat7/kausal."}
{"id": "2505.14765", "pdf": "https://arxiv.org/pdf/2505.14765", "abs": "https://arxiv.org/abs/2505.14765", "authors": ["Orhun Vural", "Bunyamin Ozaydin", "Khalid Y. Aram", "James Booth", "Brittany F. Lindsey", "Abdulaziz Ahmed"], "title": "Deep Learning-Based Forecasting of Boarding Patient Counts to Address ED Overcrowding", "categories": ["cs.LG", "cs.AI", "68T07", "I.2.6; J.3"], "comment": null, "summary": "This study develops deep learning models to forecast the number of patients\nin the emergency department (ED) boarding phase six hours in advance, aiming to\nsupport proactive operational decision-making using only non-clinical,\noperational, and contextual features. Data were collected from five sources: ED\ntracking systems, inpatient census records, weather reports, federal holiday\ncalendars, and local event schedules. After feature engineering, the data were\naggregated at an hourly level, cleaned, and merged into a unified dataset for\nmodel training. Several time series deep learning models, including ResNetPlus,\nTSTPlus, TSiTPlus (from the tsai library), and N-BEATSx, were trained using\nOptuna and grid search for hyperparameter tuning. The average ED boarding count\nwas 28.7, with a standard deviation of 11.2. N-BEATSx achieved the best\nperformance, with a mean absolute error of 2.10, mean squared error of 7.08,\nroot mean squared error of 2.66, and a coefficient of determination of 0.95.\nThe model maintained stable accuracy even during periods of extremely high\nboarding counts, defined as values exceeding one, two, or three standard\ndeviations above the mean. Results show that accurate six-hour-ahead forecasts\nare achievable without using patient-level clinical data. While strong\nperformance was observed even with a basic feature set, the inclusion of\nadditional features improved prediction stability under extreme conditions.\nThis framework offers a practical and generalizable approach for hospital\nsystems to anticipate boarding levels and help mitigate ED overcrowding."}
{"id": "2505.15423", "pdf": "https://arxiv.org/pdf/2505.15423", "abs": "https://arxiv.org/abs/2505.15423", "authors": ["Marcell T. Kurbucz", "Nikolaos Tzivanakis", "Nilufer Sari Aslam", "Adam M. Sykulski"], "title": "SplitWise Regression: Stepwise Modeling with Adaptive Dummy Encoding", "categories": ["cs.LG", "econ.EM", "stat.AP", "stat.ME", "stat.ML", "62H20, 62J05, 68T05", "G.3; I.2.6; I.5.1; I.5.2"], "comment": "15 pages, 1 figure, 3 tables", "summary": "Capturing nonlinear relationships without sacrificing interpretability\nremains a persistent challenge in regression modeling. We introduce SplitWise,\na novel framework that enhances stepwise regression. It adaptively transforms\nnumeric predictors into threshold-based binary features using shallow decision\ntrees, but only when such transformations improve model fit, as assessed by the\nAkaike Information Criterion (AIC) or Bayesian Information Criterion (BIC).\nThis approach preserves the transparency of linear models while flexibly\ncapturing nonlinear effects. Implemented as a user-friendly R package,\nSplitWise is evaluated on both synthetic and real-world datasets. The results\nshow that it consistently produces more parsimonious and generalizable models\nthan traditional stepwise and penalized regression techniques."}
{"id": "2505.14840", "pdf": "https://arxiv.org/pdf/2505.14840", "abs": "https://arxiv.org/abs/2505.14840", "authors": ["Shreya Gupta", "Boyang Huang", "Barna Saha", "Yinzhan Xu", "Christopher Ye"], "title": "Subquadratic Algorithms and Hardness for Attention with Any Temperature", "categories": ["cs.LG", "cs.CC", "F.2.1"], "comment": "34 pages, 2 figures, abstract shortened to meet arXiv requirements", "summary": "Despite the popularity of the Transformer architecture, the standard\nalgorithm for computing Attention suffers from quadratic time complexity in\ncontext length $n$. Alman and Song [NeurIPS 2023] showed that when the head\ndimension $d = \\Theta(\\log n)$, subquadratic Attention is possible if and only\nif the inputs have small entries bounded by $B = o(\\sqrt{\\log n})$ in absolute\nvalues, under the Strong Exponential Time Hypothesis ($\\mathsf{SETH}$).\nEquivalently, subquadratic Attention is possible if and only if the softmax is\napplied with high temperature for $d=\\Theta(\\log n)$. Running times of these\nalgorithms depend exponentially on $B$ and thus they do not lead to even a\npolynomial-time algorithm outside the specific range of $B$.\n  This naturally leads to the question: when can Attention be computed\nefficiently without strong assumptions on temperature? Are there fast attention\nalgorithms that scale polylogarithmically with entry size $B$? In this work, we\nresolve this question and characterize when fast Attention for arbitrary\ntemperatures is possible. First, for all constant $d = O(1)$, we give the first\nsubquadratic $\\tilde{O}(n^{2 - 1/d} \\cdot \\mathrm{polylog}(B))$ time algorithm\nfor Attention with large $B$. Our result holds even for matrices with large\nhead dimension if they have low rank. In this regime, we also give a similar\nrunning time for Attention gradient computation, and therefore for the full LLM\ntraining process. Furthermore, we show that any substantial improvement on our\nalgorithm is unlikely. In particular, we show that even when $d =\n2^{\\Theta(\\log^* n)}$, Attention requires $n^{2 - o(1)}$ time under\n$\\mathsf{SETH}$.\n  Finally, in the regime where $d = \\mathrm{poly}(n)$, we show that the\nstandard algorithm is optimal under popular fine-grained complexity\nassumptions."}
{"id": "2505.14766", "pdf": "https://arxiv.org/pdf/2505.14766", "abs": "https://arxiv.org/abs/2505.14766", "authors": ["Ben Cohen", "Emaad Khwaja", "Youssef Doubli", "Salahidine Lemaachi", "Chris Lettieri", "Charles Masson", "Hugo Miccinilli", "Elise Ramé", "Qiqi Ren", "Afshin Rostamizadeh", "Jean Ogier du Terrail", "Anna-Monica Toon", "Kan Wang", "Stephan Xie", "David Asker", "Ameet Talwalkar", "Othmane Abou-Amal"], "title": "This Time is Different: An Observability Perspective on Time Series Foundation Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce Toto, a time series forecasting foundation model with 151\nmillion parameters. Toto uses a modern decoder-only architecture coupled with\narchitectural innovations designed to account for specific challenges found in\nmultivariate observability time series data. Toto's pre-training corpus is a\nmixture of observability data, open datasets, and synthetic data, and is\n4-10$\\times$ larger than those of leading time series foundation models.\nAdditionally, we introduce BOOM, a large-scale benchmark consisting of 350\nmillion observations across 2,807 real-world time series. For both Toto and\nBOOM, we source observability data exclusively from Datadog's own telemetry and\ninternal observability metrics. Extensive evaluations demonstrate that Toto\nachieves state-of-the-art performance on both BOOM and on established general\npurpose time series forecasting benchmarks. Toto's model weights, inference\ncode, and evaluation scripts, as well as BOOM's data and evaluation code, are\nall available as open source under the Apache 2.0 License available at\nhttps://huggingface.co/Datadog/Toto-Open-Base-1.0 and\nhttps://github.com/DataDog/toto."}
{"id": "2505.15496", "pdf": "https://arxiv.org/pdf/2505.15496", "abs": "https://arxiv.org/abs/2505.15496", "authors": ["Hossein Zakerinia", "Christoph H. Lampert"], "title": "Fast Rate Bounds for Multi-Task and Meta-Learning with Different Sample Sizes", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We present new fast-rate generalization bounds for multi-task and\nmeta-learning in the unbalanced setting, i.e. when the tasks have training sets\nof different sizes, as is typically the case in real-world scenarios.\nPreviously, only standard-rate bounds were known for this situation, while\nfast-rate bounds were limited to the setting where all training sets are of\nequal size. Our new bounds are numerically computable as well as interpretable,\nand we demonstrate their flexibility in handling a number of cases where they\ngive stronger guarantees than previous bounds. Besides the bounds themselves,\nwe also make conceptual contributions: we demonstrate that the unbalanced\nmulti-task setting has different statistical properties than the balanced\nsituation, specifically that proofs from the balanced situation do not carry\nover to the unbalanced setting. Additionally, we shed light on the fact that\nthe unbalanced situation allows two meaningful definitions of multi-task risk,\ndepending on whether if all tasks should be considered equally important or if\nsample-rich tasks should receive more weight than sample-poor ones."}
{"id": "2505.14877", "pdf": "https://arxiv.org/pdf/2505.14877", "abs": "https://arxiv.org/abs/2505.14877", "authors": ["Francisco Pérez-Galarce", "Jorge Martínez-Palomera", "Karim Pichara", "Pablo Huijse", "Márcio Catelan"], "title": "A self-regulated convolutional neural network for classifying variable stars", "categories": ["cs.LG", "astro-ph.SR"], "comment": null, "summary": "Over the last two decades, machine learning models have been widely applied\nand have proven effective in classifying variable stars, particularly with the\nadoption of deep learning architectures such as convolutional neural networks,\nrecurrent neural networks, and transformer models. While these models have\nachieved high accuracy, they require high-quality, representative data and a\nlarge number of labelled samples for each star type to generalise well, which\ncan be challenging in time-domain surveys. This challenge often leads to models\nlearning and reinforcing biases inherent in the training data, an issue that is\nnot easily detectable when validation is performed on subsamples from the same\ncatalogue. The problem of biases in variable star data has been largely\noverlooked, and a definitive solution has yet to be established. In this paper,\nwe propose a new approach to improve the reliability of classifiers in variable\nstar classification by introducing a self-regulated training process. This\nprocess utilises synthetic samples generated by a physics-enhanced latent space\nvariational autoencoder, incorporating six physical parameters from Gaia Data\nRelease 3. Our method features a dynamic interaction between a classifier and a\ngenerative model, where the generative model produces ad-hoc synthetic light\ncurves to reduce confusion during classifier training and populate\nunderrepresented regions in the physical parameter space. Experiments conducted\nunder various scenarios demonstrate that our self-regulated training approach\noutperforms traditional training methods for classifying variable stars on\nbiased datasets, showing statistically significant improvements."}
{"id": "2505.14777", "pdf": "https://arxiv.org/pdf/2505.14777", "abs": "https://arxiv.org/abs/2505.14777", "authors": ["Mingquan Feng", "Yixin Huang", "Yifan Fu", "Shaobo Wang", "Junchi Yan"], "title": "KO: Kinetics-inspired Neural Optimizer with PDE Simulation Approaches", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The design of optimization algorithms for neural networks remains a critical\nchallenge, with most existing methods relying on heuristic adaptations of\ngradient-based approaches. This paper introduces KO (Kinetics-inspired\nOptimizer), a novel neural optimizer inspired by kinetic theory and partial\ndifferential equation (PDE) simulations. We reimagine the training dynamics of\nnetwork parameters as the evolution of a particle system governed by kinetic\nprinciples, where parameter updates are simulated via a numerical scheme for\nthe Boltzmann transport equation (BTE) that models stochastic particle\ncollisions. This physics-driven approach inherently promotes parameter\ndiversity during optimization, mitigating the phenomenon of parameter\ncondensation, i.e. collapse of network parameters into low-dimensional\nsubspaces, through mechanisms analogous to thermal diffusion in physical\nsystems. We analyze this property, establishing both a mathematical proof and a\nphysical interpretation. Extensive experiments on image classification\n(CIFAR-10/100, ImageNet) and text classification (IMDB, Snips) tasks\ndemonstrate that KO consistently outperforms baseline optimizers (e.g., Adam,\nSGD), achieving accuracy improvements while computation cost remains\ncomparable."}
{"id": "2505.15579", "pdf": "https://arxiv.org/pdf/2505.15579", "abs": "https://arxiv.org/abs/2505.15579", "authors": ["Hossein Zakerinia", "Jonathan Scott", "Christoph H. Lampert"], "title": "Federated Learning with Unlabeled Clients: Personalization Can Happen in Low Dimensions", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Personalized federated learning has emerged as a popular approach to training\non devices holding statistically heterogeneous data, known as clients. However,\nmost existing approaches require a client to have labeled data for training or\nfinetuning in order to obtain their own personalized model. In this paper we\naddress this by proposing FLowDUP, a novel method that is able to generate a\npersonalized model using only a forward pass with unlabeled data. The generated\nmodel parameters reside in a low-dimensional subspace, enabling efficient\ncommunication and computation. FLowDUP's learning objective is theoretically\nmotivated by our new transductive multi-task PAC-Bayesian generalization bound,\nthat provides performance guarantees for unlabeled clients. The objective is\nstructured in such a way that it allows both clients with labeled data and\nclients with only unlabeled data to contribute to the training process. To\nsupplement our theoretical results we carry out a thorough experimental\nevaluation of FLowDUP, demonstrating strong empirical performance on a range of\ndatasets with differing sorts of statistically heterogeneous clients. Through\nnumerous ablation studies, we test the efficacy of the individual components of\nthe method."}
{"id": "2505.14882", "pdf": "https://arxiv.org/pdf/2505.14882", "abs": "https://arxiv.org/abs/2505.14882", "authors": ["Abdellah Aznag", "Rachel Cummings", "Adam N. Elmachtoub"], "title": "An active learning framework for multi-group mean estimation", "categories": ["cs.LG"], "comment": null, "summary": "We study a fundamental learning problem over multiple groups with unknown\ndata distributions, where an analyst would like to learn the mean of each\ngroup. Moreover, we want to ensure that this data is collected in a relatively\nfair manner such that the noise of the estimate of each group is reasonable. In\nparticular, we focus on settings where data are collected dynamically, which is\nimportant in adaptive experimentation for online platforms or adaptive clinical\ntrials for healthcare. In our model, we employ an active learning framework to\nsequentially collect samples with bandit feedback, observing a sample in each\nperiod from the chosen group. After observing a sample, the analyst updates\ntheir estimate of the mean and variance of that group and chooses the next\ngroup accordingly. The analyst's objective is to dynamically collect samples to\nminimize the collective noise of the estimators, measured by the norm of the\nvector of variances of the mean estimators.\n  We propose an algorithm, Variance-UCB, that sequentially selects groups\naccording to an upper confidence bound on the variance estimate. We provide a\ngeneral theoretical framework for providing efficient bounds on learning from\nany underlying distribution where the variances can be estimated reasonably.\nThis framework yields upper bounds on regret that improve significantly upon\nall existing bounds, as well as a collection of new results for different\nobjectives and distributions than those previously studied."}
{"id": "2505.14803", "pdf": "https://arxiv.org/pdf/2505.14803", "abs": "https://arxiv.org/abs/2505.14803", "authors": ["Yu Liu", "Weiyao Tao", "Tong Xia", "Simon Knight", "Tingting Zhu"], "title": "SurvUnc: A Meta-Model Based Uncertainty Quantification Framework for Survival Analysis", "categories": ["cs.LG", "cs.AI", "cs.ET"], "comment": "KDD 2025", "summary": "Survival analysis, which estimates the probability of event occurrence over\ntime from censored data, is fundamental in numerous real-world applications,\nparticularly in high-stakes domains such as healthcare and risk assessment.\nDespite advances in numerous survival models, quantifying the uncertainty of\npredictions from these models remains underexplored and challenging. The lack\nof reliable uncertainty quantification limits the interpretability and\ntrustworthiness of survival models, hindering their adoption in clinical\ndecision-making and other sensitive applications. To bridge this gap, in this\nwork, we introduce SurvUnc, a novel meta-model based framework for post-hoc\nuncertainty quantification for survival models. SurvUnc introduces an\nanchor-based learning strategy that integrates concordance knowledge into\nmeta-model optimization, leveraging pairwise ranking performance to estimate\nuncertainty effectively. Notably, our framework is model-agnostic, ensuring\ncompatibility with any survival model without requiring modifications to its\narchitecture or access to its internal parameters. Especially, we design a\ncomprehensive evaluation pipeline tailored to this critical yet overlooked\nproblem. Through extensive experiments on four publicly available benchmarking\ndatasets and five representative survival models, we demonstrate the\nsuperiority of SurvUnc across multiple evaluation scenarios, including\nselective prediction, misprediction detection, and out-of-domain detection. Our\nresults highlight the effectiveness of SurvUnc in enhancing model\ninterpretability and reliability, paving the way for more trustworthy survival\npredictions in real-world applications."}
{"id": "2505.15626", "pdf": "https://arxiv.org/pdf/2505.15626", "abs": "https://arxiv.org/abs/2505.15626", "authors": ["Jacopo Teneggi", "Zhenzhen Wang", "Paul H. Yi", "Tianmin Shu", "Jeremias Sulam"], "title": "Aligning Explanations with Human Communication", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Machine learning explainability aims to make the decision-making process of\nblack-box models more transparent by finding the most important input features\nfor a given prediction task. Recent works have proposed composing explanations\nfrom semantic concepts (e.g., colors, patterns, shapes) that are inherently\ninterpretable to the user of a model. However, these methods generally ignore\nthe communicative context of explanation-the ability of the user to understand\nthe prediction of the model from the explanation. For example, while a medical\ndoctor might understand an explanation in terms of clinical markers, a patient\nmay need a more accessible explanation to make sense of the same diagnosis. In\nthis paper, we address this gap with listener-adaptive explanations. We propose\nan iterative procedure grounded in principles of pragmatic reasoning and the\nrational speech act to generate explanations that maximize communicative\nutility. Our procedure only needs access to pairwise preferences between\ncandidate explanations, relevant in real-world scenarios where a listener model\nmay not be available. We evaluate our method in image classification tasks,\ndemonstrating improved alignment between explanations and listener preferences\nacross three datasets. Furthermore, we perform a user study that demonstrates\nour explanations increase communicative utility."}
{"id": "2505.14884", "pdf": "https://arxiv.org/pdf/2505.14884", "abs": "https://arxiv.org/abs/2505.14884", "authors": ["Susav Shrestha", "Brad Settlemyer", "Nikoli Dryden", "Narasimha Reddy"], "title": "Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accelerating large language model (LLM) inference is critical for real-world\ndeployments requiring high throughput and low latency. Contextual sparsity,\nwhere each token dynamically activates only a small subset of the model\nparameters, shows promise but does not scale to large batch sizes due to union\nof active neurons quickly approaching dense computation. We introduce Polar\nSparsity, highlighting a key shift in sparsity importance from MLP to Attention\nlayers as we scale batch size and sequence length. While MLP layers become more\ncompute-efficient under batching, their sparsity vanishes. In contrast,\nattention becomes increasingly more expensive at scale, while their head\nsparsity remains stable and batch-invariant. We develop hardware-efficient,\nsparsity-aware GPU kernels for selective MLP and Attention computations,\ndelivering up to \\(2.2\\times\\) end-to-end speedups for models like OPT, LLaMA-2\n\\& 3, across various batch sizes and sequence lengths without compromising\naccuracy. To our knowledge, this is the first work to demonstrate that\ncontextual sparsity can scale effectively to large batch sizes, delivering\nsubstantial inference acceleration with minimal changes, making Polar Sparsity\npractical for large-scale, high-throughput LLM deployment systems. Our code is\navailable at: https://github.com/susavlsh10/Polar-Sparsity."}
{"id": "2505.14821", "pdf": "https://arxiv.org/pdf/2505.14821", "abs": "https://arxiv.org/abs/2505.14821", "authors": ["Runze Zhao", "Yue Yu", "Adams Yiyue Zhu", "Chen Yang", "Dongruo Zhou"], "title": "Sample and Computationally Efficient Continuous-Time Reinforcement Learning with General Function Approximation", "categories": ["cs.LG", "cs.AI"], "comment": "28 pages, 4 figures, 5 tables. Accepted to UAI 2025", "summary": "Continuous-time reinforcement learning (CTRL) provides a principled framework\nfor sequential decision-making in environments where interactions evolve\ncontinuously over time. Despite its empirical success, the theoretical\nunderstanding of CTRL remains limited, especially in settings with general\nfunction approximation. In this work, we propose a model-based CTRL algorithm\nthat achieves both sample and computational efficiency. Our approach leverages\noptimism-based confidence sets to establish the first sample complexity\nguarantee for CTRL with general function approximation, showing that a\nnear-optimal policy can be learned with a suboptimality gap of\n$\\tilde{O}(\\sqrt{d_{\\mathcal{R}} + d_{\\mathcal{F}}}N^{-1/2})$ using $N$\nmeasurements, where $d_{\\mathcal{R}}$ and $d_{\\mathcal{F}}$ denote the\ndistributional Eluder dimensions of the reward and dynamic functions,\nrespectively, capturing the complexity of general function approximation in\nreinforcement learning. Moreover, we introduce structured policy updates and an\nalternative measurement strategy that significantly reduce the number of policy\nupdates and rollouts while maintaining competitive sample efficiency. We\nimplemented experiments to backup our proposed algorithms on continuous control\ntasks and diffusion model fine-tuning, demonstrating comparable performance\nwith significantly fewer policy updates and rollouts."}
{"id": "2505.15638", "pdf": "https://arxiv.org/pdf/2505.15638", "abs": "https://arxiv.org/abs/2505.15638", "authors": ["Daniel Waxman", "Fernando Llorente", "Petar M. Djurić"], "title": "Bayesian Ensembling: Insights from Online Optimization and Empirical Bayes", "categories": ["cs.LG", "stat.CO", "stat.ME", "stat.ML"], "comment": "25 pages, 12 figures", "summary": "We revisit the classical problem of Bayesian ensembles and address the\nchallenge of learning optimal combinations of Bayesian models in an online,\ncontinual learning setting. To this end, we reinterpret existing approaches\nsuch as Bayesian model averaging (BMA) and Bayesian stacking through a novel\nempirical Bayes lens, shedding new light on the limitations and pathologies of\nBMA. Further motivated by insights from online optimization, we propose Online\nBayesian Stacking (OBS), a method that optimizes the log-score over predictive\ndistributions to adaptively combine Bayesian models. A key contribution of our\nwork is establishing a novel connection between OBS and portfolio selection,\nbridging Bayesian ensemble learning with a rich, well-studied theoretical\nframework that offers efficient algorithms and extensive regret analysis. We\nfurther clarify the relationship between OBS and online BMA, showing that they\noptimize related but distinct cost functions. Through theoretical analysis and\nempirical evaluation, we identify scenarios where OBS outperforms online BMA\nand provide principled guidance on when practitioners should prefer one\napproach over the other."}
{"id": "2505.14896", "pdf": "https://arxiv.org/pdf/2505.14896", "abs": "https://arxiv.org/abs/2505.14896", "authors": ["Hootan Mahmoodiyan", "Maryam Ahang", "Mostafa Abbasi", "Homayoun Najjaran"], "title": "Feature-Weighted MMD-CORAL for Domain Adaptation in Power Transformer Fault Diagnosis", "categories": ["cs.LG"], "comment": null, "summary": "Ensuring the reliable operation of power transformers is critical to grid\nstability. Dissolved Gas Analysis (DGA) is widely used for fault diagnosis, but\ntraditional methods rely on heuristic rules, which may lead to inconsistent\nresults. Machine learning (ML)-based approaches have improved diagnostic\naccuracy; however, power transformers operate under varying conditions, and\ndifferences in transformer type, environmental factors, and operational\nsettings create distribution shifts in diagnostic data. Consequently, direct\nmodel transfer between transformers often fails, making techniques for domain\nadaptation a necessity. To tackle this issue, this work proposes a\nfeature-weighted domain adaptation technique that combines Maximum Mean\nDiscrepancy (MMD) and Correlation Alignment (CORAL) with feature-specific\nweighting (MCW). Kolmogorov-Smirnov (K-S) statistics are used to assign\nadaptable weights, prioritizing features with larger distributional\ndiscrepancies and thereby improving source and target domain alignment.\nExperimental evaluations on datasets for power transformers demonstrate the\neffectiveness of the proposed method, which achieves a 7.9% improvement over\nFine-Tuning and a 2.2% improvement over MMD-CORAL (MC). Furthermore, it\noutperforms both techniques across various training sample sizes, confirming\nits robustness for domain adaptation."}
{"id": "2505.14884", "pdf": "https://arxiv.org/pdf/2505.14884", "abs": "https://arxiv.org/abs/2505.14884", "authors": ["Susav Shrestha", "Brad Settlemyer", "Nikoli Dryden", "Narasimha Reddy"], "title": "Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accelerating large language model (LLM) inference is critical for real-world\ndeployments requiring high throughput and low latency. Contextual sparsity,\nwhere each token dynamically activates only a small subset of the model\nparameters, shows promise but does not scale to large batch sizes due to union\nof active neurons quickly approaching dense computation. We introduce Polar\nSparsity, highlighting a key shift in sparsity importance from MLP to Attention\nlayers as we scale batch size and sequence length. While MLP layers become more\ncompute-efficient under batching, their sparsity vanishes. In contrast,\nattention becomes increasingly more expensive at scale, while their head\nsparsity remains stable and batch-invariant. We develop hardware-efficient,\nsparsity-aware GPU kernels for selective MLP and Attention computations,\ndelivering up to \\(2.2\\times\\) end-to-end speedups for models like OPT, LLaMA-2\n\\& 3, across various batch sizes and sequence lengths without compromising\naccuracy. To our knowledge, this is the first work to demonstrate that\ncontextual sparsity can scale effectively to large batch sizes, delivering\nsubstantial inference acceleration with minimal changes, making Polar Sparsity\npractical for large-scale, high-throughput LLM deployment systems. Our code is\navailable at: https://github.com/susavlsh10/Polar-Sparsity."}
{"id": "2505.15643", "pdf": "https://arxiv.org/pdf/2505.15643", "abs": "https://arxiv.org/abs/2505.15643", "authors": ["Lan V. Truong"], "title": "Optimal Best-Arm Identification under Fixed Confidence with Multiple Optima", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "comment": "22 pages", "summary": "We study the problem of best-arm identification in stochastic multi-armed\nbandits under the fixed-confidence setting, with a particular focus on\ninstances that admit multiple optimal arms. While the Track-and-Stop algorithm\nof Garivier and Kaufmann (2016) is widely conjectured to be instance-optimal,\nits performance in the presence of multiple optima has remained insufficiently\nunderstood. In this work, we revisit the Track-and-Stop strategy and propose a\nmodified stopping rule that ensures instance-optimality even when the set of\noptimal arms is not a singleton. Our analysis introduces a new\ninformation-theoretic lower bound that explicitly accounts for multiple optimal\narms, and we demonstrate that our stopping rule tightly matches this bound."}
{"id": "2505.14897", "pdf": "https://arxiv.org/pdf/2505.14897", "abs": "https://arxiv.org/abs/2505.14897", "authors": ["Ali Mohajerzarrinkelk", "Maryam Ahang", "Mehran Zoravar", "Mostafa Abbasi", "Homayoun Najjaran"], "title": "Multi-Channel Swin Transformer Framework for Bearing Remaining Useful Life Prediction", "categories": ["cs.LG"], "comment": null, "summary": "Precise estimation of the Remaining Useful Life (RUL) of rolling bearings is\nan important consideration to avoid unexpected failures, reduce downtime, and\npromote safety and efficiency in industrial systems. Complications in\ndegradation trends, noise presence, and the necessity to detect faults in\nadvance make estimation of RUL a challenging task. This paper introduces a\nnovel framework that combines wavelet-based denoising method, Wavelet Packet\nDecomposition (WPD), and a customized multi-channel Swin Transformer model\n(MCSFormer) to address these problems. With attention mechanisms incorporated\nfor feature fusion, the model is designed to learn global and local degradation\npatterns utilizing hierarchical representations for enhancing predictive\nperformance. Additionally, a customized loss function is developed as a key\ndistinction of this work to differentiate between early and late predictions,\nprioritizing accurate early detection and minimizing the high operation risks\nof late predictions. The proposed model was evaluated with the PRONOSTIA\ndataset using three experiments. Intra-condition experiments demonstrated that\nMCSFormer outperformed state-of-the-art models, including the Adaptive\nTransformer, MDAN, and CNN-SRU, achieving 41%, 64%, and 69% lower MAE on\naverage across different operating conditions, respectively. In terms of\ncross-condition testing, it achieved superior generalization under varying\noperating conditions compared to the adapted ViT and Swin Transformer. Lastly,\nthe custom loss function effectively reduced late predictions, as evidenced in\na 6.3% improvement in the scoring metric while maintaining competitive overall\nperformance. The model's robust noise resistance, generalization capability,\nand focus on safety make MCSFormer a trustworthy and effective predictive\nmaintenance tool in industrial applications."}
{"id": "2505.14943", "pdf": "https://arxiv.org/pdf/2505.14943", "abs": "https://arxiv.org/abs/2505.14943", "authors": ["Ross Nordby"], "title": "Soft Prompts for Evaluation: Measuring Conditional Distance of Capabilities", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "To help evaluate and understand the latent capabilities of language models,\nthis paper introduces an approach using optimized input embeddings, or 'soft\nprompts,' as a metric of conditional distance between a model and a target\nbehavior. The technique aims to facilitate latent capability discovery as a\npart of automated red teaming/evaluation suites and to provide quantitative\nfeedback about the accessibility of potentially concerning behaviors in a way\nthat may scale to powerful future models, including those which may otherwise\nbe capable of deceptive alignment. An evaluation framework using soft prompts\nis demonstrated in natural language, chess, and pathfinding, and the technique\nis extended with generalized conditional soft prompts to aid in constructing\ntask evaluations."}
{"id": "2505.15721", "pdf": "https://arxiv.org/pdf/2505.15721", "abs": "https://arxiv.org/abs/2505.15721", "authors": ["Coby Penso", "Bar Mahpud", "Jacob Goldberger", "Or Sheffet"], "title": "Privacy-Preserving Conformal Prediction Under Local Differential Privacy", "categories": ["cs.LG", "stat.ML"], "comment": "Preprint. Under review", "summary": "Conformal prediction (CP) provides sets of candidate classes with a\nguaranteed probability of containing the true class. However, it typically\nrelies on a calibration set with clean labels. We address privacy-sensitive\nscenarios where the aggregator is untrusted and can only access a perturbed\nversion of the true labels. We propose two complementary approaches under local\ndifferential privacy (LDP). In the first approach, users do not access the\nmodel but instead provide their input features and a perturbed label using a\nk-ary randomized response. In the second approach, which enforces stricter\nprivacy constraints, users add noise to their conformity score by binary search\nresponse. This method requires access to the classification model but preserves\nboth data and label privacy. Both approaches compute the conformal threshold\ndirectly from noisy data without accessing the true labels. We prove\nfinite-sample coverage guarantees and demonstrate robust coverage even under\nsevere randomization. This approach unifies strong local privacy with\npredictive uncertainty control, making it well-suited for sensitive\napplications such as medical imaging or large language model queries,\nregardless of whether users can (or are willing to) compute their own scores."}
{"id": "2505.14903", "pdf": "https://arxiv.org/pdf/2505.14903", "abs": "https://arxiv.org/abs/2505.14903", "authors": ["Regol Florence", "Schwinn Leo", "Sprague Kyle", "Coates Mark", "Markovich Thomas"], "title": "When to retrain a machine learning model", "categories": ["cs.LG"], "comment": null, "summary": "A significant challenge in maintaining real-world machine learning models is\nresponding to the continuous and unpredictable evolution of data. Most\npractitioners are faced with the difficult question: when should I retrain or\nupdate my machine learning model? This seemingly straightforward problem is\nparticularly challenging for three reasons: 1) decisions must be made based on\nvery limited information - we usually have access to only a few examples, 2)\nthe nature, extent, and impact of the distribution shift are unknown, and 3) it\ninvolves specifying a cost ratio between retraining and poor performance, which\ncan be hard to characterize. Existing works address certain aspects of this\nproblem, but none offer a comprehensive solution. Distribution shift detection\nfalls short as it cannot account for the cost trade-off; the scarcity of the\ndata, paired with its unusual structure, makes it a poor fit for existing\noffline reinforcement learning methods, and the online learning formulation\noverlooks key practical considerations. To address this, we present a\nprincipled formulation of the retraining problem and propose an\nuncertainty-based method that makes decisions by continually forecasting the\nevolution of model performance evaluated with a bounded metric. Our experiments\naddressing classification tasks show that the method consistently outperforms\nexisting baselines on 7 datasets."}
{"id": "2505.14964", "pdf": "https://arxiv.org/pdf/2505.14964", "abs": "https://arxiv.org/abs/2505.14964", "authors": ["Dave Cook", "Tim Klawa"], "title": "The Achilles Heel of AI: Fundamentals of Risk-Aware Training Data for High-Consequence Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "AI systems in high-consequence domains such as defense, intelligence, and\ndisaster response must detect rare, high-impact events while operating under\ntight resource constraints. Traditional annotation strategies that prioritize\nlabel volume over informational value introduce redundancy and noise, limiting\nmodel generalization. This paper introduces smart-sizing, a training data\nstrategy that emphasizes label diversity, model-guided selection, and marginal\nutility-based stopping. We implement this through Adaptive Label Optimization\n(ALO), combining pre-labeling triage, annotator disagreement analysis, and\niterative feedback to prioritize labels that meaningfully improve model\nperformance. Experiments show that models trained on 20 to 40 percent of\ncurated data can match or exceed full-data baselines, particularly in\nrare-class recall and edge-case generalization. We also demonstrate how latent\nlabeling errors embedded in training and validation sets can distort\nevaluation, underscoring the need for embedded audit tools and\nperformance-aware governance. Smart-sizing reframes annotation as a\nfeedback-driven process aligned with mission outcomes, enabling more robust\nmodels with fewer labels and supporting efficient AI development pipelines for\nfrontier models and operational systems."}
{"id": "2505.15808", "pdf": "https://arxiv.org/pdf/2505.15808", "abs": "https://arxiv.org/abs/2505.15808", "authors": ["Carlos Rodriguez-Pardo", "Leonardo Chiani", "Emanuele Borgonovo", "Massimo Tavoni"], "title": "Neural Conditional Transport Maps", "categories": ["cs.LG", "cs.AI", "math.PR", "stat.AP", "stat.ML", "49Q22 (Primary) 68T07 (Secondary)", "I.5.1; I.2.0; G.3"], "comment": "Under Review. Supplementary material included in the pdf", "summary": "We present a neural framework for learning conditional optimal transport (OT)\nmaps between probability distributions. Our approach introduces a conditioning\nmechanism capable of processing both categorical and continuous conditioning\nvariables simultaneously. At the core of our method lies a hypernetwork that\ngenerates transport layer parameters based on these inputs, creating adaptive\nmappings that outperform simpler conditioning methods. Comprehensive ablation\nstudies demonstrate the superior performance of our method over baseline\nconfigurations. Furthermore, we showcase an application to global sensitivity\nanalysis, offering high performance in computing OT-based sensitivity indices.\nThis work advances the state-of-the-art in conditional optimal transport,\nenabling broader application of optimal transport principles to complex,\nhigh-dimensional domains such as generative modeling and black-box model\nexplainability."}
{"id": "2505.14919", "pdf": "https://arxiv.org/pdf/2505.14919", "abs": "https://arxiv.org/abs/2505.14919", "authors": ["Frederik Wenkel", "Wilson Tu", "Cassandra Masschelein", "Hamed Shirzad", "Cian Eastwood", "Shawn T. Whitfield", "Ihab Bendidi", "Craig Russell", "Liam Hodgson", "Yassir El Mesbahi", "Jiarui Ding", "Marta M. Fay", "Berton Earnshaw", "Emmanuel Noutahi", "Alisandra K. Denton"], "title": "TxPert: Leveraging Biochemical Relationships for Out-of-Distribution Transcriptomic Perturbation Prediction", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Accurately predicting cellular responses to genetic perturbations is\nessential for understanding disease mechanisms and designing effective\ntherapies. Yet exhaustively exploring the space of possible perturbations\n(e.g., multi-gene perturbations or across tissues and cell types) is\nprohibitively expensive, motivating methods that can generalize to unseen\nconditions. In this work, we explore how knowledge graphs of gene-gene\nrelationships can improve out-of-distribution (OOD) prediction across three\nchallenging settings: unseen single perturbations; unseen double perturbations;\nand unseen cell lines. In particular, we present: (i) TxPert, a new\nstate-of-the-art method that leverages multiple biological knowledge networks\nto predict transcriptional responses under OOD scenarios; (ii) an in-depth\nanalysis demonstrating the impact of graphs, model architecture, and data on\nperformance; and (iii) an expanded benchmarking framework that strengthens\nevaluation standards for perturbation modeling."}
{"id": "2505.14967", "pdf": "https://arxiv.org/pdf/2505.14967", "abs": "https://arxiv.org/abs/2505.14967", "authors": ["Fangzhen Zhao", "Chenyi Zhang", "Naipeng Dong", "Ming Li", "Jinxiao Shan"], "title": "Anomaly Detection Based on Critical Paths for Deep Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": "23 pages in ACM journal latex format", "summary": "Deep neural networks (DNNs) are notoriously hard to understand and difficult\nto defend. Extracting representative paths (including the neuron activation\nvalues and the connections between neurons) from DNNs using software\nengineering approaches has recently shown to be a promising approach in\ninterpreting the decision making process of blackbox DNNs, as the extracted\npaths are often effective in capturing essential features. With this in mind,\nthis work investigates a novel approach that extracts critical paths from DNNs\nand subsequently applies the extracted paths for the anomaly detection task,\nbased on the observation that outliers and adversarial inputs do not usually\ninduce the same activation pattern on those paths as normal (in-distribution)\ninputs.\n  In our approach, we first identify critical detection paths via genetic\nevolution and mutation. Since different paths in a DNN often capture different\nfeatures for the same target class, we ensemble detection results from multiple\npaths by integrating random subspace sampling and a voting mechanism. Compared\nwith state-of-the-art methods, our experimental results suggest that our method\nnot only outperforms them, but it is also suitable for the detection of a broad\nrange of anomaly types with high accuracy."}
{"id": "2505.14933", "pdf": "https://arxiv.org/pdf/2505.14933", "abs": "https://arxiv.org/abs/2505.14933", "authors": ["Xuefeng Du"], "title": "Foundations of Unknown-aware Machine Learning", "categories": ["cs.LG"], "comment": "PhD Dissertation", "summary": "Ensuring the reliability and safety of machine learning models in open-world\ndeployment is a central challenge in AI safety. This thesis develops both\nalgorithmic and theoretical foundations to address key reliability issues\narising from distributional uncertainty and unknown classes, from standard\nneural networks to modern foundation models like large language models (LLMs).\n  Traditional learning paradigms, such as empirical risk minimization (ERM),\nassume no distribution shift between training and inference, often leading to\noverconfident predictions on out-of-distribution (OOD) inputs. This thesis\nintroduces novel frameworks that jointly optimize for in-distribution accuracy\nand reliability to unseen data. A core contribution is the development of an\nunknown-aware learning framework that enables models to recognize and handle\nnovel inputs without labeled OOD data.\n  We propose new outlier synthesis methods, VOS, NPOS, and DREAM-OOD, to\ngenerate informative unknowns during training. Building on this, we present\nSAL, a theoretical and algorithmic framework that leverages unlabeled\nin-the-wild data to enhance OOD detection under realistic deployment\nconditions. These methods demonstrate that abundant unlabeled data can be\nharnessed to recognize and adapt to unforeseen inputs, providing formal\nreliability guarantees.\n  The thesis also extends reliable learning to foundation models. We develop\nHaloScope for hallucination detection in LLMs, MLLMGuard for defending against\nmalicious prompts in multimodal models, and data cleaning methods to denoise\nhuman feedback used for better alignment. These tools target failure modes that\nthreaten the safety of large-scale models in deployment.\n  Overall, these contributions promote unknown-aware learning as a new\nparadigm, and we hope it can advance the reliability of AI systems with minimal\nhuman efforts."}
{"id": "2505.14969", "pdf": "https://arxiv.org/pdf/2505.14969", "abs": "https://arxiv.org/abs/2505.14969", "authors": ["Yangchao Wu", "Zongyue Qin", "Alex Wong", "Stefano Soatto"], "title": "STree: Speculative Tree Decoding for Hybrid State-Space Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Speculative decoding is a technique to leverage hardware concurrency to\nimprove the efficiency of large-scale autoregressive (AR) Transformer models by\nenabling multiple steps of token generation in a single forward pass.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead to\ncurrent SSM state update implementations. With the algorithm, we describe a\nhardware-aware implementation that improves naive application of AR Transformer\ntree-based speculative decoding methods to SSMs. Furthermore, we outperform\nvanilla speculative decoding with SSMs even with a baseline drafting model and\ntree structure on three different benchmarks, opening up opportunities for\nfurther speed up with SSM and hybrid model inference. Code will be released\nupon paper acceptance."}
{"id": "2505.14943", "pdf": "https://arxiv.org/pdf/2505.14943", "abs": "https://arxiv.org/abs/2505.14943", "authors": ["Ross Nordby"], "title": "Soft Prompts for Evaluation: Measuring Conditional Distance of Capabilities", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "To help evaluate and understand the latent capabilities of language models,\nthis paper introduces an approach using optimized input embeddings, or 'soft\nprompts,' as a metric of conditional distance between a model and a target\nbehavior. The technique aims to facilitate latent capability discovery as a\npart of automated red teaming/evaluation suites and to provide quantitative\nfeedback about the accessibility of potentially concerning behaviors in a way\nthat may scale to powerful future models, including those which may otherwise\nbe capable of deceptive alignment. An evaluation framework using soft prompts\nis demonstrated in natural language, chess, and pathfinding, and the technique\nis extended with generalized conditional soft prompts to aid in constructing\ntask evaluations."}
{"id": "2505.14975", "pdf": "https://arxiv.org/pdf/2505.14975", "abs": "https://arxiv.org/abs/2505.14975", "authors": ["John L. Zhou", "Jonathan C. Kao"], "title": "Flattening Hierarchies with Policy Bootstrapping", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail."}
{"id": "2505.14945", "pdf": "https://arxiv.org/pdf/2505.14945", "abs": "https://arxiv.org/abs/2505.14945", "authors": ["O. Deniz Kose", "Gonzalo Mateos", "Yanning Shen"], "title": "Unlearning Algorithmic Biases over Graphs", "categories": ["cs.LG"], "comment": null, "summary": "The growing enforcement of the right to be forgotten regulations has\npropelled recent advances in certified (graph) unlearning strategies to comply\nwith data removal requests from deployed machine learning (ML) models.\nMotivated by the well-documented bias amplification predicament inherent to\ngraph data, here we take a fresh look at graph unlearning and leverage it as a\nbias mitigation tool. Given a pre-trained graph ML model, we develop a\ntraining-free unlearning procedure that offers certifiable bias mitigation via\na single-step Newton update on the model weights. This way, we contribute a\ncomputationally lightweight alternative to the prevalent training- and\noptimization-based fairness enhancement approaches, with quantifiable\nperformance guarantees. We first develop a novel fairness-aware nodal feature\nunlearning strategy along with refined certified unlearning bounds for this\nsetting, whose impact extends beyond the realm of graph unlearning. We then\ndesign structural unlearning methods endowed with principled selection\nmechanisms over nodes and edges informed by rigorous bias analyses. Unlearning\nthese judiciously selected elements can mitigate algorithmic biases with\nminimal impact on downstream utility (e.g., node classification accuracy).\nExperimental results over real networks corroborate the bias mitigation\nefficacy of our unlearning strategies, and delineate markedly favorable\nutility-complexity trade-offs relative to retraining from scratch using\naugmented graph data obtained via removals."}
{"id": "2505.14999", "pdf": "https://arxiv.org/pdf/2505.14999", "abs": "https://arxiv.org/abs/2505.14999", "authors": ["Eric Hanchen Jiang", "Haozheng Luo", "Shengyuan Pang", "Xiaomin Li", "Zhenting Qi", "Hengli Li", "Cheng-Fu Yang", "Zongyu Lin", "Xinfeng Li", "Hao Xu", "Kai-Wei Chang", "Ying Nian Wu"], "title": "Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Mathematical reasoning presents a significant challenge for Large Language\nModels (LLMs), often requiring robust multi step logical consistency. While\nChain of Thought (CoT) prompting elicits reasoning steps, it doesn't guarantee\ncorrectness, and improving reliability via extensive sampling is\ncomputationally costly. This paper introduces the Energy Outcome Reward Model\n(EORM), an effective, lightweight, post hoc verifier. EORM leverages Energy\nBased Models (EBMs) to simplify the training of reward models by learning to\nassign a scalar energy score to CoT solutions using only outcome labels,\nthereby avoiding detailed annotations. It achieves this by interpreting\ndiscriminator output logits as negative energies, effectively ranking\ncandidates where lower energy is assigned to solutions leading to correct final\noutcomes implicitly favoring coherent reasoning. On mathematical benchmarks\n(GSM8k, MATH), EORM significantly improves final answer accuracy (e.g., with\nLlama 3 8B, achieving 90.7% on GSM8k and 63.7% on MATH). EORM effectively\nleverages a given pool of candidate solutions to match or exceed the\nperformance of brute force sampling, thereby enhancing LLM reasoning outcome\nreliability through its streamlined post hoc verification process."}
{"id": "2505.14959", "pdf": "https://arxiv.org/pdf/2505.14959", "abs": "https://arxiv.org/abs/2505.14959", "authors": ["Kungang Li", "Xiangyi Chen", "Ling Leng", "Jiajing Xu", "Jiankai Sun", "Behnam Rezaei"], "title": "Privacy Preserving Conversion Modeling in Data Clean Room", "categories": ["cs.LG", "cs.IR", "H.4"], "comment": "Published in Proceedings of the 18th ACM Conference on Recommender\n  Systems. 2024 (RecSys '24)", "summary": "In the realm of online advertising, accurately predicting the conversion rate\n(CVR) is crucial for enhancing advertising efficiency and user satisfaction.\nThis paper addresses the challenge of CVR prediction while adhering to user\nprivacy preferences and advertiser requirements. Traditional methods face\nobstacles such as the reluctance of advertisers to share sensitive conversion\ndata and the limitations of model training in secure environments like data\nclean rooms. We propose a novel model training framework that enables\ncollaborative model training without sharing sample-level gradients with the\nadvertising platform. Our approach introduces several innovative components:\n(1) utilizing batch-level aggregated gradients instead of sample-level\ngradients to minimize privacy risks; (2) applying adapter-based\nparameter-efficient fine-tuning and gradient compression to reduce\ncommunication costs; and (3) employing de-biasing techniques to train the model\nunder label differential privacy, thereby maintaining accuracy despite\nprivacy-enhanced label perturbations. Our experimental results, conducted on\nindustrial datasets, demonstrate that our method achieves competitive ROCAUC\nperformance while significantly decreasing communication overhead and complying\nwith both advertiser privacy requirements and user privacy choices. This\nframework establishes a new standard for privacy-preserving, high-performance\nCVR prediction in the digital advertising landscape."}
{"id": "2505.15008", "pdf": "https://arxiv.org/pdf/2505.15008", "abs": "https://arxiv.org/abs/2505.15008", "authors": ["Alvin Heng", "Harold Soh"], "title": "Know When to Abstain: Optimal Selective Classification with Likelihood Ratios", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Selective classification enhances the reliability of predictive models by\nallowing them to abstain from making uncertain predictions. In this work, we\nrevisit the design of optimal selection functions through the lens of the\nNeyman--Pearson lemma, a classical result in statistics that characterizes the\noptimal rejection rule as a likelihood ratio test. We show that this\nperspective not only unifies the behavior of several post-hoc selection\nbaselines, but also motivates new approaches to selective classification which\nwe propose here. A central focus of our work is the setting of covariate shift,\nwhere the input distribution at test time differs from that at training. This\nrealistic and challenging scenario remains relatively underexplored in the\ncontext of selective classification. We evaluate our proposed methods across a\nrange of vision and language tasks, including both supervised learning and\nvision-language models. Our experiments demonstrate that our\nNeyman--Pearson-informed methods consistently outperform existing baselines,\nindicating that likelihood ratio-based selection offers a robust mechanism for\nimproving selective classification under covariate shifts. Our code is publicly\navailable at https://github.com/clear-nus/sc-likelihood-ratios."}
{"id": "2505.14964", "pdf": "https://arxiv.org/pdf/2505.14964", "abs": "https://arxiv.org/abs/2505.14964", "authors": ["Dave Cook", "Tim Klawa"], "title": "The Achilles Heel of AI: Fundamentals of Risk-Aware Training Data for High-Consequence Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "AI systems in high-consequence domains such as defense, intelligence, and\ndisaster response must detect rare, high-impact events while operating under\ntight resource constraints. Traditional annotation strategies that prioritize\nlabel volume over informational value introduce redundancy and noise, limiting\nmodel generalization. This paper introduces smart-sizing, a training data\nstrategy that emphasizes label diversity, model-guided selection, and marginal\nutility-based stopping. We implement this through Adaptive Label Optimization\n(ALO), combining pre-labeling triage, annotator disagreement analysis, and\niterative feedback to prioritize labels that meaningfully improve model\nperformance. Experiments show that models trained on 20 to 40 percent of\ncurated data can match or exceed full-data baselines, particularly in\nrare-class recall and edge-case generalization. We also demonstrate how latent\nlabeling errors embedded in training and validation sets can distort\nevaluation, underscoring the need for embedded audit tools and\nperformance-aware governance. Smart-sizing reframes annotation as a\nfeedback-driven process aligned with mission outcomes, enabling more robust\nmodels with fewer labels and supporting efficient AI development pipelines for\nfrontier models and operational systems."}
{"id": "2505.15009", "pdf": "https://arxiv.org/pdf/2505.15009", "abs": "https://arxiv.org/abs/2505.15009", "authors": ["Quan Nguyen", "Thanh Nguyen-Tang"], "title": "One-Layer Transformers are Provably Optimal for In-context Reasoning and Distributional Association Learning in Next-Token Prediction Tasks", "categories": ["cs.LG", "cs.AI"], "comment": "27 pages", "summary": "We study the approximation capabilities and on-convergence behaviors of\none-layer transformers on the noiseless and noisy in-context reasoning of\nnext-token prediction. Existing theoretical results focus on understanding the\nin-context reasoning behaviors for either the first gradient step or when the\nnumber of samples is infinite. Furthermore, no convergence rates nor\ngeneralization abilities were known. Our work addresses these gaps by showing\nthat there exists a class of one-layer transformers that are provably\nBayes-optimal with both linear and ReLU attention. When being trained with\ngradient descent, we show via a finite-sample analysis that the expected loss\nof these transformers converges at linear rate to the Bayes risk. Moreover, we\nprove that the trained models generalize to unseen samples as well as exhibit\nlearning behaviors that were empirically observed in previous works. Our\ntheoretical findings are further supported by extensive empirical validations."}
{"id": "2505.14967", "pdf": "https://arxiv.org/pdf/2505.14967", "abs": "https://arxiv.org/abs/2505.14967", "authors": ["Fangzhen Zhao", "Chenyi Zhang", "Naipeng Dong", "Ming Li", "Jinxiao Shan"], "title": "Anomaly Detection Based on Critical Paths for Deep Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": "23 pages in ACM journal latex format", "summary": "Deep neural networks (DNNs) are notoriously hard to understand and difficult\nto defend. Extracting representative paths (including the neuron activation\nvalues and the connections between neurons) from DNNs using software\nengineering approaches has recently shown to be a promising approach in\ninterpreting the decision making process of blackbox DNNs, as the extracted\npaths are often effective in capturing essential features. With this in mind,\nthis work investigates a novel approach that extracts critical paths from DNNs\nand subsequently applies the extracted paths for the anomaly detection task,\nbased on the observation that outliers and adversarial inputs do not usually\ninduce the same activation pattern on those paths as normal (in-distribution)\ninputs.\n  In our approach, we first identify critical detection paths via genetic\nevolution and mutation. Since different paths in a DNN often capture different\nfeatures for the same target class, we ensemble detection results from multiple\npaths by integrating random subspace sampling and a voting mechanism. Compared\nwith state-of-the-art methods, our experimental results suggest that our method\nnot only outperforms them, but it is also suitable for the detection of a broad\nrange of anomaly types with high accuracy."}
{"id": "2505.15034", "pdf": "https://arxiv.org/pdf/2505.15034", "abs": "https://arxiv.org/abs/2505.15034", "authors": ["Kaiwen Zha", "Zhengqi Gao", "Maohao Shen", "Zhang-Wei Hong", "Duane S. Boning", "Dina Katabi"], "title": "RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Tech report. The first two authors contributed equally", "summary": "Reinforcement learning (RL) has recently emerged as a compelling approach for\nenhancing the reasoning capabilities of large language models (LLMs), where an\nLLM generator serves as a policy guided by a verifier (reward model). However,\ncurrent RL post-training methods for LLMs typically use verifiers that are\nfixed (rule-based or frozen pretrained) or trained discriminatively via\nsupervised fine-tuning (SFT). Such designs are susceptible to reward hacking\nand generalize poorly beyond their training distributions. To overcome these\nlimitations, we propose Tango, a novel framework that uses RL to concurrently\ntrain both an LLM generator and a verifier in an interleaved manner. A central\ninnovation of Tango is its generative, process-level LLM verifier, which is\ntrained via RL and co-evolves with the generator. Importantly, the verifier is\ntrained solely based on outcome-level verification correctness rewards without\nrequiring explicit process-level annotations. This generative RL-trained\nverifier exhibits improved robustness and superior generalization compared to\ndeterministic or SFT-trained verifiers, fostering effective mutual\nreinforcement with the generator. Extensive experiments demonstrate that both\ncomponents of Tango achieve state-of-the-art results among 7B/8B-scale models:\nthe generator attains best-in-class performance across five competition-level\nmath benchmarks and four challenging out-of-domain reasoning tasks, while the\nverifier leads on the ProcessBench dataset. Remarkably, both components exhibit\nparticularly substantial improvements on the most difficult mathematical\nreasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango."}
{"id": "2505.14969", "pdf": "https://arxiv.org/pdf/2505.14969", "abs": "https://arxiv.org/abs/2505.14969", "authors": ["Yangchao Wu", "Zongyue Qin", "Alex Wong", "Stefano Soatto"], "title": "STree: Speculative Tree Decoding for Hybrid State-Space Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Speculative decoding is a technique to leverage hardware concurrency to\nimprove the efficiency of large-scale autoregressive (AR) Transformer models by\nenabling multiple steps of token generation in a single forward pass.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead to\ncurrent SSM state update implementations. With the algorithm, we describe a\nhardware-aware implementation that improves naive application of AR Transformer\ntree-based speculative decoding methods to SSMs. Furthermore, we outperform\nvanilla speculative decoding with SSMs even with a baseline drafting model and\ntree structure on three different benchmarks, opening up opportunities for\nfurther speed up with SSM and hybrid model inference. Code will be released\nupon paper acceptance."}
{"id": "2505.15047", "pdf": "https://arxiv.org/pdf/2505.15047", "abs": "https://arxiv.org/abs/2505.15047", "authors": ["Yingming Pu", "Tao Lin", "Hongyu Chen"], "title": "PiFlow: Principle-aware Scientific Discovery with Multi-Agent Collaboration", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate\nremarkable potential for scientific discovery. Existing approaches, however,\noften automate scientific discovery using predefined workflows that lack\nrationality constraints. This often leads to aimless hypothesizing and a\nfailure to consistently link hypotheses with evidence, thereby hindering\nsystematic uncertainty reduction. Overcoming these limitations fundamentally\nrequires systematic uncertainty reduction. We introduce \\texttt{PiFlow}, an\ninformation-theoretical framework, treating automated scientific discovery as a\nstructured uncertainty reduction problem guided by principles (e.g., scientific\nlaws). In evaluations across three distinct scientific domains -- discovering\nnanomaterial structures, bio-molecules, and superconductor candidates with\ntargeted properties -- our method significantly improves discovery efficiency,\nreflected by a 73.55\\% increase in the Area Under the Curve (AUC) of property\nvalues versus exploration steps, and enhances solution quality by 94.06\\%\ncompared to a vanilla agent system. Overall, \\texttt{PiFlow} serves as a\nPlug-and-Play method, establishing a novel paradigm shift in highly efficient\nautomated scientific discovery, paving the way for more robust and accelerated\nAI-driven research. Code is publicly available at our\n\\href{https://github.com/amair-lab/PiFlow}{GitHub}."}
{"id": "2505.14975", "pdf": "https://arxiv.org/pdf/2505.14975", "abs": "https://arxiv.org/abs/2505.14975", "authors": ["John L. Zhou", "Jonathan C. Kao"], "title": "Flattening Hierarchies with Policy Bootstrapping", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail."}
{"id": "2505.15080", "pdf": "https://arxiv.org/pdf/2505.15080", "abs": "https://arxiv.org/abs/2505.15080", "authors": ["Sergey Pankov", "Georges Harik"], "title": "SUS backprop: linear backpropagation algorithm for long inputs in transformers", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "21 pages, 9 figures", "summary": "It is straightforward to design an unbiased gradient estimator that\nstochastically cuts the backpropagation flow through any part of a\ncomputational graph. By cutting the parts that have little effect on the\ncomputation, one can potentially save a significant amount of back-propagation\ncomputation in exchange for a minimal increase in the stochastic gradient\nvariance, in some situations. Such a situation occurs in the attention\nmechanism of the transformer architecture. For long sequences, attention\nbecomes the limiting factor, as its compute requirements increase quadratically\nwith sequence length $n$. At the same time, most attention weights become very\nsmall, as most attention heads tend to connect a given token with only a small\nfraction of other tokens in the sequence. These weights become promising\ntargets for cutting backpropagation. We propose a simple probabilistic rule\ncontrolled by a single parameter $c$ that cuts backpropagation through most\nattention weights, leaving at most $c$ interactions per token per attention\nhead. This brings a factor of $c/n$ reduction in the compute required for the\nattention backpropagation, turning it from quadratic $O(n^2)$ to linear\ncomplexity $O(nc)$. We have empirically verified that, for a typical\ntransformer model, cutting $99\\%$ of the attention gradient flow (i.e. choosing\n$c \\sim 20-30$) results in relative gradient variance increase of only about\n$1\\%$ for $n \\sim 2000$, and it decreases with $n$. This approach is amenable\nto efficient sparse matrix implementation, thus being promising for making the\ncost of a backward pass negligible relative to the cost of a forward pass when\ntraining a transformer model on long sequences."}
{"id": "2505.14999", "pdf": "https://arxiv.org/pdf/2505.14999", "abs": "https://arxiv.org/abs/2505.14999", "authors": ["Eric Hanchen Jiang", "Haozheng Luo", "Shengyuan Pang", "Xiaomin Li", "Zhenting Qi", "Hengli Li", "Cheng-Fu Yang", "Zongyu Lin", "Xinfeng Li", "Hao Xu", "Kai-Wei Chang", "Ying Nian Wu"], "title": "Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Mathematical reasoning presents a significant challenge for Large Language\nModels (LLMs), often requiring robust multi step logical consistency. While\nChain of Thought (CoT) prompting elicits reasoning steps, it doesn't guarantee\ncorrectness, and improving reliability via extensive sampling is\ncomputationally costly. This paper introduces the Energy Outcome Reward Model\n(EORM), an effective, lightweight, post hoc verifier. EORM leverages Energy\nBased Models (EBMs) to simplify the training of reward models by learning to\nassign a scalar energy score to CoT solutions using only outcome labels,\nthereby avoiding detailed annotations. It achieves this by interpreting\ndiscriminator output logits as negative energies, effectively ranking\ncandidates where lower energy is assigned to solutions leading to correct final\noutcomes implicitly favoring coherent reasoning. On mathematical benchmarks\n(GSM8k, MATH), EORM significantly improves final answer accuracy (e.g., with\nLlama 3 8B, achieving 90.7% on GSM8k and 63.7% on MATH). EORM effectively\nleverages a given pool of candidate solutions to match or exceed the\nperformance of brute force sampling, thereby enhancing LLM reasoning outcome\nreliability through its streamlined post hoc verification process."}
{"id": "2505.15083", "pdf": "https://arxiv.org/pdf/2505.15083", "abs": "https://arxiv.org/abs/2505.15083", "authors": ["Jeremy Qin"], "title": "Robust Multi-Modal Forecasting: Integrating Static and Dynamic Features", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series forecasting plays a crucial role in various applications,\nparticularly in healthcare, where accurate predictions of future health\ntrajectories can significantly impact clinical decision-making. Ensuring\ntransparency and explainability of the models responsible for these tasks is\nessential for their adoption in critical settings. Recent work has explored a\ntop-down approach to bi-level transparency, focusing on understanding trends\nand properties of predicted time series using static features. In this work, we\nextend this framework by incorporating exogenous time series features alongside\nstatic features in a structured manner, while maintaining cohesive\ninterpretation. Our approach leverages the insights of trajectory comprehension\nto introduce an encoding mechanism for exogenous time series, where they are\ndecomposed into meaningful trends and properties, enabling the extraction of\ninterpretable patterns. Through experiments on several synthetic datasets, we\ndemonstrate that our approach remains predictive while preserving\ninterpretability and robustness. This work represents a step towards developing\nrobust, and generalized time series forecasting models. The code is available\nat https://github.com/jeremy-qin/TIMEVIEW"}
{"id": "2505.15008", "pdf": "https://arxiv.org/pdf/2505.15008", "abs": "https://arxiv.org/abs/2505.15008", "authors": ["Alvin Heng", "Harold Soh"], "title": "Know When to Abstain: Optimal Selective Classification with Likelihood Ratios", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Selective classification enhances the reliability of predictive models by\nallowing them to abstain from making uncertain predictions. In this work, we\nrevisit the design of optimal selection functions through the lens of the\nNeyman--Pearson lemma, a classical result in statistics that characterizes the\noptimal rejection rule as a likelihood ratio test. We show that this\nperspective not only unifies the behavior of several post-hoc selection\nbaselines, but also motivates new approaches to selective classification which\nwe propose here. A central focus of our work is the setting of covariate shift,\nwhere the input distribution at test time differs from that at training. This\nrealistic and challenging scenario remains relatively underexplored in the\ncontext of selective classification. We evaluate our proposed methods across a\nrange of vision and language tasks, including both supervised learning and\nvision-language models. Our experiments demonstrate that our\nNeyman--Pearson-informed methods consistently outperform existing baselines,\nindicating that likelihood ratio-based selection offers a robust mechanism for\nimproving selective classification under covariate shifts. Our code is publicly\navailable at https://github.com/clear-nus/sc-likelihood-ratios."}
{"id": "2505.15116", "pdf": "https://arxiv.org/pdf/2505.15116", "abs": "https://arxiv.org/abs/2505.15116", "authors": ["Zehong Wang", "Zheyuan Liu", "Tianyi Ma", "Jiazheng Li", "Zheyuan Zhang", "Xingbo Fu", "Yiyang Li", "Zhengqing Yuan", "Wei Song", "Yijun Ma", "Qingkai Zeng", "Xiusi Chen", "Jianan Zhao", "Jundong Li", "Meng Jiang", "Pietro Lio", "Nitesh Chawla", "Chuxu Zhang", "Yanfang Ye"], "title": "Graph Foundation Models: A Comprehensive Survey", "categories": ["cs.LG", "cs.AI", "cs.SI"], "comment": "Github Repo:\n  https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs. 93 pages,\n  438 references", "summary": "Graph-structured data pervades domains such as social networks, biological\nsystems, knowledge graphs, and recommender systems. While foundation models\nhave transformed natural language processing, vision, and multimodal learning\nthrough large-scale pretraining and generalization, extending these\ncapabilities to graphs -- characterized by non-Euclidean structures and complex\nrelational semantics -- poses unique challenges and opens new opportunities. To\nthis end, Graph Foundation Models (GFMs) aim to bring scalable, general-purpose\nintelligence to structured data, enabling broad transfer across graph-centric\ntasks and domains. This survey provides a comprehensive overview of GFMs,\nunifying diverse efforts under a modular framework comprising three key\ncomponents: backbone architectures, pretraining strategies, and adaptation\nmechanisms. We categorize GFMs by their generalization scope -- universal,\ntask-specific, and domain-specific -- and review representative methods, key\ninnovations, and theoretical insights within each category. Beyond methodology,\nwe examine theoretical foundations including transferability and emergent\ncapabilities, and highlight key challenges such as structural alignment,\nheterogeneity, scalability, and evaluation. Positioned at the intersection of\ngraph learning and general-purpose AI, GFMs are poised to become foundational\ninfrastructure for open-ended reasoning over structured data. This survey\nconsolidates current progress and outlines future directions to guide research\nin this rapidly evolving field. Resources are available at\nhttps://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs."}
{"id": "2505.15009", "pdf": "https://arxiv.org/pdf/2505.15009", "abs": "https://arxiv.org/abs/2505.15009", "authors": ["Quan Nguyen", "Thanh Nguyen-Tang"], "title": "One-Layer Transformers are Provably Optimal for In-context Reasoning and Distributional Association Learning in Next-Token Prediction Tasks", "categories": ["cs.LG", "cs.AI"], "comment": "27 pages", "summary": "We study the approximation capabilities and on-convergence behaviors of\none-layer transformers on the noiseless and noisy in-context reasoning of\nnext-token prediction. Existing theoretical results focus on understanding the\nin-context reasoning behaviors for either the first gradient step or when the\nnumber of samples is infinite. Furthermore, no convergence rates nor\ngeneralization abilities were known. Our work addresses these gaps by showing\nthat there exists a class of one-layer transformers that are provably\nBayes-optimal with both linear and ReLU attention. When being trained with\ngradient descent, we show via a finite-sample analysis that the expected loss\nof these transformers converges at linear rate to the Bayes risk. Moreover, we\nprove that the trained models generalize to unseen samples as well as exhibit\nlearning behaviors that were empirically observed in previous works. Our\ntheoretical findings are further supported by extensive empirical validations."}
{"id": "2505.15134", "pdf": "https://arxiv.org/pdf/2505.15134", "abs": "https://arxiv.org/abs/2505.15134", "authors": ["Shivam Agarwal", "Zimin Zhang", "Lifan Yuan", "Jiawei Han", "Hao Peng"], "title": "The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Entropy minimization (EM) trains the model to concentrate even more\nprobability mass on its most confident outputs. We show that this simple\nobjective alone, without any labeled data, can substantially improve large\nlanguage models' (LLMs) performance on challenging math, physics, and coding\ntasks. We explore three approaches: (1) EM-FT minimizes token-level entropy\nsimilarly to instruction finetuning, but on unlabeled outputs drawn from the\nmodel; (2) EM-RL: reinforcement learning with negative entropy as the only\nreward to maximize; (3) EM-INF: inference-time logit adjustment to reduce\nentropy without any training data or parameter updates. On Qwen-7B, EM-RL,\nwithout any labeled data, achieves comparable or better performance than strong\nRL baselines such as GRPO and RLOO that are trained on 60K labeled examples.\nFurthermore, EM-INF enables Qwen-32B to match or exceed the performance of\nproprietary models like GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro on the\nchallenging SciCode benchmark, while being 3x more efficient than\nself-consistency and sequential refinement. Our findings reveal that many\npretrained LLMs possess previously underappreciated reasoning capabilities that\ncan be effectively elicited through entropy minimization alone, without any\nlabeled data or even any parameter updates."}
{"id": "2505.15015", "pdf": "https://arxiv.org/pdf/2505.15015", "abs": "https://arxiv.org/abs/2505.15015", "authors": ["Longlong Li", "Cunquan Qu", "Guanghui Wang"], "title": "Beyond Node Attention: Multi-Scale Harmonic Encoding for Feature-Wise Graph Message Passing", "categories": ["cs.LG"], "comment": null, "summary": "Conventional Graph Neural Networks (GNNs) aggregate neighbor embeddings as\nholistic vectors, lacking the ability to identify fine-grained,\ndirection-specific feature relevance. We propose MSH-GNN (Multi-Scale Harmonic\nGraph Neural Network), a novel architecture that performs feature-wise adaptive\nmessage passing through node-specific harmonic projections. For each node,\nMSH-GNN dynamically projects neighbor features onto frequency-sensitive\ndirections determined by the target node's own representation. These\nprojections are further modulated using learnable sinusoidal encodings at\nmultiple frequencies, enabling the model to capture both smooth and oscillatory\nstructural patterns across scales. A frequency-aware attention pooling\nmechanism is introduced to emphasize spectrally and structurally salient nodes\nduring readout. Theoretically, we prove that MSH-GNN approximates\nshift-invariant kernels and matches the expressive power of the\n1-Weisfeiler-Lehman (1-WL) test. Empirically, MSH-GNN consistently outperforms\nstate-of-the-art models on a wide range of graph and node classification tasks.\nFurthermore, in challenging classification settings involving joint variations\nin graph topology and spectral frequency, MSH-GNN excels at capturing\nstructural asymmetries and high-frequency modulations, enabling more accurate\ngraph discrimination."}
{"id": "2505.15138", "pdf": "https://arxiv.org/pdf/2505.15138", "abs": "https://arxiv.org/abs/2505.15138", "authors": ["Yang Xu", "Swetha Ganesh", "Washim Uddin Mondal", "Qinbo Bai", "Vaneet Aggarwal"], "title": "Global Convergence for Average Reward Constrained MDPs with Primal-Dual Actor Critic Algorithm", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper investigates infinite-horizon average reward Constrained Markov\nDecision Processes (CMDPs) with general parametrization. We propose a\nPrimal-Dual Natural Actor-Critic algorithm that adeptly manages constraints\nwhile ensuring a high convergence rate. In particular, our algorithm achieves\nglobal convergence and constraint violation rates of\n$\\tilde{\\mathcal{O}}(1/\\sqrt{T})$ over a horizon of length $T$ when the mixing\ntime, $\\tau_{\\mathrm{mix}}$, is known to the learner. In absence of knowledge\nof $\\tau_{\\mathrm{mix}}$, the achievable rates change to\n$\\tilde{\\mathcal{O}}(1/T^{0.5-\\epsilon})$ provided that $T \\geq\n\\tilde{\\mathcal{O}}\\left(\\tau_{\\mathrm{mix}}^{2/\\epsilon}\\right)$. Our results\nmatch the theoretical lower bound for Markov Decision Processes and establish a\nnew benchmark in the theoretical exploration of average reward CMDPs."}
{"id": "2505.15030", "pdf": "https://arxiv.org/pdf/2505.15030", "abs": "https://arxiv.org/abs/2505.15030", "authors": ["Qingyu Song", "Peiyu Liao", "Wenqian Zhao", "Yiwen Wang", "Shoubo Hu", "Hui-Ling Zhen", "Ning Jiang", "Mingxuan Yuan"], "title": "Harnessing Large Language Models Locally: Empirical Results and Implications for AI PC", "categories": ["cs.LG"], "comment": "18 pages, 14 figures", "summary": "The increasing deployment of Large Language Models (LLMs) on edge devices,\ndriven by model advancements and hardware improvements, offers significant\nprivacy benefits. However, these on-device LLMs inherently face performance\nlimitations due to reduced model capacity and necessary compression techniques.\nTo address this, we introduce a systematic methodology -- encompassing model\ncapability, development efficiency, and system resources -- for evaluating\non-device LLMs. Our comprehensive evaluation, encompassing models from 0.5B to\n14B parameters and seven post-training quantization (PTQ) methods on commodity\nlaptops, yields several critical insights: 1) System-level metrics exhibit\nnear-linear scaling with effective bits-per-weight (BPW). 2) A practical\nthreshold exists around $\\sim$3.5 effective BPW, larger models subjected to\nlow-bit quantization consistently outperform smaller models utilizing higher\nbit-precision. 3) Quantization with low BPW incurs marginal accuracy loss but\nsignificant memory savings. 4) Determined by low-level implementation specifics\npower consumption on CPU, where computation-intensive operations spend more\npower than memory-intensive ones. These findings offer crucial insights and\npractical guidelines for the efficient deployment and optimized configuration\nof LLMs on resource-constrained edge devices. Our codebase is available at\nhttps://github.com/simmonssong/LLMOnDevice."}
{"id": "2505.15141", "pdf": "https://arxiv.org/pdf/2505.15141", "abs": "https://arxiv.org/abs/2505.15141", "authors": ["Yunlong Hou", "Fengzhuo Zhang", "Cunxiao Du", "Xuan Zhang", "Jiachun Pan", "Tianyu Pang", "Chao Du", "Vincent Y. F. Tan", "Zhuoran Yang"], "title": "BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "35 pages, 4 figures", "summary": "Speculative decoding has emerged as a popular method to accelerate the\ninference of Large Language Models (LLMs) while retaining their superior text\ngeneration performance. Previous methods either adopt a fixed speculative\ndecoding configuration regardless of the prefix tokens, or train draft models\nin an offline or online manner to align them with the context. This paper\nproposes a training-free online learning framework to adaptively choose the\nconfiguration of the hyperparameters for speculative decoding as text is being\ngenerated. We first formulate this hyperparameter selection problem as a\nMulti-Armed Bandit problem and provide a general speculative decoding framework\nBanditSpec. Furthermore, two bandit-based hyperparameter selection algorithms,\nUCBSpec and EXP3Spec, are designed and analyzed in terms of a novel quantity,\nthe stopping time regret. We upper bound this regret under both stochastic and\nadversarial reward settings. By deriving an information-theoretic impossibility\nresult, it is shown that the regret performance of UCBSpec is optimal up to\nuniversal constants. Finally, extensive empirical experiments with LLaMA3 and\nQwen2 demonstrate that our algorithms are effective compared to existing\nmethods, and the throughput is close to the oracle best hyperparameter in\nsimulated real-life LLM serving scenarios with diverse input prompts."}
{"id": "2505.15034", "pdf": "https://arxiv.org/pdf/2505.15034", "abs": "https://arxiv.org/abs/2505.15034", "authors": ["Kaiwen Zha", "Zhengqi Gao", "Maohao Shen", "Zhang-Wei Hong", "Duane S. Boning", "Dina Katabi"], "title": "RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Tech report. The first two authors contributed equally", "summary": "Reinforcement learning (RL) has recently emerged as a compelling approach for\nenhancing the reasoning capabilities of large language models (LLMs), where an\nLLM generator serves as a policy guided by a verifier (reward model). However,\ncurrent RL post-training methods for LLMs typically use verifiers that are\nfixed (rule-based or frozen pretrained) or trained discriminatively via\nsupervised fine-tuning (SFT). Such designs are susceptible to reward hacking\nand generalize poorly beyond their training distributions. To overcome these\nlimitations, we propose Tango, a novel framework that uses RL to concurrently\ntrain both an LLM generator and a verifier in an interleaved manner. A central\ninnovation of Tango is its generative, process-level LLM verifier, which is\ntrained via RL and co-evolves with the generator. Importantly, the verifier is\ntrained solely based on outcome-level verification correctness rewards without\nrequiring explicit process-level annotations. This generative RL-trained\nverifier exhibits improved robustness and superior generalization compared to\ndeterministic or SFT-trained verifiers, fostering effective mutual\nreinforcement with the generator. Extensive experiments demonstrate that both\ncomponents of Tango achieve state-of-the-art results among 7B/8B-scale models:\nthe generator attains best-in-class performance across five competition-level\nmath benchmarks and four challenging out-of-domain reasoning tasks, while the\nverifier leads on the ProcessBench dataset. Remarkably, both components exhibit\nparticularly substantial improvements on the most difficult mathematical\nreasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango."}
{"id": "2505.15201", "pdf": "https://arxiv.org/pdf/2505.15201", "abs": "https://arxiv.org/abs/2505.15201", "authors": ["Christian Walder", "Deep Karkhanis"], "title": "Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts\nfor each problem and reward them independently. This optimizes for pass@1\nperformance and prioritizes the strength of isolated samples at the expense of\nthe diversity and collective utility of sets of samples. This under-utilizes\nthe sampling capacity, limiting exploration and eventual improvement on harder\nexamples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a\ntransformation on the final rewards which leads to direct optimization of\npass@k performance, thus optimizing for sets of samples that maximize reward\nwhen considered jointly. Our contribution is to derive novel low variance\nunbiased estimators for pass@k and its gradient, in both the binary and\ncontinuous reward settings. We show optimization with our estimators reduces to\nstandard RL with rewards that have been jointly transformed by a stable and\nefficient transformation function.\n  While previous efforts are restricted to k=n, ours is the first to enable\nrobust optimization of pass@k for any arbitrary k <= n. Moreover, instead of\ntrading off pass@1 performance for pass@k gains, our method allows annealing k\nduring training, optimizing both metrics and often achieving strong pass@1\nnumbers alongside significant pass@k gains.\n  We validate our reward transformations on toy experiments, which reveal the\nvariance reducing properties of our formulations. We also include real-world\nexamples using the open-source LLM, GEMMA-2. We find that our transformation\neffectively optimizes for the target k. Furthermore, higher k values enable\nsolving more and harder problems, while annealing k boosts both the pass@1 and\npass@k . Crucially, for challenging task sets where conventional pass@1\noptimization stalls, our pass@k approach unblocks learning, likely due to\nbetter exploration by prioritizing joint utility over the utility of individual\nsamples."}
{"id": "2505.15040", "pdf": "https://arxiv.org/pdf/2505.15040", "abs": "https://arxiv.org/abs/2505.15040", "authors": ["Ivan Smirnov", "Shangding Gu"], "title": "RLBenchNet: The Right Network for the Right Reinforcement Learning Task", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) has seen significant advancements through the\napplication of various neural network architectures. In this study, we\nsystematically investigate the performance of several neural networks in RL\ntasks, including Long Short-Term Memory (LSTM), Multi-Layer Perceptron (MLP),\nMamba/Mamba-2, Transformer-XL, Gated Transformer-XL, and Gated Recurrent Unit\n(GRU). Through comprehensive evaluation across continuous control, discrete\ndecision-making, and memory-based environments, we identify\narchitecture-specific strengths and limitations. Our results reveal that: (1)\nMLPs excel in fully observable continuous control tasks, providing an optimal\nbalance of performance and efficiency; (2) recurrent architectures like LSTM\nand GRU offer robust performance in partially observable environments with\nmoderate memory requirements; (3) Mamba models achieve a 4.5x higher throughput\ncompared to LSTM and a 3.9x increase over GRU, all while maintaining comparable\nperformance; and (4) only Transformer-XL, Gated Transformer-XL, and Mamba-2\nsuccessfully solve the most challenging memory-intensive tasks, with Mamba-2\nrequiring 8x less memory than Transformer-XL. These findings provide insights\nfor researchers and practitioners, enabling more informed architecture\nselection based on specific task characteristics and computational constraints.\nCode is available at: https://github.com/SafeRL-Lab/RLBenchNet"}
{"id": "2505.15239", "pdf": "https://arxiv.org/pdf/2505.15239", "abs": "https://arxiv.org/abs/2505.15239", "authors": ["Peter Súkeník", "Christoph H. Lampert", "Marco Mondelli"], "title": "Neural Collapse is Globally Optimal in Deep Regularized ResNets and Transformers", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "The empirical emergence of neural collapse -- a surprising symmetry in the\nfeature representations of the training data in the penultimate layer of deep\nneural networks -- has spurred a line of theoretical research aimed at its\nunderstanding. However, existing work focuses on data-agnostic models or, when\ndata structure is taken into account, it remains limited to multi-layer\nperceptrons. Our paper fills both these gaps by analyzing modern architectures\nin a data-aware regime: we prove that global optima of deep regularized\ntransformers and residual networks (ResNets) with LayerNorm trained with cross\nentropy or mean squared error loss are approximately collapsed, and the\napproximation gets tighter as the depth grows. More generally, we formally\nreduce any end-to-end large-depth ResNet or transformer training into an\nequivalent unconstrained features model, thus justifying its wide use in the\nliterature even beyond data-agnostic settings. Our theoretical results are\nsupported by experiments on computer vision and language datasets showing that,\nas the depth grows, neural collapse indeed becomes more prominent."}
{"id": "2505.15047", "pdf": "https://arxiv.org/pdf/2505.15047", "abs": "https://arxiv.org/abs/2505.15047", "authors": ["Yingming Pu", "Tao Lin", "Hongyu Chen"], "title": "PiFlow: Principle-aware Scientific Discovery with Multi-Agent Collaboration", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate\nremarkable potential for scientific discovery. Existing approaches, however,\noften automate scientific discovery using predefined workflows that lack\nrationality constraints. This often leads to aimless hypothesizing and a\nfailure to consistently link hypotheses with evidence, thereby hindering\nsystematic uncertainty reduction. Overcoming these limitations fundamentally\nrequires systematic uncertainty reduction. We introduce \\texttt{PiFlow}, an\ninformation-theoretical framework, treating automated scientific discovery as a\nstructured uncertainty reduction problem guided by principles (e.g., scientific\nlaws). In evaluations across three distinct scientific domains -- discovering\nnanomaterial structures, bio-molecules, and superconductor candidates with\ntargeted properties -- our method significantly improves discovery efficiency,\nreflected by a 73.55\\% increase in the Area Under the Curve (AUC) of property\nvalues versus exploration steps, and enhances solution quality by 94.06\\%\ncompared to a vanilla agent system. Overall, \\texttt{PiFlow} serves as a\nPlug-and-Play method, establishing a novel paradigm shift in highly efficient\nautomated scientific discovery, paving the way for more robust and accelerated\nAI-driven research. Code is publicly available at our\n\\href{https://github.com/amair-lab/PiFlow}{GitHub}."}
{"id": "2505.15250", "pdf": "https://arxiv.org/pdf/2505.15250", "abs": "https://arxiv.org/abs/2505.15250", "authors": ["Suping Xu", "Lin Shang", "Keyu Liu", "Hengrong Ju", "Xibei Yang", "Witold Pedrycz"], "title": "Margin-aware Fuzzy Rough Feature Selection: Bridging Uncertainty Characterization and Pattern Classification", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Fuzzy rough feature selection (FRFS) is an effective means of addressing the\ncurse of dimensionality in high-dimensional data. By removing redundant and\nirrelevant features, FRFS helps mitigate classifier overfitting, enhance\ngeneralization performance, and lessen computational overhead. However, most\nexisting FRFS algorithms primarily focus on reducing uncertainty in pattern\nclassification, neglecting that lower uncertainty does not necessarily result\nin improved classification performance, despite it commonly being regarded as a\nkey indicator of feature selection effectiveness in the FRFS literature. To\nbridge uncertainty characterization and pattern classification, we propose a\nMargin-aware Fuzzy Rough Feature Selection (MAFRFS) framework that considers\nboth the compactness and separation of label classes. MAFRFS effectively\nreduces uncertainty in pattern classification tasks, while guiding the feature\nselection towards more separable and discriminative label class structures.\nExtensive experiments on 15 public datasets demonstrate that MAFRFS is highly\nscalable and more effective than FRFS. The algorithms developed using MAFRFS\noutperform six state-of-the-art feature selection algorithms."}
{"id": "2505.15064", "pdf": "https://arxiv.org/pdf/2505.15064", "abs": "https://arxiv.org/abs/2505.15064", "authors": ["Sho Sonoda", "Yuka Hashimoto", "Isao Ishikawa", "Masahiro Ikeda"], "title": "Generalization Through Growth: Hidden Dynamics Controls Depth Dependence", "categories": ["cs.LG", "math.DS", "stat.ML"], "comment": null, "summary": "Recent theory has reduced the depth dependence of generalization bounds from\nexponential to polynomial and even depth-independent rates, yet these results\nremain tied to specific architectures and Euclidean inputs. We present a\nunified framework for arbitrary \\blue{pseudo-metric} spaces in which a\ndepth-\\(k\\) network is the composition of continuous hidden maps\n\\(f:\\mathcal{X}\\to \\mathcal{X}\\) and an output map \\(h:\\mathcal{X}\\to\n\\mathbb{R}\\). The resulting bound $O(\\sqrt{(\\alpha + \\log \\beta(k))/n})$\nisolates the sole depth contribution in \\(\\beta(k)\\), the word-ball growth of\nthe semigroup generated by the hidden layers. By Gromov's theorem polynomial\n(resp. exponential) growth corresponds to virtually nilpotent (resp. expanding)\ndynamics, revealing a geometric dichotomy behind existing $O(\\sqrt{k})$\n(sublinear depth) and $\\tilde{O}(1)$ (depth-independent) rates. We further\nprovide covering-number estimates showing that expanding dynamics yield an\nexponential parameter saving via compositional expressivity. Our results\ndecouple specification from implementation, offering architecture-agnostic and\ndynamical-systems-aware guarantees applicable to modern deep-learning paradigms\nsuch as test-time inference and diffusion models."}
{"id": "2505.15270", "pdf": "https://arxiv.org/pdf/2505.15270", "abs": "https://arxiv.org/abs/2505.15270", "authors": ["Chenyu Zheng", "Xinyu Zhang", "Rongzhen Wang", "Wei Huang", "Zhi Tian", "Weilin Huang", "Jun Zhu", "Chongxuan Li"], "title": "Scaling Diffusion Transformers Efficiently via $μ$P", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "35 pages, 10 figures, 15 tables", "summary": "Diffusion Transformers have emerged as the foundation for vision generative\nmodels, but their scalability is limited by the high cost of hyperparameter\n(HP) tuning at large scales. Recently, Maximal Update Parametrization ($\\mu$P)\nwas proposed for vanilla Transformers, which enables stable HP transfer from\nsmall to large language models, and dramatically reduces tuning costs. However,\nit remains unclear whether $\\mu$P of vanilla Transformers extends to diffusion\nTransformers, which differ architecturally and objectively. In this work, we\ngeneralize standard $\\mu$P to diffusion Transformers and validate its\neffectiveness through large-scale experiments. First, we rigorously prove that\n$\\mu$P of mainstream diffusion Transformers, including DiT, U-ViT,\nPixArt-$\\alpha$, and MMDiT, aligns with that of the vanilla Transformer,\nenabling the direct application of existing $\\mu$P methodologies. Leveraging\nthis result, we systematically demonstrate that DiT-$\\mu$P enjoys robust HP\ntransferability. Notably, DiT-XL-2-$\\mu$P with transferred learning rate\nachieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we\nvalidate the effectiveness of $\\mu$P on text-to-image generation by scaling\nPixArt-$\\alpha$ from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases,\nmodels under $\\mu$P outperform their respective baselines while requiring small\ntuning cost, only 5.5% of one training run for PixArt-$\\alpha$ and 3% of\nconsumption by human experts for MMDiT-18B. These results establish $\\mu$P as a\nprincipled and efficient framework for scaling diffusion Transformers."}
{"id": "2505.15072", "pdf": "https://arxiv.org/pdf/2505.15072", "abs": "https://arxiv.org/abs/2505.15072", "authors": ["Xin Zhou", "Weiqing Wang", "Francisco J. Baldán", "Wray Buntine", "Christoph Bergmeir"], "title": "MoTime: A Dataset Suite for Multimodal Time Series Forecasting", "categories": ["cs.LG", "cs.CL", "cs.DB", "cs.IR"], "comment": null, "summary": "While multimodal data sources are increasingly available from real-world\nforecasting, most existing research remains on unimodal time series. In this\nwork, we present MoTime, a suite of multimodal time series forecasting datasets\nthat pair temporal signals with external modalities such as text, metadata, and\nimages. Covering diverse domains, MoTime supports structured evaluation of\nmodality utility under two scenarios: 1) the common forecasting task, where\nvarying-length history is available, and 2) cold-start forecasting, where no\nhistorical data is available. Experiments show that external modalities can\nimprove forecasting performance in both scenarios, with particularly strong\nbenefits for short series in some datasets, though the impact varies depending\non data characteristics. By making datasets and findings publicly available, we\naim to support more comprehensive and realistic benchmarks in future multimodal\ntime series forecasting research."}
{"id": "2505.15293", "pdf": "https://arxiv.org/pdf/2505.15293", "abs": "https://arxiv.org/abs/2505.15293", "authors": ["Qianyue Hao", "Yiwen Song", "Qingmin Liao", "Jian Yuan", "Yong Li"], "title": "LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Policy exploration is critical in reinforcement learning (RL), where existing\napproaches include greedy, Gaussian process, etc. However, these approaches\nutilize preset stochastic processes and are indiscriminately applied in all\nkinds of RL tasks without considering task-specific features that influence\npolicy exploration. Moreover, during RL training, the evolution of such\nstochastic processes is rigid, which typically only incorporates a decay in the\nvariance, failing to adjust flexibly according to the agent's real-time\nlearning status. Inspired by the analyzing and reasoning capability of large\nlanguage models (LLMs), we design LLM-Explorer to adaptively generate\ntask-specific exploration strategies with LLMs, enhancing the policy\nexploration in RL. In our design, we sample the learning trajectory of the\nagent during the RL training in a given task and prompt the LLM to analyze the\nagent's current policy learning status and then generate a probability\ndistribution for future policy exploration. Updating the probability\ndistribution periodically, we derive a stochastic process specialized for the\nparticular task and dynamically adjusted to adapt to the learning process. Our\ndesign is a plug-in module compatible with various widely applied RL\nalgorithms, including the DQN series, DDPG, TD3, and any possible variants\ndeveloped based on them. Through extensive experiments on the Atari and MuJoCo\nbenchmarks, we demonstrate LLM-Explorer's capability to enhance RL policy\nexploration, achieving an average performance improvement up to 37.27%. Our\ncode is open-source at https://anonymous.4open.science/r/LLM-Explorer-19BE for\nreproducibility."}
{"id": "2505.15076", "pdf": "https://arxiv.org/pdf/2505.15076", "abs": "https://arxiv.org/abs/2505.15076", "authors": ["Nanxu Gong", "Sixun Dong", "Haoyue Bai", "Xinyuan Wang", "Wangyang Ying", "Yanjie Fu"], "title": "Agentic Feature Augmentation: Unifying Selection and Generation with Teaming, Planning, and Memories", "categories": ["cs.LG"], "comment": null, "summary": "As a widely-used and practical tool, feature engineering transforms raw data\ninto discriminative features to advance AI model performance. However, existing\nmethods usually apply feature selection and generation separately, failing to\nstrive a balance between reducing redundancy and adding meaningful dimensions.\nTo fill this gap, we propose an agentic feature augmentation concept, where the\nunification of feature generation and selection is modeled as agentic teaming\nand planning. Specifically, we develop a Multi-Agent System with Long and\nShort-Term Memory (MAGS), comprising a selector agent to eliminate redundant\nfeatures, a generator agent to produce informative new dimensions, and a router\nagent that strategically coordinates their actions. We leverage in-context\nlearning with short-term memory for immediate feedback refinement and long-term\nmemory for globally optimal guidance. Additionally, we employ offline Proximal\nPolicy Optimization (PPO) reinforcement fine-tuning to train the router agent\nfor effective decision-making to navigate a vast discrete feature space.\nExtensive experiments demonstrate that this unified agentic framework\nconsistently achieves superior task performance by intelligently orchestrating\nfeature selection and generation."}
{"id": "2505.15303", "pdf": "https://arxiv.org/pdf/2505.15303", "abs": "https://arxiv.org/abs/2505.15303", "authors": ["Johannes Kaiser", "Kristian Schwethelm", "Daniel Rueckert", "Georgios Kaissis"], "title": "Laplace Sample Information: Data Informativeness Through a Bayesian Lens", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Accurately estimating the informativeness of individual samples in a dataset\nis an important objective in deep learning, as it can guide sample selection,\nwhich can improve model efficiency and accuracy by removing redundant or\npotentially harmful samples. We propose Laplace Sample Information (LSI)\nmeasure of sample informativeness grounded in information theory widely\napplicable across model architectures and learning settings. LSI leverages a\nBayesian approximation to the weight posterior and the KL divergence to measure\nthe change in the parameter distribution induced by a sample of interest from\nthe dataset. We experimentally show that LSI is effective in ordering the data\nwith respect to typicality, detecting mislabeled samples, measuring class-wise\ninformativeness, and assessing dataset difficulty. We demonstrate these\ncapabilities of LSI on image and text data in supervised and unsupervised\nsettings. Moreover, we show that LSI can be computed efficiently through probes\nand transfers well to the training of large models."}
{"id": "2505.15080", "pdf": "https://arxiv.org/pdf/2505.15080", "abs": "https://arxiv.org/abs/2505.15080", "authors": ["Sergey Pankov", "Georges Harik"], "title": "SUS backprop: linear backpropagation algorithm for long inputs in transformers", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "21 pages, 9 figures", "summary": "It is straightforward to design an unbiased gradient estimator that\nstochastically cuts the backpropagation flow through any part of a\ncomputational graph. By cutting the parts that have little effect on the\ncomputation, one can potentially save a significant amount of back-propagation\ncomputation in exchange for a minimal increase in the stochastic gradient\nvariance, in some situations. Such a situation occurs in the attention\nmechanism of the transformer architecture. For long sequences, attention\nbecomes the limiting factor, as its compute requirements increase quadratically\nwith sequence length $n$. At the same time, most attention weights become very\nsmall, as most attention heads tend to connect a given token with only a small\nfraction of other tokens in the sequence. These weights become promising\ntargets for cutting backpropagation. We propose a simple probabilistic rule\ncontrolled by a single parameter $c$ that cuts backpropagation through most\nattention weights, leaving at most $c$ interactions per token per attention\nhead. This brings a factor of $c/n$ reduction in the compute required for the\nattention backpropagation, turning it from quadratic $O(n^2)$ to linear\ncomplexity $O(nc)$. We have empirically verified that, for a typical\ntransformer model, cutting $99\\%$ of the attention gradient flow (i.e. choosing\n$c \\sim 20-30$) results in relative gradient variance increase of only about\n$1\\%$ for $n \\sim 2000$, and it decreases with $n$. This approach is amenable\nto efficient sparse matrix implementation, thus being promising for making the\ncost of a backward pass negligible relative to the cost of a forward pass when\ntraining a transformer model on long sequences."}
{"id": "2505.15306", "pdf": "https://arxiv.org/pdf/2505.15306", "abs": "https://arxiv.org/abs/2505.15306", "authors": ["Yiwen Song", "Qianyue Hao", "Qingmin Liao", "Jian Yuan", "Yong Li"], "title": "Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak Reinforcement Learning Agents into a Supreme One", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Model ensemble is a useful approach in reinforcement learning (RL) for\ntraining effective agents. Despite wide success of RL, training effective\nagents remains difficult due to the multitude of factors requiring careful\ntuning, such as algorithm selection, hyperparameter settings, and even random\nseed choices, all of which can significantly influence an agent's performance.\nModel ensemble helps overcome this challenge by combining multiple weak agents\ninto a single, more powerful one, enhancing overall performance. However,\nexisting ensemble methods, such as majority voting and Boltzmann addition, are\ndesigned as fixed strategies and lack a semantic understanding of specific\ntasks, limiting their adaptability and effectiveness. To address this, we\npropose LLM-Ens, a novel approach that enhances RL model ensemble with\ntask-specific semantic understandings driven by large language models (LLMs).\nGiven a task, we first design an LLM to categorize states in this task into\ndistinct 'situations', incorporating high-level descriptions of the task\nconditions. Then, we statistically analyze the strengths and weaknesses of each\nindividual agent to be used in the ensemble in each situation. During the\ninference time, LLM-Ens dynamically identifies the changing task situation and\nswitches to the agent that performs best in the current situation, ensuring\ndynamic model selection in the evolving task condition. Our approach is\ndesigned to be compatible with agents trained with different random seeds,\nhyperparameter settings, and various RL algorithms. Extensive experiments on\nthe Atari benchmark show that LLM-Ens significantly improves the RL model\nensemble, surpassing well-known baselines by up to 20.9%. For reproducibility,\nour code is open-source at\nhttps://anonymous.4open.science/r/LLM4RLensemble-F7EE."}
{"id": "2505.15083", "pdf": "https://arxiv.org/pdf/2505.15083", "abs": "https://arxiv.org/abs/2505.15083", "authors": ["Jeremy Qin"], "title": "Robust Multi-Modal Forecasting: Integrating Static and Dynamic Features", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series forecasting plays a crucial role in various applications,\nparticularly in healthcare, where accurate predictions of future health\ntrajectories can significantly impact clinical decision-making. Ensuring\ntransparency and explainability of the models responsible for these tasks is\nessential for their adoption in critical settings. Recent work has explored a\ntop-down approach to bi-level transparency, focusing on understanding trends\nand properties of predicted time series using static features. In this work, we\nextend this framework by incorporating exogenous time series features alongside\nstatic features in a structured manner, while maintaining cohesive\ninterpretation. Our approach leverages the insights of trajectory comprehension\nto introduce an encoding mechanism for exogenous time series, where they are\ndecomposed into meaningful trends and properties, enabling the extraction of\ninterpretable patterns. Through experiments on several synthetic datasets, we\ndemonstrate that our approach remains predictive while preserving\ninterpretability and robustness. This work represents a step towards developing\nrobust, and generalized time series forecasting models. The code is available\nat https://github.com/jeremy-qin/TIMEVIEW"}
{"id": "2505.15311", "pdf": "https://arxiv.org/pdf/2505.15311", "abs": "https://arxiv.org/abs/2505.15311", "authors": ["Yurun Yuan", "Fan Chen", "Zeyu Jia", "Alexander Rakhlin", "Tengyang Xie"], "title": "Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Policy-based methods currently dominate reinforcement learning (RL) pipelines\nfor large language model (LLM) reasoning, leaving value-based approaches\nlargely unexplored. We revisit the classical paradigm of Bellman Residual\nMinimization and introduce Trajectory Bellman Residual Minimization (TBRM), an\nalgorithm that naturally adapts this idea to LLMs, yielding a simple yet\neffective off-policy algorithm that optimizes a single trajectory-level Bellman\nobjective using the model's own logits as $Q$-values. TBRM removes the need for\ncritics, importance-sampling ratios, or clipping, and operates with only one\nrollout per prompt. We prove convergence to the near-optimal KL-regularized\npolicy from arbitrary off-policy data via an improved\nchange-of-trajectory-measure analysis. Experiments on standard\nmathematical-reasoning benchmarks show that TBRM consistently outperforms\npolicy-based baselines, like PPO and GRPO, with comparable or lower\ncomputational and memory overhead. Our results indicate that value-based RL\nmight be a principled and efficient alternative for enhancing reasoning\ncapabilities in LLMs."}
{"id": "2505.15101", "pdf": "https://arxiv.org/pdf/2505.15101", "abs": "https://arxiv.org/abs/2505.15101", "authors": ["Eray Can Elumar", "Cem Tekin", "Osman Yagan"], "title": "Cost-aware LLM-based Online Dataset Annotation", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled automated\ndataset labeling with minimal human supervision. While majority voting across\nmultiple LLMs can improve label reliability by mitigating individual model\nbiases, it incurs high computational costs due to repeated querying. In this\nwork, we propose a novel online framework, Cost-aware Majority Voting (CaMVo),\nfor efficient and accurate LLM-based dataset annotation. CaMVo adaptively\nselects a subset of LLMs for each data instance based on contextual embeddings,\nbalancing confidence and cost without requiring pre-training or ground-truth\nlabels. Leveraging a LinUCB-based selection mechanism and a Bayesian estimator\nover confidence scores, CaMVo estimates a lower bound on labeling accuracy for\neach LLM and aggregates responses through weighted majority voting. Our\nempirical evaluation on the MMLU and IMDB Movie Review datasets demonstrates\nthat CaMVo achieves comparable or superior accuracy to full majority voting\nwhile significantly reducing labeling costs. This establishes CaMVo as a\npractical and robust solution for cost-efficient annotation in dynamic labeling\nenvironments."}
{"id": "2505.15345", "pdf": "https://arxiv.org/pdf/2505.15345", "abs": "https://arxiv.org/abs/2505.15345", "authors": ["Jacob E. Kooi", "Zhao Yang", "Vincent François-Lavet"], "title": "Hadamax Encoding: Elevating Performance in Model-Free Atari", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Neural network architectures have a large impact in machine learning. In\nreinforcement learning, network architectures have remained notably simple, as\nchanges often lead to small gains in performance. This work introduces a novel\nencoder architecture for pixel-based model-free reinforcement learning. The\nHadamax (\\textbf{Hada}mard \\textbf{max}-pooling) encoder achieves\nstate-of-the-art performance by max-pooling Hadamard products between\nGELU-activated parallel hidden layers. Based on the recent PQN algorithm, the\nHadamax encoder achieves state-of-the-art model-free performance in the\nAtari-57 benchmark. Specifically, without applying any algorithmic\nhyperparameter modifications, Hadamax-PQN achieves an 80\\% performance gain\nover vanilla PQN and significantly surpasses Rainbow-DQN. For reproducibility,\nthe full code is available on\n\\href{https://github.com/Jacobkooi/Hadamax}{GitHub}."}
{"id": "2505.15103", "pdf": "https://arxiv.org/pdf/2505.15103", "abs": "https://arxiv.org/abs/2505.15103", "authors": ["Zihu Wang", "Boxun Xu", "Hejia Geng", "Peng Li"], "title": "Khan-GCL: Kolmogorov-Arnold Network Based Graph Contrastive Learning with Hard Negatives", "categories": ["cs.LG"], "comment": "Graph Contrastive Learning, Self-supervised Learning,\n  Kolmogorov-Arnold Network, Representation Learning", "summary": "Graph contrastive learning (GCL) has demonstrated great promise for learning\ngeneralizable graph representations from unlabeled data. However, conventional\nGCL approaches face two critical limitations: (1) the restricted expressive\ncapacity of multilayer perceptron (MLP) based encoders, and (2) suboptimal\nnegative samples that either from random augmentations-failing to provide\neffective 'hard negatives'-or generated hard negatives without addressing the\nsemantic distinctions crucial for discriminating graph data. To this end, we\npropose Khan-GCL, a novel framework that integrates the Kolmogorov-Arnold\nNetwork (KAN) into the GCL encoder architecture, substantially enhancing its\nrepresentational capacity. Furthermore, we exploit the rich information\nembedded within KAN coefficient parameters to develop two novel critical\nfeature identification techniques that enable the generation of semantically\nmeaningful hard negative samples for each graph representation. These\nstrategically constructed hard negatives guide the encoder to learn more\ndiscriminative features by emphasizing critical semantic differences between\ngraphs. Extensive experiments demonstrate that our approach achieves\nstate-of-the-art performance compared to existing GCL methods across a variety\nof datasets and tasks."}
{"id": "2505.15418", "pdf": "https://arxiv.org/pdf/2505.15418", "abs": "https://arxiv.org/abs/2505.15418", "authors": ["Yueheng Li", "Guangming Xie", "Zongqing Lu"], "title": "Guided Policy Optimization under Partial Observability", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "24 pages, 13 figures", "summary": "Reinforcement Learning (RL) in partially observable environments poses\nsignificant challenges due to the complexity of learning under uncertainty.\nWhile additional information, such as that available in simulations, can\nenhance training, effectively leveraging it remains an open problem. To address\nthis, we introduce Guided Policy Optimization (GPO), a framework that co-trains\na guider and a learner. The guider takes advantage of privileged information\nwhile ensuring alignment with the learner's policy that is primarily trained\nvia imitation learning. We theoretically demonstrate that this learning scheme\nachieves optimality comparable to direct RL, thereby overcoming key limitations\ninherent in existing approaches. Empirical evaluations show strong performance\nof GPO across various tasks, including continuous control with partial\nobservability and noise, and memory-based challenges, significantly\noutperforming existing methods."}
{"id": "2505.15116", "pdf": "https://arxiv.org/pdf/2505.15116", "abs": "https://arxiv.org/abs/2505.15116", "authors": ["Zehong Wang", "Zheyuan Liu", "Tianyi Ma", "Jiazheng Li", "Zheyuan Zhang", "Xingbo Fu", "Yiyang Li", "Zhengqing Yuan", "Wei Song", "Yijun Ma", "Qingkai Zeng", "Xiusi Chen", "Jianan Zhao", "Jundong Li", "Meng Jiang", "Pietro Lio", "Nitesh Chawla", "Chuxu Zhang", "Yanfang Ye"], "title": "Graph Foundation Models: A Comprehensive Survey", "categories": ["cs.LG", "cs.AI", "cs.SI"], "comment": "Github Repo:\n  https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs. 93 pages,\n  438 references", "summary": "Graph-structured data pervades domains such as social networks, biological\nsystems, knowledge graphs, and recommender systems. While foundation models\nhave transformed natural language processing, vision, and multimodal learning\nthrough large-scale pretraining and generalization, extending these\ncapabilities to graphs -- characterized by non-Euclidean structures and complex\nrelational semantics -- poses unique challenges and opens new opportunities. To\nthis end, Graph Foundation Models (GFMs) aim to bring scalable, general-purpose\nintelligence to structured data, enabling broad transfer across graph-centric\ntasks and domains. This survey provides a comprehensive overview of GFMs,\nunifying diverse efforts under a modular framework comprising three key\ncomponents: backbone architectures, pretraining strategies, and adaptation\nmechanisms. We categorize GFMs by their generalization scope -- universal,\ntask-specific, and domain-specific -- and review representative methods, key\ninnovations, and theoretical insights within each category. Beyond methodology,\nwe examine theoretical foundations including transferability and emergent\ncapabilities, and highlight key challenges such as structural alignment,\nheterogeneity, scalability, and evaluation. Positioned at the intersection of\ngraph learning and general-purpose AI, GFMs are poised to become foundational\ninfrastructure for open-ended reasoning over structured data. This survey\nconsolidates current progress and outlines future directions to guide research\nin this rapidly evolving field. Resources are available at\nhttps://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs."}
{"id": "2505.15429", "pdf": "https://arxiv.org/pdf/2505.15429", "abs": "https://arxiv.org/abs/2505.15429", "authors": ["Pritam Anand"], "title": "Uncertainty Quantification in SVM prediction", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper explores Uncertainty Quantification (UQ) in SVM predictions,\nparticularly for regression and forecasting tasks. Unlike the Neural Network,\nthe SVM solutions are typically more stable, sparse, optimal and interpretable.\nHowever, there are only few literature which addresses the UQ in SVM\nprediction. At first, we provide a comprehensive summary of existing Prediction\nInterval (PI) estimation and probabilistic forecasting methods developed in the\nSVM framework and evaluate them against the key properties expected from an\nideal PI model. We find that none of the existing SVM PI models achieves a\nsparse solution. To introduce sparsity in SVM model, we propose the Sparse\nSupport Vector Quantile Regression (SSVQR) model, which constructs PIs and\nprobabilistic forecasts by solving a pair of linear programs. Further, we\ndevelop a feature selection algorithm for PI estimation using SSVQR that\neffectively eliminates a significant number of features while improving PI\nquality in case of high-dimensional dataset. Finally we extend the SVM models\nin Conformal Regression setting for obtaining more stable prediction set with\nfinite test set guarantees. Extensive experiments on artificial, real-world\nbenchmark datasets compare the different characteristics of both existing and\nproposed SVM-based PI estimation methods and also highlight the advantages of\nthe feature selection in PI estimation. Furthermore, we compare both, the\nexisting and proposed SVM-based PI estimation models, with modern deep learning\nmodels for probabilistic forecasting tasks on benchmark datasets. Furthermore,\nSVM models show comparable or superior performance to modern complex deep\nlearning models for probabilistic forecasting task in our experiments."}
{"id": "2505.15130", "pdf": "https://arxiv.org/pdf/2505.15130", "abs": "https://arxiv.org/abs/2505.15130", "authors": ["Sajjad Ghiasvand", "Haniyeh Ehsani Oskouie", "Mahnoosh Alizadeh", "Ramtin Pedarsani"], "title": "Few-Shot Adversarial Low-Rank Fine-Tuning of Vision-Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Vision-Language Models (VLMs) such as CLIP have shown remarkable performance\nin cross-modal tasks through large-scale contrastive pre-training. To adapt\nthese large transformer-based models efficiently for downstream tasks,\nParameter-Efficient Fine-Tuning (PEFT) techniques like LoRA have emerged as\nscalable alternatives to full fine-tuning, especially in few-shot scenarios.\nHowever, like traditional deep neural networks, VLMs are highly vulnerable to\nadversarial attacks, where imperceptible perturbations can significantly\ndegrade model performance. Adversarial training remains the most effective\nstrategy for improving model robustness in PEFT. In this work, we propose\nAdvCLIP-LoRA, the first algorithm designed to enhance the adversarial\nrobustness of CLIP models fine-tuned with LoRA in few-shot settings. Our method\nformulates adversarial fine-tuning as a minimax optimization problem and\nprovides theoretical guarantees for convergence under smoothness and\nnonconvex-strong-concavity assumptions. Empirical results across eight datasets\nusing ViT-B/16 and ViT-B/32 models show that AdvCLIP-LoRA significantly\nimproves robustness against common adversarial attacks (e.g., FGSM, PGD),\nwithout sacrificing much clean accuracy. These findings highlight AdvCLIP-LoRA\nas a practical and theoretically grounded approach for robust adaptation of\nVLMs in resource-constrained settings."}
{"id": "2505.15433", "pdf": "https://arxiv.org/pdf/2505.15433", "abs": "https://arxiv.org/abs/2505.15433", "authors": ["Beni Egressy", "Jan Stühmer"], "title": "Set-LLM: A Permutation-Invariant LLM", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "While large language models (LLMs) demonstrate impressive capabilities across\nnumerous applications, their robustness remains a critical concern. This paper\nis motivated by a specific vulnerability: the order sensitivity of LLMs. This\nvulnerability manifests itself as the order bias observed when LLMs decide\nbetween possible options (for example, a preference for the first option) and\nthe tendency of LLMs to provide different answers when options are reordered.\nThe use cases for this scenario extend beyond the classical case of\nmultiple-choice question answering to the use of LLMs as automated evaluators\nin AI pipelines, comparing output generated by different models. We introduce\nSet-LLM, a novel architectural adaptation for pretrained LLMs that enables the\nprocessing of mixed set-text inputs with permutation invariance guarantees. The\nadaptations involve a new attention mask and new positional encodings\nspecifically designed for sets. We provide a theoretical proof of invariance\nand demonstrate through experiments that Set-LLM can be trained effectively,\nachieving comparable or improved performance and maintaining the runtime of the\noriginal model, while eliminating order sensitivity."}
{"id": "2505.15134", "pdf": "https://arxiv.org/pdf/2505.15134", "abs": "https://arxiv.org/abs/2505.15134", "authors": ["Shivam Agarwal", "Zimin Zhang", "Lifan Yuan", "Jiawei Han", "Hao Peng"], "title": "The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Entropy minimization (EM) trains the model to concentrate even more\nprobability mass on its most confident outputs. We show that this simple\nobjective alone, without any labeled data, can substantially improve large\nlanguage models' (LLMs) performance on challenging math, physics, and coding\ntasks. We explore three approaches: (1) EM-FT minimizes token-level entropy\nsimilarly to instruction finetuning, but on unlabeled outputs drawn from the\nmodel; (2) EM-RL: reinforcement learning with negative entropy as the only\nreward to maximize; (3) EM-INF: inference-time logit adjustment to reduce\nentropy without any training data or parameter updates. On Qwen-7B, EM-RL,\nwithout any labeled data, achieves comparable or better performance than strong\nRL baselines such as GRPO and RLOO that are trained on 60K labeled examples.\nFurthermore, EM-INF enables Qwen-32B to match or exceed the performance of\nproprietary models like GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro on the\nchallenging SciCode benchmark, while being 3x more efficient than\nself-consistency and sequential refinement. Our findings reveal that many\npretrained LLMs possess previously underappreciated reasoning capabilities that\ncan be effectively elicited through entropy minimization alone, without any\nlabeled data or even any parameter updates."}
{"id": "2505.15507", "pdf": "https://arxiv.org/pdf/2505.15507", "abs": "https://arxiv.org/abs/2505.15507", "authors": ["Mahesh Godavarti"], "title": "Directional Non-Commutative Monoidal Structures for Compositional Embeddings in Machine Learning", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.IR", "20-XX, 08A02", "F.4.1; I.2"], "comment": "11 pages submitted to NeurIPS 2025", "summary": "We introduce a new algebraic structure for multi-dimensional compositional\nembeddings, built on directional non-commutative monoidal operators. The core\ncontribution of this work is this novel framework, which exhibits appealing\ntheoretical properties (associativity along each dimension and an interchange\nlaw ensuring global consistency) while remaining compatible with modern machine\nlearning architectures. Our construction defines a distinct composition\noperator circ_i for each axis i, ensuring associative combination along each\naxis without imposing global commutativity. Importantly, all axis-specific\noperators commute with one another, enforcing a global interchange law that\nenables consistent crossaxis compositions. This is, to our knowledge, the first\napproach that provides a common foundation that generalizes classical\nsequence-modeling paradigms (e.g., structured state-space models (SSMs) and\ntransformer self-attention) to a unified multi-dimensional framework. For\nexample, specific one-dimensional instances of our framework can recover the\nfamiliar affine transformation algebra, vanilla self-attention, and the\nSSM-style recurrence. The higher-dimensional generalizations naturally support\nrecursive, structure-aware operations in embedding spaces. We outline several\npotential applications unlocked by this structure-including structured\npositional encodings in Transformers, directional image embeddings, and\nsymbolic modeling of sequences or grids-indicating that it could inform future\ndeep learning model designs. We formally establish the algebraic properties of\nour framework and discuss efficient implementations. Finally, as our focus is\ntheoretical, we include no experiments here and defer empirical validation to\nfuture work, which we plan to undertake."}
{"id": "2505.15138", "pdf": "https://arxiv.org/pdf/2505.15138", "abs": "https://arxiv.org/abs/2505.15138", "authors": ["Yang Xu", "Swetha Ganesh", "Washim Uddin Mondal", "Qinbo Bai", "Vaneet Aggarwal"], "title": "Global Convergence for Average Reward Constrained MDPs with Primal-Dual Actor Critic Algorithm", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper investigates infinite-horizon average reward Constrained Markov\nDecision Processes (CMDPs) with general parametrization. We propose a\nPrimal-Dual Natural Actor-Critic algorithm that adeptly manages constraints\nwhile ensuring a high convergence rate. In particular, our algorithm achieves\nglobal convergence and constraint violation rates of\n$\\tilde{\\mathcal{O}}(1/\\sqrt{T})$ over a horizon of length $T$ when the mixing\ntime, $\\tau_{\\mathrm{mix}}$, is known to the learner. In absence of knowledge\nof $\\tau_{\\mathrm{mix}}$, the achievable rates change to\n$\\tilde{\\mathcal{O}}(1/T^{0.5-\\epsilon})$ provided that $T \\geq\n\\tilde{\\mathcal{O}}\\left(\\tau_{\\mathrm{mix}}^{2/\\epsilon}\\right)$. Our results\nmatch the theoretical lower bound for Markov Decision Processes and establish a\nnew benchmark in the theoretical exploration of average reward CMDPs."}
{"id": "2505.15514", "pdf": "https://arxiv.org/pdf/2505.15514", "abs": "https://arxiv.org/abs/2505.15514", "authors": ["Soham Sane"], "title": "AM-PPO: (Advantage) Alpha-Modulation with Proximal Policy Optimization", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": "17 pages, 4 Tables, 9 Figures, 11 equations", "summary": "Proximal Policy Optimization (PPO) is a widely used reinforcement learning\nalgorithm that heavily relies on accurate advantage estimates for stable and\nefficient training. However, raw advantage signals can exhibit significant\nvariance, noise, and scale-related issues, impeding optimal learning\nperformance. To address this challenge, we introduce Advantage Modulation PPO\n(AM-PPO), a novel enhancement of PPO that adaptively modulates advantage\nestimates using a dynamic, non-linear scaling mechanism. This adaptive\nmodulation employs an alpha controller that dynamically adjusts the scaling\nfactor based on evolving statistical properties of the advantage signals, such\nas their norm, variance, and a predefined target saturation level. By\nincorporating a tanh-based gating function driven by these adaptively scaled\nadvantages, AM-PPO reshapes the advantage signals to stabilize gradient updates\nand improve the conditioning of the policy gradient landscape. Crucially, this\nmodulation also influences value function training by providing consistent and\nadaptively conditioned learning targets. Empirical evaluations across standard\ncontinuous control benchmarks demonstrate that AM-PPO achieves superior reward\ntrajectories, exhibits sustained learning progression, and significantly\nreduces the clipping required by adaptive optimizers. These findings underscore\nthe potential of advantage modulation as a broadly applicable technique for\nenhancing reinforcement learning optimization."}
{"id": "2505.15140", "pdf": "https://arxiv.org/pdf/2505.15140", "abs": "https://arxiv.org/abs/2505.15140", "authors": ["Tong Cheng", "Fu Jie", "Xinpeng Ling", "Huifa Li", "Zhili Chen"], "title": "EC-LDA : Label Distribution Inference Attack against Federated Graph Learning with Embedding Compression", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) have been widely used for graph analysis.\nFederated Graph Learning (FGL) is an emerging learning framework to\ncollaboratively train graph data from various clients. However, since clients\nare required to upload model parameters to the server in each round, this\nprovides the server with an opportunity to infer each client's data privacy. In\nthis paper, we focus on label distribution attacks(LDAs) that aim to infer the\nlabel distributions of the clients' local data. We take the first step to\nattack client's label distributions in FGL. Firstly, we observe that the\neffectiveness of LDA is closely related to the variance of node embeddings in\nGNNs. Next, we analyze the relation between them and we propose a new attack\nnamed EC-LDA, which significantly improves the attack effectiveness by\ncompressing node embeddings. Thirdly, extensive experiments on node\nclassification and link prediction tasks across six widely used graph datasets\nshow that EC-LDA outperforms the SOTA LDAs. For example, EC-LDA attains optimal\nvalues under both Cos-sim and JS-div evaluation metrics in the CoraFull and\nLastFM datasets. Finally, we explore the robustness of EC-LDA under\ndifferential privacy protection."}
{"id": "2505.15516", "pdf": "https://arxiv.org/pdf/2505.15516", "abs": "https://arxiv.org/abs/2505.15516", "authors": ["Christiaan Meijer", "E. G. Patrick Bos"], "title": "Explainable embeddings with Distance Explainer", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "68T99", "I.2.m"], "comment": "33 pages, 19 figures. Submitted to JMLR. Method implementation:\n  https://research-software-directory.org/software/distance-explainer", "summary": "While eXplainable AI (XAI) has advanced significantly, few methods address\ninterpretability in embedded vector spaces where dimensions represent complex\nabstractions. We introduce Distance Explainer, a novel method for generating\nlocal, post-hoc explanations of embedded spaces in machine learning models. Our\napproach adapts saliency-based techniques from RISE to explain the distance\nbetween two embedded data points by assigning attribution values through\nselective masking and distance-ranked mask filtering. We evaluate Distance\nExplainer on cross-modal embeddings (image-image and image-caption pairs) using\nestablished XAI metrics including Faithfulness, Sensitivity/Robustness, and\nRandomization. Experiments with ImageNet and CLIP models demonstrate that our\nmethod effectively identifies features contributing to similarity or\ndissimilarity between embedded data points while maintaining high robustness\nand consistency. We also explore how parameter tuning, particularly mask\nquantity and selection strategy, affects explanation quality. This work\naddresses a critical gap in XAI research and enhances transparency and\ntrustworthiness in deep learning applications utilizing embedded spaces."}
{"id": "2505.15141", "pdf": "https://arxiv.org/pdf/2505.15141", "abs": "https://arxiv.org/abs/2505.15141", "authors": ["Yunlong Hou", "Fengzhuo Zhang", "Cunxiao Du", "Xuan Zhang", "Jiachun Pan", "Tianyu Pang", "Chao Du", "Vincent Y. F. Tan", "Zhuoran Yang"], "title": "BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "35 pages, 4 figures", "summary": "Speculative decoding has emerged as a popular method to accelerate the\ninference of Large Language Models (LLMs) while retaining their superior text\ngeneration performance. Previous methods either adopt a fixed speculative\ndecoding configuration regardless of the prefix tokens, or train draft models\nin an offline or online manner to align them with the context. This paper\nproposes a training-free online learning framework to adaptively choose the\nconfiguration of the hyperparameters for speculative decoding as text is being\ngenerated. We first formulate this hyperparameter selection problem as a\nMulti-Armed Bandit problem and provide a general speculative decoding framework\nBanditSpec. Furthermore, two bandit-based hyperparameter selection algorithms,\nUCBSpec and EXP3Spec, are designed and analyzed in terms of a novel quantity,\nthe stopping time regret. We upper bound this regret under both stochastic and\nadversarial reward settings. By deriving an information-theoretic impossibility\nresult, it is shown that the regret performance of UCBSpec is optimal up to\nuniversal constants. Finally, extensive empirical experiments with LLaMA3 and\nQwen2 demonstrate that our algorithms are effective compared to existing\nmethods, and the throughput is close to the oracle best hyperparameter in\nsimulated real-life LLM serving scenarios with diverse input prompts."}
{"id": "2505.15547", "pdf": "https://arxiv.org/pdf/2505.15547", "abs": "https://arxiv.org/abs/2505.15547", "authors": ["Adrian Arnaiz-Rodriguez", "Federico Errica"], "title": "Oversmoothing, \"Oversquashing\", Heterophily, Long-Range, and more: Demystifying Common Beliefs in Graph Machine Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "After a renaissance phase in which researchers revisited the message-passing\nparadigm through the lens of deep learning, the graph machine learning\ncommunity shifted its attention towards a deeper and practical understanding of\nmessage-passing's benefits and limitations. In this position paper, we notice\nhow the fast pace of progress around the topics of oversmoothing and\noversquashing, the homophily-heterophily dichotomy, and long-range tasks, came\nwith the consolidation of commonly accepted beliefs and assumptions that are\nnot always true nor easy to distinguish from each other. We argue that this has\nled to ambiguities around the investigated problems, preventing researchers\nfrom focusing on and addressing precise research questions while causing a good\namount of misunderstandings. Our contribution wants to make such common beliefs\nexplicit and encourage critical thinking around these topics, supported by\nsimple but noteworthy counterexamples. The hope is to clarify the distinction\nbetween the different issues and promote separate but intertwined research\ndirections to address them."}
{"id": "2505.15143", "pdf": "https://arxiv.org/pdf/2505.15143", "abs": "https://arxiv.org/abs/2505.15143", "authors": ["Weiqin Chen", "Xinjie Zhang", "Dharmashankar Subramanian", "Santiago Paternain"], "title": "Filtering Learning Histories Enhances In-Context Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Transformer models (TMs) have exhibited remarkable in-context reinforcement\nlearning (ICRL) capabilities, allowing them to generalize to and improve in\npreviously unseen environments without re-training or fine-tuning. This is\ntypically accomplished by imitating the complete learning histories of a source\nRL algorithm over a substantial amount of pretraining environments, which,\nhowever, may transfer suboptimal behaviors inherited from the source\nalgorithm/dataset. Therefore, in this work, we address the issue of inheriting\nsuboptimality from the perspective of dataset preprocessing. Motivated by the\nsuccess of the weighted empirical risk minimization, we propose a simple yet\neffective approach, learning history filtering (LHF), to enhance ICRL by\nreweighting and filtering the learning histories based on their improvement and\nstability characteristics. To the best of our knowledge, LHF is the first\napproach to avoid source suboptimality by dataset preprocessing, and can be\ncombined with the current state-of-the-art (SOTA) ICRL algorithms. We\nsubstantiate the effectiveness of LHF through a series of experiments conducted\non the well-known ICRL benchmarks, encompassing both discrete environments and\ncontinuous robotic manipulation tasks, with three SOTA ICRL algorithms (AD,\nDPT, DICP) as the backbones. LHF exhibits robust performance across a variety\nof suboptimal scenarios, as well as under varying hyperparameters and sampling\nstrategies. Notably, the superior performance of LHF becomes more pronounced in\nthe presence of noisy data, indicating the significance of filtering learning\nhistories."}
{"id": "2505.15572", "pdf": "https://arxiv.org/pdf/2505.15572", "abs": "https://arxiv.org/abs/2505.15572", "authors": ["Wangyang Ying", "Haoyue Bai", "Nanxu Gong", "Xinyuan Wang", "Sixun Dong", "Haifeng Chen", "Yanjie Fu"], "title": "Bridging the Domain Gap in Equation Distillation with Reinforcement Feedback", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The data-to-equation (Data2Eqn) task aims to discover interpretable\nmathematical equations that map observed values to labels, offering physical\ninsights and broad applicability across academic and industrial domains.\nGenetic programming and traditional deep learning-based approaches suffer from\nsearch inefficiency and poor generalization on small task-specific datasets.\nFoundation models showed promise in this area, but existing approaches suffer\nfrom: 1) They are pretrained on general-purpose data distributions, making them\nless effective for domain-specific tasks; and 2) their training objectives\nfocus on token-level alignment, overlooking mathematical semantics, which can\nlead to inaccurate equations. To address these issues, we aim to enhance the\ndomain adaptability of foundation models for Data2Eqn tasks. In this work, we\npropose a reinforcement learning-based finetuning framework that directly\noptimizes the generation policy of a pretrained model through reward signals\nderived from downstream numerical fitness. Our method allows the model to adapt\nto specific and complex data distributions and generate mathematically\nmeaningful equations. Extensive experiments demonstrate that our approach\nimproves both the accuracy and robustness of equation generation under complex\ndistributions."}
{"id": "2505.15151", "pdf": "https://arxiv.org/pdf/2505.15151", "abs": "https://arxiv.org/abs/2505.15151", "authors": ["Xiaohou Shi", "Ke Li", "Aobo Liang", "Yan Sun"], "title": "Time Tracker: Mixture-of-Experts-Enhanced Foundation Time Series Forecasting Model with Decoupled Training Pipelines", "categories": ["cs.LG"], "comment": null, "summary": "In the past few years, time series foundation models have achieved superior\npredicting accuracy. However, real-world time series often exhibit significant\ndiversity in their temporal patterns across different time spans and domains,\nmaking it challenging for a single model architecture to fit all complex\nscenarios. In addition, time series data may have multiple variables exhibiting\ncomplex correlations between each other. Recent mainstream works have focused\non modeling times series in a channel-independent manner in both pretraining\nand finetuning stages, overlooking the valuable inter-series dependencies. To\nthis end, we propose \\textbf{Time Tracker} for better predictions on\nmultivariate time series data. Firstly, we leverage sparse mixture of experts\n(MoE) within Transformers to handle the modeling of diverse time series\npatterns, thereby alleviating the learning difficulties of a single model while\nimproving its generalization. Besides, we propose Any-variate Attention,\nenabling a unified model structure to seamlessly handle both univariate and\nmultivariate time series, thereby supporting channel-independent modeling\nduring pretraining and channel-mixed modeling for finetuning. Furthermore, we\ndesign a graph learning module that constructs relations among sequences from\nfrequency-domain features, providing more precise guidance to capture\ninter-series dependencies in channel-mixed modeling. Based on these\nadvancements, Time Tracker achieves state-of-the-art performance in predicting\naccuracy, model generalization and adaptability."}
{"id": "2505.15589", "pdf": "https://arxiv.org/pdf/2505.15589", "abs": "https://arxiv.org/abs/2505.15589", "authors": ["Carlos Stein Brito", "Daniel McNamee"], "title": "World Models as Reference Trajectories for Rapid Motor Adaptation", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Deploying learned control policies in real-world environments poses a\nfundamental challenge. When system dynamics change unexpectedly, performance\ndegrades until models are retrained on new data. We introduce Reflexive World\nModels (RWM), a dual control framework that uses world model predictions as\nimplicit reference trajectories for rapid adaptation. Our method separates the\ncontrol problem into long-term reward maximization through reinforcement\nlearning and robust motor execution through rapid latent control. This dual\narchitecture achieves significantly faster adaptation with low online\ncomputational cost compared to model-based RL baselines, while maintaining\nnear-optimal performance. The approach combines the benefits of flexible policy\nlearning through reinforcement learning with rapid error correction\ncapabilities, providing a principled approach to maintaining performance in\nhigh-dimensional continuous control tasks under varying dynamics."}
{"id": "2505.15152", "pdf": "https://arxiv.org/pdf/2505.15152", "abs": "https://arxiv.org/abs/2505.15152", "authors": ["Nanxu Gong", "Zijun Li", "Sixun Dong", "Haoyue Bai", "Wangyang Ying", "Xinyuan Wang", "Yanjie Fu"], "title": "Sculpting Features from Noise: Reward-Guided Hierarchical Diffusion for Task-Optimal Feature Transformation", "categories": ["cs.LG"], "comment": null, "summary": "Feature Transformation (FT) crafts new features from original ones via\nmathematical operations to enhance dataset expressiveness for downstream\nmodels. However, existing FT methods exhibit critical limitations: discrete\nsearch struggles with enormous combinatorial spaces, impeding practical use;\nand continuous search, being highly sensitive to initialization and step sizes,\noften becomes trapped in local optima, restricting global exploration. To\novercome these limitations, DIFFT redefines FT as a reward-guided generative\ntask. It first learns a compact and expressive latent space for feature sets\nusing a Variational Auto-Encoder (VAE). A Latent Diffusion Model (LDM) then\nnavigates this space to generate high-quality feature embeddings, its\ntrajectory guided by a performance evaluator towards task-specific optima. This\nsynthesis of global distribution learning (from LDM) and targeted optimization\n(reward guidance) produces potent embeddings, which a novel semi-autoregressive\ndecoder efficiently converts into structured, discrete features, preserving\nintra-feature dependencies while allowing parallel inter-feature generation.\nExtensive experiments on 14 benchmark datasets show DIFFT consistently\noutperforms state-of-the-art baselines in predictive accuracy and robustness,\nwith significantly lower training and inference times."}
{"id": "2505.15594", "pdf": "https://arxiv.org/pdf/2505.15594", "abs": "https://arxiv.org/abs/2505.15594", "authors": ["Yury Belousov", "Brian Pulfer", "Vitaliy Kinakh", "Slava Voloshynovskiy"], "title": "Beyond Classification: Evaluating Diffusion Denoised Smoothing for Security-Utility Trade off", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Paper accepted at the 33rd European Signal Processing Conference\n  (EUSIPCO 2025)", "summary": "While foundation models demonstrate impressive performance across various\ntasks, they remain vulnerable to adversarial inputs. Current research explores\nvarious approaches to enhance model robustness, with Diffusion Denoised\nSmoothing emerging as a particularly promising technique. This method employs a\npretrained diffusion model to preprocess inputs before model inference. Yet,\nits effectiveness remains largely unexplored beyond classification. We aim to\naddress this gap by analyzing three datasets with four distinct downstream\ntasks under three different adversarial attack algorithms. Our findings reveal\nthat while foundation models maintain resilience against conventional\ntransformations, applying high-noise diffusion denoising to clean images\nwithout any distortions significantly degrades performance by as high as 57%.\nLow-noise diffusion settings preserve performance but fail to provide adequate\nprotection across all attack types. Moreover, we introduce a novel attack\nstrategy specifically targeting the diffusion process itself, capable of\ncircumventing defenses in the low-noise regime. Our results suggest that the\ntrade-off between adversarial robustness and performance remains a challenge to\nbe addressed."}
{"id": "2505.15174", "pdf": "https://arxiv.org/pdf/2505.15174", "abs": "https://arxiv.org/abs/2505.15174", "authors": ["Bo-Han Lai", "Pin-Han Huang", "Bo-Han Kung", "Shang-Tse Chen"], "title": "Enhancing Certified Robustness via Block Reflector Orthogonal Layers and Logit Annealing Loss", "categories": ["cs.LG"], "comment": "ICML 2025 Spotlight", "summary": "Lipschitz neural networks are well-known for providing certified robustness\nin deep learning. In this paper, we present a novel, efficient Block Reflector\nOrthogonal (BRO) layer that enhances the capability of orthogonal layers on\nconstructing more expressive Lipschitz neural architectures. In addition, by\ntheoretically analyzing the nature of Lipschitz neural networks, we introduce a\nnew loss function that employs an annealing mechanism to increase margin for\nmost data points. This enables Lipschitz models to provide better certified\nrobustness. By employing our BRO layer and loss function, we design BRONet - a\nsimple yet effective Lipschitz neural network that achieves state-of-the-art\ncertified robustness. Extensive experiments and empirical analysis on\nCIFAR-10/100, Tiny-ImageNet, and ImageNet validate that our method outperforms\nexisting baselines. The implementation is available at\n\\href{https://github.com/ntuaislab/BRONet}{https://github.com/ntuaislab/BRONet}."}
{"id": "2505.15647", "pdf": "https://arxiv.org/pdf/2505.15647", "abs": "https://arxiv.org/abs/2505.15647", "authors": ["Youming Tao", "Zuyuan Zhang", "Dongxiao Yu", "Xiuzhen Cheng", "Falko Dressler", "Di Wang"], "title": "Second-Order Convergence in Private Stochastic Non-Convex Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We investigate the problem of finding second-order stationary points (SOSP)\nin differentially private (DP) stochastic non-convex optimization. Existing\nmethods suffer from two key limitations: (i) inaccurate convergence error rate\ndue to overlooking gradient variance in the saddle point escape analysis, and\n(ii) dependence on auxiliary private model selection procedures for identifying\nDP-SOSP, which can significantly impair utility, particularly in distributed\nsettings. To address these issues, we propose a generic perturbed stochastic\ngradient descent (PSGD) framework built upon Gaussian noise injection and\ngeneral gradient oracles. A core innovation of our framework is using model\ndrift distance to determine whether PSGD escapes saddle points, ensuring\nconvergence to approximate local minima without relying on second-order\ninformation or additional DP-SOSP identification. By leveraging the adaptive\nDP-SPIDER estimator as a specific gradient oracle, we develop a new DP\nalgorithm that rectifies the convergence error rates reported in prior work. We\nfurther extend this algorithm to distributed learning with arbitrarily\nheterogeneous data, providing the first formal guarantees for finding DP-SOSP\nin such settings. Our analysis also highlights the detrimental impacts of\nprivate selection procedures in distributed learning under high-dimensional\nmodels, underscoring the practical benefits of our design. Numerical\nexperiments on real-world datasets validate the efficacy of our approach."}
{"id": "2505.15177", "pdf": "https://arxiv.org/pdf/2505.15177", "abs": "https://arxiv.org/abs/2505.15177", "authors": ["Jiawei Gu", "Ziyue Qiao", "Zechao Li"], "title": "SpectralGap: Graph-Level Out-of-Distribution Detection via Laplacian Eigenvalue Gaps", "categories": ["cs.LG"], "comment": "Accepted to IJCAI 20205", "summary": "The task of graph-level out-of-distribution (OOD) detection is crucial for\ndeploying graph neural networks in real-world settings. In this paper, we\nobserve a significant difference in the relationship between the largest and\nsecond-largest eigenvalues of the Laplacian matrix for in-distribution (ID) and\nOOD graph samples: \\textit{OOD samples often exhibit anomalous spectral gaps\n(the difference between the largest and second-largest eigenvalues)}. This\nobservation motivates us to propose SpecGap, an effective post-hoc approach for\nOOD detection on graphs. SpecGap adjusts features by subtracting the component\nassociated with the second-largest eigenvalue, scaled by the spectral gap, from\nthe high-level features (i.e., $\\mathbf{X}-\\left(\\lambda_n-\\lambda_{n-1}\\right)\n\\mathbf{u}_{n-1} \\mathbf{v}_{n-1}^T$). SpecGap achieves state-of-the-art\nperformance across multiple benchmark datasets. We present extensive ablation\nstudies and comprehensive theoretical analyses to support our empirical\nresults. As a parameter-free post-hoc method, SpecGap can be easily integrated\ninto existing graph neural network models without requiring any additional\ntraining or model modification."}
{"id": "2505.15657", "pdf": "https://arxiv.org/pdf/2505.15657", "abs": "https://arxiv.org/abs/2505.15657", "authors": ["Cheng Yan", "Felix Mohr", "Tom Viering"], "title": "LCDB 1.1: A Database Illustrating Learning Curves Are More Ill-Behaved Than Previously Thought", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Sample-wise learning curves plot performance versus training set size. They\nare useful for studying scaling laws and speeding up hyperparameter tuning and\nmodel selection. Learning curves are often assumed to be well-behaved: monotone\n(i.e. improving with more data) and convex. By constructing the Learning Curves\nDatabase 1.1 (LCDB 1.1), a large-scale database with high-resolution learning\ncurves, we show that learning curves are less often well-behaved than\npreviously thought. Using statistically rigorous methods, we observe\nsignificant ill-behavior in approximately 14% of the learning curves, almost\ntwice as much as in previous estimates. We also identify which learners are to\nblame and show that specific learners are more ill-behaved than others.\nAdditionally, we demonstrate that different feature scalings rarely resolve\nill-behavior. We evaluate the impact of ill-behavior on downstream tasks, such\nas learning curve fitting and model selection, and find it poses significant\nchallenges, underscoring the relevance and potential of LCDB 1.1 as a\nchallenging benchmark for future research."}
{"id": "2505.15178", "pdf": "https://arxiv.org/pdf/2505.15178", "abs": "https://arxiv.org/abs/2505.15178", "authors": ["Zhehao Huang", "Xinwen Cheng", "Jie Zhang", "Jinghao Zheng", "Haoran Wang", "Zhengbao He", "Tao Li", "Xiaolin Huang"], "title": "A Unified Gradient-based Framework for Task-agnostic Continual Learning-Unlearning", "categories": ["cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2409.19732", "summary": "Recent advancements in deep models have highlighted the need for intelligent\nsystems that combine continual learning (CL) for knowledge acquisition with\nmachine unlearning (MU) for data removal, forming the Continual\nLearning-Unlearning (CLU) paradigm. While existing work treats CL and MU as\nseparate processes, we reveal their intrinsic connection through a unified\noptimization framework based on Kullback-Leibler divergence minimization. This\nframework decomposes gradient updates for approximate CLU into four components:\nlearning new knowledge, unlearning targeted data, preserving existing\nknowledge, and modulation via weight saliency. A critical challenge lies in\nbalancing knowledge update and retention during sequential learning-unlearning\ncycles. To resolve this stability-plasticity dilemma, we introduce a\nremain-preserved manifold constraint to induce a remaining Hessian compensation\nfor CLU iterations. A fast-slow weight adaptation mechanism is designed to\nefficiently approximate the second-order optimization direction, combined with\nadaptive weighting coefficients and a balanced weight saliency mask, proposing\na unified implementation framework for gradient-based CLU. Furthermore, we\npioneer task-agnostic CLU scenarios that support fine-grained unlearning at the\ncross-task category and random sample levels beyond the traditional task-aware\nsetups. Experiments demonstrate that the proposed UG-CLU framework effectively\ncoordinates incremental learning, precise unlearning, and knowledge stability\nacross multiple datasets and model architectures, providing a theoretical\nfoundation and methodological support for dynamic, compliant intelligent\nsystems."}
{"id": "2505.15694", "pdf": "https://arxiv.org/pdf/2505.15694", "abs": "https://arxiv.org/abs/2505.15694", "authors": ["Xingyu Zhou", "Yulian Wu", "Francesco Orabona"], "title": "A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this paper, we theoretically investigate the effects of noisy labels in\noffline alignment, with a focus on the interplay between privacy and robustness\nagainst adversarial corruption. Specifically, under linear modeling\nassumptions, we present a unified analysis covering both reinforcement learning\nfrom human feedback (RLHF) and direct preference optimization (DPO) under\ndifferent privacy-corruption scenarios, such as Local differential\nprivacy-then-Corruption (LTC), where human preference labels are privatized\nbefore being corrupted by an adversary, and Corruption-then-Local differential\nprivacy (CTL), where labels are corrupted before privacy protection. Our\nanalysis leverages a reduction framework that reduces the offline alignment\nproblem under linear modeling assumptions to parameter estimation in logistic\nregression. This framework allows us to establish an interesting separation\nresult between LTC and CTL, demonstrating that LTC presents a greater challenge\nthan CTL in offline alignment, even under linear models. As important\nby-products, our findings also advance the state-of-the-art theoretical results\nin offline alignment under privacy-only or corruption-only scenarios."}
{"id": "2505.15180", "pdf": "https://arxiv.org/pdf/2505.15180", "abs": "https://arxiv.org/abs/2505.15180", "authors": ["Jiawei Gu", "Ziyue Qiao", "Xiao Luo"], "title": "NeuBM: Mitigating Model Bias in Graph Neural Networks through Neutral Input Calibration", "categories": ["cs.LG"], "comment": "Accepted to IJCAI 20205", "summary": "Graph Neural Networks (GNNs) have shown remarkable performance across various\ndomains, yet they often struggle with model bias, particularly in the presence\nof class imbalance. This bias can lead to suboptimal performance and unfair\npredictions, especially for underrepresented classes. We introduce NeuBM\n(Neutral Bias Mitigation), a novel approach to mitigate model bias in GNNs\nthrough neutral input calibration. NeuBM leverages a dynamically updated\nneutral graph to estimate and correct the inherent biases of the model. By\nsubtracting the logits obtained from the neutral graph from those of the input\ngraph, NeuBM effectively recalibrates the model's predictions, reducing bias\nacross different classes. Our method integrates seamlessly into existing GNN\narchitectures and training procedures, requiring minimal computational\noverhead. Extensive experiments on multiple benchmark datasets demonstrate that\nNeuBM significantly improves the balanced accuracy and recall of minority\nclasses, while maintaining strong overall performance. The effectiveness of\nNeuBM is particularly pronounced in scenarios with severe class imbalance and\nlimited labeled data, where traditional methods often struggle. We provide\ntheoretical insights into how NeuBM achieves bias mitigation, relating it to\nthe concept of representation balancing. Our analysis reveals that NeuBM not\nonly adjusts the final predictions but also influences the learning of balanced\nfeature representations throughout the network."}
{"id": "2505.15746", "pdf": "https://arxiv.org/pdf/2505.15746", "abs": "https://arxiv.org/abs/2505.15746", "authors": ["Jingzhe Liu", "Zhigang Hua", "Yan Xie", "Bingheng Li", "Harry Shomer", "Yu Song", "Kaveh Hassani", "Jiliang Tang"], "title": "Higher-order Structure Boosts Link Prediction on Temporal Graphs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Temporal Graph Neural Networks (TGNNs) have gained growing attention for\nmodeling and predicting structures in temporal graphs. However, existing TGNNs\nprimarily focus on pairwise interactions while overlooking higher-order\nstructures that are integral to link formation and evolution in real-world\ntemporal graphs. Meanwhile, these models often suffer from efficiency\nbottlenecks, further limiting their expressive power. To tackle these\nchallenges, we propose a Higher-order structure Temporal Graph Neural Network,\nwhich incorporates hypergraph representations into temporal graph learning. In\nparticular, we develop an algorithm to identify the underlying higher-order\nstructures, enhancing the model's ability to capture the group interactions.\nFurthermore, by aggregating multiple edge features into hyperedge\nrepresentations, HTGN effectively reduces memory cost during training. We\ntheoretically demonstrate the enhanced expressiveness of our approach and\nvalidate its effectiveness and efficiency through extensive experiments on\nvarious real-world temporal graphs. Experimental results show that HTGN\nachieves superior performance on dynamic link prediction while reducing memory\ncosts by up to 50\\% compared to existing methods."}
{"id": "2505.15195", "pdf": "https://arxiv.org/pdf/2505.15195", "abs": "https://arxiv.org/abs/2505.15195", "authors": ["Adel Javanmard", "Rudrajit Das", "Alessandro Epasto", "Vahab Mirrokni"], "title": "Self-Boost via Optimal Retraining: An Analysis via Approximate Message Passing", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": "31 pages, 6 figures, 5 tables", "summary": "Retraining a model using its own predictions together with the original,\npotentially noisy labels is a well-known strategy for improving the model\nperformance. While prior works have demonstrated the benefits of specific\nheuristic retraining schemes, the question of how to optimally combine the\nmodel's predictions and the provided labels remains largely open. This paper\naddresses this fundamental question for binary classification tasks. We develop\na principled framework based on approximate message passing (AMP) to analyze\niterative retraining procedures for two ground truth settings: Gaussian mixture\nmodel (GMM) and generalized linear model (GLM). Our main contribution is the\nderivation of the Bayes optimal aggregator function to combine the current\nmodel's predictions and the given labels, which when used to retrain the same\nmodel, minimizes its prediction error. We also quantify the performance of this\noptimal retraining strategy over multiple rounds. We complement our theoretical\nresults by proposing a practically usable version of the theoretically-optimal\naggregator function for linear probing with the cross-entropy loss, and\ndemonstrate its superiority over baseline methods in the high label noise\nregime."}
{"id": "2505.15747", "pdf": "https://arxiv.org/pdf/2505.15747", "abs": "https://arxiv.org/abs/2505.15747", "authors": ["Kanan Kiguchi", "Yunhao Tu", "Katsuhiro Ajito", "Fady Alnajjar", "Kazuyuki Murase"], "title": "Multi-modal Integration Analysis of Alzheimer's Disease Using Large Language Models and Knowledge Graphs", "categories": ["cs.LG", "cs.AI", "I.2.6; I.2.1; H.3.1; J.3"], "comment": "38 pages, 8 figures, 4 tables", "summary": "We propose a novel framework for integrating fragmented multi-modal data in\nAlzheimer's disease (AD) research using large language models (LLMs) and\nknowledge graphs. While traditional multimodal analysis requires matched\npatient IDs across datasets, our approach demonstrates population-level\nintegration of MRI, gene expression, biomarkers, EEG, and clinical indicators\nfrom independent cohorts. Statistical analysis identified significant features\nin each modality, which were connected as nodes in a knowledge graph. LLMs then\nanalyzed the graph to extract potential correlations and generate hypotheses in\nnatural language. This approach revealed several novel relationships, including\na potential pathway linking metabolic risk factors to tau protein abnormalities\nvia neuroinflammation (r>0.6, p<0.001), and unexpected correlations between\nfrontal EEG channels and specific gene expression profiles (r=0.42-0.58,\np<0.01). Cross-validation with independent datasets confirmed the robustness of\nmajor findings, with consistent effect sizes across cohorts (variance <15%).\nThe reproducibility of these findings was further supported by expert review\n(Cohen's k=0.82) and computational validation. Our framework enables cross\nmodal integration at a conceptual level without requiring patient ID matching,\noffering new possibilities for understanding AD pathology through fragmented\ndata reuse and generating testable hypotheses for future research."}
{"id": "2505.15201", "pdf": "https://arxiv.org/pdf/2505.15201", "abs": "https://arxiv.org/abs/2505.15201", "authors": ["Christian Walder", "Deep Karkhanis"], "title": "Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts\nfor each problem and reward them independently. This optimizes for pass@1\nperformance and prioritizes the strength of isolated samples at the expense of\nthe diversity and collective utility of sets of samples. This under-utilizes\nthe sampling capacity, limiting exploration and eventual improvement on harder\nexamples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a\ntransformation on the final rewards which leads to direct optimization of\npass@k performance, thus optimizing for sets of samples that maximize reward\nwhen considered jointly. Our contribution is to derive novel low variance\nunbiased estimators for pass@k and its gradient, in both the binary and\ncontinuous reward settings. We show optimization with our estimators reduces to\nstandard RL with rewards that have been jointly transformed by a stable and\nefficient transformation function.\n  While previous efforts are restricted to k=n, ours is the first to enable\nrobust optimization of pass@k for any arbitrary k <= n. Moreover, instead of\ntrading off pass@1 performance for pass@k gains, our method allows annealing k\nduring training, optimizing both metrics and often achieving strong pass@1\nnumbers alongside significant pass@k gains.\n  We validate our reward transformations on toy experiments, which reveal the\nvariance reducing properties of our formulations. We also include real-world\nexamples using the open-source LLM, GEMMA-2. We find that our transformation\neffectively optimizes for the target k. Furthermore, higher k values enable\nsolving more and harder problems, while annealing k boosts both the pass@1 and\npass@k . Crucially, for challenging task sets where conventional pass@1\noptimization stalls, our pass@k approach unblocks learning, likely due to\nbetter exploration by prioritizing joint utility over the utility of individual\nsamples."}
{"id": "2505.15754", "pdf": "https://arxiv.org/pdf/2505.15754", "abs": "https://arxiv.org/abs/2505.15754", "authors": ["Palash Chatterjee", "Roni Khardon"], "title": "Improving planning and MBRL with temporally-extended actions", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Continuous time systems are often modeled using discrete time dynamics but\nthis requires a small simulation step to maintain accuracy. In turn, this\nrequires a large planning horizon which leads to computationally demanding\nplanning problems and reduced performance. Previous work in model free\nreinforcement learning has partially addressed this issue using action repeats\nwhere a policy is learned to determine a discrete action duration. Instead we\npropose to control the continuous decision timescale directly by using\ntemporally-extended actions and letting the planner treat the duration of the\naction as an additional optimization variable along with the standard action\nvariables. This additional structure has multiple advantages. It speeds up\nsimulation time of trajectories and, importantly, it allows for deep horizon\nsearch in terms of primitive actions while using a shallow search depth in the\nplanner. In addition, in the model based reinforcement learning (MBRL) setting,\nit reduces compounding errors from model learning and improves training time\nfor models. We show that this idea is effective and that the range for action\ndurations can be automatically selected using a multi-armed bandit formulation\nand integrated into the MBRL framework. An extensive experimental evaluation\nboth in planning and in MBRL, shows that our approach yields faster planning,\nbetter solutions, and that it enables solutions to problems that are not solved\nin the standard formulation."}
{"id": "2505.15212", "pdf": "https://arxiv.org/pdf/2505.15212", "abs": "https://arxiv.org/abs/2505.15212", "authors": ["Haomin Bai", "Dingzhi Yu", "Shuai Li", "Haipeng Luo", "Lijun Zhang"], "title": "Group Distributionally Robust Optimization with Flexible Sample Queries", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Group distributionally robust optimization (GDRO) aims to develop models that\nperform well across $m$ distributions simultaneously. Existing GDRO algorithms\ncan only process a fixed number of samples per iteration, either 1 or $m$, and\ntherefore can not support scenarios where the sample size varies dynamically.\nTo address this limitation, we investigate GDRO with flexible sample queries\nand cast it as a two-player game: one player solves an online convex\noptimization problem, while the other tackles a prediction with limited advice\n(PLA) problem. Within such a game, we propose a novel PLA algorithm,\nconstructing appropriate loss estimators for cases where the sample size is\neither 1 or not, and updating the decision using follow-the-regularized-leader.\nThen, we establish the first high-probability regret bound for non-oblivious\nPLA. Building upon the above approach, we develop a GDRO algorithm that allows\nan arbitrary and varying sample size per round, achieving a high-probability\noptimization error bound of $O\\left(\\frac{1}{t}\\sqrt{\\sum_{j=1}^t\n\\frac{m}{r_j}\\log m}\\right)$, where $r_t$ denotes the sample size at round $t$.\nThis result demonstrates that the optimization error decreases as the number of\nsamples increases and implies a consistent sample complexity of $O(m\\log\n(m)/\\epsilon^2)$ for any fixed sample size $r\\in[m]$, aligning with existing\nbounds for cases of $r=1$ or $m$. We validate our approach on synthetic binary\nand real-world multi-class datasets."}
{"id": "2505.15784", "pdf": "https://arxiv.org/pdf/2505.15784", "abs": "https://arxiv.org/abs/2505.15784", "authors": ["Jun Wan", "Lingrui Mei"], "title": "Large Language Models as Computable Approximations to Solomonoff Induction", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Both authors contributed equally", "summary": "The rapid advancement of large language models (LLMs) calls for a rigorous\ntheoretical framework to explain their empirical success. While significant\nprogress has been made in understanding LLM behaviors, existing theoretical\nframeworks remain fragmented in explaining emergent phenomena through a unified\nmathematical lens. We establish the first formal connection between LLM\narchitectures and Algorithmic Information Theory (AIT) by proving two\nfundamental results: (1) the training process computationally approximates\nSolomonoff prior through loss minimization interpreted as program length\noptimization, and (2) next-token prediction implements approximate Solomonoff\ninduction. We leverage AIT to provide a unified theoretical explanation for\nin-context learning, few-shot learning, and scaling laws. Furthermore, our\ntheoretical insights lead to a principled method for few-shot example selection\nthat prioritizes samples where models exhibit lower predictive confidence. We\ndemonstrate through experiments on diverse text classification benchmarks that\nthis strategy yields significant performance improvements, particularly for\nsmaller model architectures, when compared to selecting high-confidence\nexamples. Our framework bridges the gap between theoretical foundations and\npractical LLM behaviors, providing both explanatory power and actionable\ninsights for future model development."}
{"id": "2505.15213", "pdf": "https://arxiv.org/pdf/2505.15213", "abs": "https://arxiv.org/abs/2505.15213", "authors": ["Sampanna Yashwant Kahu"], "title": "KernelOracle: Predicting the Linux Scheduler's Next Move with Deep Learning", "categories": ["cs.LG", "cs.OS", "D.4.1; I.2.0; I.2.1"], "comment": "7 pages, 11 figures, pre-print. The source code and data used in this\n  work is available at: https://github.com/SampannaKahu/KernelOracle", "summary": "Efficient task scheduling is paramount in the Linux kernel, where the\nCompletely Fair Scheduler (CFS) meticulously manages CPU resources to balance\nhigh utilization with interactive responsiveness. This research pioneers the\nuse of deep learning techniques to predict the sequence of tasks selected by\nCFS, aiming to evaluate the feasibility of a more generalized and potentially\nmore adaptive task scheduler for diverse workloads. Our core contributions are\ntwofold: first, the systematic generation and curation of a novel scheduling\ndataset from a running Linux kernel, capturing real-world CFS behavior; and\nsecond, the development, training, and evaluation of a Long Short-Term Memory\n(LSTM) network designed to accurately forecast the next task to be scheduled.\nThis paper further discusses the practical pathways and implications of\nintegrating such a predictive model into the kernel's scheduling framework. The\nfindings and methodologies presented herein open avenues for data-driven\nadvancements in kernel scheduling, with the full source code provided for\nreproducibility and further exploration."}
{"id": "2505.15808", "pdf": "https://arxiv.org/pdf/2505.15808", "abs": "https://arxiv.org/abs/2505.15808", "authors": ["Carlos Rodriguez-Pardo", "Leonardo Chiani", "Emanuele Borgonovo", "Massimo Tavoni"], "title": "Neural Conditional Transport Maps", "categories": ["cs.LG", "cs.AI", "math.PR", "stat.AP", "stat.ML", "49Q22 (Primary) 68T07 (Secondary)", "I.5.1; I.2.0; G.3"], "comment": "Under Review. Supplementary material included in the pdf", "summary": "We present a neural framework for learning conditional optimal transport (OT)\nmaps between probability distributions. Our approach introduces a conditioning\nmechanism capable of processing both categorical and continuous conditioning\nvariables simultaneously. At the core of our method lies a hypernetwork that\ngenerates transport layer parameters based on these inputs, creating adaptive\nmappings that outperform simpler conditioning methods. Comprehensive ablation\nstudies demonstrate the superior performance of our method over baseline\nconfigurations. Furthermore, we showcase an application to global sensitivity\nanalysis, offering high performance in computing OT-based sensitivity indices.\nThis work advances the state-of-the-art in conditional optimal transport,\nenabling broader application of optimal transport principles to complex,\nhigh-dimensional domains such as generative modeling and black-box model\nexplainability."}
{"id": "2505.15228", "pdf": "https://arxiv.org/pdf/2505.15228", "abs": "https://arxiv.org/abs/2505.15228", "authors": ["Mathew Vanherreweghe", "Lirandë Pira", "Patrick Rebentrost"], "title": "Degree-Optimized Cumulative Polynomial Kolmogorov-Arnold Networks", "categories": ["cs.LG", "cs.CE", "cs.NE"], "comment": null, "summary": "We introduce cumulative polynomial Kolmogorov-Arnold networks (CP-KAN), a\nneural architecture combining Chebyshev polynomial basis functions and\nquadratic unconstrained binary optimization (QUBO). Our primary contribution\ninvolves reformulating the degree selection problem as a QUBO task, reducing\nthe complexity from $O(D^N)$ to a single optimization step per layer. This\napproach enables efficient degree selection across neurons while maintaining\ncomputational tractability. The architecture performs well in regression tasks\nwith limited data, showing good robustness to input scales and natural\nregularization properties from its polynomial basis. Additionally, theoretical\nanalysis establishes connections between CP-KAN's performance and properties of\nfinancial time series. Our empirical validation across multiple domains\ndemonstrates competitive performance compared to several traditional\narchitectures tested, especially in scenarios where data efficiency and\nnumerical stability are important. Our implementation, including strategies for\nmanaging computational overhead in larger networks is available in\nRef.~\\citep{cpkan_implementation}."}
{"id": "2505.15231", "pdf": "https://arxiv.org/pdf/2505.15231", "abs": "https://arxiv.org/abs/2505.15231", "authors": ["Kabir V. Dabholkar", "Omri Barak"], "title": "Finding separatrices of dynamical flows with Deep Koopman Eigenfunctions", "categories": ["cs.LG"], "comment": null, "summary": "Many natural systems, including neural circuits involved in decision making,\ncan be modeled as high-dimensional dynamical systems with multiple stable\nstates. While existing analytical tools primarily describe behavior near stable\nequilibria, characterizing separatrices -- the manifolds that delineate\nboundaries between different basins of attraction -- remains challenging,\nparticularly in high-dimensional settings. Here, we introduce a numerical\nframework leveraging Koopman Theory combined with Deep Neural Networks to\neffectively characterize separatrices. Specifically, we approximate Koopman\nEigenfunctions (KEFs) associated with real positive eigenvalues, which vanish\nprecisely at the separatrices. Utilizing these scalar KEFs, optimization\nmethods efficiently locate separatrices even in complex systems. We demonstrate\nour approach on synthetic benchmarks, ecological network models, and recurrent\nneural networks trained on neuroscience-inspired tasks. Moreover, we illustrate\nthe practical utility of our method by designing optimal perturbations that can\nshift systems across separatrices, enabling predictions relevant to optogenetic\nstimulation experiments in neuroscience."}
{"id": "2505.15239", "pdf": "https://arxiv.org/pdf/2505.15239", "abs": "https://arxiv.org/abs/2505.15239", "authors": ["Peter Súkeník", "Christoph H. Lampert", "Marco Mondelli"], "title": "Neural Collapse is Globally Optimal in Deep Regularized ResNets and Transformers", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "The empirical emergence of neural collapse -- a surprising symmetry in the\nfeature representations of the training data in the penultimate layer of deep\nneural networks -- has spurred a line of theoretical research aimed at its\nunderstanding. However, existing work focuses on data-agnostic models or, when\ndata structure is taken into account, it remains limited to multi-layer\nperceptrons. Our paper fills both these gaps by analyzing modern architectures\nin a data-aware regime: we prove that global optima of deep regularized\ntransformers and residual networks (ResNets) with LayerNorm trained with cross\nentropy or mean squared error loss are approximately collapsed, and the\napproximation gets tighter as the depth grows. More generally, we formally\nreduce any end-to-end large-depth ResNet or transformer training into an\nequivalent unconstrained features model, thus justifying its wide use in the\nliterature even beyond data-agnostic settings. Our theoretical results are\nsupported by experiments on computer vision and language datasets showing that,\nas the depth grows, neural collapse indeed becomes more prominent."}
{"id": "2505.15244", "pdf": "https://arxiv.org/pdf/2505.15244", "abs": "https://arxiv.org/abs/2505.15244", "authors": ["Mohamad Mestoukirdi", "Mourad Khanfouci"], "title": "Reliable Vertical Federated Learning in 5G Core Network Architecture", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "Globecom Submission", "summary": "This work proposes a new algorithm to mitigate model generalization loss in\nVertical Federated Learning (VFL) operating under client reliability\nconstraints within 5G Core Networks (CNs). Recently studied and endorsed by\n3GPP, VFL enables collaborative and load-balanced model training and inference\nacross the CN. However, the performance of VFL significantly degrades when the\nNetwork Data Analytics Functions (NWDAFs) - which serve as primary clients for\nVFL model training and inference - experience reliability issues stemming from\nresource constraints and operational overhead. Unlike edge environments, CN\nenvironments adopt fundamentally different data management strategies,\ncharacterized by more centralized data orchestration capabilities. This\npresents opportunities to implement better distributed solutions that take full\nadvantage of the CN data handling flexibility. Leveraging this flexibility, we\npropose a method that optimizes the vertical feature split among clients while\ncentrally defining their local models based on reliability metrics. Our\nempirical evaluation demonstrates the effectiveness of our proposed algorithm,\nshowing improved performance over traditional baseline methods."}
{"id": "2505.15246", "pdf": "https://arxiv.org/pdf/2505.15246", "abs": "https://arxiv.org/abs/2505.15246", "authors": ["Xiaoling Zhou", "Wei Ye", "Rui Xie", "Shikun Zhang"], "title": "Mitigating Spurious Correlations with Causal Logit Perturbation", "categories": ["cs.LG", "K.3.2", "F.4.1"], "comment": "34 pages,9 figures", "summary": "Deep learning has seen widespread success in various domains such as science,\nindustry, and society. However, it is acknowledged that certain approaches\nsuffer from non-robustness, relying on spurious correlations for predictions.\nAddressing these limitations is of paramount importance, necessitating the\ndevelopment of methods that can disentangle spurious correlations. {This study\nattempts to implement causal models via logit perturbations and introduces a\nnovel Causal Logit Perturbation (CLP) framework to train classifiers with\ngenerated causal logit perturbations for individual samples, thereby mitigating\nthe spurious associations between non-causal attributes (i.e., image\nbackgrounds) and classes.} {Our framework employs a} perturbation network to\ngenerate sample-wise logit perturbations using a series of training\ncharacteristics of samples as inputs. The whole framework is optimized by an\nonline meta-learning-based learning algorithm and leverages human causal\nknowledge by augmenting metadata in both counterfactual and factual manners.\nEmpirical evaluations on four typical biased learning scenarios, including\nlong-tail learning, noisy label learning, generalized long-tail learning, and\nsubpopulation shift learning, demonstrate that CLP consistently achieves\nstate-of-the-art performance. Moreover, visualization results support the\neffectiveness of the generated causal perturbations in redirecting model\nattention towards causal image attributes and dismantling spurious\nassociations."}
{"id": "2505.15250", "pdf": "https://arxiv.org/pdf/2505.15250", "abs": "https://arxiv.org/abs/2505.15250", "authors": ["Suping Xu", "Lin Shang", "Keyu Liu", "Hengrong Ju", "Xibei Yang", "Witold Pedrycz"], "title": "Margin-aware Fuzzy Rough Feature Selection: Bridging Uncertainty Characterization and Pattern Classification", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Fuzzy rough feature selection (FRFS) is an effective means of addressing the\ncurse of dimensionality in high-dimensional data. By removing redundant and\nirrelevant features, FRFS helps mitigate classifier overfitting, enhance\ngeneralization performance, and lessen computational overhead. However, most\nexisting FRFS algorithms primarily focus on reducing uncertainty in pattern\nclassification, neglecting that lower uncertainty does not necessarily result\nin improved classification performance, despite it commonly being regarded as a\nkey indicator of feature selection effectiveness in the FRFS literature. To\nbridge uncertainty characterization and pattern classification, we propose a\nMargin-aware Fuzzy Rough Feature Selection (MAFRFS) framework that considers\nboth the compactness and separation of label classes. MAFRFS effectively\nreduces uncertainty in pattern classification tasks, while guiding the feature\nselection towards more separable and discriminative label class structures.\nExtensive experiments on 15 public datasets demonstrate that MAFRFS is highly\nscalable and more effective than FRFS. The algorithms developed using MAFRFS\noutperform six state-of-the-art feature selection algorithms."}
{"id": "2505.15251", "pdf": "https://arxiv.org/pdf/2505.15251", "abs": "https://arxiv.org/abs/2505.15251", "authors": ["Idriss Malek", "Abhijit Sharma", "Salem Lahlou"], "title": "Loss-Guided Auxiliary Agents for Overcoming Mode Collapse in GFlowNets", "categories": ["cs.LG"], "comment": null, "summary": "Although Generative Flow Networks (GFlowNets) are designed to capture\nmultiple modes of a reward function, they often suffer from mode collapse in\npractice, getting trapped in early discovered modes and requiring prolonged\ntraining to find diverse solutions. Existing exploration techniques may rely on\nheuristic novelty signals. We propose Loss-Guided GFlowNets (LGGFN), a novel\napproach where an auxiliary GFlowNet's exploration is directly driven by the\nmain GFlowNet's training loss. By prioritizing trajectories where the main\nmodel exhibits high loss, LGGFN focuses sampling on poorly understood regions\nof the state space. This targeted exploration significantly accelerates the\ndiscovery of diverse, high-reward samples. Empirically, across various\nbenchmarks including grid environments, structured sequence generation, and\nBayesian structure learning, LGGFN consistently enhances exploration efficiency\nand sample diversity compared to baselines. For instance, on a challenging\nsequence generation task, it discovered over 40 times more unique valid modes\nwhile simultaneously reducing the exploration error metric by approximately\n99\\%."}
{"id": "2505.15259", "pdf": "https://arxiv.org/pdf/2505.15259", "abs": "https://arxiv.org/abs/2505.15259", "authors": ["Hyunseok Lee", "Jeonghoon Kim", "Beomjun Kim", "Jihoon Tack", "Chansong Jo", "Jaehong Lee", "Cheonbok Park", "Sookyo In", "Jinwoo Shin", "Kang Min Yoo"], "title": "ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have enabled\nautonomous agents to interact with computers via Graphical User Interfaces\n(GUIs), where accurately localizing the coordinates of interface elements\n(e.g., buttons) is often required for fine-grained actions. However, this\nremains significantly challenging, leading prior works to rely on large-scale\nweb datasets to improve the grounding accuracy. In this work, we propose\nReasoning Graphical User Interface Grounding for Data Efficiency (ReGUIDE), a\nnovel and effective framework for web grounding that enables MLLMs to learn\ndata efficiently through self-generated reasoning and spatial-aware criticism.\nMore specifically, ReGUIDE learns to (i) self-generate a language reasoning\nprocess for the localization via online reinforcement learning, and (ii)\ncriticize the prediction using spatial priors that enforce equivariance under\ninput transformations. At inference time, ReGUIDE further boosts performance\nthrough a test-time scaling strategy, which combines spatial search with\ncoordinate aggregation. Our experiments demonstrate that ReGUIDE significantly\nadvances web grounding performance across multiple benchmarks, outperforming\nbaselines with substantially fewer training data points (e.g., only 0.2%\nsamples compared to the best open-sourced baselines)."}
{"id": "2505.15270", "pdf": "https://arxiv.org/pdf/2505.15270", "abs": "https://arxiv.org/abs/2505.15270", "authors": ["Chenyu Zheng", "Xinyu Zhang", "Rongzhen Wang", "Wei Huang", "Zhi Tian", "Weilin Huang", "Jun Zhu", "Chongxuan Li"], "title": "Scaling Diffusion Transformers Efficiently via $μ$P", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "35 pages, 10 figures, 15 tables", "summary": "Diffusion Transformers have emerged as the foundation for vision generative\nmodels, but their scalability is limited by the high cost of hyperparameter\n(HP) tuning at large scales. Recently, Maximal Update Parametrization ($\\mu$P)\nwas proposed for vanilla Transformers, which enables stable HP transfer from\nsmall to large language models, and dramatically reduces tuning costs. However,\nit remains unclear whether $\\mu$P of vanilla Transformers extends to diffusion\nTransformers, which differ architecturally and objectively. In this work, we\ngeneralize standard $\\mu$P to diffusion Transformers and validate its\neffectiveness through large-scale experiments. First, we rigorously prove that\n$\\mu$P of mainstream diffusion Transformers, including DiT, U-ViT,\nPixArt-$\\alpha$, and MMDiT, aligns with that of the vanilla Transformer,\nenabling the direct application of existing $\\mu$P methodologies. Leveraging\nthis result, we systematically demonstrate that DiT-$\\mu$P enjoys robust HP\ntransferability. Notably, DiT-XL-2-$\\mu$P with transferred learning rate\nachieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we\nvalidate the effectiveness of $\\mu$P on text-to-image generation by scaling\nPixArt-$\\alpha$ from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases,\nmodels under $\\mu$P outperform their respective baselines while requiring small\ntuning cost, only 5.5% of one training run for PixArt-$\\alpha$ and 3% of\nconsumption by human experts for MMDiT-18B. These results establish $\\mu$P as a\nprincipled and efficient framework for scaling diffusion Transformers."}
{"id": "2505.15284", "pdf": "https://arxiv.org/pdf/2505.15284", "abs": "https://arxiv.org/abs/2505.15284", "authors": ["Kun Fang", "Qinghua Tao", "Mingzhen He", "Kexin Lv", "Runze Yang", "Haibo Hu", "Xiaolin Huang", "Jie Yang", "Longbin Cao"], "title": "Kernel PCA for Out-of-Distribution Detection: Non-Linear Kernel Selections and Approximations", "categories": ["cs.LG", "cs.CV"], "comment": "This study is an extension of its conference version published in\n  NeurIPS'24, see\n  https://proceedings.neurips.cc/paper_files/paper/2024/hash/f2543511e5f4d4764857f9ad833a977d-Abstract-Conference.html", "summary": "Out-of-Distribution (OoD) detection is vital for the reliability of deep\nneural networks, the key of which lies in effectively characterizing the\ndisparities between OoD and In-Distribution (InD) data. In this work, such\ndisparities are exploited through a fresh perspective of non-linear feature\nsubspace. That is, a discriminative non-linear subspace is learned from InD\nfeatures to capture representative patterns of InD, while informative patterns\nof OoD features cannot be well captured in such a subspace due to their\ndifferent distribution. Grounded on this perspective, we exploit the deviations\nof InD and OoD features in such a non-linear subspace for effective OoD\ndetection. To be specific, we leverage the framework of Kernel Principal\nComponent Analysis (KPCA) to attain the discriminative non-linear subspace and\ndeploy the reconstruction error on such subspace to distinguish InD and OoD\ndata. Two challenges emerge: (i) the learning of an effective non-linear\nsubspace, i.e., the selection of kernel function in KPCA, and (ii) the\ncomputation of the kernel matrix with large-scale InD data. For the former, we\nreveal two vital non-linear patterns that closely relate to the InD-OoD\ndisparity, leading to the establishment of a Cosine-Gaussian kernel for\nconstructing the subspace. For the latter, we introduce two techniques to\napproximate the Cosine-Gaussian kernel with significantly cheap computations.\nIn particular, our approximation is further tailored by incorporating the InD\ndata confidence, which is demonstrated to promote the learning of\ndiscriminative subspaces for OoD data. Our study presents new insights into the\nnon-linear feature subspace for OoD detection and contributes practical\nexplorations on the associated kernel design and efficient computations,\nyielding a KPCA detection method with distinctively improved efficacy and\nefficiency."}
{"id": "2505.15293", "pdf": "https://arxiv.org/pdf/2505.15293", "abs": "https://arxiv.org/abs/2505.15293", "authors": ["Qianyue Hao", "Yiwen Song", "Qingmin Liao", "Jian Yuan", "Yong Li"], "title": "LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Policy exploration is critical in reinforcement learning (RL), where existing\napproaches include greedy, Gaussian process, etc. However, these approaches\nutilize preset stochastic processes and are indiscriminately applied in all\nkinds of RL tasks without considering task-specific features that influence\npolicy exploration. Moreover, during RL training, the evolution of such\nstochastic processes is rigid, which typically only incorporates a decay in the\nvariance, failing to adjust flexibly according to the agent's real-time\nlearning status. Inspired by the analyzing and reasoning capability of large\nlanguage models (LLMs), we design LLM-Explorer to adaptively generate\ntask-specific exploration strategies with LLMs, enhancing the policy\nexploration in RL. In our design, we sample the learning trajectory of the\nagent during the RL training in a given task and prompt the LLM to analyze the\nagent's current policy learning status and then generate a probability\ndistribution for future policy exploration. Updating the probability\ndistribution periodically, we derive a stochastic process specialized for the\nparticular task and dynamically adjusted to adapt to the learning process. Our\ndesign is a plug-in module compatible with various widely applied RL\nalgorithms, including the DQN series, DDPG, TD3, and any possible variants\ndeveloped based on them. Through extensive experiments on the Atari and MuJoCo\nbenchmarks, we demonstrate LLM-Explorer's capability to enhance RL policy\nexploration, achieving an average performance improvement up to 37.27%. Our\ncode is open-source at https://anonymous.4open.science/r/LLM-Explorer-19BE for\nreproducibility."}
{"id": "2505.15303", "pdf": "https://arxiv.org/pdf/2505.15303", "abs": "https://arxiv.org/abs/2505.15303", "authors": ["Johannes Kaiser", "Kristian Schwethelm", "Daniel Rueckert", "Georgios Kaissis"], "title": "Laplace Sample Information: Data Informativeness Through a Bayesian Lens", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Accurately estimating the informativeness of individual samples in a dataset\nis an important objective in deep learning, as it can guide sample selection,\nwhich can improve model efficiency and accuracy by removing redundant or\npotentially harmful samples. We propose Laplace Sample Information (LSI)\nmeasure of sample informativeness grounded in information theory widely\napplicable across model architectures and learning settings. LSI leverages a\nBayesian approximation to the weight posterior and the KL divergence to measure\nthe change in the parameter distribution induced by a sample of interest from\nthe dataset. We experimentally show that LSI is effective in ordering the data\nwith respect to typicality, detecting mislabeled samples, measuring class-wise\ninformativeness, and assessing dataset difficulty. We demonstrate these\ncapabilities of LSI on image and text data in supervised and unsupervised\nsettings. Moreover, we show that LSI can be computed efficiently through probes\nand transfers well to the training of large models."}
{"id": "2505.15306", "pdf": "https://arxiv.org/pdf/2505.15306", "abs": "https://arxiv.org/abs/2505.15306", "authors": ["Yiwen Song", "Qianyue Hao", "Qingmin Liao", "Jian Yuan", "Yong Li"], "title": "Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak Reinforcement Learning Agents into a Supreme One", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Model ensemble is a useful approach in reinforcement learning (RL) for\ntraining effective agents. Despite wide success of RL, training effective\nagents remains difficult due to the multitude of factors requiring careful\ntuning, such as algorithm selection, hyperparameter settings, and even random\nseed choices, all of which can significantly influence an agent's performance.\nModel ensemble helps overcome this challenge by combining multiple weak agents\ninto a single, more powerful one, enhancing overall performance. However,\nexisting ensemble methods, such as majority voting and Boltzmann addition, are\ndesigned as fixed strategies and lack a semantic understanding of specific\ntasks, limiting their adaptability and effectiveness. To address this, we\npropose LLM-Ens, a novel approach that enhances RL model ensemble with\ntask-specific semantic understandings driven by large language models (LLMs).\nGiven a task, we first design an LLM to categorize states in this task into\ndistinct 'situations', incorporating high-level descriptions of the task\nconditions. Then, we statistically analyze the strengths and weaknesses of each\nindividual agent to be used in the ensemble in each situation. During the\ninference time, LLM-Ens dynamically identifies the changing task situation and\nswitches to the agent that performs best in the current situation, ensuring\ndynamic model selection in the evolving task condition. Our approach is\ndesigned to be compatible with agents trained with different random seeds,\nhyperparameter settings, and various RL algorithms. Extensive experiments on\nthe Atari benchmark show that LLM-Ens significantly improves the RL model\nensemble, surpassing well-known baselines by up to 20.9%. For reproducibility,\nour code is open-source at\nhttps://anonymous.4open.science/r/LLM4RLensemble-F7EE."}
{"id": "2505.15311", "pdf": "https://arxiv.org/pdf/2505.15311", "abs": "https://arxiv.org/abs/2505.15311", "authors": ["Yurun Yuan", "Fan Chen", "Zeyu Jia", "Alexander Rakhlin", "Tengyang Xie"], "title": "Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Policy-based methods currently dominate reinforcement learning (RL) pipelines\nfor large language model (LLM) reasoning, leaving value-based approaches\nlargely unexplored. We revisit the classical paradigm of Bellman Residual\nMinimization and introduce Trajectory Bellman Residual Minimization (TBRM), an\nalgorithm that naturally adapts this idea to LLMs, yielding a simple yet\neffective off-policy algorithm that optimizes a single trajectory-level Bellman\nobjective using the model's own logits as $Q$-values. TBRM removes the need for\ncritics, importance-sampling ratios, or clipping, and operates with only one\nrollout per prompt. We prove convergence to the near-optimal KL-regularized\npolicy from arbitrary off-policy data via an improved\nchange-of-trajectory-measure analysis. Experiments on standard\nmathematical-reasoning benchmarks show that TBRM consistently outperforms\npolicy-based baselines, like PPO and GRPO, with comparable or lower\ncomputational and memory overhead. Our results indicate that value-based RL\nmight be a principled and efficient alternative for enhancing reasoning\ncapabilities in LLMs."}
{"id": "2505.15312", "pdf": "https://arxiv.org/pdf/2505.15312", "abs": "https://arxiv.org/abs/2505.15312", "authors": ["Yuxuan Shu", "Vasileios Lampos"], "title": "Sonnet: Spectral Operator Neural Network for Multivariable Time Series Forecasting", "categories": ["cs.LG"], "comment": "The code is available at https://github.com/ClaudiaShu/Sonnet", "summary": "Multivariable time series forecasting methods can integrate information from\nexogenous variables, leading to significant prediction accuracy gains.\nTransformer architecture has been widely applied in various time series\nforecasting models due to its ability to capture long-range sequential\ndependencies. However, a na\\\"ive application of transformers often struggles to\neffectively model complex relationships among variables over time. To mitigate\nagainst this, we propose a novel architecture, namely the Spectral Operator\nNeural Network (Sonnet). Sonnet applies learnable wavelet transformations to\nthe input and incorporates spectral analysis using the Koopman operator. Its\npredictive skill relies on the Multivariable Coherence Attention (MVCA), an\noperation that leverages spectral coherence to model variable dependencies. Our\nempirical analysis shows that Sonnet yields the best performance on $34$ out of\n$47$ forecasting tasks with an average mean absolute error (MAE) reduction of\n$1.1\\%$ against the most competitive baseline (different per task). We further\nshow that MVCA -- when put in place of the na\\\"ive attention used in various\ndeep learning models -- can remedy its deficiencies, reducing MAE by $10.7\\%$\non average in the most challenging forecasting tasks."}
{"id": "2505.15329", "pdf": "https://arxiv.org/pdf/2505.15329", "abs": "https://arxiv.org/abs/2505.15329", "authors": ["Anqiao Ouyang", "Hongyi Ke", "Qi Wang"], "title": "Fourier-Invertible Neural Encoder (FINE) for Homogeneous Flows", "categories": ["cs.LG"], "comment": null, "summary": "Invertible neural architectures have recently attracted attention for their\ncompactness, interpretability, and information-preserving properties. In this\nwork, we propose the Fourier-Invertible Neural Encoder (FINE), which combines\ninvertible monotonic activation functions with reversible filter structures,\nand could be extended using Invertible ResNets. This architecture is examined\nin learning low-dimensional representations of one-dimensional nonlinear wave\ninteractions and exact circular translation symmetry. Dimensionality is\npreserved across layers, except for a Fourier truncation step in the latent\nspace, which enables dimensionality reduction while maintaining shift\nequivariance and interpretability. Our results demonstrate that FINE\nsignificantly outperforms classical linear methods such as Discrete Fourier\nTransformation (DFT) and Proper Orthogonal Decomposition (POD), and achieves\nreconstruction accuracy better than conventional deep autoencoders with\nconvolutional layers (CNN) - while using substantially smaller models and\noffering superior physical interpretability. These findings suggest that\ninvertible single-neuron networks, when combined with spectral truncation,\noffer a promising framework for learning compact and interpretable\nrepresentations of physics datasets, and symmetry-aware representation learning\nin physics-informed machine learning."}
{"id": "2505.15340", "pdf": "https://arxiv.org/pdf/2505.15340", "abs": "https://arxiv.org/abs/2505.15340", "authors": ["Yuanlin Chu", "Bo Wang", "Xiang Liu", "Hong Chen", "Aiwei Liu", "Xuming Hu"], "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) have achieved impressive results on multi-step\nmathematical reasoning, yet at the cost of high computational overhead. This\nchallenge is particularly acute for test-time scaling methods such as parallel\ndecoding, which increase answer diversity but scale poorly in efficiency. To\naddress this efficiency-accuracy trade-off, we propose SSR (Speculative\nParallel Scaling Reasoning), a training-free framework that leverages a key\ninsight: by introducing speculative decoding at the step level, we can\naccelerate reasoning without sacrificing correctness. SSR integrates two\ncomponents: a Selective Parallel Module (SPM) that identifies a small set of\npromising reasoning strategies via model-internal scoring, and Step-level\nSpeculative Decoding (SSD), which enables efficient draft-target collaboration\nfor fine-grained reasoning acceleration. Experiments on three mathematical\nbenchmarks-AIME 2024, MATH-500, and LiveMathBench - demonstrate that SSR\nachieves strong gains over baselines. For instance, on LiveMathBench, SSR\nimproves pass@1 accuracy by 13.84% while reducing computation to 80.5% of the\nbaseline FLOPs. On MATH-500, SSR reduces compute to only 30% with no loss in\naccuracy."}
{"id": "2505.15345", "pdf": "https://arxiv.org/pdf/2505.15345", "abs": "https://arxiv.org/abs/2505.15345", "authors": ["Jacob E. Kooi", "Zhao Yang", "Vincent François-Lavet"], "title": "Hadamax Encoding: Elevating Performance in Model-Free Atari", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Neural network architectures have a large impact in machine learning. In\nreinforcement learning, network architectures have remained notably simple, as\nchanges often lead to small gains in performance. This work introduces a novel\nencoder architecture for pixel-based model-free reinforcement learning. The\nHadamax (\\textbf{Hada}mard \\textbf{max}-pooling) encoder achieves\nstate-of-the-art performance by max-pooling Hadamard products between\nGELU-activated parallel hidden layers. Based on the recent PQN algorithm, the\nHadamax encoder achieves state-of-the-art model-free performance in the\nAtari-57 benchmark. Specifically, without applying any algorithmic\nhyperparameter modifications, Hadamax-PQN achieves an 80\\% performance gain\nover vanilla PQN and significantly surpasses Rainbow-DQN. For reproducibility,\nthe full code is available on\n\\href{https://github.com/Jacobkooi/Hadamax}{GitHub}."}
{"id": "2505.15354", "pdf": "https://arxiv.org/pdf/2505.15354", "abs": "https://arxiv.org/abs/2505.15354", "authors": ["Malik Tiomoko", "Hamza Cherkaoui", "Giuseppe Paolo", "Zhang Yili", "Yu Meng", "Zhang Keli", "Hafiz Tiomoko Ali"], "title": "Human in the Loop Adaptive Optimization for Improved Time Series Forecasting", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Time series forecasting models often produce systematic, predictable errors\neven in critical domains such as energy, finance, and healthcare. We introduce\na novel post training adaptive optimization framework that improves forecast\naccuracy without retraining or architectural changes. Our method automatically\napplies expressive transformations optimized via reinforcement learning,\ncontextual bandits, or genetic algorithms to correct model outputs in a\nlightweight and model agnostic way. Theoretically, we prove that affine\ncorrections always reduce the mean squared error; practically, we extend this\nidea with dynamic action based optimization. The framework also supports an\noptional human in the loop component: domain experts can guide corrections\nusing natural language, which is parsed into actions by a language model.\nAcross multiple benchmarks (e.g., electricity, weather, traffic), we observe\nconsistent accuracy gains with minimal computational overhead. Our interactive\ndemo shows the framework's real time usability. By combining automated post hoc\nrefinement with interpretable and extensible mechanisms, our approach offers a\npowerful new direction for practical forecasting systems."}
{"id": "2505.15371", "pdf": "https://arxiv.org/pdf/2505.15371", "abs": "https://arxiv.org/abs/2505.15371", "authors": ["Mounssif Krouka", "Chaouki Ben Issaid", "Mehdi Bennis"], "title": "Distributionally Robust Federated Learning with Client Drift Minimization", "categories": ["cs.LG"], "comment": null, "summary": "Federated learning (FL) faces critical challenges, particularly in\nheterogeneous environments where non-independent and identically distributed\ndata across clients can lead to unfair and inefficient model performance. In\nthis work, we introduce \\textit{DRDM}, a novel algorithm that addresses these\nissues by combining a distributionally robust optimization (DRO) framework with\ndynamic regularization to mitigate client drift. \\textit{DRDM} frames the\ntraining as a min-max optimization problem aimed at maximizing performance for\nthe worst-case client, thereby promoting robustness and fairness. This robust\nobjective is optimized through an algorithm leveraging dynamic regularization\nand efficient local updates, which significantly reduces the required number of\ncommunication rounds. Moreover, we provide a theoretical convergence analysis\nfor convex smooth objectives under partial participation. Extensive experiments\non three benchmark datasets, covering various model architectures and data\nheterogeneity levels, demonstrate that \\textit{DRDM} significantly improves\nworst-case test accuracy while requiring fewer communication rounds than\nexisting state-of-the-art baselines. Furthermore, we analyze the impact of\nsignal-to-noise ratio (SNR) and bandwidth on the energy consumption of\nparticipating clients, demonstrating that the number of local update steps can\nbe adaptively selected to achieve a target worst-case test accuracy with\nminimal total energy cost across diverse communication environments."}
{"id": "2505.15391", "pdf": "https://arxiv.org/pdf/2505.15391", "abs": "https://arxiv.org/abs/2505.15391", "authors": ["Duncan Bart", "Bruno Endres Forlin", "Ana-Lucia Varbanescu", "Marco Ottavi", "Kuan-Hsun Chen"], "title": "InTreeger: An End-to-End Framework for Integer-Only Decision Tree Inference", "categories": ["cs.LG"], "comment": null, "summary": "Integer quantization has emerged as a critical technique to facilitate\ndeployment on resource-constrained devices. Although they do reduce the\ncomplexity of the learning models, their inference performance is often prone\nto quantization-induced errors. To this end, we introduce InTreeger: an\nend-to-end framework that takes a training dataset as input, and outputs an\narchitecture-agnostic integer-only C implementation of tree-based machine\nlearning model, without loss of precision. This framework enables anyone, even\nthose without prior experience in machine learning, to generate a highly\noptimized integer-only classification model that can run on any hardware simply\nby providing an input dataset and target variable. We evaluated our generated\nimplementations across three different architectures (ARM, x86, and RISC-V),\nresulting in significant improvements in inference latency. In addition, we\nshow the energy efficiency compared to typical decision tree implementations\nthat rely on floating-point arithmetic. The results underscore the advantages\nof integer-only inference, making it particularly suitable for energy- and\narea-constrained devices such as embedded systems and edge computing platforms,\nwhile also enabling the execution of decision trees on existing ultra-low power\ndevices."}
{"id": "2505.15405", "pdf": "https://arxiv.org/pdf/2505.15405", "abs": "https://arxiv.org/abs/2505.15405", "authors": ["Martin Carrasco", "Guillermo Bernardez", "Marco Montagna", "Nina Miolane", "Lev Telyatnikov"], "title": "HOPSE: Scalable Higher-Order Positional and Structural Encoder for Combinatorial Representations", "categories": ["cs.LG"], "comment": null, "summary": "While Graph Neural Networks (GNNs) have proven highly effective at modeling\nrelational data, pairwise connections cannot fully capture multi-way\nrelationships naturally present in complex real-world systems. In response to\nthis, Topological Deep Learning (TDL) leverages more general combinatorial\nrepresentations --such as simplicial or cellular complexes-- to accommodate\nhigher-order interactions. Existing TDL methods often extend GNNs through\nHigher-Order Message Passing (HOMP), but face critical \\emph{scalability\nchallenges} due to \\textit{(i)} a combinatorial explosion of message-passing\nroutes, and \\textit{(ii)} significant complexity overhead from the propagation\nmechanism. To overcome these limitations, we propose HOPSE (Higher-Order\nPositional and Structural Encoder)--a \\emph{message passing-free} framework\nthat uses Hasse graph decompositions to derive efficient and expressive\nencodings over \\emph{arbitrary higher-order domains}. Notably, HOPSE scales\nlinearly with dataset size while preserving expressive power and permutation\nequivariance. Experiments on molecular, expressivity and topological benchmarks\nshow that HOPSE matches or surpasses state-of-the-art performance while\nachieving up to 7 $times$ speedups over HOMP-based models, opening a new path\nfor scalable TDL."}
{"id": "2505.15407", "pdf": "https://arxiv.org/pdf/2505.15407", "abs": "https://arxiv.org/abs/2505.15407", "authors": ["Naiqi Li", "Yuqiu Xie", "Peiyuan Liu", "Tao Dai", "Yong Jiang", "Shu-Tao Xia"], "title": "Efficient Differentiable Approximation of Generalized Low-rank Regularization", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": "Accepted by IJCAI-25", "summary": "Low-rank regularization (LRR) has been widely applied in various machine\nlearning tasks, but the associated optimization is challenging. Directly\noptimizing the rank function under constraints is NP-hard in general. To\novercome this difficulty, various relaxations of the rank function were\nstudied. However, optimization of these relaxed LRRs typically depends on\nsingular value decomposition, which is a time-consuming and nondifferentiable\noperator that cannot be optimized with gradient-based techniques. To address\nthese challenges, in this paper we propose an efficient differentiable\napproximation of the generalized LRR. The considered LRR form subsumes many\npopular choices like the nuclear norm, the Schatten-$p$ norm, and various\nnonconvex relaxations. Our method enables LRR terms to be appended to loss\nfunctions in a plug-and-play fashion, and the GPU-friendly operations enable\nefficient and convenient implementation. Furthermore, convergence analysis is\npresented, which rigorously shows that both the bias and the variance of our\nrank estimator rapidly reduce with increased sample size and iteration steps.\nIn the experimental study, the proposed method is applied to various tasks,\nwhich demonstrates its versatility and efficiency. Code is available at\nhttps://github.com/naiqili/EDLRR."}
{"id": "2505.15418", "pdf": "https://arxiv.org/pdf/2505.15418", "abs": "https://arxiv.org/abs/2505.15418", "authors": ["Yueheng Li", "Guangming Xie", "Zongqing Lu"], "title": "Guided Policy Optimization under Partial Observability", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "24 pages, 13 figures", "summary": "Reinforcement Learning (RL) in partially observable environments poses\nsignificant challenges due to the complexity of learning under uncertainty.\nWhile additional information, such as that available in simulations, can\nenhance training, effectively leveraging it remains an open problem. To address\nthis, we introduce Guided Policy Optimization (GPO), a framework that co-trains\na guider and a learner. The guider takes advantage of privileged information\nwhile ensuring alignment with the learner's policy that is primarily trained\nvia imitation learning. We theoretically demonstrate that this learning scheme\nachieves optimality comparable to direct RL, thereby overcoming key limitations\ninherent in existing approaches. Empirical evaluations show strong performance\nof GPO across various tasks, including continuous control with partial\nobservability and noise, and memory-based challenges, significantly\noutperforming existing methods."}
{"id": "2505.15423", "pdf": "https://arxiv.org/pdf/2505.15423", "abs": "https://arxiv.org/abs/2505.15423", "authors": ["Marcell T. Kurbucz", "Nikolaos Tzivanakis", "Nilufer Sari Aslam", "Adam M. Sykulski"], "title": "SplitWise Regression: Stepwise Modeling with Adaptive Dummy Encoding", "categories": ["cs.LG", "econ.EM", "stat.AP", "stat.ME", "stat.ML", "62H20, 62J05, 68T05", "G.3; I.2.6; I.5.1; I.5.2"], "comment": "15 pages, 1 figure, 3 tables", "summary": "Capturing nonlinear relationships without sacrificing interpretability\nremains a persistent challenge in regression modeling. We introduce SplitWise,\na novel framework that enhances stepwise regression. It adaptively transforms\nnumeric predictors into threshold-based binary features using shallow decision\ntrees, but only when such transformations improve model fit, as assessed by the\nAkaike Information Criterion (AIC) or Bayesian Information Criterion (BIC).\nThis approach preserves the transparency of linear models while flexibly\ncapturing nonlinear effects. Implemented as a user-friendly R package,\nSplitWise is evaluated on both synthetic and real-world datasets. The results\nshow that it consistently produces more parsimonious and generalizable models\nthan traditional stepwise and penalized regression techniques."}
{"id": "2505.15433", "pdf": "https://arxiv.org/pdf/2505.15433", "abs": "https://arxiv.org/abs/2505.15433", "authors": ["Beni Egressy", "Jan Stühmer"], "title": "Set-LLM: A Permutation-Invariant LLM", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "While large language models (LLMs) demonstrate impressive capabilities across\nnumerous applications, their robustness remains a critical concern. This paper\nis motivated by a specific vulnerability: the order sensitivity of LLMs. This\nvulnerability manifests itself as the order bias observed when LLMs decide\nbetween possible options (for example, a preference for the first option) and\nthe tendency of LLMs to provide different answers when options are reordered.\nThe use cases for this scenario extend beyond the classical case of\nmultiple-choice question answering to the use of LLMs as automated evaluators\nin AI pipelines, comparing output generated by different models. We introduce\nSet-LLM, a novel architectural adaptation for pretrained LLMs that enables the\nprocessing of mixed set-text inputs with permutation invariance guarantees. The\nadaptations involve a new attention mask and new positional encodings\nspecifically designed for sets. We provide a theoretical proof of invariance\nand demonstrate through experiments that Set-LLM can be trained effectively,\nachieving comparable or improved performance and maintaining the runtime of the\noriginal model, while eliminating order sensitivity."}
{"id": "2505.15496", "pdf": "https://arxiv.org/pdf/2505.15496", "abs": "https://arxiv.org/abs/2505.15496", "authors": ["Hossein Zakerinia", "Christoph H. Lampert"], "title": "Fast Rate Bounds for Multi-Task and Meta-Learning with Different Sample Sizes", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We present new fast-rate generalization bounds for multi-task and\nmeta-learning in the unbalanced setting, i.e. when the tasks have training sets\nof different sizes, as is typically the case in real-world scenarios.\nPreviously, only standard-rate bounds were known for this situation, while\nfast-rate bounds were limited to the setting where all training sets are of\nequal size. Our new bounds are numerically computable as well as interpretable,\nand we demonstrate their flexibility in handling a number of cases where they\ngive stronger guarantees than previous bounds. Besides the bounds themselves,\nwe also make conceptual contributions: we demonstrate that the unbalanced\nmulti-task setting has different statistical properties than the balanced\nsituation, specifically that proofs from the balanced situation do not carry\nover to the unbalanced setting. Additionally, we shed light on the fact that\nthe unbalanced situation allows two meaningful definitions of multi-task risk,\ndepending on whether if all tasks should be considered equally important or if\nsample-rich tasks should receive more weight than sample-poor ones."}
{"id": "2505.15497", "pdf": "https://arxiv.org/pdf/2505.15497", "abs": "https://arxiv.org/abs/2505.15497", "authors": ["Frederik Baymler Mathiesen", "Nikolaus Vertovec", "Francesco Fabiano", "Luca Laurenti", "Alessandro Abate"], "title": "Certified Neural Approximations of Nonlinear Dynamics", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "first and second author contributed equally", "summary": "Neural networks hold great potential to act as approximate models of\nnonlinear dynamical systems, with the resulting neural approximations enabling\nverification and control of such systems. However, in safety-critical contexts,\nthe use of neural approximations requires formal bounds on their closeness to\nthe underlying system. To address this fundamental challenge, we propose a\nnovel, adaptive, and parallelizable verification method based on certified\nfirst-order models. Our approach provides formal error bounds on the neural\napproximations of dynamical systems, allowing them to be safely employed as\nsurrogates by interpreting the error bound as bounded disturbances acting on\nthe approximated dynamics. We demonstrate the effectiveness and scalability of\nour method on a range of established benchmarks from the literature, showing\nthat it outperforms the state-of-the-art. Furthermore, we highlight the\nflexibility of our framework by applying it to two novel scenarios not\npreviously explored in this context: neural network compression and an\nautoencoder-based deep learning architecture for learning Koopman operators,\nboth yielding compelling results."}
{"id": "2505.15507", "pdf": "https://arxiv.org/pdf/2505.15507", "abs": "https://arxiv.org/abs/2505.15507", "authors": ["Mahesh Godavarti"], "title": "Directional Non-Commutative Monoidal Structures for Compositional Embeddings in Machine Learning", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.IR", "20-XX, 08A02", "F.4.1; I.2"], "comment": "11 pages submitted to NeurIPS 2025", "summary": "We introduce a new algebraic structure for multi-dimensional compositional\nembeddings, built on directional non-commutative monoidal operators. The core\ncontribution of this work is this novel framework, which exhibits appealing\ntheoretical properties (associativity along each dimension and an interchange\nlaw ensuring global consistency) while remaining compatible with modern machine\nlearning architectures. Our construction defines a distinct composition\noperator circ_i for each axis i, ensuring associative combination along each\naxis without imposing global commutativity. Importantly, all axis-specific\noperators commute with one another, enforcing a global interchange law that\nenables consistent crossaxis compositions. This is, to our knowledge, the first\napproach that provides a common foundation that generalizes classical\nsequence-modeling paradigms (e.g., structured state-space models (SSMs) and\ntransformer self-attention) to a unified multi-dimensional framework. For\nexample, specific one-dimensional instances of our framework can recover the\nfamiliar affine transformation algebra, vanilla self-attention, and the\nSSM-style recurrence. The higher-dimensional generalizations naturally support\nrecursive, structure-aware operations in embedding spaces. We outline several\npotential applications unlocked by this structure-including structured\npositional encodings in Transformers, directional image embeddings, and\nsymbolic modeling of sequences or grids-indicating that it could inform future\ndeep learning model designs. We formally establish the algebraic properties of\nour framework and discuss efficient implementations. Finally, as our focus is\ntheoretical, we include no experiments here and defer empirical validation to\nfuture work, which we plan to undertake."}
{"id": "2505.15511", "pdf": "https://arxiv.org/pdf/2505.15511", "abs": "https://arxiv.org/abs/2505.15511", "authors": ["Brandon Duderstadt", "Zach Nussbaum", "Laurens van der Maaten"], "title": "NOMAD Projection", "categories": ["cs.LG"], "comment": null, "summary": "The rapid adoption of generative AI has driven an explosion in the size of\ndatasets consumed and produced by AI models. Traditional methods for\nunstructured data visualization, such as t-SNE and UMAP, have not kept up with\nthe pace of dataset scaling. This presents a significant challenge for AI\nexplainability, which relies on methods such as t-SNE and UMAP for exploratory\ndata analysis. In this paper, we introduce Negative Or Mean Affinity\nDiscrimination (NOMAD) Projection, the first method for unstructured data\nvisualization via nonlinear dimensionality reduction that can run on multiple\nGPUs at train time. We provide theory that situates NOMAD Projection as an\napproximate upper bound on the InfoNC-t-SNE loss, and empirical results that\ndemonstrate NOMAD Projection's superior performance and speed profile compared\nto existing state-of-the-art methods. We demonstrate the scalability of NOMAD\nProjection by computing the first complete data map of Multilingual Wikipedia."}
{"id": "2505.15514", "pdf": "https://arxiv.org/pdf/2505.15514", "abs": "https://arxiv.org/abs/2505.15514", "authors": ["Soham Sane"], "title": "AM-PPO: (Advantage) Alpha-Modulation with Proximal Policy Optimization", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": "17 pages, 4 Tables, 9 Figures, 11 equations", "summary": "Proximal Policy Optimization (PPO) is a widely used reinforcement learning\nalgorithm that heavily relies on accurate advantage estimates for stable and\nefficient training. However, raw advantage signals can exhibit significant\nvariance, noise, and scale-related issues, impeding optimal learning\nperformance. To address this challenge, we introduce Advantage Modulation PPO\n(AM-PPO), a novel enhancement of PPO that adaptively modulates advantage\nestimates using a dynamic, non-linear scaling mechanism. This adaptive\nmodulation employs an alpha controller that dynamically adjusts the scaling\nfactor based on evolving statistical properties of the advantage signals, such\nas their norm, variance, and a predefined target saturation level. By\nincorporating a tanh-based gating function driven by these adaptively scaled\nadvantages, AM-PPO reshapes the advantage signals to stabilize gradient updates\nand improve the conditioning of the policy gradient landscape. Crucially, this\nmodulation also influences value function training by providing consistent and\nadaptively conditioned learning targets. Empirical evaluations across standard\ncontinuous control benchmarks demonstrate that AM-PPO achieves superior reward\ntrajectories, exhibits sustained learning progression, and significantly\nreduces the clipping required by adaptive optimizers. These findings underscore\nthe potential of advantage modulation as a broadly applicable technique for\nenhancing reinforcement learning optimization."}
{"id": "2505.15516", "pdf": "https://arxiv.org/pdf/2505.15516", "abs": "https://arxiv.org/abs/2505.15516", "authors": ["Christiaan Meijer", "E. G. Patrick Bos"], "title": "Explainable embeddings with Distance Explainer", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "68T99", "I.2.m"], "comment": "33 pages, 19 figures. Submitted to JMLR. Method implementation:\n  https://research-software-directory.org/software/distance-explainer", "summary": "While eXplainable AI (XAI) has advanced significantly, few methods address\ninterpretability in embedded vector spaces where dimensions represent complex\nabstractions. We introduce Distance Explainer, a novel method for generating\nlocal, post-hoc explanations of embedded spaces in machine learning models. Our\napproach adapts saliency-based techniques from RISE to explain the distance\nbetween two embedded data points by assigning attribution values through\nselective masking and distance-ranked mask filtering. We evaluate Distance\nExplainer on cross-modal embeddings (image-image and image-caption pairs) using\nestablished XAI metrics including Faithfulness, Sensitivity/Robustness, and\nRandomization. Experiments with ImageNet and CLIP models demonstrate that our\nmethod effectively identifies features contributing to similarity or\ndissimilarity between embedded data points while maintaining high robustness\nand consistency. We also explore how parameter tuning, particularly mask\nquantity and selection strategy, affects explanation quality. This work\naddresses a critical gap in XAI research and enhances transparency and\ntrustworthiness in deep learning applications utilizing embedded spaces."}
{"id": "2505.15544", "pdf": "https://arxiv.org/pdf/2505.15544", "abs": "https://arxiv.org/abs/2505.15544", "authors": ["Haruki Settai", "Naoya Takeishi", "Takehisa Yairi"], "title": "A Temporal Difference Method for Stochastic Continuous Dynamics", "categories": ["cs.LG"], "comment": null, "summary": "For continuous systems modeled by dynamical equations such as ODEs and SDEs,\nBellman's principle of optimality takes the form of the Hamilton-Jacobi-Bellman\n(HJB) equation, which provides the theoretical target of reinforcement learning\n(RL). Although recent advances in RL successfully leverage this formulation,\nthe existing methods typically assume the underlying dynamics are known a\npriori because they need explicit access to the coefficient functions of\ndynamical equations to update the value function following the HJB equation. We\naddress this inherent limitation of HJB-based RL; we propose a model-free\napproach still targeting the HJB equation and propose the corresponding\ntemporal difference method. We demonstrate its potential advantages over\ntransition kernel-based formulations, both qualitatively and empirically. The\nproposed formulation paves the way toward bridging stochastic optimal control\nand model-free reinforcement learning."}
{"id": "2505.15547", "pdf": "https://arxiv.org/pdf/2505.15547", "abs": "https://arxiv.org/abs/2505.15547", "authors": ["Adrian Arnaiz-Rodriguez", "Federico Errica"], "title": "Oversmoothing, \"Oversquashing\", Heterophily, Long-Range, and more: Demystifying Common Beliefs in Graph Machine Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "After a renaissance phase in which researchers revisited the message-passing\nparadigm through the lens of deep learning, the graph machine learning\ncommunity shifted its attention towards a deeper and practical understanding of\nmessage-passing's benefits and limitations. In this position paper, we notice\nhow the fast pace of progress around the topics of oversmoothing and\noversquashing, the homophily-heterophily dichotomy, and long-range tasks, came\nwith the consolidation of commonly accepted beliefs and assumptions that are\nnot always true nor easy to distinguish from each other. We argue that this has\nled to ambiguities around the investigated problems, preventing researchers\nfrom focusing on and addressing precise research questions while causing a good\namount of misunderstandings. Our contribution wants to make such common beliefs\nexplicit and encourage critical thinking around these topics, supported by\nsimple but noteworthy counterexamples. The hope is to clarify the distinction\nbetween the different issues and promote separate but intertwined research\ndirections to address them."}
{"id": "2505.15548", "pdf": "https://arxiv.org/pdf/2505.15548", "abs": "https://arxiv.org/abs/2505.15548", "authors": ["Suvadeep Hajra"], "title": "Short-Range Dependency Effects on Transformer Instability and a Decomposed Attention Solution", "categories": ["cs.LG"], "comment": null, "summary": "Transformer language models have driven significant progress across various\nfields, including natural language processing and computer vision. A central\ncomponent of these models is the self-attention (SA) mechanism, which learns\nrich vector representations of tokens by modeling their relationships with\nothers in a sequence. However, despite extensive research, transformers\ncontinue to suffer from training instability -- often manifesting as spikes or\ndivergence in the training loss during a run.\n  In this work, we identify one source of this instability: SA's limited\nability to capture short-range dependencies, especially in tasks like language\nmodeling, where almost every token heavily relies on its nearby neighbors. This\nlimitation causes the pre-softmax logits of SA to grow rapidly, destabilizing\ntraining. To address this, we propose decomposing the SA into local\n(short-range) and global (long-range) attention heads. This decomposed\nattention, referred to as Long Short-attention (LS-attention), mitigates logit\nexplosion and results in more stable training compared to an equivalent\nmulti-head self-attention (MHSA). Empirical comparisons with two alternative\ntraining stabilization methods show that LS-attention reduces the validation\nperplexity to nearly 2/5 of that achieved by one method and reaches a similar\nperplexity as the other method using only 1/20 of the GPU hours. Additionally,\nour experiments demonstrate that LS-attention reduces inference latency by up\nto 36% compared to a state-of-the-art implementation of equivalent MHSA."}
{"id": "2505.15560", "pdf": "https://arxiv.org/pdf/2505.15560", "abs": "https://arxiv.org/abs/2505.15560", "authors": ["Julian Oelhaf", "Georg Kordowich", "Changhun Kim", "Paula Andrea Perez-Toro", "Andreas Maier", "Johann Jager", "Siming Bayer"], "title": "Impact of Data Sparsity on Machine Learning for Fault Detection in Power System Protection", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Germany's transition to a renewable energy-based power system is reshaping\ngrid operations, requiring advanced monitoring and control to manage\ndecentralized generation. Machine learning (ML) has emerged as a powerful tool\nfor power system protection, particularly for fault detection (FD) and fault\nline identification (FLI) in transmission grids. However, ML model reliability\ndepends on data quality and availability. Data sparsity resulting from sensor\nfailures, communication disruptions, or reduced sampling rates poses a\nchallenge to ML-based FD and FLI. Yet, its impact has not been systematically\nvalidated prior to this work. In response, we propose a framework to assess the\nimpact of data sparsity on ML-based FD and FLI performance. We simulate\nrealistic data sparsity scenarios, evaluate their impact, derive quantitative\ninsights, and demonstrate the effectiveness of this evaluation strategy by\napplying it to an existing ML-based framework. Results show the ML model\nremains robust for FD, maintaining an F1-score of 0.999 $\\pm$ 0.000 even after\na 50x data reduction. In contrast, FLI is more sensitive, with performance\ndecreasing by 55.61% for missing voltage measurements and 9.73% due to\ncommunication failures at critical network points. These findings offer\nactionable insights for optimizing ML models for real-world grid protection.\nThis enables more efficient FD and supports targeted improvements in FLI."}
{"id": "2505.15570", "pdf": "https://arxiv.org/pdf/2505.15570", "abs": "https://arxiv.org/abs/2505.15570", "authors": ["Marko Tuononen", "Duy Vu", "Dani Korpi", "Vesa Starck", "Ville Hautamäki"], "title": "Refining Neural Activation Patterns for Layer-Level Concept Discovery in Neural Network-Based Receivers", "categories": ["cs.LG", "68T07 (Primary) 62H30, 94A05 (Secondary)", "I.2.6; I.5.3; C.2.1"], "comment": "46 pages, 40 figures, 28 tables, 10 equations, and 5 listings", "summary": "Concept discovery in neural networks often targets individual neurons or\nhuman-interpretable features, overlooking distributed layer-wide patterns. We\nstudy the Neural Activation Pattern (NAP) methodology, which clusters\nfull-layer activation distributions to identify such layer-level concepts.\nApplied to visual object recognition and radio receiver models, we propose\nimproved normalization, distribution estimation, distance metrics, and varied\ncluster selection. In the radio receiver model, distinct concepts did not\nemerge; instead, a continuous activation manifold shaped by Signal-to-Noise\nRatio (SNR) was observed -- highlighting SNR as a key learned factor,\nconsistent with classical receiver behavior and supporting physical\nplausibility. Our enhancements to NAP improved in-distribution vs.\nout-of-distribution separation, suggesting better generalization and indirectly\nvalidating clustering quality. These results underscore the importance of\nclustering design and activation manifolds in interpreting and troubleshooting\nneural network behavior."}
{"id": "2505.15572", "pdf": "https://arxiv.org/pdf/2505.15572", "abs": "https://arxiv.org/abs/2505.15572", "authors": ["Wangyang Ying", "Haoyue Bai", "Nanxu Gong", "Xinyuan Wang", "Sixun Dong", "Haifeng Chen", "Yanjie Fu"], "title": "Bridging the Domain Gap in Equation Distillation with Reinforcement Feedback", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The data-to-equation (Data2Eqn) task aims to discover interpretable\nmathematical equations that map observed values to labels, offering physical\ninsights and broad applicability across academic and industrial domains.\nGenetic programming and traditional deep learning-based approaches suffer from\nsearch inefficiency and poor generalization on small task-specific datasets.\nFoundation models showed promise in this area, but existing approaches suffer\nfrom: 1) They are pretrained on general-purpose data distributions, making them\nless effective for domain-specific tasks; and 2) their training objectives\nfocus on token-level alignment, overlooking mathematical semantics, which can\nlead to inaccurate equations. To address these issues, we aim to enhance the\ndomain adaptability of foundation models for Data2Eqn tasks. In this work, we\npropose a reinforcement learning-based finetuning framework that directly\noptimizes the generation policy of a pretrained model through reward signals\nderived from downstream numerical fitness. Our method allows the model to adapt\nto specific and complex data distributions and generate mathematically\nmeaningful equations. Extensive experiments demonstrate that our approach\nimproves both the accuracy and robustness of equation generation under complex\ndistributions."}
{"id": "2505.15579", "pdf": "https://arxiv.org/pdf/2505.15579", "abs": "https://arxiv.org/abs/2505.15579", "authors": ["Hossein Zakerinia", "Jonathan Scott", "Christoph H. Lampert"], "title": "Federated Learning with Unlabeled Clients: Personalization Can Happen in Low Dimensions", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Personalized federated learning has emerged as a popular approach to training\non devices holding statistically heterogeneous data, known as clients. However,\nmost existing approaches require a client to have labeled data for training or\nfinetuning in order to obtain their own personalized model. In this paper we\naddress this by proposing FLowDUP, a novel method that is able to generate a\npersonalized model using only a forward pass with unlabeled data. The generated\nmodel parameters reside in a low-dimensional subspace, enabling efficient\ncommunication and computation. FLowDUP's learning objective is theoretically\nmotivated by our new transductive multi-task PAC-Bayesian generalization bound,\nthat provides performance guarantees for unlabeled clients. The objective is\nstructured in such a way that it allows both clients with labeled data and\nclients with only unlabeled data to contribute to the training process. To\nsupplement our theoretical results we carry out a thorough experimental\nevaluation of FLowDUP, demonstrating strong empirical performance on a range of\ndatasets with differing sorts of statistically heterogeneous clients. Through\nnumerous ablation studies, we test the efficacy of the individual components of\nthe method."}
{"id": "2505.15589", "pdf": "https://arxiv.org/pdf/2505.15589", "abs": "https://arxiv.org/abs/2505.15589", "authors": ["Carlos Stein Brito", "Daniel McNamee"], "title": "World Models as Reference Trajectories for Rapid Motor Adaptation", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Deploying learned control policies in real-world environments poses a\nfundamental challenge. When system dynamics change unexpectedly, performance\ndegrades until models are retrained on new data. We introduce Reflexive World\nModels (RWM), a dual control framework that uses world model predictions as\nimplicit reference trajectories for rapid adaptation. Our method separates the\ncontrol problem into long-term reward maximization through reinforcement\nlearning and robust motor execution through rapid latent control. This dual\narchitecture achieves significantly faster adaptation with low online\ncomputational cost compared to model-based RL baselines, while maintaining\nnear-optimal performance. The approach combines the benefits of flexible policy\nlearning through reinforcement learning with rapid error correction\ncapabilities, providing a principled approach to maintaining performance in\nhigh-dimensional continuous control tasks under varying dynamics."}
{"id": "2505.15594", "pdf": "https://arxiv.org/pdf/2505.15594", "abs": "https://arxiv.org/abs/2505.15594", "authors": ["Yury Belousov", "Brian Pulfer", "Vitaliy Kinakh", "Slava Voloshynovskiy"], "title": "Beyond Classification: Evaluating Diffusion Denoised Smoothing for Security-Utility Trade off", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Paper accepted at the 33rd European Signal Processing Conference\n  (EUSIPCO 2025)", "summary": "While foundation models demonstrate impressive performance across various\ntasks, they remain vulnerable to adversarial inputs. Current research explores\nvarious approaches to enhance model robustness, with Diffusion Denoised\nSmoothing emerging as a particularly promising technique. This method employs a\npretrained diffusion model to preprocess inputs before model inference. Yet,\nits effectiveness remains largely unexplored beyond classification. We aim to\naddress this gap by analyzing three datasets with four distinct downstream\ntasks under three different adversarial attack algorithms. Our findings reveal\nthat while foundation models maintain resilience against conventional\ntransformations, applying high-noise diffusion denoising to clean images\nwithout any distortions significantly degrades performance by as high as 57%.\nLow-noise diffusion settings preserve performance but fail to provide adequate\nprotection across all attack types. Moreover, we introduce a novel attack\nstrategy specifically targeting the diffusion process itself, capable of\ncircumventing defenses in the low-noise regime. Our results suggest that the\ntrade-off between adversarial robustness and performance remains a challenge to\nbe addressed."}
{"id": "2505.15602", "pdf": "https://arxiv.org/pdf/2505.15602", "abs": "https://arxiv.org/abs/2505.15602", "authors": ["Patrick Cheridito", "Jean-Loup Dupret", "Donatien Hainaut"], "title": "Deep Learning for Continuous-time Stochastic Control with Jumps", "categories": ["cs.LG", "cs.SY", "eess.SY", "math.OC", "q-fin.PM", "I.2.8; I.2.6"], "comment": null, "summary": "In this paper, we introduce a model-based deep-learning approach to solve\nfinite-horizon continuous-time stochastic control problems with jumps. We\niteratively train two neural networks: one to represent the optimal policy and\nthe other to approximate the value function. Leveraging a continuous-time\nversion of the dynamic programming principle, we derive two different training\nobjectives based on the Hamilton-Jacobi-Bellman equation, ensuring that the\nnetworks capture the underlying stochastic dynamics. Empirical evaluations on\ndifferent problems illustrate the accuracy and scalability of our approach,\ndemonstrating its effectiveness in solving complex, high-dimensional stochastic\ncontrol tasks."}
{"id": "2505.15622", "pdf": "https://arxiv.org/pdf/2505.15622", "abs": "https://arxiv.org/abs/2505.15622", "authors": ["Pietro Bartoli", "Christian Veronesi", "Andrea Giudici", "David Siorpaes", "Diana Trojaniello", "Franco Zappa"], "title": "Benchmarking Energy and Latency in TinyML: A Novel Method for Resource-Constrained AI", "categories": ["cs.LG"], "comment": "8 pages, 6 figures The article is already accepted for International\n  Joint Conference on Neural Networks (IJCNN) 2025", "summary": "The rise of IoT has increased the need for on-edge machine learning, with\nTinyML emerging as a promising solution for resource-constrained devices such\nas MCU. However, evaluating their performance remains challenging due to\ndiverse architectures and application scenarios. Current solutions have many\nnon-negligible limitations. This work introduces an alternative benchmarking\nmethodology that integrates energy and latency measurements while\ndistinguishing three execution phases pre-inference, inference, and\npost-inference. Additionally, the setup ensures that the device operates\nwithout being powered by an external measurement unit, while automated testing\ncan be leveraged to enhance statistical significance. To evaluate our setup, we\ntested the STM32N6 MCU, which includes a NPU for executing neural networks. Two\nconfigurations were considered: high-performance and Low-power. The variation\nof the EDP was analyzed separately for each phase, providing insights into the\nimpact of hardware configurations on energy efficiency. Each model was tested\n1000 times to ensure statistically relevant results. Our findings demonstrate\nthat reducing the core voltage and clock frequency improve the efficiency of\npre- and post-processing without significantly affecting network execution\nperformance. This approach can also be used for cross-platform comparisons to\ndetermine the most efficient inference platform and to quantify how pre- and\npost-processing overhead varies across different hardware implementations."}
{"id": "2505.15624", "pdf": "https://arxiv.org/pdf/2505.15624", "abs": "https://arxiv.org/abs/2505.15624", "authors": ["H. V. AlquBoj", "Hilal AlQuabeh", "Velibor Bojkovic", "Munachiso Nwadike", "Kentaro Inui"], "title": "Mechanistic Insights into Grokking from the Embedding Layer", "categories": ["cs.LG", "cs.CL"], "comment": "Mechanistic view of embedding layers", "summary": "Grokking, a delayed generalization in neural networks after perfect training\nperformance, has been observed in Transformers and MLPs, but the components\ndriving it remain underexplored. We show that embeddings are central to\ngrokking: introducing them into MLPs induces delayed generalization in modular\narithmetic tasks, whereas MLPs without embeddings can generalize immediately.\nOur analysis identifies two key mechanisms: (1) Embedding update dynamics,\nwhere rare tokens stagnate due to sparse gradient updates and weight decay, and\n(2) Bilinear coupling, where the interaction between embeddings and downstream\nweights introduces saddle points and increases sensitivity to initialization.\nTo confirm these mechanisms, we investigate frequency-aware sampling, which\nbalances token updates by minimizing gradient variance, and embedding-specific\nlearning rates, derived from the asymmetric curvature of the bilinear loss\nlandscape. We prove that an adaptive learning rate ratio,\n\\(\\frac{\\eta_E}{\\eta_W} \\propto \\frac{\\sigma_{\\max}(E)}{\\sigma_{\\max}(W)} \\cdot\n\\frac{f_W}{f_E}\\), mitigates bilinear coupling effects, accelerating\nconvergence. Our methods not only improve grokking dynamics but also extend to\nbroader challenges in Transformer optimization, where bilinear interactions\nhinder efficient training."}
{"id": "2505.15626", "pdf": "https://arxiv.org/pdf/2505.15626", "abs": "https://arxiv.org/abs/2505.15626", "authors": ["Jacopo Teneggi", "Zhenzhen Wang", "Paul H. Yi", "Tianmin Shu", "Jeremias Sulam"], "title": "Aligning Explanations with Human Communication", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Machine learning explainability aims to make the decision-making process of\nblack-box models more transparent by finding the most important input features\nfor a given prediction task. Recent works have proposed composing explanations\nfrom semantic concepts (e.g., colors, patterns, shapes) that are inherently\ninterpretable to the user of a model. However, these methods generally ignore\nthe communicative context of explanation-the ability of the user to understand\nthe prediction of the model from the explanation. For example, while a medical\ndoctor might understand an explanation in terms of clinical markers, a patient\nmay need a more accessible explanation to make sense of the same diagnosis. In\nthis paper, we address this gap with listener-adaptive explanations. We propose\nan iterative procedure grounded in principles of pragmatic reasoning and the\nrational speech act to generate explanations that maximize communicative\nutility. Our procedure only needs access to pairwise preferences between\ncandidate explanations, relevant in real-world scenarios where a listener model\nmay not be available. We evaluate our method in image classification tasks,\ndemonstrating improved alignment between explanations and listener preferences\nacross three datasets. Furthermore, we perform a user study that demonstrates\nour explanations increase communicative utility."}
{"id": "2505.15631", "pdf": "https://arxiv.org/pdf/2505.15631", "abs": "https://arxiv.org/abs/2505.15631", "authors": ["Nick Kocher", "Christian Wassermann", "Leona Hennig", "Jonas Seng", "Holger Hoos", "Kristian Kersting", "Marius Lindauer", "Matthias Müller"], "title": "Guidelines for the Quality Assessment of Energy-Aware NAS Benchmarks", "categories": ["cs.LG"], "comment": null, "summary": "Neural Architecture Search (NAS) accelerates progress in deep learning\nthrough systematic refinement of model architectures. The downside is\nincreasingly large energy consumption during the search process.\nSurrogate-based benchmarking mitigates the cost of full training by querying a\npre-trained surrogate to obtain an estimate for the quality of the model.\nSpecifically, energy-aware benchmarking aims to make it possible for NAS to\nfavourably trade off model energy consumption against accuracy. Towards this\nend, we propose three design principles for such energy-aware benchmarks: (i)\nreliable power measurements, (ii) a wide range of GPU usage, and (iii) holistic\ncost reporting. We analyse EA-HAS-Bench based on these principles and find that\nthe choice of GPU measurement API has a large impact on the quality of results.\nUsing the Nvidia System Management Interface (SMI) on top of its underlying\nlibrary influences the sampling rate during the initial data collection,\nreturning faulty low-power estimations. This results in poor correlation with\naccurate measurements obtained from an external power meter. With this study,\nwe bring to attention several key considerations when performing energy-aware\nsurrogate-based benchmarking and derive first guidelines that can help design\nnovel benchmarks. We show a narrow usage range of the four GPUs attached to our\ndevice, ranging from 146 W to 305 W in a single-GPU setting, and narrowing down\neven further when using all four GPUs. To improve holistic energy reporting, we\npropose calibration experiments over assumptions made in popular tools, such as\nCode Carbon, thus achieving reductions in the maximum inaccuracy from 10.3 % to\n8.9 % without and to 6.6 % with prior estimation of the expected load on the\ndevice."}
{"id": "2505.15638", "pdf": "https://arxiv.org/pdf/2505.15638", "abs": "https://arxiv.org/abs/2505.15638", "authors": ["Daniel Waxman", "Fernando Llorente", "Petar M. Djurić"], "title": "Bayesian Ensembling: Insights from Online Optimization and Empirical Bayes", "categories": ["cs.LG", "stat.CO", "stat.ME", "stat.ML"], "comment": "25 pages, 12 figures", "summary": "We revisit the classical problem of Bayesian ensembles and address the\nchallenge of learning optimal combinations of Bayesian models in an online,\ncontinual learning setting. To this end, we reinterpret existing approaches\nsuch as Bayesian model averaging (BMA) and Bayesian stacking through a novel\nempirical Bayes lens, shedding new light on the limitations and pathologies of\nBMA. Further motivated by insights from online optimization, we propose Online\nBayesian Stacking (OBS), a method that optimizes the log-score over predictive\ndistributions to adaptively combine Bayesian models. A key contribution of our\nwork is establishing a novel connection between OBS and portfolio selection,\nbridging Bayesian ensemble learning with a rich, well-studied theoretical\nframework that offers efficient algorithms and extensive regret analysis. We\nfurther clarify the relationship between OBS and online BMA, showing that they\noptimize related but distinct cost functions. Through theoretical analysis and\nempirical evaluation, we identify scenarios where OBS outperforms online BMA\nand provide principled guidance on when practitioners should prefer one\napproach over the other."}
{"id": "2505.15643", "pdf": "https://arxiv.org/pdf/2505.15643", "abs": "https://arxiv.org/abs/2505.15643", "authors": ["Lan V. Truong"], "title": "Optimal Best-Arm Identification under Fixed Confidence with Multiple Optima", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "comment": "22 pages", "summary": "We study the problem of best-arm identification in stochastic multi-armed\nbandits under the fixed-confidence setting, with a particular focus on\ninstances that admit multiple optimal arms. While the Track-and-Stop algorithm\nof Garivier and Kaufmann (2016) is widely conjectured to be instance-optimal,\nits performance in the presence of multiple optima has remained insufficiently\nunderstood. In this work, we revisit the Track-and-Stop strategy and propose a\nmodified stopping rule that ensures instance-optimality even when the set of\noptimal arms is not a singleton. Our analysis introduces a new\ninformation-theoretic lower bound that explicitly accounts for multiple optimal\narms, and we demonstrate that our stopping rule tightly matches this bound."}
{"id": "2505.15647", "pdf": "https://arxiv.org/pdf/2505.15647", "abs": "https://arxiv.org/abs/2505.15647", "authors": ["Youming Tao", "Zuyuan Zhang", "Dongxiao Yu", "Xiuzhen Cheng", "Falko Dressler", "Di Wang"], "title": "Second-Order Convergence in Private Stochastic Non-Convex Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We investigate the problem of finding second-order stationary points (SOSP)\nin differentially private (DP) stochastic non-convex optimization. Existing\nmethods suffer from two key limitations: (i) inaccurate convergence error rate\ndue to overlooking gradient variance in the saddle point escape analysis, and\n(ii) dependence on auxiliary private model selection procedures for identifying\nDP-SOSP, which can significantly impair utility, particularly in distributed\nsettings. To address these issues, we propose a generic perturbed stochastic\ngradient descent (PSGD) framework built upon Gaussian noise injection and\ngeneral gradient oracles. A core innovation of our framework is using model\ndrift distance to determine whether PSGD escapes saddle points, ensuring\nconvergence to approximate local minima without relying on second-order\ninformation or additional DP-SOSP identification. By leveraging the adaptive\nDP-SPIDER estimator as a specific gradient oracle, we develop a new DP\nalgorithm that rectifies the convergence error rates reported in prior work. We\nfurther extend this algorithm to distributed learning with arbitrarily\nheterogeneous data, providing the first formal guarantees for finding DP-SOSP\nin such settings. Our analysis also highlights the detrimental impacts of\nprivate selection procedures in distributed learning under high-dimensional\nmodels, underscoring the practical benefits of our design. Numerical\nexperiments on real-world datasets validate the efficacy of our approach."}
{"id": "2505.15648", "pdf": "https://arxiv.org/pdf/2505.15648", "abs": "https://arxiv.org/abs/2505.15648", "authors": ["Harmender Gahlawat", "Meirav Zehavi"], "title": "Learning Small Decision Trees with Few Outliers: A Parameterized Perspective", "categories": ["cs.LG", "cs.DS"], "comment": null, "summary": "Decision trees are a fundamental tool in machine learning for representing,\nclassifying, and generalizing data. It is desirable to construct ``small''\ndecision trees, by minimizing either the \\textit{size} ($s$) or the\n\\textit{depth} $(d)$ of the \\textit{decision tree} (\\textsc{DT}). Recently, the\nparameterized complexity of \\textsc{Decision Tree Learning} has attracted a lot\nof attention. We consider a generalization of \\textsc{Decision Tree Learning}\nwhere given a \\textit{classification instance} $E$ and an integer $t$, the task\nis to find a ``small'' \\textsc{DT} that disagrees with $E$ in at most $t$\nexamples. We consider two problems: \\textsc{DTSO} and \\textsc{DTDO}, where the\ngoal is to construct a \\textsc{DT} minimizing $s$ and $d$, respectively. We\nfirst establish that both \\textsc{DTSO} and \\textsc{DTDO} are W[1]-hard when\nparameterized by $s+\\delta_{max}$ and $d+\\delta_{max}$, respectively, where\n$\\delta_{max}$ is the maximum number of features in which two differently\nlabeled examples can differ. We complement this result by showing that these\nproblems become \\textsc{FPT} if we include the parameter $t$. We also consider\nthe kernelization complexity of these problems and establish several positive\nand negative results for both \\textsc{DTSO} and \\textsc{DTDO}."}
{"id": "2505.15657", "pdf": "https://arxiv.org/pdf/2505.15657", "abs": "https://arxiv.org/abs/2505.15657", "authors": ["Cheng Yan", "Felix Mohr", "Tom Viering"], "title": "LCDB 1.1: A Database Illustrating Learning Curves Are More Ill-Behaved Than Previously Thought", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Sample-wise learning curves plot performance versus training set size. They\nare useful for studying scaling laws and speeding up hyperparameter tuning and\nmodel selection. Learning curves are often assumed to be well-behaved: monotone\n(i.e. improving with more data) and convex. By constructing the Learning Curves\nDatabase 1.1 (LCDB 1.1), a large-scale database with high-resolution learning\ncurves, we show that learning curves are less often well-behaved than\npreviously thought. Using statistically rigorous methods, we observe\nsignificant ill-behavior in approximately 14% of the learning curves, almost\ntwice as much as in previous estimates. We also identify which learners are to\nblame and show that specific learners are more ill-behaved than others.\nAdditionally, we demonstrate that different feature scalings rarely resolve\nill-behavior. We evaluate the impact of ill-behavior on downstream tasks, such\nas learning curve fitting and model selection, and find it poses significant\nchallenges, underscoring the relevance and potential of LCDB 1.1 as a\nchallenging benchmark for future research."}
{"id": "2505.15661", "pdf": "https://arxiv.org/pdf/2505.15661", "abs": "https://arxiv.org/abs/2505.15661", "authors": ["Sina Mohammad-Taheri", "Matthew J. Colbrook", "Simone Brugiapaglia"], "title": "Deep greedy unfolding: Sorting out argsorting in greedy sparse recovery algorithms", "categories": ["cs.LG", "cs.NA", "cs.NE", "math.NA"], "comment": null, "summary": "Gradient-based learning imposes (deep) neural networks to be differentiable\nat all steps. This includes model-based architectures constructed by unrolling\niterations of an iterative algorithm onto layers of a neural network, known as\nalgorithm unrolling. However, greedy sparse recovery algorithms depend on the\nnon-differentiable argsort operator, which hinders their integration into\nneural networks. In this paper, we address this challenge in Orthogonal\nMatching Pursuit (OMP) and Iterative Hard Thresholding (IHT), two popular\nrepresentative algorithms in this class. We propose permutation-based variants\nof these algorithms and approximate permutation matrices using \"soft\"\npermutation matrices derived from softsort, a continuous relaxation of argsort.\nWe demonstrate--both theoretically and numerically--that Soft-OMP and Soft-IHT,\nas differentiable counterparts of OMP and IHT and fully compatible with neural\nnetwork training, effectively approximate these algorithms with a controllable\ndegree of accuracy. This leads to the development of OMP- and IHT-Net, fully\ntrainable network architectures based on Soft-OMP and Soft-IHT, respectively.\nFinally, by choosing weights as \"structure-aware\" trainable parameters, we\nconnect our approach to structured sparse recovery and demonstrate its ability\nto extract latent sparsity patterns from data."}
{"id": "2505.15668", "pdf": "https://arxiv.org/pdf/2505.15668", "abs": "https://arxiv.org/abs/2505.15668", "authors": ["Davide Scassola", "Sebastiano Saccani", "Luca Bortolussi"], "title": "Graph Conditional Flow Matching for Relational Data Generation", "categories": ["cs.LG", "68T07"], "comment": "9 pages of main content, submitted to a conference", "summary": "Data synthesis is gaining momentum as a privacy-enhancing technology. While\nsingle-table tabular data generation has seen considerable progress, current\nmethods for multi-table data often lack the flexibility and expressiveness\nneeded to capture complex relational structures. In particular, they struggle\nwith long-range dependencies and complex foreign-key relationships, such as\ntables with multiple parent tables or multiple types of links between the same\npair of tables. We propose a generative model for relational data that\ngenerates the content of a relational dataset given the graph formed by the\nforeign-key relationships. We do this by learning a deep generative model of\nthe content of the whole relational database by flow matching, where the neural\nnetwork trained to denoise records leverages a graph neural network to obtain\ninformation from connected records. Our method is flexible, as it can support\nrelational datasets with complex structures, and expressive, as the generation\nof each record can be influenced by any other record within the same connected\ncomponent. We evaluate our method on several benchmark datasets and show that\nit achieves state-of-the-art performance in terms of synthetic data fidelity."}
{"id": "2505.15688", "pdf": "https://arxiv.org/pdf/2505.15688", "abs": "https://arxiv.org/abs/2505.15688", "authors": ["Leonardo N. Coregliano", "Maryanthe Malliaris"], "title": "A packing lemma for VCN${}_k$-dimension and learning high-dimensional data", "categories": ["cs.LG", "math.ST", "stat.TH", "Primary: 68Q32. Secondary: 68T05"], "comment": "29 pages, 1 figure", "summary": "Recently, the authors introduced the theory of high-arity PAC learning, which\nis well-suited for learning graphs, hypergraphs and relational structures. In\nthe same initial work, the authors proved a high-arity analogue of the\nFundamental Theorem of Statistical Learning that almost completely\ncharacterizes all notions of high-arity PAC learning in terms of a\ncombinatorial dimension, called the Vapnik--Chervonenkis--Natarajan (VCN${}_k$)\n$k$-dimension, leaving as an open problem only the characterization of\nnon-partite, non-agnostic high-arity PAC learnability.\n  In this work, we complete this characterization by proving that non-partite\nnon-agnostic high-arity PAC learnability implies a high-arity version of the\nHaussler packing property, which in turn implies finiteness of\nVCN${}_k$-dimension. This is done by obtaining direct proofs that classic PAC\nlearnability implies classic Haussler packing property, which in turn implies\nfinite Natarajan dimension and noticing that these direct proofs nicely lift to\nhigh-arity."}
{"id": "2505.15694", "pdf": "https://arxiv.org/pdf/2505.15694", "abs": "https://arxiv.org/abs/2505.15694", "authors": ["Xingyu Zhou", "Yulian Wu", "Francesco Orabona"], "title": "A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this paper, we theoretically investigate the effects of noisy labels in\noffline alignment, with a focus on the interplay between privacy and robustness\nagainst adversarial corruption. Specifically, under linear modeling\nassumptions, we present a unified analysis covering both reinforcement learning\nfrom human feedback (RLHF) and direct preference optimization (DPO) under\ndifferent privacy-corruption scenarios, such as Local differential\nprivacy-then-Corruption (LTC), where human preference labels are privatized\nbefore being corrupted by an adversary, and Corruption-then-Local differential\nprivacy (CTL), where labels are corrupted before privacy protection. Our\nanalysis leverages a reduction framework that reduces the offline alignment\nproblem under linear modeling assumptions to parameter estimation in logistic\nregression. This framework allows us to establish an interesting separation\nresult between LTC and CTL, demonstrating that LTC presents a greater challenge\nthan CTL in offline alignment, even under linear models. As important\nby-products, our findings also advance the state-of-the-art theoretical results\nin offline alignment under privacy-only or corruption-only scenarios."}
{"id": "2505.15721", "pdf": "https://arxiv.org/pdf/2505.15721", "abs": "https://arxiv.org/abs/2505.15721", "authors": ["Coby Penso", "Bar Mahpud", "Jacob Goldberger", "Or Sheffet"], "title": "Privacy-Preserving Conformal Prediction Under Local Differential Privacy", "categories": ["cs.LG", "stat.ML"], "comment": "Preprint. Under review", "summary": "Conformal prediction (CP) provides sets of candidate classes with a\nguaranteed probability of containing the true class. However, it typically\nrelies on a calibration set with clean labels. We address privacy-sensitive\nscenarios where the aggregator is untrusted and can only access a perturbed\nversion of the true labels. We propose two complementary approaches under local\ndifferential privacy (LDP). In the first approach, users do not access the\nmodel but instead provide their input features and a perturbed label using a\nk-ary randomized response. In the second approach, which enforces stricter\nprivacy constraints, users add noise to their conformity score by binary search\nresponse. This method requires access to the classification model but preserves\nboth data and label privacy. Both approaches compute the conformal threshold\ndirectly from noisy data without accessing the true labels. We prove\nfinite-sample coverage guarantees and demonstrate robust coverage even under\nsevere randomization. This approach unifies strong local privacy with\npredictive uncertainty control, making it well-suited for sensitive\napplications such as medical imaging or large language model queries,\nregardless of whether users can (or are willing to) compute their own scores."}
{"id": "2505.15746", "pdf": "https://arxiv.org/pdf/2505.15746", "abs": "https://arxiv.org/abs/2505.15746", "authors": ["Jingzhe Liu", "Zhigang Hua", "Yan Xie", "Bingheng Li", "Harry Shomer", "Yu Song", "Kaveh Hassani", "Jiliang Tang"], "title": "Higher-order Structure Boosts Link Prediction on Temporal Graphs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Temporal Graph Neural Networks (TGNNs) have gained growing attention for\nmodeling and predicting structures in temporal graphs. However, existing TGNNs\nprimarily focus on pairwise interactions while overlooking higher-order\nstructures that are integral to link formation and evolution in real-world\ntemporal graphs. Meanwhile, these models often suffer from efficiency\nbottlenecks, further limiting their expressive power. To tackle these\nchallenges, we propose a Higher-order structure Temporal Graph Neural Network,\nwhich incorporates hypergraph representations into temporal graph learning. In\nparticular, we develop an algorithm to identify the underlying higher-order\nstructures, enhancing the model's ability to capture the group interactions.\nFurthermore, by aggregating multiple edge features into hyperedge\nrepresentations, HTGN effectively reduces memory cost during training. We\ntheoretically demonstrate the enhanced expressiveness of our approach and\nvalidate its effectiveness and efficiency through extensive experiments on\nvarious real-world temporal graphs. Experimental results show that HTGN\nachieves superior performance on dynamic link prediction while reducing memory\ncosts by up to 50\\% compared to existing methods."}
{"id": "2505.15747", "pdf": "https://arxiv.org/pdf/2505.15747", "abs": "https://arxiv.org/abs/2505.15747", "authors": ["Kanan Kiguchi", "Yunhao Tu", "Katsuhiro Ajito", "Fady Alnajjar", "Kazuyuki Murase"], "title": "Multi-modal Integration Analysis of Alzheimer's Disease Using Large Language Models and Knowledge Graphs", "categories": ["cs.LG", "cs.AI", "I.2.6; I.2.1; H.3.1; J.3"], "comment": "38 pages, 8 figures, 4 tables", "summary": "We propose a novel framework for integrating fragmented multi-modal data in\nAlzheimer's disease (AD) research using large language models (LLMs) and\nknowledge graphs. While traditional multimodal analysis requires matched\npatient IDs across datasets, our approach demonstrates population-level\nintegration of MRI, gene expression, biomarkers, EEG, and clinical indicators\nfrom independent cohorts. Statistical analysis identified significant features\nin each modality, which were connected as nodes in a knowledge graph. LLMs then\nanalyzed the graph to extract potential correlations and generate hypotheses in\nnatural language. This approach revealed several novel relationships, including\na potential pathway linking metabolic risk factors to tau protein abnormalities\nvia neuroinflammation (r>0.6, p<0.001), and unexpected correlations between\nfrontal EEG channels and specific gene expression profiles (r=0.42-0.58,\np<0.01). Cross-validation with independent datasets confirmed the robustness of\nmajor findings, with consistent effect sizes across cohorts (variance <15%).\nThe reproducibility of these findings was further supported by expert review\n(Cohen's k=0.82) and computational validation. Our framework enables cross\nmodal integration at a conceptual level without requiring patient ID matching,\noffering new possibilities for understanding AD pathology through fragmented\ndata reuse and generating testable hypotheses for future research."}
{"id": "2505.15754", "pdf": "https://arxiv.org/pdf/2505.15754", "abs": "https://arxiv.org/abs/2505.15754", "authors": ["Palash Chatterjee", "Roni Khardon"], "title": "Improving planning and MBRL with temporally-extended actions", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Continuous time systems are often modeled using discrete time dynamics but\nthis requires a small simulation step to maintain accuracy. In turn, this\nrequires a large planning horizon which leads to computationally demanding\nplanning problems and reduced performance. Previous work in model free\nreinforcement learning has partially addressed this issue using action repeats\nwhere a policy is learned to determine a discrete action duration. Instead we\npropose to control the continuous decision timescale directly by using\ntemporally-extended actions and letting the planner treat the duration of the\naction as an additional optimization variable along with the standard action\nvariables. This additional structure has multiple advantages. It speeds up\nsimulation time of trajectories and, importantly, it allows for deep horizon\nsearch in terms of primitive actions while using a shallow search depth in the\nplanner. In addition, in the model based reinforcement learning (MBRL) setting,\nit reduces compounding errors from model learning and improves training time\nfor models. We show that this idea is effective and that the range for action\ndurations can be automatically selected using a multi-armed bandit formulation\nand integrated into the MBRL framework. An extensive experimental evaluation\nboth in planning and in MBRL, shows that our approach yields faster planning,\nbetter solutions, and that it enables solutions to problems that are not solved\nin the standard formulation."}
{"id": "2505.15777", "pdf": "https://arxiv.org/pdf/2505.15777", "abs": "https://arxiv.org/abs/2505.15777", "authors": ["Jorge Bacca"], "title": "Projection-Based Correction for Enhancing Deep Inverse Networks", "categories": ["cs.LG", "physics.comp-ph"], "comment": null, "summary": "Deep learning-based models have demonstrated remarkable success in solving\nillposed inverse problems; however, many fail to strictly adhere to the\nphysical constraints imposed by the measurement process. In this work, we\nintroduce a projection-based correction method to enhance the inference of deep\ninverse networks by ensuring consistency with the forward model. Specifically,\ngiven an initial estimate from a learned reconstruction network, we apply a\nprojection step that constrains the solution to lie within the valid solution\nspace of the inverse problem. We theoretically demonstrate that if the recovery\nmodel is a well-trained deep inverse network, the solution can be decomposed\ninto range-space and null-space components, where the projection-based\ncorrection reduces to an identity transformation. Extensive simulations and\nexperiments validate the proposed method, demonstrating improved reconstruction\naccuracy across diverse inverse problems and deep network architectures."}
{"id": "2505.15782", "pdf": "https://arxiv.org/pdf/2505.15782", "abs": "https://arxiv.org/abs/2505.15782", "authors": ["Pedro P. Santos", "Alberto Sardinha", "Francisco S. Melo"], "title": "Solving General-Utility Markov Decision Processes in the Single-Trial Regime with Online Planning", "categories": ["cs.LG"], "comment": null, "summary": "In this work, we contribute the first approach to solve infinite-horizon\ndiscounted general-utility Markov decision processes (GUMDPs) in the\nsingle-trial regime, i.e., when the agent's performance is evaluated based on a\nsingle trajectory. First, we provide some fundamental results regarding policy\noptimization in the single-trial regime, investigating which class of policies\nsuffices for optimality, casting our problem as a particular MDP that is\nequivalent to our original problem, as well as studying the computational\nhardness of policy optimization in the single-trial regime. Second, we show how\nwe can leverage online planning techniques, in particular a Monte-Carlo tree\nsearch algorithm, to solve GUMDPs in the single-trial regime. Third, we provide\nexperimental results showcasing the superior performance of our approach in\ncomparison to relevant baselines."}
{"id": "2505.15784", "pdf": "https://arxiv.org/pdf/2505.15784", "abs": "https://arxiv.org/abs/2505.15784", "authors": ["Jun Wan", "Lingrui Mei"], "title": "Large Language Models as Computable Approximations to Solomonoff Induction", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Both authors contributed equally", "summary": "The rapid advancement of large language models (LLMs) calls for a rigorous\ntheoretical framework to explain their empirical success. While significant\nprogress has been made in understanding LLM behaviors, existing theoretical\nframeworks remain fragmented in explaining emergent phenomena through a unified\nmathematical lens. We establish the first formal connection between LLM\narchitectures and Algorithmic Information Theory (AIT) by proving two\nfundamental results: (1) the training process computationally approximates\nSolomonoff prior through loss minimization interpreted as program length\noptimization, and (2) next-token prediction implements approximate Solomonoff\ninduction. We leverage AIT to provide a unified theoretical explanation for\nin-context learning, few-shot learning, and scaling laws. Furthermore, our\ntheoretical insights lead to a principled method for few-shot example selection\nthat prioritizes samples where models exhibit lower predictive confidence. We\ndemonstrate through experiments on diverse text classification benchmarks that\nthis strategy yields significant performance improvements, particularly for\nsmaller model architectures, when compared to selecting high-confidence\nexamples. Our framework bridges the gap between theoretical foundations and\npractical LLM behaviors, providing both explanatory power and actionable\ninsights for future model development."}
{"id": "2505.15788", "pdf": "https://arxiv.org/pdf/2505.15788", "abs": "https://arxiv.org/abs/2505.15788", "authors": ["Zahra Khatti", "Daniel P. Robinson", "Frank E. Curtis"], "title": "Fair Supervised Learning Through Constraints on Smooth Nonconvex Unfairness-Measure Surrogates", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "A new strategy for fair supervised machine learning is proposed. The main\nadvantages of the proposed strategy as compared to others in the literature are\nas follows. (a) We introduce a new smooth nonconvex surrogate to approximate\nthe Heaviside functions involved in discontinuous unfairness measures. The\nsurrogate is based on smoothing methods from the optimization literature, and\nis new for the fair supervised learning literature. The surrogate is a tight\napproximation which ensures the trained prediction models are fair, as opposed\nto other (e.g., convex) surrogates that can fail to lead to a fair prediction\nmodel in practice. (b) Rather than rely on regularizers (that lead to\noptimization problems that are difficult to solve) and corresponding\nregularization parameters (that can be expensive to tune), we propose a\nstrategy that employs hard constraints so that specific tolerances for\nunfairness can be enforced without the complications associated with the use of\nregularization. (c)~Our proposed strategy readily allows for constraints on\nmultiple (potentially conflicting) unfairness measures at the same time.\nMultiple measures can be considered with a regularization approach, but at the\ncost of having even more difficult optimization problems to solve and further\nexpense for tuning. By contrast, through hard constraints, our strategy leads\nto optimization models that can be solved tractably with minimal tuning."}
{"id": "2505.15798", "pdf": "https://arxiv.org/pdf/2505.15798", "abs": "https://arxiv.org/abs/2505.15798", "authors": ["Taehoon Kim", "Henry Gouk", "Minyoung Kim", "Timothy Hospedales"], "title": "Model Merging is Secretly Certifiable: Non-Vacuous Generalisation Bounds for Low-Shot Learning", "categories": ["cs.LG"], "comment": null, "summary": "Certifying the IID generalisation ability of deep networks is the first of\nmany requirements for trusting AI in high-stakes applications from medicine to\nsecurity. However, when instantiating generalisation bounds for deep networks\nit remains challenging to obtain non-vacuous guarantees, especially when\napplying contemporary large models on the small scale data prevalent in such\nhigh-stakes fields. In this paper, we draw a novel connection between a family\nof learning methods based on model fusion and generalisation certificates, and\nsurprisingly show that with minor adjustment several existing learning\nstrategies already provide non-trivial generalisation guarantees. Essentially,\nby focusing on data-driven learning of downstream tasks by fusion rather than\nfine-tuning, the certified generalisation gap becomes tiny and independent of\nthe base network size, facilitating its certification. Our results show for the\nfirst time non-trivial generalisation guarantees for learning with as low as\n100 examples, while using vision models such as VIT-B and language models such\nas mistral-7B. This observation is significant as it has immediate implications\nfor facilitating the certification of existing systems as trustworthy, and\nopens up new directions for research at the intersection of practice and\ntheory."}
{"id": "2505.15802", "pdf": "https://arxiv.org/pdf/2505.15802", "abs": "https://arxiv.org/abs/2505.15802", "authors": ["Sarah E. Wessinger", "Leslie N. Smith", "Jacob Gull", "Jonathan Gehman", "Zachary Beever", "Andrew J. Kammerer"], "title": "A Deep Learning Framework for Two-Dimensional, Multi-Frequency Propagation Factor Estimation", "categories": ["cs.LG", "eess.SP", "physics.ao-ph"], "comment": "Submitted for publication", "summary": "Accurately estimating the refractive environment over multiple frequencies\nwithin the marine atmospheric boundary layer is crucial for the effective\ndeployment of radar technologies. Traditional parabolic equation simulations,\nwhile effective, can be computationally expensive and time-intensive, limiting\ntheir practical application. This communication explores a novel approach using\ndeep neural networks to estimate the pattern propagation factor, a critical\nparameter for characterizing environmental impacts on signal propagation.\nImage-to-image translation generators designed to ingest modified refractivity\ndata and generate predictions of pattern propagation factors over the same\ndomain were developed. Findings demonstrate that deep neural networks can be\ntrained to analyze multiple frequencies and reasonably predict the pattern\npropagation factor, offering an alternative to traditional methods."}
{"id": "2505.15803", "pdf": "https://arxiv.org/pdf/2505.15803", "abs": "https://arxiv.org/abs/2505.15803", "authors": ["Dheeraj Baby", "Yifei Tang", "Hieu Duy Nguyen", "Yu-Xiang Wang", "Rohit Pyati"], "title": "Adaptive Estimation and Learning under Temporal Distribution Shift", "categories": ["cs.LG"], "comment": "Accepted at ICML 2025", "summary": "In this paper, we study the problem of estimation and learning under temporal\ndistribution shift. Consider an observation sequence of length $n$, which is a\nnoisy realization of a time-varying groundtruth sequence. Our focus is to\ndevelop methods to estimate the groundtruth at the final time-step while\nproviding sharp point-wise estimation error rates. We show that, without prior\nknowledge on the level of temporal shift, a wavelet soft-thresholding estimator\nprovides an optimal estimation error bound for the groundtruth. Our proposed\nestimation method generalizes existing researches Mazzetto and Upfal (2023) by\nestablishing a connection between the sequence's non-stationarity level and the\nsparsity in the wavelet-transformed domain. Our theoretical findings are\nvalidated by numerical experiments. Additionally, we applied the estimator to\nderive sparsity-aware excess risk bounds for binary classification under\ndistribution shift and to develop computationally efficient training\nobjectives. As a final contribution, we draw parallels between our results and\nthe classical signal processing problem of total-variation denoising (Mammen\nand van de Geer,1997; Tibshirani, 2014), uncovering novel optimal algorithms\nfor such task."}
{"id": "2505.15808", "pdf": "https://arxiv.org/pdf/2505.15808", "abs": "https://arxiv.org/abs/2505.15808", "authors": ["Carlos Rodriguez-Pardo", "Leonardo Chiani", "Emanuele Borgonovo", "Massimo Tavoni"], "title": "Neural Conditional Transport Maps", "categories": ["cs.LG", "cs.AI", "math.PR", "stat.AP", "stat.ML", "49Q22 (Primary) 68T07 (Secondary)", "I.5.1; I.2.0; G.3"], "comment": "Under Review. Supplementary material included in the pdf", "summary": "We present a neural framework for learning conditional optimal transport (OT)\nmaps between probability distributions. Our approach introduces a conditioning\nmechanism capable of processing both categorical and continuous conditioning\nvariables simultaneously. At the core of our method lies a hypernetwork that\ngenerates transport layer parameters based on these inputs, creating adaptive\nmappings that outperform simpler conditioning methods. Comprehensive ablation\nstudies demonstrate the superior performance of our method over baseline\nconfigurations. Furthermore, we showcase an application to global sensitivity\nanalysis, offering high performance in computing OT-based sensitivity indices.\nThis work advances the state-of-the-art in conditional optimal transport,\nenabling broader application of optimal transport principles to complex,\nhigh-dimensional domains such as generative modeling and black-box model\nexplainability."}
{"id": "2505.15811", "pdf": "https://arxiv.org/pdf/2505.15811", "abs": "https://arxiv.org/abs/2505.15811", "authors": ["Eric J. Michaud", "Asher Parker-Sartori", "Max Tegmark"], "title": "On the creation of narrow AI: hierarchy and nonlocality of neural network skills", "categories": ["cs.LG"], "comment": "19 pages, 13 figures", "summary": "We study the problem of creating strong, yet narrow, AI systems. While recent\nAI progress has been driven by the training of large general-purpose foundation\nmodels, the creation of smaller models specialized for narrow domains could be\nvaluable for both efficiency and safety. In this work, we explore two\nchallenges involved in creating such systems, having to do with basic\nproperties of how neural networks learn and structure their representations.\nThe first challenge regards when it is possible to train narrow models from\nscratch. Through experiments on a synthetic task, we find that it is sometimes\nnecessary to train networks on a wide distribution of data to learn certain\nnarrow skills within that distribution. This effect arises when skills depend\non each other hierarchically, and training on a broad distribution introduces a\ncurriculum which substantially accelerates learning. The second challenge\nregards how to transfer particular skills from large general models into small\nspecialized models. We find that model skills are often not perfectly localized\nto a particular set of prunable components. However, we find that methods based\non pruning can still outperform distillation. We investigate the use of a\nregularization objective to align desired skills with prunable components while\nunlearning unnecessary skills."}
{"id": "2505.15813", "pdf": "https://arxiv.org/pdf/2505.15813", "abs": "https://arxiv.org/abs/2505.15813", "authors": ["Muquan Yu", "Mu Nan", "Hossein Adeli", "Jacob S. Prince", "John A. Pyles", "Leila Wehbe", "Margaret M. Henderson", "Michael J. Tarr", "Andrew F. Luo"], "title": "Meta-Learning an In-Context Transformer Model of Human Higher Visual Cortex", "categories": ["cs.LG", "q-bio.NC"], "comment": null, "summary": "Understanding functional representations within higher visual cortex is a\nfundamental question in computational neuroscience. While artificial neural\nnetworks pretrained on large-scale datasets exhibit striking representational\nalignment with human neural responses, learning image-computable models of\nvisual cortex relies on individual-level, large-scale fMRI datasets. The\nnecessity for expensive, time-intensive, and often impractical data acquisition\nlimits the generalizability of encoders to new subjects and stimuli. BraInCoRL\nuses in-context learning to predict voxelwise neural responses from few-shot\nexamples without any additional finetuning for novel subjects and stimuli. We\nleverage a transformer architecture that can flexibly condition on a variable\nnumber of in-context image stimuli, learning an inductive bias over multiple\nsubjects. During training, we explicitly optimize the model for in-context\nlearning. By jointly conditioning on image features and voxel activations, our\nmodel learns to directly generate better performing voxelwise models of higher\nvisual cortex. We demonstrate that BraInCoRL consistently outperforms existing\nvoxelwise encoder designs in a low-data regime when evaluated on entirely novel\nimages, while also exhibiting strong test-time scaling behavior. The model also\ngeneralizes to an entirely new visual fMRI dataset, which uses different\nsubjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates\nbetter interpretability of neural signals in higher visual cortex by attending\nto semantically relevant stimuli. Finally, we show that our framework enables\ninterpretable mappings from natural language queries to voxel selectivity."}
{"id": "2505.14808", "pdf": "https://arxiv.org/pdf/2505.14808", "abs": "https://arxiv.org/abs/2505.14808", "authors": ["Soo Min Kwon", "Alec S. Xu", "Can Yaras", "Laura Balzano", "Qing Qu"], "title": "Out-of-Distribution Generalization of In-Context Learning: A Low-Dimensional Subspace Perspective", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "This work aims to demystify the out-of-distribution (OOD) capabilities of\nin-context learning (ICL) by studying linear regression tasks parameterized\nwith low-rank covariance matrices. With such a parameterization, we can model\ndistribution shifts as a varying angle between the subspace of the training and\ntesting covariance matrices. We prove that a single-layer linear attention\nmodel incurs a test risk with a non-negligible dependence on the angle,\nillustrating that ICL is not robust to such distribution shifts. However, using\nthis framework, we also prove an interesting property of ICL: when trained on\ntask vectors drawn from a union of low-dimensional subspaces, ICL can\ngeneralize to any subspace within their span, given sufficiently long prompt\nlengths. This suggests that the OOD generalization ability of Transformers may\nactually stem from the new task lying within the span of those encountered\nduring training. We empirically show that our results also hold for models such\nas GPT-2, and conclude with (i) experiments on how our observations extend to\nnonlinear function classes and (ii) results on how LoRA has the ability to\ncapture distribution shifts."}
{"id": "2505.14867", "pdf": "https://arxiv.org/pdf/2505.14867", "abs": "https://arxiv.org/abs/2505.14867", "authors": ["So Won Jeong", "Claire Donnat"], "title": "LOBSTUR: A Local Bootstrap Framework for Tuning Unsupervised Representations in Graph Neural Networks", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) are increasingly used in conjunction with\nunsupervised learning techniques to learn powerful node representations, but\ntheir deployment is hindered by their high sensitivity to hyperparameter tuning\nand the absence of established methodologies for selecting the optimal models.\nTo address these challenges, we propose LOBSTUR-GNN ({\\bf Lo}cal {\\bf B}oot{\\bf\ns}trap for {\\bf T}uning {\\bf U}nsupervised {\\bf R}epresentations in GNNs) i), a\nnovel framework designed to adapt bootstrapping techniques for unsupervised\ngraph representation learning. LOBSTUR-GNN tackles two main challenges: (a)\nadapting the bootstrap edge and feature resampling process to account for local\ngraph dependencies in creating alternative versions of the same graph, and (b)\nestablishing robust metrics for evaluating learned representations without\nground-truth labels. Using locally bootstrapped resampling and leveraging\nCanonical Correlation Analysis (CCA) to assess embedding consistency, LOBSTUR\nprovides a principled approach for hyperparameter tuning in unsupervised GNNs.\nWe validate the effectiveness and efficiency of our proposed method through\nextensive experiments on established academic datasets, showing an 65.9\\%\nimprovement in the classification accuracy compared to an uninformed selection\nof hyperparameters. Finally, we deploy our framework on a real-world\napplication, thereby demonstrating its validity and practical utility in\nvarious settings. \\footnote{The code is available at\n\\href{https://github.com/sowonjeong/lobstur-graph-bootstrap}{github.com/sowonjeong/lobstur-graph-bootstrap}.}"}
{"id": "2505.14970", "pdf": "https://arxiv.org/pdf/2505.14970", "abs": "https://arxiv.org/abs/2505.14970", "authors": ["Xiaoyin Chen", "Jiarui Lu", "Minsu Kim", "Dinghuai Zhang", "Jian Tang", "Alexandre Piché", "Nicolas Gontier", "Yoshua Bengio", "Ehsan Kamalloo"], "title": "Self-Evolving Curriculum for LLM Reasoning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) has proven effective for fine-tuning large\nlanguage models (LLMs), significantly enhancing their reasoning abilities in\ndomains such as mathematics and code generation. A crucial factor influencing\nRL fine-tuning success is the training curriculum: the order in which training\nproblems are presented. While random curricula serve as common baselines, they\nremain suboptimal; manually designed curricula often rely heavily on\nheuristics, and online filtering methods can be computationally prohibitive. To\naddress these limitations, we propose Self-Evolving Curriculum (SEC), an\nautomatic curriculum learning method that learns a curriculum policy\nconcurrently with the RL fine-tuning process. Our approach formulates\ncurriculum selection as a non-stationary Multi-Armed Bandit problem, treating\neach problem category (e.g., difficulty level or problem type) as an individual\narm. We leverage the absolute advantage from policy gradient methods as a proxy\nmeasure for immediate learning gain. At each training step, the curriculum\npolicy selects categories to maximize this reward signal and is updated using\nthe TD(0) method. Across three distinct reasoning domains: planning, inductive\nreasoning, and mathematics, our experiments demonstrate that SEC significantly\nimproves models' reasoning capabilities, enabling better generalization to\nharder, out-of-distribution test problems. Additionally, our approach achieves\nbetter skill balance when fine-tuning simultaneously on multiple reasoning\ndomains. These findings highlight SEC as a promising strategy for RL\nfine-tuning of LLMs."}
{"id": "2505.15013", "pdf": "https://arxiv.org/pdf/2505.15013", "abs": "https://arxiv.org/abs/2505.15013", "authors": ["Anupama Sridhar", "Alexander Johansen"], "title": "Convergence of Adam in Deep ReLU Networks via Directional Complexity and Kakeya Bounds", "categories": ["stat.ML", "cs.LG"], "comment": "9 pages main paper", "summary": "First-order adaptive optimization methods like Adam are the default choices\nfor training modern deep neural networks. Despite their empirical success, the\ntheoretical understanding of these methods in non-smooth settings, particularly\nin Deep ReLU networks, remains limited. ReLU activations create exponentially\nmany region boundaries where standard smoothness assumptions break down.\n\\textbf{We derive the first\n\\(\\tilde{O}\\!\\bigl(\\sqrt{d_{\\mathrm{eff}}/n}\\bigr)\\) generalization bound for\nAdam in Deep ReLU networks and the first global-optimal convergence for Adam in\nthe non smooth, non convex relu landscape without a global PL or convexity\nassumption.} Our analysis is based on stratified Morse theory and novel results\nin Kakeya sets. We develop a multi-layer refinement framework that\nprogressively tightens bounds on region crossings. We prove that the number of\nregion crossings collapses from exponential to near-linear in the effective\ndimension. Using a Kakeya based method, we give a tighter generalization bound\nthan PAC-Bayes approaches and showcase convergence using a mild uniform low\nbarrier assumption."}
{"id": "2505.15022", "pdf": "https://arxiv.org/pdf/2505.15022", "abs": "https://arxiv.org/abs/2505.15022", "authors": ["Ya-Yun Huang", "Joseph McClernon", "Jason A. Oliver", "Matthew M. Engelhard"], "title": "Infinite hierarchical contrastive clustering for personal digital envirotyping", "categories": ["stat.ML", "cs.LG"], "comment": "10pages, 5 figures, Machine Learning four Health(ML4H 2024)", "summary": "Daily environments have profound influence on our health and behavior. Recent\nwork has shown that digital envirotyping, where computer vision is applied to\nimages of daily environments taken during ecological momentary assessment\n(EMA), can be used to identify meaningful relationships between environmental\nfeatures and health outcomes of interest. To systematically study such effects\non an individual level, it is helpful to group images into distinct\nenvironments encountered in an individual's daily life; these may then be\nanalyzed, further grouped into related environments with similar features, and\nlinked to health outcomes. Here we introduce infinite hierarchical contrastive\nclustering to address this challenge. Building on the established contrastive\nclustering framework, our method a) allows an arbitrary number of clusters\nwithout requiring the full Dirichlet Process machinery by placing a\nstick-breaking prior on predicted cluster probabilities; and b) encourages\ndistinct environments to form well-defined sub-clusters within each cluster of\nrelated environments by incorporating a participant-specific prediction loss.\nOur experiments show that our model effectively identifies distinct personal\nenvironments and groups these environments into meaningful environment types.\nWe then illustrate how the resulting clusters can be linked to various health\noutcomes, highlighting the potential of our approach to advance the\nenvirotyping paradigm."}
{"id": "2505.15068", "pdf": "https://arxiv.org/pdf/2505.15068", "abs": "https://arxiv.org/abs/2505.15068", "authors": ["Cheng Qian", "Hongyi Du", "Hongru Wang", "Xiusi Chen", "Yuji Zhang", "Avirup Sil", "Chengxiang Zhai", "Kathleen McKeown", "Heng Ji"], "title": "ModelingAgent: Bridging LLMs and Mathematical Modeling for Real-World Challenges", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "36 Pages, 26 Figures, 5 Tables", "summary": "Recent progress in large language models (LLMs) has enabled substantial\nadvances in solving mathematical problems. However, existing benchmarks often\nfail to reflect the complexity of real-world problems, which demand open-ended,\ninterdisciplinary reasoning and integration of computational tools. To address\nthis gap, we introduce ModelingBench, a novel benchmark featuring\nreal-world-inspired, open-ended problems from math modeling competitions across\ndiverse domains, ranging from urban traffic optimization to ecosystem resource\nplanning. These tasks require translating natural language into formal\nmathematical formulations, applying appropriate tools, and producing\nstructured, defensible reports. ModelingBench also supports multiple valid\nsolutions, capturing the ambiguity and creativity of practical modeling. We\nalso present ModelingAgent, a multi-agent framework that coordinates tool use,\nsupports structured workflows, and enables iterative self-refinement to\ngenerate well-grounded, creative solutions. To evaluate outputs, we further\npropose ModelingJudge, an expert-in-the-loop system leveraging LLMs as\ndomain-specialized judges assessing solutions from multiple expert\nperspectives. Empirical results show that ModelingAgent substantially\noutperforms strong baselines and often produces solutions indistinguishable\nfrom those of human experts. Together, our work provides a comprehensive\nframework for evaluating and advancing real-world problem-solving in\nopen-ended, interdisciplinary modeling challenges."}
{"id": "2505.15175", "pdf": "https://arxiv.org/pdf/2505.15175", "abs": "https://arxiv.org/abs/2505.15175", "authors": ["Diego Granziol", "Donald Flynn"], "title": "A Linear Approach to Data Poisoning", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": "9 pages, 9 Figures", "summary": "We investigate the theoretical foundations of data poisoning attacks in\nmachine learning models. Our analysis reveals that the Hessian with respect to\nthe input serves as a diagnostic tool for detecting poisoning, exhibiting\nspectral signatures that characterize compromised datasets. We use random\nmatrix theory (RMT) to develop a theory for the impact of poisoning proportion\nand regularisation on attack efficacy in linear regression. Through QR stepwise\nregression, we study the spectral signatures of the Hessian in multi-output\nregression. We perform experiments on deep networks to show experimentally that\nthis theory extends to modern convolutional and transformer networks under the\ncross-entropy loss. Based on these insights we develop preliminary algorithms\nto determine if a network has been poisoned and remedies which do not require\nfurther training."}
{"id": "2505.15215", "pdf": "https://arxiv.org/pdf/2505.15215", "abs": "https://arxiv.org/abs/2505.15215", "authors": ["Otto Tabell", "Santtu Tikka", "Juha Karvanen"], "title": "Clustering and Pruning in Causal Data Fusion", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Data fusion, the process of combining observational and experimental data,\ncan enable the identification of causal effects that would otherwise remain\nnon-identifiable. Although identification algorithms have been developed for\nspecific scenarios, do-calculus remains the only general-purpose tool for\ncausal data fusion, particularly when variables are present in some data\nsources but not others. However, approaches based on do-calculus may encounter\ncomputational challenges as the number of variables increases and the causal\ngraph grows in complexity. Consequently, there exists a need to reduce the size\nof such models while preserving the essential features. For this purpose, we\npropose pruning (removing unnecessary variables) and clustering (combining\nvariables) as preprocessing operations for causal data fusion. We generalize\nearlier results on a single data source and derive conditions for applying\npruning and clustering in the case of multiple data sources. We give sufficient\nconditions for inferring the identifiability or non-identifiability of a causal\neffect in a larger graph based on a smaller graph and show how to obtain the\ncorresponding identifying functional for identifiable causal effects. Examples\nfrom epidemiology and social science demonstrate the use of the results."}
{"id": "2505.15240", "pdf": "https://arxiv.org/pdf/2505.15240", "abs": "https://arxiv.org/abs/2505.15240", "authors": ["Yassir Fathullah", "Mark J. F. Gales"], "title": "Generalised Probabilistic Modelling and Improved Uncertainty Estimation in Comparative LLM-as-a-judge", "categories": ["cs.AI", "cs.LG", "stat.ML"], "comment": "To appear in UAI 2025", "summary": "This paper explores generalised probabilistic modelling and uncertainty\nestimation in comparative LLM-as-a-judge frameworks. We show that existing\nProduct-of-Experts methods are specific cases of a broader framework, enabling\ndiverse modelling options. Furthermore, we propose improved uncertainty\nestimates for individual comparisons, enabling more efficient selection and\nachieving strong performance with fewer evaluations. We also introduce a method\nfor estimating overall ranking uncertainty. Finally, we demonstrate that\ncombining absolute and comparative scoring improves performance. Experiments\nshow that the specific expert model has a limited impact on final rankings but\nour proposed uncertainty estimates, especially the probability of reordering,\nsignificantly improve the efficiency of systems reducing the number of needed\ncomparisons by ~50%. Furthermore, ranking-level uncertainty metrics can be used\nto identify low-performing predictions, where the nature of the probabilistic\nmodel has a notable impact on the quality of the overall uncertainty."}
{"id": "2505.15342", "pdf": "https://arxiv.org/pdf/2505.15342", "abs": "https://arxiv.org/abs/2505.15342", "authors": ["Kaito Ariu", "Po-An Wang", "Alexandre Proutiere", "Kenshi Abe"], "title": "Policy Testing in Markov Decision Processes", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "We study the policy testing problem in discounted Markov decision processes\n(MDPs) under the fixed-confidence setting. The goal is to determine whether the\nvalue of a given policy exceeds a specified threshold while minimizing the\nnumber of observations. We begin by deriving an instance-specific lower bound\nthat any algorithm must satisfy. This lower bound is characterized as the\nsolution to an optimization problem with non-convex constraints. We propose a\npolicy testing algorithm inspired by this optimization problem--a common\napproach in pure exploration problems such as best-arm identification, where\nasymptotically optimal algorithms often stem from such optimization-based\ncharacterizations. As for other pure exploration tasks in MDPs, however, the\nnon-convex constraints in the lower-bound problem present significant\nchallenges, raising doubts about whether statistically optimal and\ncomputationally tractable algorithms can be designed. To address this, we\nreformulate the lower-bound problem by interchanging the roles of the objective\nand the constraints, yielding an alternative problem with a non-convex\nobjective but convex constraints. Strikingly, this reformulated problem admits\nan interpretation as a policy optimization task in a newly constructed reversed\nMDP. Leveraging recent advances in policy gradient methods, we efficiently\nsolve this problem and use it to design a policy testing algorithm that is\nstatistically optimal--matching the instance-specific lower bound on sample\ncomplexity--while remaining computationally tractable. We validate our approach\nwith numerical experiments."}
{"id": "2505.15417", "pdf": "https://arxiv.org/pdf/2505.15417", "abs": "https://arxiv.org/abs/2505.15417", "authors": ["Leon Chlon", "Maggie Chlon", "MarcAntonio M. Awada"], "title": "Robust Multimodal Learning via Entropy-Gated Contrastive Fusion", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Real-world multimodal systems routinely face missing-input scenarios, and in\nreality, robots lose audio in a factory or a clinical record omits lab tests at\ninference time. Standard fusion layers either preserve robustness or\ncalibration but never both. We introduce Adaptive Entropy-Gated Contrastive\nFusion (AECF), a single light-weight layer that (i) adapts its entropy\ncoefficient per instance, (ii) enforces monotone calibration across all\nmodality subsets, and (iii) drives a curriculum mask directly from\ntraining-time entropy. On AV-MNIST and MS-COCO, AECF improves masked-input mAP\nby +18 pp at a 50% drop rate while reducing ECE by up to 200%, yet adds 1%\nrun-time. All back-bones remain frozen, making AECF an easy drop-in layer for\nrobust, calibrated multimodal inference."}
{"id": "2505.15429", "pdf": "https://arxiv.org/pdf/2505.15429", "abs": "https://arxiv.org/abs/2505.15429", "authors": ["Pritam Anand"], "title": "Uncertainty Quantification in SVM prediction", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper explores Uncertainty Quantification (UQ) in SVM predictions,\nparticularly for regression and forecasting tasks. Unlike the Neural Network,\nthe SVM solutions are typically more stable, sparse, optimal and interpretable.\nHowever, there are only few literature which addresses the UQ in SVM\nprediction. At first, we provide a comprehensive summary of existing Prediction\nInterval (PI) estimation and probabilistic forecasting methods developed in the\nSVM framework and evaluate them against the key properties expected from an\nideal PI model. We find that none of the existing SVM PI models achieves a\nsparse solution. To introduce sparsity in SVM model, we propose the Sparse\nSupport Vector Quantile Regression (SSVQR) model, which constructs PIs and\nprobabilistic forecasts by solving a pair of linear programs. Further, we\ndevelop a feature selection algorithm for PI estimation using SSVQR that\neffectively eliminates a significant number of features while improving PI\nquality in case of high-dimensional dataset. Finally we extend the SVM models\nin Conformal Regression setting for obtaining more stable prediction set with\nfinite test set guarantees. Extensive experiments on artificial, real-world\nbenchmark datasets compare the different characteristics of both existing and\nproposed SVM-based PI estimation methods and also highlight the advantages of\nthe feature selection in PI estimation. Furthermore, we compare both, the\nexisting and proposed SVM-based PI estimation models, with modern deep learning\nmodels for probabilistic forecasting tasks on benchmark datasets. Furthermore,\nSVM models show comparable or superior performance to modern complex deep\nlearning models for probabilistic forecasting task in our experiments."}
{"id": "2505.15437", "pdf": "https://arxiv.org/pdf/2505.15437", "abs": "https://arxiv.org/abs/2505.15437", "authors": ["Nikita Kotelevskii", "Mohsen Guizani", "Eric Moulines", "Maxim Panov"], "title": "Adaptive Temperature Scaling with Conformal Prediction", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Conformal prediction enables the construction of high-coverage prediction\nsets for any pre-trained model, guaranteeing that the true label lies within\nthe set with a specified probability. However, these sets do not provide\nprobability estimates for individual labels, limiting their practical use. In\nthis paper, we propose, to the best of our knowledge, the first method for\nassigning calibrated probabilities to elements of a conformal prediction set.\nOur approach frames this as an adaptive calibration problem, selecting an\ninput-specific temperature parameter to match the desired coverage level.\nExperiments on several challenging image classification datasets demonstrate\nthat our method maintains coverage guarantees while significantly reducing\nexpected calibration error."}
{"id": "2505.15728", "pdf": "https://arxiv.org/pdf/2505.15728", "abs": "https://arxiv.org/abs/2505.15728", "authors": ["Luqin Gan", "Tarek M. Zikry", "Genevera I. Allen"], "title": "Are machine learning interpretations reliable? A stability study on global interpretations", "categories": ["stat.ML", "cs.LG", "stat.AP"], "comment": "17 pages main text, 5 main text figures. 57 pages in total with\n  Appendix and Bibliography", "summary": "As machine learning systems are increasingly used in high-stakes domains,\nthere is a growing emphasis placed on making them interpretable to improve\ntrust in these systems. In response, a range of interpretable machine learning\n(IML) methods have been developed to generate human-understandable insights\ninto otherwise black box models. With these methods, a fundamental question\narises: Are these interpretations reliable? Unlike with prediction accuracy or\nother evaluation metrics for supervised models, the proximity to the true\ninterpretation is difficult to define. Instead, we ask a closely related\nquestion that we argue is a prerequisite for reliability: Are these\ninterpretations stable? We define stability as findings that are consistent or\nreliable under small random perturbations to the data or algorithms. In this\nstudy, we conduct the first systematic, large-scale empirical stability study\non popular machine learning global interpretations for both supervised and\nunsupervised tasks on tabular data. Our findings reveal that popular\ninterpretation methods are frequently unstable, notably less stable than the\npredictions themselves, and that there is no association between the accuracy\nof machine learning predictions and the stability of their associated\ninterpretations. Moreover, we show that no single method consistently provides\nthe most stable interpretations across a range of benchmark datasets. Overall,\nthese results suggest that interpretability alone does not warrant trust, and\nunderscores the need for rigorous evaluation of interpretation stability in\nfuture work. To support these principles, we have developed and released an\nopen source IML dashboard and Python package to enable researchers to assess\nthe stability and reliability of their own data-driven interpretations and\ndiscoveries."}
{"id": "2505.15742", "pdf": "https://arxiv.org/pdf/2505.15742", "abs": "https://arxiv.org/abs/2505.15742", "authors": ["Adam Gould", "Francesca Toni"], "title": "Neuro-Argumentative Learning with Case-Based Reasoning", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted to NeSy25", "summary": "We introduce Gradual Abstract Argumentation for Case-Based Reasoning (Gradual\nAA-CBR), a data-driven, neurosymbolic classification model in which the outcome\nis determined by an argumentation debate structure that is learned\nsimultaneously with neural-based feature extractors. Each argument in the\ndebate is an observed case from the training data, favouring their labelling.\nCases attack or support those with opposing or agreeing labellings, with the\nstrength of each argument and relationship learned through gradient-based\nmethods. This argumentation debate structure provides human-aligned reasoning,\nimproving model interpretability compared to traditional neural networks (NNs).\nUnlike the existing purely symbolic variant, Abstract Argumentation for\nCase-Based Reasoning (AA-CBR), Gradual AA-CBR is capable of multi-class\nclassification, automatic learning of feature and data point importance,\nassigning uncertainty values to outcomes, using all available data points, and\ndoes not require binary features. We show that Gradual AA-CBR performs\ncomparably to NNs whilst significantly outperforming existing AA-CBR\nformulations."}
