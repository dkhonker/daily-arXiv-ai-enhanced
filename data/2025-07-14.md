<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 64]
- [cs.AI](#cs.AI) [总数: 22]
- [cs.CR](#cs.CR) [总数: 7]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [An Enhanced Privacy-preserving Federated Few-shot Learning Framework for Respiratory Disease Diagnosis](https://arxiv.org/abs/2507.08050)
*Ming Wang, Zhaoyang Duan, Dong Xue, Fangzhou Liu, Zhongheng Zhang*

**主要类别:** cs.LG

**AI概要:** This study proposes a federated few-shot learning framework with privacy-preserving mechanisms to address the issues of limited labeled data and privacy protection in diagnosing respiratory diseases.


<details>
  <summary>更多</summary>
  
**动机:** The labor-intensive nature of medical data annotation and patient privacy concerns present significant challenges for respiratory disease diagnosis, particularly in resource-constrained settings. Existing centralized data-driven approaches often compromise data privacy.

**方法:** A federated few-shot learning framework is proposed. Within this framework, a meta-stochastic gradient descent algorithm is used to mitigate overfitting problems from insufficient data. Differential privacy noise is integrated into the gradients during training to ensure data privacy against gradient leakage. A weighted average algorithm aggregates local diagnostic models from different clients.

**结果:** Experimental results show that the proposed method yields compelling results with the implementation of differential privacy, while effectively diagnosing respiratory diseases using data from different structures, categories, and distributions.

**结论:** The proposed federated few-shot learning framework with privacy-preserving mechanisms effectively addresses the issues of limited labeled data and privacy protection in diagnosing respiratory diseases, yielding compelling results with the implementation of differential privacy.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Enhanced+Privacy-preserving+Federated+Few-shot+Learning+Framework+for+Respiratory+Disease+Diagnosis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08050，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08050&send_immediately=true&force_search=false)

**原文摘要:** The labor-intensive nature of medical data annotation presents a significant
challenge for respiratory disease diagnosis, resulting in a scarcity of
high-quality labeled datasets in resource-constrained settings. Moreover,
patient privacy concerns complicate the direct sharing of local medical data
across institutions, and existing centralized data-driven approaches, which
rely on amounts of available data, often compromise data privacy. This study
proposes a federated few-shot learning framework with privacy-preserving
mechanisms to address the issues of limited labeled data and privacy protection
in diagnosing respiratory diseases. In particular, a meta-stochastic gradient
descent algorithm is proposed to mitigate the overfitting problem that arises
from insufficient data when employing traditional gradient descent methods for
neural network training. Furthermore, to ensure data privacy against gradient
leakage, differential privacy noise from a standard Gaussian distribution is
integrated into the gradients during the training of private models with local
data, thereby preventing the reconstruction of medical images. Given the
impracticality of centralizing respiratory disease data dispersed across
various medical institutions, a weighted average algorithm is employed to
aggregate local diagnostic models from different clients, enhancing the
adaptability of a model across diverse scenarios. Experimental results show
that the proposed method yields compelling results with the implementation of
differential privacy, while effectively diagnosing respiratory diseases using
data from different structures, categories, and distributions.

</details>


### [2] [Tree-Structured Parzen Estimator Can Solve Black-Box Combinatorial Optimization More Efficiently](https://arxiv.org/abs/2507.08053)
*Kenshin Abe, Yunzhuo Wang, Shuhei Watanabe*

**主要类别:** cs.LG

**AI概要:** This paper proposes an efficient combinatorial optimization algorithm for TPE, which generalizes the categorical kernel and reduces time complexity.


<details>
  <summary>更多</summary>
  
**动机:** Black-box combinatorial optimization is actively utilized in domains such as chemistry and biology but has not been addressed in TPE.

**方法:** The categorical kernel is generalized with the numerical kernel in TPE, introducing a distance structure to the categorical kernel. Modifications are made to handle large combinatorial search spaces.

**结果:** Experiments using synthetic problems show that the proposed method identifies better solutions with fewer evaluations than the original TPE.

**结论:** The proposed method for combinatorial optimization in TPE is effective and available in Optuna.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Tree-Structured+Parzen+Estimator+Can+Solve+Black-Box+Combinatorial+Optimization+More+Efficiently，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08053，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08053&send_immediately=true&force_search=false)

**原文摘要:** Tree-structured Parzen estimator (TPE) is a versatile hyperparameter
optimization (HPO) method supported by popular HPO tools. Since these HPO tools
have been developed in line with the trend of deep learning (DL), the problem
setups often used in the DL domain have been discussed for TPE such as
multi-objective optimization and multi-fidelity optimization. However, the
practical applications of HPO are not limited to DL, and black-box
combinatorial optimization is actively utilized in some domains, e.g.,
chemistry and biology. As combinatorial optimization has been an untouched, yet
very important, topic in TPE, we propose an efficient combinatorial
optimization algorithm for TPE. In this paper, we first generalize the
categorical kernel with the numerical kernel in TPE, enabling us to introduce a
distance structure to the categorical kernel. Then we discuss modifications for
the newly developed kernel to handle a large combinatorial search space. These
modifications reduce the time complexity of the kernel calculation with respect
to the size of a combinatorial search space. In the experiments using synthetic
problems, we verified that our proposed method identifies better solutions with
fewer evaluations than the original TPE. Our algorithm is available in Optuna,
an open-source framework for HPO.

</details>


### [3] [Quantile Reward Policy Optimization: Alignment with Pointwise Regression and Exact Partition Functions](https://arxiv.org/abs/2507.08068)
*Simon Matrenok, Skander Moalla, Caglar Gulcehre*

**主要类别:** cs.LG

**AI概要:** This paper introduces QRPO, a new method for aligning large language models with pointwise absolute rewards that preserves simplicity and offline applicability, achieving top performance and reducing length bias.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to bridge the gap between online, on-policy algorithms and simpler offline or off-policy methods for aligning large language models with pointwise absolute rewards.

**方法:** The paper introduces Quantile Reward Policy Optimization (QRPO), which uses quantile rewards to enable regression to the closed-form solution of the KL-regularized RL objective.

**结果:** Empirically, QRPO consistently achieves top performance on chat and coding evaluations across diverse datasets and 8B-scale models. Training with robust rewards instead of converting them to preferences induces less length bias.

**结论:** QRPO provides a new method for aligning large language models with pointwise absolute rewards, outperforming existing methods and reducing length bias.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quantile+Reward+Policy+Optimization%3A+Alignment+with+Pointwise+Regression+and+Exact+Partition+Functions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08068，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08068&send_immediately=true&force_search=false)

**原文摘要:** Aligning large language models with pointwise absolute rewards has so far
required online, on-policy algorithms such as PPO and GRPO. In contrast,
simpler methods that can leverage offline or off-policy data, such as DPO and
REBEL, are limited to learning from preference pairs or relative signals. To
bridge this gap, we introduce \emph{Quantile Reward Policy Optimization}
(QRPO), which learns from pointwise absolute rewards while preserving the
simplicity and offline applicability of DPO-like methods. QRPO uses quantile
rewards to enable regression to the closed-form solution of the KL-regularized
RL objective. This reward yields an analytically tractable partition function,
removing the need for relative signals to cancel this term. Moreover, QRPO
scales with increased compute to estimate quantile rewards, opening a new
dimension for pre-computation scaling. Empirically, QRPO consistently achieves
top performance on chat and coding evaluations -- reward model scores,
AlpacaEval 2, and LeetCode -- compared to DPO, REBEL, and SimPO across diverse
datasets and 8B-scale models. Finally, we find that training with robust
rewards instead of converting them to preferences induces less length bias.

</details>


### [4] [Low-rank Momentum Factorization for Memory Efficient Training](https://arxiv.org/abs/2507.08091)
*Pouria Mahdavinia, Mehrdad Mahdavi*

**主要类别:** cs.LG

**AI概要:** This paper introduces Momentum Factorized SGD (MoFaSGD), a memory-efficient fine-tuning method that dynamically updates the optimization subspace and performs spectrally normalized updates. MoFaSGD achieves optimal convergence rates and shows good performance in reducing memory usage while maintaining competitive model performance.


<details>
  <summary>更多</summary>
  
**动机:** Fine-tuning large foundation models faces memory challenges due to stateful optimizers like AdamW. Existing methods either struggle with fixed subspaces or are computationally costly. There is a need for a more efficient and adaptive method that can reduce memory usage while maintaining performance.

**方法:** The proposed method, Momentum Factorized SGD (MoFaSGD), maintains a dynamically updated low-rank SVD representation of the first-order momentum to closely approximate its full-rank counterpart. It uses this factorization to adaptively update the optimization subspace at each iteration and leverages the low-rank momentum factors for efficient spectrally normalized updates.

**结果:** MoFaSGD achieves an optimal rate for non-convex stochastic optimization and demonstrates competitive performance on large language model alignment benchmarks, with memory reduction comparable to LoRA.

**结论:** MoFaSGD provides an efficient and effective method for fine-tuning large models, offering a good trade-off between memory reduction and performance.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Low-rank+Momentum+Factorization+for+Memory+Efficient+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08091，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08091&send_immediately=true&force_search=false)

**原文摘要:** Fine-tuning large foundation models presents significant memory challenges
due to stateful optimizers like AdamW, often requiring several times more GPU
memory than inference. While memory-efficient methods like parameter-efficient
fine-tuning (e.g., LoRA) and optimizer state compression exist, recent
approaches like GaLore bridge these by using low-rank gradient projections and
subspace moment accumulation. However, such methods may struggle with fixed
subspaces or computationally costly offline resampling (e.g., requiring
full-matrix SVDs). We propose Momentum Factorized SGD (MoFaSGD), which
maintains a dynamically updated low-rank SVD representation of the first-order
momentum, closely approximating its full-rank counterpart throughout training.
This factorization enables a memory-efficient fine-tuning method that
adaptively updates the optimization subspace at each iteration. Crucially,
MoFaSGD leverages the computed low-rank momentum factors to perform efficient
spectrally normalized updates, offering an alternative to subspace moment
accumulation. We establish theoretical convergence guarantees for MoFaSGD,
proving it achieves an optimal rate for non-convex stochastic optimization
under standard assumptions. Empirically, we demonstrate MoFaSGD's effectiveness
on large language model alignment benchmarks, achieving a competitive trade-off
between memory reduction (comparable to LoRA) and performance compared to
state-of-the-art low-rank optimization methods. Our implementation is available
at https://github.com/pmahdavi/MoFaSGD.

</details>


### [5] [PDE-aware Optimizer for Physics-informed Neural Networks](https://arxiv.org/abs/2507.08118)
*Hardik Shukla, Manurag Khullar, Vismay Churiwala*

**主要类别:** cs.LG

**AI概要:** A new PDE-aware optimizer improves training stability and accuracy in Physics-Informed Neural Networks.


<details>
  <summary>更多</summary>
  
**动机:** Standard optimizers struggle with stiff or ill-conditioned systems in PINNs, prompting the need for a specialized optimizer.

**方法:** A novel PDE-aware optimizer that adapts parameter updates based on per-sample PDE residual gradients.

**结果:** The PDE-aware optimizer achieves better convergence and lower errors compared to Adam and SOAP.

**结论:** The PDE-aware optimizer shows potential for enhancing PINN stability and accuracy, though further scaling is needed.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PDE-aware+Optimizer+for+Physics-informed+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08118，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08118&send_immediately=true&force_search=false)

**原文摘要:** Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework
for solving partial differential equations (PDEs) by embedding physical
constraints into the loss function. However, standard optimizers such as Adam
often struggle to balance competing loss terms, particularly in stiff or
ill-conditioned systems. In this work, we propose a PDE-aware optimizer that
adapts parameter updates based on the variance of per-sample PDE residual
gradients. This method addresses gradient misalignment without incurring the
heavy computational costs of second-order optimizers such as SOAP. We benchmark
the PDE-aware optimizer against Adam and SOAP on 1D Burgers', Allen-Cahn and
Korteweg-de Vries(KdV) equations. Across both PDEs, the PDE-aware optimizer
achieves smoother convergence and lower absolute errors, particularly in
regions with sharp gradients. Our results demonstrate the effectiveness of PDE
residual-aware adaptivity in enhancing stability in PINNs training. While
promising, further scaling on larger architectures and hardware accelerators
remains an important direction for future research.

</details>


### [6] [Quasi-Random Physics-informed Neural Networks](https://arxiv.org/abs/2507.08121)
*Tianchi Yu, Ivan Oseledets*

**主要类别:** cs.LG

**AI概要:** This paper introduces Quasi-Random Physics-Informed Neural Networks (QRPINNs), which utilize low-discrepancy sequences for improved sampling efficiency during the training of physics-informed neural networks.


<details>
  <summary>更多</summary>
  
**动机:** Physics-informed neural networks' performance is sensitive to point sampling. The impressive performance of quasi Monte-Carlo methods in high dimensional problems inspires the exploration of better sampling strategies.

**方法:** The paper proposes Quasi-Random Physics-Informed Neural Networks (QRPINNs), using low-discrepancy sequences for sampling instead of random points directly from the domain.

**结果:** QRPINNs have been theoretically proven to have a better convergence rate than PINNs and empirically shown to outperform PINNs and some adaptive sampling methods, especially in high-dimensional PDEs.

**结论:** Quasi-Random Physics-Informed Neural Networks (QRPINNs) provide a more effective sampling method for training physics-informed neural networks, particularly in high-dimensional problems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quasi-Random+Physics-informed+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08121，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08121&send_immediately=true&force_search=false)

**原文摘要:** Physics-informed neural networks have shown promise in solving partial
differential equations (PDEs) by integrating physical constraints into neural
network training, but their performance is sensitive to the sampling of points.
Based on the impressive performance of quasi Monte-Carlo methods in high
dimensional problems, this paper proposes Quasi-Random Physics-Informed Neural
Networks (QRPINNs), which use low-discrepancy sequences for sampling instead of
random points directly from the domain. Theoretically, QRPINNs have been proven
to have a better convergence rate than PINNs. Empirically, experiments
demonstrate that QRPINNs significantly outperform PINNs and some representative
adaptive sampling methods, especially in high-dimensional PDEs. Furthermore,
combining QRPINNs with adaptive sampling can further improve the performance.

</details>


### [7] [Physics-Informed Neural Networks with Hard Nonlinear Equality and Inequality Constraints](https://arxiv.org/abs/2507.08124)
*Ashfaq Iftakher, Rahul Golder, M. M. Faruque Hasan*

**主要类别:** cs.LG

**AI概要:** This paper introduces KKT-Hardnet, a PINN architecture that enforces strict constraint satisfaction up to machine precision, achieving higher accuracy and reliability in complex system modeling.


<details>
  <summary>更多</summary>
  
**动机:** Traditional PINNs do not guarantee strict constraint satisfaction, which can significantly degrade the reliability and consistency of model predictions in engineering systems.

**方法:** KKT-Hardnet is developed to enforce linear and nonlinear equality and inequality constraints up to machine precision using KKT conditions and log-exponential transformation.

**结果:** KKT-Hardnet achieves higher accuracy and strict constraint satisfaction compared to multilayer perceptrons and PINNs.

**结论:** KKT-Hardnet achieves higher accuracy and strict constraint satisfaction, allowing the integration of domain knowledge into machine learning for reliable hybrid modeling of complex systems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Physics-Informed+Neural+Networks+with+Hard+Nonlinear+Equality+and+Inequality+Constraints，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08124，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08124&send_immediately=true&force_search=false)

**原文摘要:** Traditional physics-informed neural networks (PINNs) do not guarantee strict
constraint satisfaction. This is problematic in engineering systems where minor
violations of governing laws can significantly degrade the reliability and
consistency of model predictions. In this work, we develop KKT-Hardnet, a PINN
architecture that enforces both linear and nonlinear equality and inequality
constraints up to machine precision. It leverages a projection onto the
feasible region through solving Karush-Kuhn-Tucker (KKT) conditions of a
distance minimization problem. Furthermore, we reformulate the nonlinear KKT
conditions using log-exponential transformation to construct a general sparse
system with only linear and exponential terms, thereby making the projection
differentiable. We apply KKT-Hardnet on both test problems and a real-world
chemical process simulation. Compared to multilayer perceptrons and PINNs,
KKT-Hardnet achieves higher accuracy and strict constraint satisfaction. This
approach allows the integration of domain knowledge into machine learning
towards reliable hybrid modeling of complex systems.

</details>


### [8] [ALCo-FM: Adaptive Long-Context Foundation Model for Accident Prediction](https://arxiv.org/abs/2507.08153)
*Pinaki Prasad Guha Neogi, Ahmad Mohammadshirazi, Rajiv Ramnath*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一个名为ALCo-FM的模型，它是一种统一的自适应长上下文基础模型，用于进行交通意外风险预测。


<details>
  <summary>更多</summary>
  
**动机:** 交通事故是罕见但具有高影响力事件，需要长上下文多模式推理来进行准确的风险预测。

**方法:** 该论文提出了一种统一的自适应长上下文基础模型ALCo-FM，通过计算波动性预评分来动态选择输入数据的上下文窗口，并通过浅交叉注意力编码和融合多模态数据。模型结合了局部GAT层和BigBird风格的稀疏全局变压器以及H3六边形网格，并使用蒙特卡洛dropout提高置信度。

**结果:** ALCo-FM模型在美国15个城市的数据显示下，取得了0.94的准确率、0.92的F1值和0.04的ECE，超过了20多个最先进的基线模型。

**结论:** ALCo-FM模型在大规模城市风险预测中表现优越，能够准确预测交通事故风险。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ALCo-FM%3A+Adaptive+Long-Context+Foundation+Model+for+Accident+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08153，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08153&send_immediately=true&force_search=false)

**原文摘要:** Traffic accidents are rare, yet high-impact events that require long-context
multimodal reasoning for accurate risk forecasting. In this paper, we introduce
ALCo-FM, a unified adaptive long-context foundation model that computes a
volatility pre-score to dynamically select context windows for input data and
encodes and fuses these multimodal data via shallow cross attention. Following
a local GAT layer and a BigBird-style sparse global transformer over H3
hexagonal grids, coupled with Monte Carlo dropout for confidence, the model
yields superior, well-calibrated predictions. Trained on data from 15 US cities
with a class-weighted loss to counter label imbalance, and fine-tuned with
minimal data on held-out cities, ALCo-FM achieves 0.94 accuracy, 0.92 F1, and
an ECE of 0.04, outperforming more than 20 state-of-the-art baselines in
large-scale urban risk prediction. Code and dataset are available at:
https://github.com/PinakiPrasad12/ALCo-FM

</details>


### [9] [Just Read the Question: Enabling Generalization to New Assessment Items with Text Awareness](https://arxiv.org/abs/2507.08154)
*Arisha Khan, Nathaniel Li, Tori Shen, Anna N. Rafferty*

**主要类别:** cs.LG

**AI概要:** This paper presents Text-LENS, an extension of the LENS model that leverages item text embeddings for educational assessment. It performs as well as LENS on seen items and better on unseen items, effectively predicting student performance on new items.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to tackle the challenge of incorporating new items in machine learning approaches in educational assessment, which traditionally rely heavily on historical data.

**方法:** Text-LENS is developed by extending the LENS partial variational auto-encoder for educational assessment to leverage item text embeddings.

**结果:** Text-LENS matches LENS' performance on seen items and improves upon it in a variety of conditions involving unseen items.

**结论:** Text-LENS effectively learns student proficiency from and makes predictions about student performance on new items.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Just+Read+the+Question%3A+Enabling+Generalization+to+New+Assessment+Items+with+Text+Awareness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08154，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08154&send_immediately=true&force_search=false)

**原文摘要:** Machine learning has been proposed as a way to improve educational assessment
by making fine-grained predictions about student performance and learning
relationships between items. One challenge with many machine learning
approaches is incorporating new items, as these approaches rely heavily on
historical data. We develop Text-LENS by extending the LENS partial variational
auto-encoder for educational assessment to leverage item text embeddings, and
explore the impact on predictive performance and generalization to previously
unseen items. We examine performance on two datasets: Eedi, a publicly
available dataset that includes item content, and LLM-Sim, a novel dataset with
test items produced by an LLM. We find that Text-LENS matches LENS' performance
on seen items and improves upon it in a variety of conditions involving unseen
items; it effectively learns student proficiency from and makes predictions
about student performance on new items.

</details>


### [10] [Emotion Recognition in Older Adults with Quantum Machine Learning and Wearable Sensors](https://arxiv.org/abs/2507.08175)
*Md. Saif Hassan Onim, Travis S. Humble, Himanshu Thapliyal*

**主要类别:** cs.LG

**AI概要:** This paper presents a method for inferring emotional states from physiological signals using quantum machine learning, offering better privacy and accuracy than classical methods.


<details>
  <summary>更多</summary>
  
**动机:** To explore a privacy-preserving alternative to conventional facial recognition techniques for inferring emotional states exclusively from physiological signals.

**方法:** Comparison of classical machine learning algorithms and hybrid quantum machine learning methods with a quantum kernel-based model.

**结果:** Quantum-enhanced SVM surpasses classical counterparts in classification performance across all emotion categories, even when trained on limited datasets. F1 scores over all classes are over 80% with around a maximum of 36% improvement in the recall values.

**结论:** The integration of wearable sensor data with quantum machine learning enhances accuracy and robustness in emotion recognition, holding promise for individuals with impaired communication abilities.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Emotion+Recognition+in+Older+Adults+with+Quantum+Machine+Learning+and+Wearable+Sensors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08175，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08175&send_immediately=true&force_search=false)

**原文摘要:** We investigate the feasibility of inferring emotional states exclusively from
physiological signals, thereby presenting a privacy-preserving alternative to
conventional facial recognition techniques. We conduct a performance comparison
of classical machine learning algorithms and hybrid quantum machine learning
(QML) methods with a quantum kernel-based model. Our results indicate that the
quantum-enhanced SVM surpasses classical counterparts in classification
performance across all emotion categories, even when trained on limited
datasets. The F1 scores over all classes are over 80% with around a maximum of
36% improvement in the recall values. The integration of wearable sensor data
with quantum machine learning not only enhances accuracy and robustness but
also facilitates unobtrusive emotion recognition. This methodology holds
promise for populations with impaired communication abilities, such as
individuals with Alzheimer's Disease and Related Dementias (ADRD) and veterans
with Post-Traumatic Stress Disorder (PTSD). The findings establish an early
foundation for passive emotional monitoring in clinical and assisted living
conditions.

</details>


### [11] [ADAPT: A Pseudo-labeling Approach to Combat Concept Drift in Malware Detection](https://arxiv.org/abs/2507.08597)
*Md Tanvirul Alam, Aritran Piplai, Nidhi Rastogi*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新的伪标签半监督算法ADAPT，用于解决恶意软件检测中的概念漂移问题，并在五个不同的数据集上进行了广泛的实验，结果表明该方法性能优越。


<details>
  <summary>更多</summary>
  
**动机:** 机器学习模型在恶意软件分类中被普遍使用，但它们因概念漂移而随时间退化的问题，需要频繁更新和昂贵的真实标注。尽管主动学习可以减少标注负担，但通过半监督学习利用未标记数据在恶意软件检测领域仍然是相对未被探索的方法。

**方法:** 本研究引入了ADAPT，一种新颖的伪标签半监督算法来应对概念漂移。

**结果:** 实验结果表明，我们的方法始终优于基线模型和竞争基准。

**结论:** ADAPT算法在五个不同的恶意软件检测数据集上表现优异，为机器学习模型对概念漂移的适应提供了有效的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ADAPT%3A+A+Pseudo-labeling+Approach+to+Combat+Concept+Drift+in+Malware+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08597，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08597&send_immediately=true&force_search=false)

**原文摘要:** Machine learning models are commonly used for malware classification;
however, they suffer from performance degradation over time due to concept
drift. Adapting these models to changing data distributions requires frequent
updates, which rely on costly ground truth annotations. While active learning
can reduce the annotation burden, leveraging unlabeled data through
semi-supervised learning remains a relatively underexplored approach in the
context of malware detection. In this research, we introduce \texttt{ADAPT}, a
novel pseudo-labeling semi-supervised algorithm for addressing concept drift.
Our model-agnostic method can be applied to various machine learning models,
including neural networks and tree-based algorithms. We conduct extensive
experiments on five diverse malware detection datasets spanning Android,
Windows, and PDF domains. The results demonstrate that our method consistently
outperforms baseline models and competitive benchmarks. This work paves the way
for more effective adaptation of machine learning models to concept drift in
malware detection.

</details>


### [12] [Rethinking Spatio-Temporal Anomaly Detection: A Vision for Causality-Driven Cybersecurity](https://arxiv.org/abs/2507.08177)
*Arun Vignesh Malarkkan, Haoyue Bai, Xinyuan Wang, Anjali Kaushik, Dongjie Wang, Yanjie Fu*

**主要类别:** cs.LG

**AI概要:** This paper addresses the resilience of cyber-physical systems against cyberattacks through spatio-temporal anomaly detection. It advocates for a causal learning approach to overcome the limitations of current data-driven methods, focusing on interpretability, adaptability, and robustness.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this paper is the increasing interconnection and spatial distribution of cyber-physical systems, which necessitates enhanced resilience against evolving cyberattacks. Current data-driven approaches face challenges in interpretability, adaptability to distribution shifts, and robustness under evolving system dynamics.

**方法:** The paper advocates for a causal learning perspective to advance anomaly detection in spatially distributed infrastructures, emphasizing structural cause-effect relationships. It identifies three key directions: causal graph profiling, multi-view fusion, and continual causal graph learning.

**结果:** By drawing on real-world insights from systems such as water treatment infrastructures, the paper illustrates how causal models provide early warning signals and root cause attribution, addressing the limitations of black-box detectors.

**结论:** The paper concludes by outlining a future research agenda focused on multi-modality, generative AI-driven, and scalable adaptive causal frameworks. The objective is to inspire a new research trajectory towards scalable, adaptive, explainable, and spatially grounded anomaly detection systems, promoting a paradigm shift in cybersecurity research.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+Spatio-Temporal+Anomaly+Detection%3A+A+Vision+for+Causality-Driven+Cybersecurity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08177，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08177&send_immediately=true&force_search=false)

**原文摘要:** As cyber-physical systems grow increasingly interconnected and spatially
distributed, ensuring their resilience against evolving cyberattacks has become
a critical priority. Spatio-Temporal Anomaly detection plays an important role
in ensuring system security and operational integrity. However, current
data-driven approaches, largely driven by black-box deep learning, face
challenges in interpretability, adaptability to distribution shifts, and
robustness under evolving system dynamics. In this paper, we advocate for a
causal learning perspective to advance anomaly detection in spatially
distributed infrastructures that grounds detection in structural cause-effect
relationships. We identify and formalize three key directions: causal graph
profiling, multi-view fusion, and continual causal graph learning, each
offering distinct advantages in uncovering dynamic cause-effect structures
across time and space. Drawing on real-world insights from systems such as
water treatment infrastructures, we illustrate how causal models provide early
warning signals and root cause attribution, addressing the limitations of
black-box detectors. Looking ahead, we outline the future research agenda
centered on multi-modality, generative AI-driven, and scalable adaptive causal
frameworks. Our objective is to lay a new research trajectory toward scalable,
adaptive, explainable, and spatially grounded anomaly detection systems. We
hope to inspire a paradigm shift in cybersecurity research, promoting
causality-driven approaches to address evolving threats in interconnected
infrastructures.

</details>


### [13] [CTRLS: Chain-of-Thought Reasoning via Latent State-Transition](https://arxiv.org/abs/2507.08182)
*Junda Wu, Yuxin Xiong, Xintong Li, Zhengmian Hu, Tong Yu, Rui Wang, Xiang Chen, Jingbo Shang, Julian McAuley*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新框架CTRLS，通过将链式思维推理形式化为带有潜在状态转移的马尔可夫决策过程，实现了更系统、更多样化的推理路径探索。


<details>
  <summary>更多</summary>
  
**动机:** 常规的CoT方法依赖于启发式采样，缺乏对推理转移的结构化建模，限制了系统地探索和发现多样且有效的推理轨迹的能力。因此，需要一种新的方法来改善这一状况。

**方法:** 本文提出了一种新的框架CTRLS，将链式思维推理形式化为带有潜在状态转移的马尔可夫决策过程，并采用分布强化学习进行有原则的、对状态敏感的探索。

**结果:** 实验表明，CTRLS在基准推理任务上提高了推理准确性、多样性和探索效率。理论分析提供了证据下界(ELBO)，从理论上支持了对潜在推理动态的转换感知建模。

**结论:** CTRLS框架通过强化学习策略提高了推理任务的准确性和多样性，为未来研究提供了理论和实践基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CTRLS%3A+Chain-of-Thought+Reasoning+via+Latent+State-Transition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08182，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08182&send_immediately=true&force_search=false)

**原文摘要:** Chain-of-thought (CoT) reasoning enables large language models (LLMs) to
break down complex problems into interpretable intermediate steps,
significantly enhancing model transparency and performance in reasoning tasks.
However, conventional CoT methods rely on heuristic sampling without structured
modeling of reasoning transitions, constraining their ability to systematically
explore and discover diverse and effective reasoning trajectories. In this
work, we introduce CTRLS, a framework that formulates CoT reasoning as a Markov
decision process (MDP) with latent state transitions, enabling principled and
state-aware exploration via distributional reinforcement learning. By modelling
reasoning actions as explicit probability distributions in latent space, our
approach explicitly models epistemic uncertainty, facilitating robust
exploration of the reasoning space. As part of our framework, we introduce an
on-policy reinforcement learning strategy incorporating epsilon-greedy
exploration and entropy-based regularization to iteratively refine latent state
transitions without requiring additional fine-tuning of the underlying LLM.
Theoretical analyses provide evidence lower bounds (ELBO), theoretically
grounding our transition-aware modeling of latent reasoning dynamics. Further
experiments demonstrate improvements in reasoning accuracy, diversity, and
exploration efficiency across benchmark reasoning tasks.

</details>


### [14] [EvA: Evolutionary Attacks on Graphs](https://arxiv.org/abs/2507.08212)
*Mohammad Sadegh Akhondzadeh, Soroush H. Zargarbashi, Jimin Cao, Aleksandar Bojchevski*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种进化攻击（EvA）方法，解决了现有攻击方法存在的问题，并在实验中取得了更好的效果。


<details>
  <summary>更多</summary>
  
**动机:** 即使图结构的微小扰动也可能导致图神经网络（GNNs）的准确性大幅下降。大多数现有的攻击方法利用梯度信息来扰动边，这将攻击的优化问题从离散空间放宽到连续空间，导致解决方案远离最优解，并限制了攻击对非可微目标的适应性。

**方法:** 我们引入了一种基于进化算法的简单而有效的增强方法来直接解决离散优化问题。我们的进化攻击（Evolutionary Attack，简称EvA）可以与任何黑盒模型和目标一起使用，消除了对可微代理损失的需求。

**结果:** EvA可以设计出两种新颖的攻击方法，减少鲁棒性证书的有效性并破坏共形集。该攻击的记忆复杂度与攻击预算呈线性关系。在我们的实验中，平均而言，EvA相比之前的最佳攻击方法额外降低了约11%的准确率。

**结论:** EvA在实验中平均比之前的最佳攻击方法额外降低了约11%的准确率，揭示了设计攻击方法中存在显著的未开发潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EvA%3A+Evolutionary+Attacks+on+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08212，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08212&send_immediately=true&force_search=false)

**原文摘要:** Even a slight perturbation in the graph structure can cause a significant
drop in the accuracy of graph neural networks (GNNs). Most existing attacks
leverage gradient information to perturb edges. This relaxes the attack's
optimization problem from a discrete to a continuous space, resulting in
solutions far from optimal. It also restricts the adaptability of the attack to
non-differentiable objectives. Instead, we introduce a few simple yet effective
enhancements of an evolutionary-based algorithm to solve the discrete
optimization problem directly. Our Evolutionary Attack (EvA) works with any
black-box model and objective, eliminating the need for a differentiable proxy
loss. This allows us to design two novel attacks that reduce the effectiveness
of robustness certificates and break conformal sets. The memory complexity of
our attack is linear in the attack budget. Among our experiments, EvA shows
$\sim$11\% additional drop in accuracy on average compared to the best previous
attack, revealing significant untapped potential in designing attacks.

</details>


### [15] [InsightBuild: LLM-Powered Causal Reasoning in Smart Building Systems](https://arxiv.org/abs/2507.08235)
*Pinaki Prasad Guha Neogi, Ahmad Mohammadshirazi, Rajiv Ramnath*

**主要类别:** cs.LG

**AI概要:** This paper proposes InsightBuild, a two-stage framework integrating causality analysis with a fine-tuned large language model to provide clear and actionable explanations of energy consumption patterns.


<details>
  <summary>更多</summary>
  
**动机:** Smart buildings generate vast streams of sensor and control data, but facility managers often lack clear explanations for anomalous energy usage.

**方法:** InsightBuild, a two-stage framework that integrates causality analysis with a fine-tuned large language model (LLM) to provide human-readable, causal explanations of energy consumption patterns.

**结果:** The evaluation on two real-world datasets demonstrates that combining explicit causal discovery with LLM-based natural language generation yields clear, precise explanations.

**结论:** Combining explicit causal discovery with LLM-based natural language generation yields clear, precise explanations that assist facility managers in diagnosing and mitigating energy inefficiencies.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是InsightBuild%3A+LLM-Powered+Causal+Reasoning+in+Smart+Building+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08235，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08235&send_immediately=true&force_search=false)

**原文摘要:** Smart buildings generate vast streams of sensor and control data, but
facility managers often lack clear explanations for anomalous energy usage. We
propose InsightBuild, a two-stage framework that integrates causality analysis
with a fine-tuned large language model (LLM) to provide human-readable, causal
explanations of energy consumption patterns. First, a lightweight causal
inference module applies Granger causality tests and structural causal
discovery on building telemetry (e.g., temperature, HVAC settings, occupancy)
drawn from Google Smart Buildings and Berkeley Office datasets. Next, an LLM,
fine-tuned on aligned pairs of sensor-level causes and textual explanations,
receives as input the detected causal relations and generates concise,
actionable explanations. We evaluate InsightBuild on two real-world datasets
(Google: 2017-2022; Berkeley: 2018-2020), using expert-annotated ground-truth
causes for a held-out set of anomalies. Our results demonstrate that combining
explicit causal discovery with LLM-based natural language generation yields
clear, precise explanations that assist facility managers in diagnosing and
mitigating energy inefficiencies.

</details>


### [16] [Self-Supervised Learning-Based Multimodal Prediction on Prosocial Behavior Intentions](https://arxiv.org/abs/2507.08238)
*Abinay Reddy Naini, Zhaobo K. Zheng, Teruhisa Misu, Kumar Akash*

**主要类别:** cs.LG

**AI概要:** 这篇论文针对移动场景中亲社会行为意图预测这一未被充分探索的领域，提出了一种自监督学习方法，通过利用现有生理和行为数据集的多模态数据，解决了数据稀缺的问题，为亲社会行为预测提供了更有效的基准，并为改进智能车辆系统和人机交互提供了宝贵的见解。


<details>
  <summary>更多</summary>
  
**动机:** 预测移动场景中的亲社会行为意图（如在路上帮助他人）是一个未被充分探索的领域，当前的研究面临一个主要限制：没有大型的、标记的亲社会行为数据集，小规模的数据集难以有效训练深度学习模型。

**方法:** 提出了一种自监督学习方法，利用现有的生理和行为数据集的多模态数据，通过在各种任务上预训练模型并使用较小的手动标记亲社会行为数据集进行微调来解决数据稀缺问题。

**结果:** 通过在各种任务上预训练模型并使用较小的手动标记亲社会行为数据集进行微调，显著提高了模型的性能。

**结论:** 提出的自监督学习方法可以显著提高模型性能，为亲社会行为预测提供了更有效的基准，并为改进智能车辆系统和人机交互提供了宝贵的见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-Supervised+Learning-Based+Multimodal+Prediction+on+Prosocial+Behavior+Intentions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08238，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08238&send_immediately=true&force_search=false)

**原文摘要:** Human state detection and behavior prediction have seen significant
advancements with the rise of machine learning and multimodal sensing
technologies. However, predicting prosocial behavior intentions in mobility
scenarios, such as helping others on the road, is an underexplored area.
Current research faces a major limitation. There are no large, labeled datasets
available for prosocial behavior, and small-scale datasets make it difficult to
train deep-learning models effectively. To overcome this, we propose a
self-supervised learning approach that harnesses multi-modal data from existing
physiological and behavioral datasets. By pre-training our model on diverse
tasks and fine-tuning it with a smaller, manually labeled prosocial behavior
dataset, we significantly enhance its performance. This method addresses the
data scarcity issue, providing a more effective benchmark for prosocial
behavior prediction, and offering valuable insights for improving intelligent
vehicle systems and human-machine interaction.

</details>


### [17] [Data Generation without Function Estimation](https://arxiv.org/abs/2507.08239)
*Hadi Daneshmand, Ashkan Soleymani*

**主要类别:** cs.LG

**AI概要:** This paper proposes an estimation-free generative method that uses deterministic updates with (inverse) gradient descent to transport a uniform distribution to arbitrary data distribution, avoiding function estimation, training neural networks, and even noise injection.


<details>
  <summary>更多</summary>
  
**动机:** Estimating the score function is a fundamental component of most generative models, but such function estimation is computationally and statistically challenging. The motivation is to avoid function estimation for data generation.

**方法:** A set of points whose locations are deterministically updated with (inverse) gradient descent is used to transport a uniform distribution to arbitrary data distribution.

**结果:** The proposed method built upon recent advances in the physics of interacting particles can leverage these advances to develop novel generative methods.

**结论:** The proposed estimation-free generative method can transport a uniform distribution to arbitrary data distribution without function estimation, training neural networks, and even noise injection.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Data+Generation+without+Function+Estimation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08239，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08239&send_immediately=true&force_search=false)

**原文摘要:** Estimating the score function (or other population-density-dependent
functions) is a fundamental component of most generative models. However, such
function estimation is computationally and statistically challenging. Can we
avoid function estimation for data generation? We propose an estimation-free
generative method: A set of points whose locations are deterministically
updated with (inverse) gradient descent can transport a uniform distribution to
arbitrary data distribution, in the mean field regime, without function
estimation, training neural networks, and even noise injection. The proposed
method is built upon recent advances in the physics of interacting particles.
We show, both theoretically and experimentally, that these advances can be
leveraged to develop novel generative methods.

</details>


### [18] [CoreSPECT: Enhancing Clustering Algorithms via an Interplay of Density and Geometry](https://arxiv.org/abs/2507.08243)
*Chandra Sekhar Mukherjee, Joonyoung Bae, Jiapeng Zhang*

**主要类别:** cs.LG

**AI概要:** CoreSPECT leverages the interaction between data distribution and geometry to enhance simple clustering algorithms, resulting in significant accuracy improvements.


<details>
  <summary>更多</summary>
  
**动机:** The interaction between distribution and geometry in clustering algorithms is often overlooked. By formalizing this interaction, an enhancement to existing clustering techniques can be achieved.

**方法:** The CoreSPECT framework applies simple clustering algorithms to strategically selected regions and uses a novel neighborhood graph based multi-layer propagation procedure to extend the partition.

**结果:** CoreSPECT improves the ARI of K-Means by 40% and of GMM by 14% on average. It consistently outperforms manifold-based and recent density-based clustering algorithms across 15 datasets from three domains.

**结论:** The CoreSPECT framework significantly enhances the performance of simple clustering algorithms like K-Means and GMM, often surpassing more complex algorithms. It is robust to noise and supported by theoretical guarantees.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CoreSPECT%3A+Enhancing+Clustering+Algorithms+via+an+Interplay+of+Density+and+Geometry，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08243，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08243&send_immediately=true&force_search=false)

**原文摘要:** Density and geometry have long served as two of the fundamental guiding
principles in clustering algorithm design, with algorithm usually focusing
either on the density structure of the data (e.g., HDBSCAN and Density Peak
Clustering) or the complexity of underlying geometry (e.g., manifold clustering
algorithms).
  In this paper, we identify and formalize a recurring but often overlooked
interaction between distribution and geometry and leverage this insight to
design our clustering enhancement framework CoreSPECT (Core Space
Projection-based Enhancement of Clustering Techniques). Our framework boosts
the performance of simple algorithms like K-Means and GMM by applying them to
strategically selected regions, then extending the partial partition to a
complete partition for the dataset using a novel neighborhood graph based
multi-layer propagation procedure.
  We apply our framework on 15 datasets from three different domains and obtain
consistent and substantial gain in clustering accuracy for both K-Means and
GMM. On average, our framework improves the ARI of K-Means by 40% and of GMM by
14%, often surpassing the performance of both manifold-based and recent
density-based clustering algorithms. We further support our framework with
initial theoretical guarantees, ablation to demonstrate the usefulness of the
individual steps and with evidence of robustness to noise.

</details>


### [19] [Quantum-Accelerated Neural Imputation with Large Language Models (LLMs)](https://arxiv.org/abs/2507.08255)
*Hossein Jamali*

**主要类别:** cs.LG

**AI概要:** This paper addresses the challenge of missing data in real-world datasets by introducing Quantum-UnIMP, which leverages quantum circuits to enhance data imputation.


<details>
  <summary>更多</summary>
  
**动机:** Missing data presents a critical challenge in real-world datasets, significantly degrading the performance of machine learning models. While Large Language Models (LLMs) have shown capabilities in tabular data imputation, their reliance on classical embedding methods often limits their ability to capture complex, non-linear correlations, particularly in mixed-type data scenarios.

**方法:** This paper introduces Quantum-UnIMP, a novel framework that integrates shallow quantum circuits into an LLM-based imputation architecture. The core innovation is replacing conventional classical input embeddings with quantum feature maps generated by an Instantaneous Quantum Polynomial (IQP) circuit.

**结果:** Experiments on benchmark mixed-type datasets show that Quantum-UnIMP reduces imputation error by up to 15.2% for numerical features (RMSE) and improves classification accuracy by 8.7% for categorical features (F1-Score) compared to state-of-the-art classical and LLM-based methods.

**结论:** Quantum-UnIMP demonstrates the potential of quantum-enhanced representations for complex data imputation tasks, even with near-term quantum hardware.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quantum-Accelerated+Neural+Imputation+with+Large+Language+Models+%28LLMs%29，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08255，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08255&send_immediately=true&force_search=false)

**原文摘要:** Missing data presents a critical challenge in real-world datasets,
significantly degrading the performance of machine learning models. While Large
Language Models (LLMs) have recently demonstrated remarkable capabilities in
tabular data imputation, exemplified by frameworks like UnIMP, their reliance
on classical embedding methods often limits their ability to capture complex,
non-linear correlations, particularly in mixed-type data scenarios encompassing
numerical, categorical, and textual features. This paper introduces
Quantum-UnIMP, a novel framework that integrates shallow quantum circuits into
an LLM-based imputation architecture. Our core innovation lies in replacing
conventional classical input embeddings with quantum feature maps generated by
an Instantaneous Quantum Polynomial (IQP) circuit. This approach enables the
model to leverage quantum phenomena such as superposition and entanglement,
thereby learning richer, more expressive representations of data and enhancing
the recovery of intricate missingness patterns. Our experiments on benchmark
mixed-type datasets demonstrate that Quantum-UnIMP reduces imputation error by
up to 15.2% for numerical features (RMSE) and improves classification accuracy
by 8.7% for categorical features (F1-Score) compared to state-of-the-art
classical and LLM-based methods. These compelling results underscore the
profound potential of quantum-enhanced representations for complex data
imputation tasks, even with near-term quantum hardware.

</details>


### [20] [A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy with SFT and Efficiency with Reinforcement Learning](https://arxiv.org/abs/2507.08267)
*Hiroshi Yoshihara, Taiki Yamaguchi, Yuichi Inoue*

**主要类别:** cs.LG

**AI概要:** The paper addresses enhancing LLMs' mathematical reasoning by combining extended SFT and RL through a new method called GRPO, showing superior model accuracy and token efficiency.


<details>
  <summary>更多</summary>
  
**动机:** Enhancing the mathematical reasoning of Large Language Models (LLMs) is a pivotal challenge in advancing AI capabilities

**方法:** a practical and effective training recipe that strategically integrates extended SFT with RL from online inference (GRPO)

**结果:** Our experiments reveal that extending SFT for as many as 10 epochs is crucial for performance breakthroughs, and that the primary role of GRPO in this framework is to optimize solution length.

**结论:** This work provides a battle-tested blueprint for developing state-of-the-art mathematical reasoners that are both exceptionally accurate and practically efficient.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Practical+Two-Stage+Recipe+for+Mathematical+LLMs%3A+Maximizing+Accuracy+with+SFT+and+Efficiency+with+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08267，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08267&send_immediately=true&force_search=false)

**原文摘要:** Enhancing the mathematical reasoning of Large Language Models (LLMs) is a
pivotal challenge in advancing AI capabilities. While Supervised Fine-Tuning
(SFT) and Reinforcement Learning (RL) are the dominant training paradigms, a
systematic methodology for combining them to maximize both accuracy and
efficiency remains largely unexplored. This paper introduces a practical and
effective training recipe that strategically integrates extended SFT with RL
from online inference (GRPO). We posit that these methods play complementary,
not competing, roles: a prolonged SFT phase first pushes the model's accuracy
to its limits, after which a GRPO phase dramatically improves token efficiency
while preserving this peak performance. Our experiments reveal that extending
SFT for as many as 10 epochs is crucial for performance breakthroughs, and that
the primary role of GRPO in this framework is to optimize solution length. The
efficacy of our recipe is rigorously validated through top-tier performance on
challenging benchmarks, including a high rank among over 2,200 teams in the
strictly leak-free AI Mathematical Olympiad (AIMO). This work provides the
community with a battle-tested blueprint for developing state-of-the-art
mathematical reasoners that are both exceptionally accurate and practically
efficient. To ensure full reproducibility and empower future research, we will
open-source our entire framework, including all code, model checkpoints, and
training configurations at
https://github.com/analokmaus/kaggle-aimo2-fast-math-r1.

</details>


### [21] [Data-Driven Dimensional Synthesis of Diverse Planar Four-bar Function Generation Mechanisms via Direct Parameterization](https://arxiv.org/abs/2507.08269)
*Woon Ryong Kim, Jaeheun Jung, Jeong Un Ha, Donghun Lee, Jae Kyung Shim*

**主要类别:** cs.LG

**AI概要:** A data-driven framework using supervised learning is proposed for dimensional synthesis of planar four-bar mechanisms, producing accurate and defect-free linkages.


<details>
  <summary>更多</summary>
  
**动机:** Dimensional synthesis of planar four-bar mechanisms is a challenging inverse problem in kinematics, requiring the determination of mechanism dimensions from desired motion specifications.

**方法:** A data-driven framework is proposed that uses an LSTM-based neural network and a Mixture of Experts (MoE) architecture to bypass traditional equation-solving and optimization.

**结果:** Experiments show that the approach produces accurate, defect-free linkages across various configurations.

**结论:** The proposed data-driven framework enables accurate and efficient dimensional synthesis of planar four-bar mechanisms, even for non-expert users.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Data-Driven+Dimensional+Synthesis+of+Diverse+Planar+Four-bar+Function+Generation+Mechanisms+via+Direct+Parameterization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08269，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08269&send_immediately=true&force_search=false)

**原文摘要:** Dimensional synthesis of planar four-bar mechanisms is a challenging inverse
problem in kinematics, requiring the determination of mechanism dimensions from
desired motion specifications. We propose a data-driven framework that bypasses
traditional equation-solving and optimization by leveraging supervised
learning. Our method combines a synthetic dataset, an LSTM-based neural network
for handling sequential precision points, and a Mixture of Experts (MoE)
architecture tailored to different linkage types. Each expert model is trained
on type-specific data and guided by a type-specifying layer, enabling both
single-type and multi-type synthesis. A novel simulation metric evaluates
prediction quality by comparing desired and generated motions. Experiments show
our approach produces accurate, defect-free linkages across various
configurations. This enables intuitive and efficient mechanism design, even for
non-expert users, and opens new possibilities for scalable and flexible
synthesis in kinematic design.

</details>


### [22] [Lightweight Safety Guardrails via Synthetic Data and RL-guided Adversarial Training](https://arxiv.org/abs/2507.08284)
*Aleksei Ilin, Gor Matevosyan, Xueying Ma, Vladimir Eremin, Suhaa Dada, Muqun Li, Riyaaz Shaik, Haluk Noyan Tokgozoglu*

**主要类别:** cs.LG

**AI概要:** This paper introduces a safety guardrail framework for language models using synthetic data generation and adversarial training, allowing small models to excel at content moderation.


<details>
  <summary>更多</summary>
  
**动机:** To introduce a lightweight yet highly effective safety guardrail framework for language models.

**方法:** The framework uses high-fidelity synthetic data generation and adversarial training. The synthetic data generation process begins with human-curated seed data, which undergoes query augmentation and paraphrasing to create diverse and contextually rich examples. Adversarial training employs reinforcement learning to guide a generator that produces challenging synthetic examples.

**结果:** Small-scale language models can achieve, and even surpass, the performance of larger counterparts in content moderation tasks.

**结论:** The framework enables small language models to serve as robust safety guardrails, offering a scalable and efficient solution for content moderation in AI systems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lightweight+Safety+Guardrails+via+Synthetic+Data+and+RL-guided+Adversarial+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08284，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08284&send_immediately=true&force_search=false)

**原文摘要:** We introduce a lightweight yet highly effective safety guardrail framework
for language models, demonstrating that small-scale language models can
achieve, and even surpass, the performance of larger counterparts in content
moderation tasks. This is accomplished through high-fidelity synthetic data
generation and adversarial training. The synthetic data generation process
begins with human-curated seed data, which undergoes query augmentation and
paraphrasing to create diverse and contextually rich examples. This augmented
data is then subjected to multiple rounds of curation, ensuring high fidelity
and relevance. Inspired by recent advances in the Generative Adversarial
Network (GAN) architecture, our adversarial training employs reinforcement
learning to guide a generator that produces challenging synthetic examples.
These examples are used to fine-tune the safety classifier, enhancing its
ability to detect and mitigate harmful content. Additionally, we incorporate
strategies from recent research on efficient LLM training, leveraging the
capabilities of smaller models to improve the performance of larger generative
models. With iterative adversarial training and the generation of diverse,
high-quality synthetic data, our framework enables small language models (SLMs)
to serve as robust safety guardrails. This approach not only reduces
computational overhead but also enhances resilience against adversarial
attacks, offering a scalable and efficient solution for content moderation in
AI systems.

</details>


### [23] [CAS Condensed and Accelerated Silhouette: An Efficient Method for Determining the Optimal K in K-Means Clustering](https://arxiv.org/abs/2507.08311)
*Krishnendu Das, Sumit Gupta, Awadhesh Kumar*

**主要类别:** cs.LG

**AI概要:** This paper introduces improvements to clustering techniques for text and image data to provide insights into better computational performance and cluster validity.


<details>
  <summary>更多</summary>
  
**动机:** Clustering accuracy remains a major challenge in large datasets. This paper presents a comprehensive overview of strategies for selecting the optimal value of k in clustering, with a focus on achieving a balance between clustering precision and computational efficiency in complex data environments.

**方法:** The proposed approach is based on the Condensed Silhouette method, along with statistical methods such as Local Structures, Gap Statistics, Class Consistency Ratio, and a Cluster Overlap Index CCR and COI-based algorithm to calculate the best value of k for K-Means clustering.

**结果:** The results of comparative experiments show that the proposed approach achieves up to 99 percent faster execution times on high-dimensional datasets while retaining both precision and scalability.

**结论:** The proposed approach is highly suitable for real-time clustering needs or scenarios demanding efficient clustering with minimal resource utilization.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CAS+Condensed+and+Accelerated+Silhouette%3A+An+Efficient+Method+for+Determining+the+Optimal+K+in+K-Means+Clustering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08311，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08311&send_immediately=true&force_search=false)

**原文摘要:** Clustering is a critical component of decision-making in todays data-driven
environments. It has been widely used in a variety of fields such as
bioinformatics, social network analysis, and image processing. However,
clustering accuracy remains a major challenge in large datasets. This paper
presents a comprehensive overview of strategies for selecting the optimal value
of k in clustering, with a focus on achieving a balance between clustering
precision and computational efficiency in complex data environments. In
addition, this paper introduces improvements to clustering techniques for text
and image data to provide insights into better computational performance and
cluster validity. The proposed approach is based on the Condensed Silhouette
method, along with statistical methods such as Local Structures, Gap
Statistics, Class Consistency Ratio, and a Cluster Overlap Index CCR and
COIbased algorithm to calculate the best value of k for K-Means clustering. The
results of comparative experiments show that the proposed approach achieves up
to 99 percent faster execution times on high-dimensional datasets while
retaining both precision and scalability, making it highly suitable for real
time clustering needs or scenarios demanding efficient clustering with minimal
resource utilization.

</details>


### [24] [A Comprehensively Adaptive Architectural Optimization-Ingrained Quantum Neural Network Model for Cloud Workloads Prediction](https://arxiv.org/abs/2507.08317)
*Jitendra Kumar, Deepika Saxena, Kishu Gupta, Satyam Kumar, Ashutosh Kumar Singh*

**主要类别:** cs.LG

**AI概要:** To address challenges with diverse, high-dimensional workloads during sudden resource demand changes, this work proposes a novel CA-QNN model.


<details>
  <summary>更多</summary>
  
**动机:** Accurate workload prediction and advanced resource reservation are indispensably crucial for managing dynamic cloud services.

**方法:** This work proposes a novel Comprehensively Adaptive Architectural Optimization-based Variable Quantum Neural Network (CA-QNN), which combines the efficiency of quantum computing with complete structural and qubit vector parametric learning.

**结果:** The performance of CA-QNN model is thoroughly investigated against seven state-of-the-art methods across four benchmark datasets of heterogeneous cloud workloads.

**结论:** The proposed CA-QNN model demonstrates superior prediction accuracy, reducing prediction errors by up to 93.40% and 91.27% compared to existing deep learning and QNN-based approaches.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Comprehensively+Adaptive+Architectural+Optimization-Ingrained+Quantum+Neural+Network+Model+for+Cloud+Workloads+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08317，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08317&send_immediately=true&force_search=false)

**原文摘要:** Accurate workload prediction and advanced resource reservation are
indispensably crucial for managing dynamic cloud services. Traditional neural
networks and deep learning models frequently encounter challenges with diverse,
high-dimensional workloads, especially during sudden resource demand changes,
leading to inefficiencies. This issue arises from their limited optimization
during training, relying only on parametric (inter-connection weights)
adjustments using conventional algorithms. To address this issue, this work
proposes a novel Comprehensively Adaptive Architectural Optimization-based
Variable Quantum Neural Network (CA-QNN), which combines the efficiency of
quantum computing with complete structural and qubit vector parametric
learning. The model converts workload data into qubits, processed through qubit
neurons with Controlled NOT-gated activation functions for intuitive pattern
recognition. In addition, a comprehensive architecture optimization algorithm
for networks is introduced to facilitate the learning and propagation of the
structure and parametric values in variable-sized QNNs. This algorithm
incorporates quantum adaptive modulation and size-adaptive recombination during
training process. The performance of CA-QNN model is thoroughly investigated
against seven state-of-the-art methods across four benchmark datasets of
heterogeneous cloud workloads. The proposed model demonstrates superior
prediction accuracy, reducing prediction errors by up to 93.40% and 91.27%
compared to existing deep learning and QNN-based approaches.

</details>


### [25] [scE$^2$TM: Toward Interpretable Single-Cell Embedding via Topic Modeling](https://arxiv.org/abs/2507.08355)
*Hegang Chen, Yuyin Lu, Zhiming Dai, Fu Lee Wang, Qing Li, Yanghui Rao*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的外部知识指导的单细胞嵌入主题模型scE2TM，该模型提供了高质量的细胞嵌入和强大的解释性，有助于全面的scRNA-seq数据分析。


<details>
  <summary>更多</summary>
  
**动机:** 随着深度学习模型复杂性和性能的快速提升，可解释性变得越来越重要。然而，之前的单细胞嵌入主题模型研究存在解释崩溃的问题，并且忽视了外部生物学知识，这限制了分析性能。

**方法:** 提出了一种新的外部知识指导的单细胞嵌入主题模型scE2TM，并提出了一种新的解释性评估基准，引入了10个指标来定量评估单细胞嵌入主题模型的解释性。

**结果:** 通过对20个scRNA-seq数据集的综合评估，相比于7种最先进的方法，scE2TM实现了显著的聚类性能提升。scE2TM在多样性以及与潜在生物学信号的一致性方面表现良好。

**结论:** scE2TM在单细胞嵌入主题模型的解释性方面表现优异，有助于更好地揭示潜在的生物学机制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是scE%24%5E2%24TM%3A+Toward+Interpretable+Single-Cell+Embedding+via+Topic+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08355，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08355&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in sequencing technologies have enabled researchers to
explore cellular heterogeneity at single-cell resolution. Meanwhile,
interpretability has gained prominence parallel to the rapid increase in the
complexity and performance of deep learning models. In recent years, topic
models have been widely used for interpretable single-cell embedding learning
and clustering analysis, which we refer to as single-cell embedded topic
models. However, previous studies evaluated the interpretability of the models
mainly through qualitative analysis, and these single-cell embedded topic
models suffer from the potential problem of interpretation collapse.
Furthermore, their neglect of external biological knowledge constrains
analytical performance. Here, we present scE2TM, an external knowledge-guided
single-cell embedded topic model that provides a high-quality cell embedding
and strong interpretation, contributing to comprehensive scRNA-seq data
analysis. Our comprehensive evaluation across 20 scRNA-seq datasets
demonstrates that scE2TM achieves significant clustering performance gains
compared to 7 state-of-the-art methods. In addition, we propose a new
interpretability evaluation benchmark that introduces 10 metrics to
quantitatively assess the interpretability of single-cell embedded topic
models. The results show that the interpretation provided by scE2TM performs
encouragingly in terms of diversity and consistency with the underlying
biological signals, contributing to a better revealing of the underlying
biological mechanisms.

</details>


### [26] [Leveraging Machine Learning and Enhanced Parallelism Detection for BPMN Model Generation from Text](https://arxiv.org/abs/2507.08362)
*Phuong Nam Lê, Charlotte Schneider-Depré, Alexandre Goossens, Alexander Stevens, Aurélie Leribaux, Johannes De Smedt*

**主要类别:** cs.LG

**AI概要:** This paper presents a machine learning-based automated pipeline for extracting BPMN models from text, introducing a newly annotated dataset that significantly enhances training.


<details>
  <summary>更多</summary>
  
**动机:** Efficient planning, resource management, and consistent operations often rely on converting textual process documents into formal Business Process Model and Notation (BPMN) models. However, this conversion process remains time-intensive and costly.

**方法:** This paper introduces an automated pipeline for extracting BPMN models from text, leveraging the use of machine learning and large language models.

**结果:** The proposed approach demonstrates adequate performance in terms of reconstruction accuracy.

**结论:** The proposed approach offers a promising foundation for organizations to accelerate BPMN model creation.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leveraging+Machine+Learning+and+Enhanced+Parallelism+Detection+for+BPMN+Model+Generation+from+Text，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08362，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08362&send_immediately=true&force_search=false)

**原文摘要:** Efficient planning, resource management, and consistent operations often rely
on converting textual process documents into formal Business Process Model and
Notation (BPMN) models. However, this conversion process remains time-intensive
and costly. Existing approaches, whether rule-based or machine-learning-based,
still struggle with writing styles and often fail to identify parallel
structures in process descriptions.
  This paper introduces an automated pipeline for extracting BPMN models from
text, leveraging the use of machine learning and large language models. A key
contribution of this work is the introduction of a newly annotated dataset,
which significantly enhances the training process. Specifically, we augment the
PET dataset with 15 newly annotated documents containing 32 parallel gateways
for model training, a critical feature often overlooked in existing datasets.
This addition enables models to better capture parallel structures, a common
but complex aspect of process descriptions. The proposed approach demonstrates
adequate performance in terms of reconstruction accuracy, offering a promising
foundation for organizations to accelerate BPMN model creation.

</details>


### [27] [Prediction of Lane Change Intentions of Human Drivers using an LSTM, a CNN and a Transformer](https://arxiv.org/abs/2507.08365)
*Francesco De Cristofaro, Felix Hofbaur, Aixi Yang, Arno Eichberger*

**主要类别:** cs.LG

**AI概要:** 本文探讨了在复杂交通环境中预测司机变道意图的不同神经网络架构，特别是LSTM、CNN和Transformer网络，并发现变压器网络性能最佳，尤其在减少过度拟合方面。


<details>
  <summary>更多</summary>
  
**动机:** 前车的车道变换对自动驾驶车辆的运动规划有重大影响，尤其是在复杂的交通情况下。预测车道变换将有利于公众的安全和效率。然而，与设定的预测时间相比，在一定时间间隔内预测操作的研究较少。此外，缺乏不同架构之间的性能对比以及如何正确选择此类模型的输入。

**方法:** 描述并实现了一个LSTM、CNN和变压器网络的结构，以预测驾驶员进行车道变换的意图。使用公开数据集（highD）准备数据，设计网络，并比较三种网络在不同输入数据配置下的结果。

**结果:** 研究发现变压器网络比其他网络表现更好，受过拟合的影响较小。不同输入配置的准确度从82.79%到96.73%不等，在精度和召回率方面总体表现良好。

**结论:** 变压器网络在预测车道变换意图方面优于其他网络，并且受过拟合影响较小。不同输入配置的准确度从82.79%到96.73%不等，在精度和召回率方面总体表现良好。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Prediction+of+Lane+Change+Intentions+of+Human+Drivers+using+an+LSTM%2C+a+CNN+and+a+Transformer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08365，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08365&send_immediately=true&force_search=false)

**原文摘要:** Lane changes of preceding vehicles have a great impact on the motion planning
of automated vehicles especially in complex traffic situations. Predicting them
would benefit the public in terms of safety and efficiency. While many research
efforts have been made in this direction, few concentrated on predicting
maneuvers within a set time interval compared to predicting at a set prediction
time. In addition, there exist a lack of comparisons between different
architectures to try to determine the best performing one and to assess how to
correctly choose the input for such models. In this paper the structure of an
LSTM, a CNN and a Transformer network are described and implemented to predict
the intention of human drivers to perform a lane change. We show how the data
was prepared starting from a publicly available dataset (highD), which features
were used, how the networks were designed and finally we compare the results of
the three networks with different configurations of input data. We found that
transformer networks performed better than the other networks and was less
affected by overfitting. The accuracy of the method spanned from $82.79\%$ to
$96.73\%$ for different input configurations and showed overall good
performances considering also precision and recall.

</details>


### [28] [Advances in Machine Learning: Where Can Quantum Techniques Help?](https://arxiv.org/abs/2507.08379)
*Samarth Kashyap, Rohit K Ramakrishnan, Kumari Jyoti, Apoorva D Patel*

**主要类别:** cs.LG

**AI概要:** This review explores the potential of Quantum Machine Learning (QML) to address the computational bottlenecks of classical machine learning, introducing the theoretical foundations and categorizing QML approaches. It critically evaluates key developments and discusses the challenges posed by NISQ devices, outlining future directions.


<details>
  <summary>更多</summary>
  
**动机:** QML aims to leverage quantum computational advantages to enhance data-driven tasks and address the computational bottlenecks of classical machine learning, particularly in processing complex datasets.

**方法:** Theoretical foundations of QML are introduced, including quantum data encoding, quantum learning theory and optimization techniques. The paper categorizes QML approaches based on data type and computational architecture and critically evaluates key developments such as Quantum Principal Component Analysis, quantum-enhanced sensing and applications in material science.

**结果:** Quantum computational advantages are problem-dependent. The paper evaluates the theoretical speed-ups and practical limitations of key developments such as Quantum Principal Component Analysis, quantum-enhanced sensing and applications in material science. It also discusses the challenges posed by Noisy Intermediate-Scale Quantum (NISQ) devices.

**结论:** While QML has significant potential for specific applications, its broader utility in real-world scenarios remains contingent on overcoming technological and methodological hurdles.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Advances+in+Machine+Learning%3A+Where+Can+Quantum+Techniques+Help%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08379，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08379&send_immediately=true&force_search=false)

**原文摘要:** Quantum Machine Learning (QML) represents a promising frontier at the
intersection of quantum computing and artificial intelligence, aiming to
leverage quantum computational advantages to enhance data-driven tasks. This
review explores the potential of QML to address the computational bottlenecks
of classical machine learning, particularly in processing complex datasets. We
introduce the theoretical foundations of QML, including quantum data encoding,
quantum learning theory and optimization techniques, while categorizing QML
approaches based on data type and computational architecture. It is
well-established that quantum computational advantages are problem-dependent,
and so potentially useful directions for QML need to be systematically
identified. Key developments, such as Quantum Principal Component Analysis,
quantum-enhanced sensing and applications in material science, are critically
evaluated for their theoretical speed-ups and practical limitations. The
challenges posed by Noisy Intermediate-Scale Quantum (NISQ) devices, including
hardware noise, scalability constraints and data encoding overheads, are
discussed in detail. We also outline future directions, emphasizing the need
for quantum-native algorithms, improved error correction, and realistic
benchmarks to bridge the gap between theoretical promise and practical
deployment. This comprehensive analysis underscores that while QML has
significant potential for specific applications such as quantum chemistry and
sensing, its broader utility in real-world scenarios remains contingent on
overcoming technological and methodological hurdles.

</details>


### [29] [Two-cluster test](https://arxiv.org/abs/2507.08382)
*Xinying Liu, Lianyu Hu, Mudi Jiang, Simen Zhang, Jun Lou, Zengyou He*

**主要类别:** cs.LG

**AI概要:** This paper introduces a new method for two-cluster testing in cluster analysis to reduce Type-I error rate.


<details>
  <summary>更多</summary>
  
**动机:** To overcome the bias of classic two-sample tests in the context of cluster analysis, which yield extremely smaller p-values and inflated Type-I error rate.

**方法:** A new method based on the boundary points between two subsets to derive an analytical p-value for the purpose of significance quantification.

**结果:** Experiments on both synthetic and real data sets show that the proposed test is able to significantly reduce the Type-I error rate, in comparison with several classic two-sample testing methods.

**结论:** The proposed two-cluster test is able to significantly reduce the Type-I error rate and has practical usage in tree-based interpretable clustering and significance-based hierarchical clustering.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Two-cluster+test，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08382，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08382&send_immediately=true&force_search=false)

**原文摘要:** Cluster analysis is a fundamental research issue in statistics and machine
learning. In many modern clustering methods, we need to determine whether two
subsets of samples come from the same cluster. Since these subsets are usually
generated by certain clustering procedures, the deployment of classic
two-sample tests in this context would yield extremely smaller p-values,
leading to inflated Type-I error rate. To overcome this bias, we formally
introduce the two-cluster test issue and argue that it is a totally different
significance testing issue from conventional two-sample test. Meanwhile, we
present a new method based on the boundary points between two subsets to derive
an analytical p-value for the purpose of significance quantification.
Experiments on both synthetic and real data sets show that the proposed test is
able to significantly reduce the Type-I error rate, in comparison with several
classic two-sample testing methods. More importantly, the practical usage of
such two-cluster test is further verified through its applications in
tree-based interpretable clustering and significance-based hierarchical
clustering.

</details>


### [30] [Online Pre-Training for Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2507.08387)
*Yongjae Shin, Jeonghye Kim, Whiyoung Jung, Sunghoon Hong, Deunsol Yoon, Youngsoo Jang, Geonhyeong Kim, Jongseong Chae, Youngchul Sung, Kanghoon Lee, Woohyung Lim*

**主要类别:** cs.LG

**AI概要:** This paper proposes a novel method, Online Pre-Training (OPT), to address the issue of inaccurate value estimation in offline pre-trained agents for Offline-to-Online RL, resulting in significant performance improvements.


<details>
  <summary>更多</summary>
  
**动机:** Offline pre-trained agents often underperform during online fine-tuning due to inaccurate value estimation caused by distribution shift. Random initialization can be more effective in certain cases.

**方法:** A new learning phase, Online Pre-Training, is introduced to train a new value function specifically for effective online fine-tuning.

**结果:** Implementation of OPT on TD3 and SPOT demonstrates an average 30% improvement in performance across a wide range of D4RL environments.

**结论:** The proposed method, Online Pre-Training (OPT), effectively addresses the issue of inaccurate value estimation in offline pre-trained agents, leading to significant performance improvements in Offline-to-Online RL.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Online+Pre-Training+for+Offline-to-Online+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08387，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08387&send_immediately=true&force_search=false)

**原文摘要:** Offline-to-online reinforcement learning (RL) aims to integrate the
complementary strengths of offline and online RL by pre-training an agent
offline and subsequently fine-tuning it through online interactions. However,
recent studies reveal that offline pre-trained agents often underperform during
online fine-tuning due to inaccurate value estimation caused by distribution
shift, with random initialization proving more effective in certain cases. In
this work, we propose a novel method, Online Pre-Training for Offline-to-Online
RL (OPT), explicitly designed to address the issue of inaccurate value
estimation in offline pre-trained agents. OPT introduces a new learning phase,
Online Pre-Training, which allows the training of a new value function tailored
specifically for effective online fine-tuning. Implementation of OPT on TD3 and
SPOT demonstrates an average 30% improvement in performance across a wide range
of D4RL environments, including MuJoCo, Antmaze, and Adroit.

</details>


### [31] [Space filling positionality and the Spiroformer](https://arxiv.org/abs/2507.08456)
*M. Maurin, M. Á. Evangelista-Alvarado, P. Suárez-Serrato*

**主要类别:** cs.LG

**AI概要:** 提出了一种在几何域（例如流形）上推广Transformer模型的解决方案，通过沿着空间填充曲线的注意力头。作为第一个实验示例，展示了遵循二维球面上极螺旋的Spiroformer。


<details>
  <summary>更多</summary>
  
**动机:** 在处理序列数据时，Transformer表现出色。然而，将其模型推广到几何域（如流形）时，我们遇到了没有明确定义的全局顺序的问题。

**方法:** 通过沿空间填充曲线的注意力头来解决这个问题。

**结果:** 作为第一个实验示例，展示了遵循二维球面上极螺旋的Spiroformer。

**结论:** 这种方法为在几何域上应用Transformer提供了一种新的方式。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Space+filling+positionality+and+the+Spiroformer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08456，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08456&send_immediately=true&force_search=false)

**原文摘要:** Transformers excel when dealing with sequential data. Generalizing
transformer models to geometric domains, such as manifolds, we encounter the
problem of not having a well-defined global order. We propose a solution with
attention heads following a space-filling curve. As a first experimental
example, we present the Spiroformer, a transformer that follows a polar spiral
on the $2$-sphere.

</details>


### [32] [Inference-Time Scaling of Diffusion Language Models with Particle Gibbs Sampling](https://arxiv.org/abs/2507.08390)
*Meihua Dang, Jiaqi Han, Minkai Xu, Kai Xu, Akash Srivastava, Stefano Ermon*

**主要类别:** cs.LG

**AI概要:** This paper explores sampling-based approaches for high-quality text generation from discrete diffusion models in reward-guided settings and proposes a novel inference-time scaling approach based on particle Gibbs sampling.


<details>
  <summary>更多</summary>
  
**动机:** Discrete diffusion models have not been extensively explored for inference-time scaling, especially in reward-guided settings. The authors aim to address this gap by studying sampling-based approaches for high-quality text generation.

**方法:** The paper introduces a novel inference-time scaling approach based on particle Gibbs sampling for discrete diffusion models, which iteratively refines full diffusion trajectories using conditional Sequential Monte Carlo as its transition mechanism.

**结果:** The method consistently outperforms prior inference-time strategies on reward-guided text generation tasks, achieving significant improvement in accuracy under varying compute budgets.

**结论:** The particle Gibbs sampling algorithm for discrete diffusion models significantly improves the accuracy of reward-guided text generation under varying compute budgets.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Inference-Time+Scaling+of+Diffusion+Language+Models+with+Particle+Gibbs+Sampling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08390，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08390&send_immediately=true&force_search=false)

**原文摘要:** Discrete diffusion models have emerged as a powerful paradigm for language
modeling, rivaling auto-regressive models by training-time scaling. However,
inference-time scaling in discrete diffusion models remains relatively
under-explored. In this work, we study sampling-based approaches for achieving
high-quality text generation from discrete diffusion models in reward-guided
settings. We introduce a novel inference-time scaling approach based on
particle Gibbs sampling for discrete diffusion models. The particle Gibbs
sampling algorithm iteratively refines full diffusion trajectories using
conditional Sequential Monte Carlo as its transition mechanism. This process
ensures that the updated samples progressively improve and move closer to the
reward-weighted target distribution. Unlike existing inference-time scaling
methods, which are often limited to single diffusion trajectories, our approach
leverages iterative refinement across multiple trajectories. Within this
framework, we further analyze the trade-offs between four key axes for
inference-time scaling under fixed compute budgets: particle Gibbs iterations,
particle count, denoising steps, and reward estimation cost. Empirically, our
method consistently outperforms prior inference-time strategies on
reward-guided text generation tasks, achieving significant improvement in
accuracy under varying compute budgets.

</details>


### [33] [Pre-Training LLMs on a budget: A comparison of three optimizers](https://arxiv.org/abs/2507.08472)
*Joel Schlotthauer, Christian Kroos, Chris Hinze, Viktor Hangya, Luzian Hahn, Fabian Küch*

**主要类别:** cs.LG

**AI概要:** This study compares three optimizers - AdamW, Lion, and Sophia - for pre-training LLMs. While all three optimizers performed similarly, Sophia had the lowest training and validation loss, Lion was fastest in terms of training GPU hours, and AdamW led to the best downstream evaluation results.


<details>
  <summary>更多</summary>
  
**动机:** To determine the best optimizer for reducing pre-training times for LLMs and achieving better-performing models.

**方法:** Comparison of three optimizers: AdamW, Lion, and Sophia with two different base architectures using Maximal Update Parametrization and smaller proxy models for hyperparameter tuning.

**结果:** Sophia optimizer had the lowest training and validation loss, Lion was fastest in terms of training GPU hours, and AdamW led to the best downstream evaluation results.

**结论:** Sophia optimizer exhibited the lowest training and validation loss, Lion was fastest in terms of training GPU hours but AdamW led to the best downstream evaluation results.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pre-Training+LLMs+on+a+budget%3A+A+comparison+of+three+optimizers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08472，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08472&send_immediately=true&force_search=false)

**原文摘要:** Optimizers play a decisive role in reducing pre-training times for LLMs and
achieving better-performing models. In this study, we compare three major
variants: the de-facto standard AdamW, the simpler Lion, developed through an
evolutionary search, and the second-order optimizer Sophia. For better
generalization, we train with two different base architectures and use a
single- and a multiple-epoch approach while keeping the number of tokens
constant. Using the Maximal Update Parametrization and smaller proxy models, we
tune relevant hyperparameters separately for each combination of base
architecture and optimizer. We found that while the results from all three
optimizers were in approximately the same range, Sophia exhibited the lowest
training and validation loss, Lion was fastest in terms of training GPU hours
but AdamW led to the best downstream evaluation results.

</details>


### [34] [RTNinja: a generalized machine learning framework for analyzing random telegraph noise signals in nanoelectronic devices](https://arxiv.org/abs/2507.08424)
*Anirudh Varanasi, Robin Degraeve, Philippe Roussel, Clement Merckling*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种名为RTNinja的全自动机器学习框架，用于无监督分析随机电报噪声信号。


<details>
  <summary>更多</summary>
  
**动机:** 随机电报噪声是纳米电子器件中普遍存在的变异性现象，传统的分析技术常常依赖于限制性假设或人工干预，限制了它们在复杂、有噪声的数据集上的适用性。

**方法:** RTNinja框架由两个模块化组件构成：LevelsExtractor（使用贝叶斯推断和模型选择来去噪并离散化信号）和SourcesMapper（通过概率聚类和优化推断源配置）。为了评估性能，研究人员开发了一个蒙特卡洛模拟器，生成涵盖广泛信噪比和源复杂性的标签数据集。

**结果:** 在7000个这样的数据集中，RTNinja始终表现出高保真信号重建和准确提取源幅度及活动模式的能力。

**结论:** RTNinja提供了一种强大、可扩展且与设备无关的工具，用于随机电报噪声的特性分析，能够实现大规模统计基准测试、以可靠性为中心的技术鉴定、预测故障建模和下一代纳米电子学中的器件物理探索。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RTNinja%3A+a+generalized+machine+learning+framework+for+analyzing+random+telegraph+noise+signals+in+nanoelectronic+devices，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08424，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08424&send_immediately=true&force_search=false)

**原文摘要:** Random telegraph noise is a prevalent variability phenomenon in
nanoelectronic devices, arising from stochastic carrier exchange at defect
sites and critically impacting device reliability and performance. Conventional
analysis techniques often rely on restrictive assumptions or manual
interventions, limiting their applicability to complex, noisy datasets. Here,
we introduce RTNinja, a generalized, fully automated machine learning framework
for the unsupervised analysis of random telegraph noise signals. RTNinja
deconvolves complex signals to identify the number and characteristics of
hidden individual sources, without requiring prior knowledge of the system. The
framework comprises two modular components: LevelsExtractor, which uses
Bayesian inference and model selection to denoise and discretize the signal;
and SourcesMapper, which infers source configurations through probabilistic
clustering and optimization. To evaluate performance, we developed a Monte
Carlo simulator that generates labeled datasets spanning broad signal-to-noise
ratios and source complexities; across 7000 such datasets, RTNinja consistently
demonstrated high-fidelity signal reconstruction and accurate extraction of
source amplitudes and activity patterns. Our results demonstrate that RTNinja
offers a robust, scalable, and device-agnostic tool for random telegraph noise
characterization, enabling large-scale statistical benchmarking,
reliability-centric technology qualification, predictive failure modeling, and
device physics exploration in next-generation nanoelectronics.

</details>


### [35] [KGRAG-Ex: Explainable Retrieval-Augmented Generation with Knowledge Graph-based Perturbations](https://arxiv.org/abs/2507.08443)
*Georgios Balanos, Evangelos Chasanis, Konstantinos Skianis, Evaggelia Pitoura*

**主要类别:** cs.LG

**AI概要:** This paper presents KGRAG-Ex, a RAG system that leverages a domain-specific knowledge graph to improve both factual grounding and explainability.


<details>
  <summary>更多</summary>
  
**动机:** Despite RAG's enhancement of language models through external information, explainability remains a challenge. Knowledge graphs offer structured representations that enable transparent retrieval paths and interpretable reasoning.

**方法:** The system identifies relevant entities and semantic paths in the KG, transforms them into pseudo-paragraphs to guide corpus retrieval, and incorporates perturbation-based explanation methods to assess the influence of specific KG-derived components on the generated answers.

**结果:** KGRAG-Ex shows improvements in both factual grounding and explainability. The experiments analyze the sensitivity of the system to different perturbation methods, the relationship between graph component importance and structural positions, the influence of semantic node types, and how graph metrics correspond to the influence of components within the explanations process.

**结论:** KGRAG-Ex improves the factual grounding and explainability of RAG systems by leveraging a domain-specific knowledge graph.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是KGRAG-Ex%3A+Explainable+Retrieval-Augmented+Generation+with+Knowledge+Graph-based+Perturbations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08443，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08443&send_immediately=true&force_search=false)

**原文摘要:** Retrieval-Augmented Generation (RAG) enhances language models by grounding
responses in external information, yet explainability remains a critical
challenge, particularly when retrieval relies on unstructured text. Knowledge
graphs (KGs) offer a solution by introducing structured, semantically rich
representations of entities and their relationships, enabling transparent
retrieval paths and interpretable reasoning. In this work, we present KGRAG-Ex,
a RAG system that improves both factual grounding and explainability by
leveraging a domain-specific KG constructed via prompt-based information
extraction. Given a user query, KGRAG-Ex identifies relevant entities and
semantic paths in the graph, which are then transformed into pseudo-paragraphs:
natural language representations of graph substructures that guide corpus
retrieval. To improve interpretability and support reasoning transparency, we
incorporate perturbation-based explanation methods that assess the influence of
specific KG-derived components on the generated answers. We conduct a series of
experiments to analyze the sensitivity of the system to different perturbation
methods, the relationship between graph component importance and their
structural positions, the influence of semantic node types, and how graph
metrics correspond to the influence of components within the explanations
process.

</details>


### [36] [Towards Collaborative Fairness in Federated Learning Under Imbalanced Covariate Shift](https://arxiv.org/abs/2507.08617)
*Tianrun Yu, Jiaqi Wang, Haoyu Wang, Mingquan Lin, Han Liu, Nelson S. Yee, Fenglong Ma*

**主要类别:** cs.LG

**AI概要:** This paper proposes FedAKD, a method designed to balance accurate prediction with collaborative fairness in federated learning by addressing the issue of imbalanced covariate shift.


<details>
  <summary>更多</summary>
  
**动机:** Existing approaches in federated learning often overlook imbalanced covariate shift. This motivates the design of FedAKD which balances accurate prediction with collaborative fairness.

**方法:** FedAKD consists of client and server updates. In the client update, it introduces an asynchronous knowledge distillation strategy based on the analysis that imbalanced covariate shift primarily arises from misclassified samples. The approach first applies traditional knowledge distillation to update client models while keeping the global model fixed. Next, it selects correctly predicted high-confidence samples and update the global model using these samples while keeping client models fixed. The server update aggregates all client models.

**结果:** Experimental results on public datasets (FashionMNIST and CIFAR10) and a real-world Electronic Health Records (EHR) dataset demonstrate that FedAKD significantly improves collaborative fairness, enhances predictive accuracy, and fosters client participation even under highly heterogeneous data distributions.

**结论:** FedAKD improves collaborative fairness, enhances predictive accuracy, and fosters client participation under heterogeneous data distributions.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Collaborative+Fairness+in+Federated+Learning+Under+Imbalanced+Covariate+Shift，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08617，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08617&send_immediately=true&force_search=false)

**原文摘要:** Collaborative fairness is a crucial challenge in federated learning. However,
existing approaches often overlook a practical yet complex form of
heterogeneity: imbalanced covariate shift. We provide a theoretical analysis of
this setting, which motivates the design of FedAKD (Federated Asynchronous
Knowledge Distillation)- simple yet effective approach that balances accurate
prediction with collaborative fairness. FedAKD consists of client and server
updates. In the client update, we introduce a novel asynchronous knowledge
distillation strategy based on our preliminary analysis, which reveals that
while correctly predicted samples exhibit similar feature distributions across
clients, incorrectly predicted samples show significant variability. This
suggests that imbalanced covariate shift primarily arises from misclassified
samples. Leveraging this insight, our approach first applies traditional
knowledge distillation to update client models while keeping the global model
fixed. Next, we select correctly predicted high-confidence samples and update
the global model using these samples while keeping client models fixed. The
server update simply aggregates all client models. We further provide a
theoretical proof of FedAKD's convergence. Experimental results on public
datasets (FashionMNIST and CIFAR10) and a real-world Electronic Health Records
(EHR) dataset demonstrate that FedAKD significantly improves collaborative
fairness, enhances predictive accuracy, and fosters client participation even
under highly heterogeneous data distributions.

</details>


### [37] [Scaling Attention to Very Long Sequences in Linear Time with Wavelet-Enhanced Random Spectral Attention (WERSA)](https://arxiv.org/abs/2507.08637)
*Vincenzo Dentamaro*

**主要类别:** cs.LG

**AI概要:** This paper introduces WERSA, a new method that reduces computational loads without compromising accuracy, making long-context models more practical and affordable, especially on low-resource hardware.


<details>
  <summary>更多</summary>
  
**动机:** Transformer models are computationally costly on long sequences since regular attention has quadratic $O(n^2)$ time complexity. The motivation is to enable successful long-sequence processing without the performance trade-off.

**方法:** Wavelet-Enhanced Random Spectral Attention (WERSA), a novel mechanism of linear $O(n)$ time complexity that merges content-adaptive random spectral features together with multi-resolution Haar wavelets and learnable parameters to selectively attend to informative scales of data while preserving linear efficiency.

**结果:** WERSA achieves best accuracy in all tests. On ArXiv classification, WERSA improves accuracy over vanilla attention by 1.2% (86.2% vs 85.0%) while cutting training time by 81% (296s vs 1554s) and FLOPS by 73.4% (26.2G vs 98.4G).

**结论:** WERSA makes possible more practical, more affordable, long-context models, in particular on low-resource hardware, for more sustainable and more scalable AI development.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scaling+Attention+to+Very+Long+Sequences+in+Linear+Time+with+Wavelet-Enhanced+Random+Spectral+Attention+%28WERSA%29，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08637，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08637&send_immediately=true&force_search=false)

**原文摘要:** Transformer models are computationally costly on long sequences since regular
attention has quadratic $O(n^2)$ time complexity. We introduce Wavelet-Enhanced
Random Spectral Attention (WERSA), a novel mechanism of linear $O(n)$ time
complexity that is pivotal to enable successful long-sequence processing
without the performance trade-off. WERSA merges content-adaptive random
spectral features together with multi-resolution Haar wavelets and learnable
parameters to selectively attend to informative scales of data while preserving
linear efficiency.
  Large-scale comparisons \textbf{on single GPU} and across various benchmarks
(vision, NLP, hierarchical reasoning) and various attention mechanisms (like
Multiheaded Attention, Flash-Attention-2, FNet, Linformer, Performer,
Waveformer), reveal uniform advantages of WERSA. It achieves best accuracy in
all tests. On ArXiv classification, WERSA improves accuracy over vanilla
attention by 1.2\% (86.2\% vs 85.0\%) while cutting training time by 81\% (296s
vs 1554s) and FLOPS by 73.4\% (26.2G vs 98.4G). Significantly, WERSA excels
where vanilla and FlashAttention-2 fail: on ArXiv-128k's extremely lengthy
sequences, it achieves best accuracy (79.1\%) and AUC (0.979) among viable
methods, operating on data that gives Out-Of-Memory errors to quadratic methods
while being \textbf{twice as fast} as Waveformer, its next-best competitor.
  By significantly reducing computational loads without compromising accuracy,
WERSA makes possible more practical, more affordable, long-context models, in
particular on low-resource hardware, for more sustainable and more scalable AI
development.

</details>


### [38] [Ranked Set Sampling-Based Multilayer Perceptron: Improving Generalization via Variance-Based Bounds](https://arxiv.org/abs/2507.08465)
*Feijiang Li, Liuya Zhang, Jieting Wang, Tao Yan, Yuhua Qian*

**主要类别:** cs.LG

**AI概要:** This paper proposes a new method called RSS-MLP that uses Rank Set Sampling to reduce the variance of empirical loss, enhancing the generalization ability of multilayer perceptrons.


<details>
  <summary>更多</summary>
  
**动机:** To enhance the generalization ability of MLP by reducing the variance of empirical loss, inspired by a new generalization error bound.

**方法:** Introduces an ordered structure in the training data set by Rank Set Sampling (RSS) to further reduce the variance of loss and develops a RSS-MLP method.

**结果:** Theoretical results show that the variance of empirical exponential loss and the logistic loss estimated by RSS are smaller than those estimated by SRS. Experimental results on twelve benchmark data sets validate the performance of RSS-MLP.

**结论:** The proposed RSS-MLP method is effective and rational, supported by both theoretical and experimental results.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Ranked+Set+Sampling-Based+Multilayer+Perceptron%3A+Improving+Generalization+via+Variance-Based+Bounds，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08465，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08465&send_immediately=true&force_search=false)

**原文摘要:** Multilayer perceptron (MLP), one of the most fundamental neural networks, is
extensively utilized for classification and regression tasks. In this paper, we
establish a new generalization error bound, which reveals how the variance of
empirical loss influences the generalization ability of the learning model.
Inspired by this learning bound, we advocate to reduce the variance of
empirical loss to enhance the ability of MLP. As is well-known, bagging is a
popular ensemble method to realize variance reduction. However, bagging
produces the base training data sets by the Simple Random Sampling (SRS)
method, which exhibits a high degree of randomness. To handle this issue, we
introduce an ordered structure in the training data set by Rank Set Sampling
(RSS) to further reduce the variance of loss and develop a RSS-MLP method.
Theoretical results show that the variance of empirical exponential loss and
the logistic loss estimated by RSS are smaller than those estimated by SRS,
respectively. To validate the performance of RSS-MLP, we conduct comparison
experiments on twelve benchmark data sets in terms of the two convex loss
functions under two fusion methods. Extensive experimental results and analysis
illustrate the effectiveness and rationality of the propose method.

</details>


### [39] [Monitoring Risks in Test-Time Adaptation](https://arxiv.org/abs/2507.08721)
*Mona Schirmer, Metod Jazbec, Christian A. Naesseth, Eric Nalisnick*

**主要类别:** cs.LG

**AI概要:** Proposing a TTA monitoring framework paired with risk monitoring tools to detect points of ultimate failure when deploying predictive models.


<details>
  <summary>更多</summary>
  
**动机:** To detect points of ultimate failure when deploying predictive models by pairing TTA with risk monitoring frameworks.

**方法:** Extending existing monitoring tools based on sequential testing with confidence sequences to accommodate scenarios in which the model is updated at test time and no test labels are available.

**结果:** Demonstrating the effectiveness of the proposed TTA monitoring framework across different datasets, distribution shift types, and TTA methods.

**结论:** The proposed TTA monitoring framework is effective across a representative set of datasets, distribution shift types, and TTA methods.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Monitoring+Risks+in+Test-Time+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08721，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08721&send_immediately=true&force_search=false)

**原文摘要:** Encountering shifted data at test time is a ubiquitous challenge when
deploying predictive models. Test-time adaptation (TTA) methods address this
issue by continuously adapting a deployed model using only unlabeled test data.
While TTA can extend the model's lifespan, it is only a temporary solution.
Eventually the model might degrade to the point that it must be taken offline
and retrained. To detect such points of ultimate failure, we propose pairing
TTA with risk monitoring frameworks that track predictive performance and raise
alerts when predefined performance criteria are violated. Specifically, we
extend existing monitoring tools based on sequential testing with confidence
sequences to accommodate scenarios in which the model is updated at test time
and no test labels are available to estimate the performance metrics of
interest. Our extensions unlock the application of rigorous statistical risk
monitoring to TTA, and we demonstrate the effectiveness of our proposed TTA
monitoring framework across a representative set of datasets, distribution
shift types, and TTA methods.

</details>


### [40] [Catastrophic Forgetting Mitigation Through Plateau Phase Activity Profiling](https://arxiv.org/abs/2507.08736)
*Idan Mashiach, Oren Glickman, Tom Tirer*

**主要类别:** cs.LG

**AI概要:** This paper proposes tracking parameters during the final training plateau to mitigate catastrophic forgetting, demonstrating superior performance in balancing forgetting prevention with strong performance on new tasks.


<details>
  <summary>更多</summary>
  
**动机:** Catastrophic forgetting in deep neural networks occurs when learning new tasks degrades performance on previously learned tasks due to knowledge overwriting. Regularization techniques aim to identify and constrain 'important' parameters to preserve previous knowledge.

**方法:** Proposes a novel perspective of tracking parameters during the final training plateau instead of throughout the entire training process.

**结果:** The approach achieves superior performance in balancing catastrophic forgetting mitigation with strong performance on newly learned tasks.

**结论:** Tracking parameters during the final training plateau is more effective for mitigating catastrophic forgetting while maintaining strong performance on new tasks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Catastrophic+Forgetting+Mitigation+Through+Plateau+Phase+Activity+Profiling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08736，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08736&send_immediately=true&force_search=false)

**原文摘要:** Catastrophic forgetting in deep neural networks occurs when learning new
tasks degrades performance on previously learned tasks due to knowledge
overwriting. Among the approaches to mitigate this issue, regularization
techniques aim to identify and constrain "important" parameters to preserve
previous knowledge. In the highly nonconvex optimization landscape of deep
learning, we propose a novel perspective: tracking parameters during the final
training plateau is more effective than monitoring them throughout the entire
training process. We argue that parameters that exhibit higher activity
(movement and variability) during this plateau reveal directions in the loss
landscape that are relatively flat, making them suitable for adaptation to new
tasks while preserving knowledge from previous ones. Our comprehensive
experiments demonstrate that this approach achieves superior performance in
balancing catastrophic forgetting mitigation with strong performance on newly
learned tasks.

</details>


### [41] [Evaluating SAE interpretability without explanations](https://arxiv.org/abs/2507.08473)
*Gonçalo Paulo, Nora Belrose*

**主要类别:** cs.LG

**AI概要:** This paper proposes a new method for evaluating the interpretability of sparse coders without relying on natural language explanations.


<details>
  <summary>更多</summary>
  
**动机:** There is a challenge in measuring the interpretability of Sparse autoencoders (SAEs) and transcoders due to the lack of consensus on benchmarks and the difficulty in separating the explanation generation process from the actual interpretability of the latents.

**方法:** Existing methods are adapted to assess the interpretability of sparse coders without requiring the generation of natural language explanations.

**结果:** The scores produced by the interpretability metrics were compared with human evaluations, providing insights for the community on improving evaluation techniques.

**结论:** The proposed method offers a more direct and potentially standardized way to assess the interpretability of sparse coders, which could help in improving the evaluation of these techniques.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+SAE+interpretability+without+explanations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08473，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08473&send_immediately=true&force_search=false)

**原文摘要:** Sparse autoencoders (SAEs) and transcoders have become important tools for
machine learning interpretability. However, measuring how interpretable they
are remains challenging, with weak consensus about which benchmarks to use.
Most evaluation procedures start by producing a single-sentence explanation for
each latent. These explanations are then evaluated based on how well they
enable an LLM to predict the activation of a latent in new contexts. This
method makes it difficult to disentangle the explanation generation and
evaluation process from the actual interpretability of the latents discovered.
In this work, we adapt existing methods to assess the interpretability of
sparse coders, with the advantage that they do not require generating natural
language explanations as an intermediate step. This enables a more direct and
potentially standardized assessment of interpretability. Furthermore, we
compare the scores produced by our interpretability metrics with human
evaluations across similar tasks and varying setups, offering suggestions for
the community on improving the evaluation of these techniques.

</details>


### [42] [Adaptive Nonlinear Vector Autoregression: Robust Forecasting for Noisy Chaotic Time Series](https://arxiv.org/abs/2507.08738)
*Azimov Sherkhon, Susana Lopez-Moreno, Eric Dolores-Cuenca, Sieun Lee, Sangil Kim*

**主要类别:** cs.LG

**AI概要:** This paper proposes an adaptive NVAR model that combines delay-embedded linear inputs with features generated by a shallow, learnable multi-layer perceptron (MLP), which enables the model to learn data-driven nonlinearities while preserving a simple readout structure. The model outperforms the standard NVAR in predictive accuracy and shows robust forecasting under noisy conditions.


<details>
  <summary>更多</summary>
  
**动机:** Nonlinear vector autoregression (NVAR) and reservoir computing (RC) have limitations in adaptability to high noise or real-world data and scale poorly in high-dimensional settings.

**方法:** The adaptive NVAR model combines delay-embedded linear inputs with features generated by a shallow, learnable multi-layer perceptron (MLP). The MLP and linear readout are jointly trained using gradient-based optimization.

**结果:** Initial experiments on chaotic systems tested under noise-free and synthetically noisy conditions showed that the adaptive model outperformed the standard NVAR in predictive accuracy.

**结论:** The adaptive NVAR model outperforms the standard NVAR in predictive accuracy and shows robust forecasting under noisy conditions with a lower observation frequency.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptive+Nonlinear+Vector+Autoregression%3A+Robust+Forecasting+for+Noisy+Chaotic+Time+Series，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08738，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08738&send_immediately=true&force_search=false)

**原文摘要:** Nonlinear vector autoregression (NVAR) and reservoir computing (RC) have
shown promise in forecasting chaotic dynamical systems, such as the Lorenz-63
model and El Nino-Southern Oscillation. However, their reliance on fixed
nonlinearities - polynomial expansions in NVAR or random feature maps in RC -
limits their adaptability to high noise or real-world data. These methods also
scale poorly in high-dimensional settings due to costly matrix inversion during
readout computation. We propose an adaptive NVAR model that combines
delay-embedded linear inputs with features generated by a shallow, learnable
multi-layer perceptron (MLP). The MLP and linear readout are jointly trained
using gradient-based optimization, enabling the model to learn data-driven
nonlinearities while preserving a simple readout structure. Unlike standard
NVAR, our approach avoids the need for an exhaustive and sensitive grid search
over ridge and delay parameters. Instead, tuning is restricted to neural
network hyperparameters, improving scalability. Initial experiments on chaotic
systems tested under noise-free and synthetically noisy conditions showed that
the adaptive model outperformed the standard NVAR in predictive accuracy and
showed robust forecasting under noisy conditions with a lower observation
frequency.

</details>


### [43] [SynBridge: Bridging Reaction States via Discrete Flow for Bidirectional Reaction Prediction](https://arxiv.org/abs/2507.08475)
*Haitao Lin, Junjie Wang, Zhifeng Gao, Xiaohong Ji, Rong Zhu, Linfeng Zhang, Guolin Ke, Weinan E*

**主要类别:** cs.LG

**AI概要:** A bidirectional flow-based generative model called SynBridge is proposed to achieve multi-task reaction prediction.


<details>
  <summary>更多</summary>
  
**动机:** To model the transition of states in chemical reactions which are inherently discrete and abrupt.

**方法:** SynBridge, a bidirectional flow-based generative model using a graph-to-graph transformer network architecture and discrete flow bridges.

**结果:** SynBridge achieves state-of-the-art performance in both forward and retrosynthesis tasks on three benchmark datasets.

**结论:** SynBridge is effective for modeling the transition of states in chemical reactions and achieves state-of-the-art performance in both forward and retrosynthesis tasks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SynBridge%3A+Bridging+Reaction+States+via+Discrete+Flow+for+Bidirectional+Reaction+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08475，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08475&send_immediately=true&force_search=false)

**原文摘要:** The essence of a chemical reaction lies in the redistribution and
reorganization of electrons, which is often manifested through electron
transfer or the migration of electron pairs. These changes are inherently
discrete and abrupt in the physical world, such as alterations in the charge
states of atoms or the formation and breaking of chemical bonds. To model the
transition of states, we propose SynBridge, a bidirectional flow-based
generative model to achieve multi-task reaction prediction. By leveraging a
graph-to-graph transformer network architecture and discrete flow bridges
between any two discrete distributions, SynBridge captures bidirectional
chemical transformations between graphs of reactants and products through the
bonds' and atoms' discrete states. We further demonstrate the effectiveness of
our method through extensive experiments on three benchmark datasets
(USPTO-50K, USPTO-MIT, Pistachio), achieving state-of-the-art performance in
both forward and retrosynthesis tasks. Our ablation studies and noise
scheduling analysis reveal the benefits of structured diffusion over discrete
spaces for reaction prediction.

</details>


### [44] [Penalizing Infeasible Actions and Reward Scaling in Reinforcement Learning with Offline Data](https://arxiv.org/abs/2507.08761)
*Jeonghye Kim, Yongjae Shin, Whiyoung Jung, Sunghoon Hong, Deunsol Yoon, Youngchul Sung, Kanghoon Lee, Woohyung Lim*

**主要类别:** cs.LG

**AI概要:** This paper addresses the issue of Q-value extrapolation errors in reinforcement learning with offline data by developing a new algorithm called PARS, which achieves superior performance on the D4RL benchmark.


<details>
  <summary>更多</summary>
  
**动机:** Reinforcement learning with offline data suffers from Q-value extrapolation errors, particularly linear extrapolation of the Q-function beyond the data range.

**方法:** The paper proposes a new algorithm called PARS, which combines RS-LN and PA to mitigate Q-value extrapolation errors.

**结果:** PARS demonstrates superior performance compared to state-of-the-art algorithms on the D4RL benchmark.

**结论:** The PARS algorithm shows superior performance in offline training and online fine-tuning, particularly excelling in the AntMaze Ultra task.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Penalizing+Infeasible+Actions+and+Reward+Scaling+in+Reinforcement+Learning+with+Offline+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08761，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08761&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning with offline data suffers from Q-value extrapolation
errors. To address this issue, we first demonstrate that linear extrapolation
of the Q-function beyond the data range is particularly problematic. To
mitigate this, we propose guiding the gradual decrease of Q-values outside the
data range, which is achieved through reward scaling with layer normalization
(RS-LN) and a penalization mechanism for infeasible actions (PA). By combining
RS-LN and PA, we develop a new algorithm called PARS. We evaluate PARS across a
range of tasks, demonstrating superior performance compared to state-of-the-art
algorithms in both offline training and online fine-tuning on the D4RL
benchmark, with notable success in the challenging AntMaze Ultra task.

</details>


### [45] [Efficient Deployment of Vision-Language Models on Mobile Devices: A Case Study on OnePlus 13R](https://arxiv.org/abs/2507.08505)
*Pablo Robin Guerrero, Yueyang Pan, Sanidhya Kashyap*

**主要类别:** cs.LG

**AI概要:** A survey of deployment frameworks for Vision-Language Models on mobile devices reveals significant performance bottlenecks.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to address the computational limitations and energy inefficiency in deploying Vision-Language Models on mobile devices, especially for real-time applications.

**方法:** The study evaluates deployment frameworks (llama.cpp, MLC-Imp, mllm) for VLMs on mobile devices by running representative workloads and measuring various performance metrics.

**结果:** Benchmarking revealed critical performance bottlenecks across frameworks with over-utilized CPUs and underutilized or saturated GPUs and NPUs.

**结论:** Current deployment frameworks for Vision-Language Models on mobile devices face significant challenges with CPU overuse and ineffective use of GPU and NPU accelerators, highlighting the need for optimized solutions.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Deployment+of+Vision-Language+Models+on+Mobile+Devices%3A+A+Case+Study+on+OnePlus+13R，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08505，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08505&send_immediately=true&force_search=false)

**原文摘要:** Vision-Language Models (VLMs) offer promising capabilities for mobile
devices, but their deployment faces significant challenges due to computational
limitations and energy inefficiency, especially for real-time applications.
This study provides a comprehensive survey of deployment frameworks for VLMs on
mobile devices, evaluating llama.cpp, MLC-Imp, and mllm in the context of
running LLaVA-1.5 7B, MobileVLM-3B, and Imp-v1.5 3B as representative workloads
on a OnePlus 13R. Each deployment framework was evaluated on the OnePlus 13R
while running VLMs, with measurements covering CPU, GPU, and NPU utilization,
temperature, inference time, power consumption, and user experience.
Benchmarking revealed critical performance bottlenecks across frameworks: CPU
resources were consistently over-utilized during token generation, while GPU
and NPU accelerators were largely unused. When the GPU was used, primarily for
image feature extraction, it was saturated, leading to degraded device
responsiveness. The study contributes framework-level benchmarks, practical
profiling tools, and an in-depth analysis of hardware utilization bottlenecks,
highlighting the consistent overuse of CPUs and the ineffective or unstable use
of GPUs and NPUs in current deployment frameworks.

</details>


### [46] [Optimistic Exploration for Risk-Averse Constrained Reinforcement Learning](https://arxiv.org/abs/2507.08793)
*James McCarthy, Radu Marinescu, Elizabeth Daly, Ivana Dusparic*

**主要类别:** cs.LG

**AI概要:** This paper proposes an exploration-based approach called Optimistic Risk-averse Actor Critic (ORAC) for Risk-averse Constrained Reinforcement Learning (RaCRL), which encourages the policy to explore uncertain regions of the environment to discover high reward states while satisfying safety constraints.


<details>
  <summary>更多</summary>
  
**动机:** Risk-aversion leads to conservative exploration of the environment, which typically results in converging to sub-optimal policies that fail to adequately maximise reward or fail to achieve the goal.

**方法:** Optimistic Risk-averse Actor Critic (ORAC) constructs an exploratory policy by maximising a local upper confidence bound of the state-action reward value function while minimising a local lower confidence bound of the risk-averse state-action cost value function.

**结果:** Experimental results demonstrate that the ORAC approach prevents convergence to sub-optimal policies and improves significantly the reward-cost trade-off in various continuous control tasks.

**结论:** The ORAC approach prevents convergence to sub-optimal policies and significantly improves the reward-cost trade-off in various continuous control tasks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimistic+Exploration+for+Risk-Averse+Constrained+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08793，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08793&send_immediately=true&force_search=false)

**原文摘要:** Risk-averse Constrained Reinforcement Learning (RaCRL) aims to learn policies
that minimise the likelihood of rare and catastrophic constraint violations
caused by an environment's inherent randomness. In general, risk-aversion leads
to conservative exploration of the environment which typically results in
converging to sub-optimal policies that fail to adequately maximise reward or,
in some cases, fail to achieve the goal. In this paper, we propose an
exploration-based approach for RaCRL called Optimistic Risk-averse Actor Critic
(ORAC), which constructs an exploratory policy by maximising a local upper
confidence bound of the state-action reward value function whilst minimising a
local lower confidence bound of the risk-averse state-action cost value
function. Specifically, at each step, the weighting assigned to the cost value
is increased or decreased if it exceeds or falls below the safety constraint
value. This way the policy is encouraged to explore uncertain regions of the
environment to discover high reward states whilst still satisfying the safety
constraints. Our experimental results demonstrate that the ORAC approach
prevents convergence to sub-optimal policies and improves significantly the
reward-cost trade-off in various continuous control tasks such as
Safety-Gymnasium and a complex building energy management environment
CityLearn.

</details>


### [47] [SFedKD: Sequential Federated Learning with Discrepancy-Aware Multi-Teacher Knowledge Distillation](https://arxiv.org/abs/2507.08508)
*Haotian Xu, Jinrui Zhou, Xichong Zhang, Mingjun Xiao, He Sun, Yin Xu*

**主要类别:** cs.LG

**AI概要:** 本文提出SFedKD，一种新的SFL框架，用以解决SFL在异构环境下的灾难性遗忘问题。


<details>
  <summary>更多</summary>
  
**动机:** SFL在异构环境中存在严重的灾难性遗忘问题，即模型容易忘记从前一客户端学到的知识。

**方法:** 提出了一种名为SFedKD的SFL框架，该框架通过选择前一轮的多个模型来指导当前轮的训练，并将单一教师解耦知识蒸馏方法扩展到多教师环境，根据教师和学生数据之间的类别分布差异对目标类和非目标类知识赋予不同的权重。此外，为了防止知识稀释，还设计了一种基于互补的教师选择机制。

**结果:** SFedKD能有效克服SFL中的灾难性遗忘问题，同时减少通信和计算成本。

**结论:** SFedKD有效地克服了SFL中的灾难性遗忘问题，并在实验中表现优于现有的FL方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SFedKD%3A+Sequential+Federated+Learning+with+Discrepancy-Aware+Multi-Teacher+Knowledge+Distillation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08508，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08508&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL) is a distributed machine learning paradigm which
coordinates multiple clients to collaboratively train a global model via a
central server. Sequential Federated Learning (SFL) is a newly-emerging FL
training framework where the global model is trained in a sequential manner
across clients. Since SFL can provide strong convergence guarantees under data
heterogeneity, it has attracted significant research attention in recent years.
However, experiments show that SFL suffers from severe catastrophic forgetting
in heterogeneous environments, meaning that the model tends to forget knowledge
learned from previous clients. To address this issue, we propose an SFL
framework with discrepancy-aware multi-teacher knowledge distillation, called
SFedKD, which selects multiple models from the previous round to guide the
current round of training. In SFedKD, we extend the single-teacher Decoupled
Knowledge Distillation approach to our multi-teacher setting and assign
distinct weights to teachers' target-class and non-target-class knowledge based
on the class distributional discrepancy between teacher and student data.
Through this fine-grained weighting strategy, SFedKD can enhance model training
efficacy while mitigating catastrophic forgetting. Additionally, to prevent
knowledge dilution, we eliminate redundant teachers for the knowledge
distillation and formalize it as a variant of the maximum coverage problem.
Based on the greedy strategy, we design a complementary-based teacher selection
mechanism to ensure that the selected teachers achieve comprehensive knowledge
space coverage while reducing communication and computational costs. Extensive
experiments show that SFedKD effectively overcomes catastrophic forgetting in
SFL and outperforms state-of-the-art FL methods.

</details>


### [48] [Recursive Reward Aggregation](https://arxiv.org/abs/2507.08537)
*Yuting Tang, Yivan Zhang, Johannes Ackermann, Yu-Jie Zhang, Soichiro Nishimori, Masashi Sugiyama*

**主要类别:** cs.LG

**AI概要:** This paper proposes an alternative approach for flexible behavior alignment in reinforcement learning by selecting appropriate reward aggregation functions, eliminating the need to modify the reward function.


<details>
  <summary>更多</summary>
  
**动机:** Aligning agent behavior with specific objectives in reinforcement learning typically requires careful design of the reward function, which can be challenging when the desired objectives are complex.

**方法:** An algebraic perspective on Markov decision processes is introduced to generalize the standard discounted sum to other recursive aggregations, allowing for flexible behavior alignment in RL.

**结果:** Experimental results demonstrate that the proposed approach effectively optimizes diverse objectives in both deterministic and stochastic settings.

**结论:** The proposed approach in this paper provides a flexible way for behavior alignment in reinforcement learning without the need to modify the reward function, which has great potential for real-world applications.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Recursive+Reward+Aggregation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08537，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08537&send_immediately=true&force_search=false)

**原文摘要:** In reinforcement learning (RL), aligning agent behavior with specific
objectives typically requires careful design of the reward function, which can
be challenging when the desired objectives are complex. In this work, we
propose an alternative approach for flexible behavior alignment that eliminates
the need to modify the reward function by selecting appropriate reward
aggregation functions. By introducing an algebraic perspective on Markov
decision processes (MDPs), we show that the Bellman equations naturally emerge
from the recursive generation and aggregation of rewards, allowing for the
generalization of the standard discounted sum to other recursive aggregations,
such as discounted max and Sharpe ratio. Our approach applies to both
deterministic and stochastic settings and integrates seamlessly with
value-based and actor-critic algorithms. Experimental results demonstrate that
our approach effectively optimizes diverse objectives, highlighting its
versatility and potential for real-world applications.

</details>


### [49] [CircFormerMoE: An End-to-End Deep Learning Framework for Circular RNA Splice Site Detection and Pairing in Plant Genomes](https://arxiv.org/abs/2507.08542)
*Tianyou Jiang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为CircFormerMoE的深度学习框架，用于直接从植物基因组DNA预测circRNAs。


<details>
  <summary>更多</summary>
  
**动机:** 现有的circRNA识别方法存在许多限制：不能直接从基因组DNA序列预测circRNAs，严重依赖RNA实验数据；涉及复杂的比对和过滤步骤，计算成本高；在大规模或全基因组circRNA预测方面效率低下。尤其在植物中，挑战更大，因为植物circRNA剪接位点常常缺乏人类mRNA剪接中看到的经典GT-AG基序，并且目前不存在具有强大泛化能力的高效深度学习模型。此外，目前已鉴定的植物circRNAs数量可能远远低于其真实丰度。

**方法:** 提出了一种基于transformers和混合专家（mixture-of-experts）的深度学习框架CircFormerMoE，用于直接从植物基因组DNA预测circRNAs。该框架包括两个子任务，即剪接位点检测（SSD）和剪接位点配对（SSP）。

**结果:** 该模型的有效性已在10个植物物种的基因数据上得到了验证。经过已知circRNA实例的训练，它还能够发现以前未注释的circRNAs。我们还对训练后的模型进行了可解释性分析，以研究导致其预测的序列模式。

**结论:** CircFormerMoE框架为植物circRNA的大规模发现提供了一种快速、准确的计算方法和工具，为植物功能基因组学和非编码RNA注释的未来研究奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CircFormerMoE%3A+An+End-to-End+Deep+Learning+Framework+for+Circular+RNA+Splice+Site+Detection+and+Pairing+in+Plant+Genomes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08542，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08542&send_immediately=true&force_search=false)

**原文摘要:** Circular RNAs (circRNAs) are important components of the non-coding RNA
regulatory network. Previous circRNA identification primarily relies on
high-throughput RNA sequencing (RNA-seq) data combined with alignment-based
algorithms that detect back-splicing signals. However, these methods face
several limitations: they can't predict circRNAs directly from genomic DNA
sequences and relies heavily on RNA experimental data; they involve high
computational costs due to complex alignment and filtering steps; and they are
inefficient for large-scale or genome-wide circRNA prediction. The challenge is
even greater in plants, where plant circRNA splice sites often lack the
canonical GT-AG motif seen in human mRNA splicing, and no efficient deep
learning model with strong generalization capability currently exists.
Furthermore, the number of currently identified plant circRNAs is likely far
lower than their true abundance. In this paper, we propose a deep learning
framework named CircFormerMoE based on transformers and mixture-of experts for
predicting circRNAs directly from plant genomic DNA. Our framework consists of
two subtasks known as splicing site detection (SSD) and splicing site pairing
(SSP). The model's effectiveness has been validated on gene data of 10 plant
species. Trained on known circRNA instances, it is also capable of discovering
previously unannotated circRNAs. In addition, we performed interpretability
analyses on the trained model to investigate the sequence patterns contributing
to its predictions. Our framework provides a fast and accurate computational
method and tool for large-scale circRNA discovery in plants, laying a
foundation for future research in plant functional genomics and non-coding RNA
annotation.

</details>


### [50] [STRAP: Spatial-Temporal Risk-Attentive Vehicle Trajectory Prediction for Autonomous Driving](https://arxiv.org/abs/2507.08563)
*Xinyi Ning, Zilin Bian, Kaan Ozbay, Semiha Ergan*

**主要类别:** cs.LG

**AI概要:** This paper proposes a spatial-temporal risk-attentive trajectory prediction framework for autonomous driving systems, which incorporates a risk potential field to improve prediction accuracy, especially in high-risk scenarios.


<details>
  <summary>更多</summary>
  
**动机:** Accurate vehicle trajectory prediction is essential for ensuring safety and efficiency in fully autonomous driving systems. Existing methods often neglect the potential risks posed by the uncertain or aggressive behaviors of surrounding vehicles.

**方法:** A novel spatial-temporal risk-attentive trajectory prediction framework that incorporates a risk potential field to assess perceived risks arising from behaviors of nearby vehicles.

**结果:** Experiments on the widely used NGSIM and HighD datasets demonstrate that our method reduces average prediction errors by 4.8% and 31.2% respectively compared to state-of-the-art approaches, especially in high-risk scenarios.

**结论:** The proposed framework provides interpretable, risk-aware predictions, contributing to more robust decision-making for autonomous driving systems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是STRAP%3A+Spatial-Temporal+Risk-Attentive+Vehicle+Trajectory+Prediction+for+Autonomous+Driving，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08563，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08563&send_immediately=true&force_search=false)

**原文摘要:** Accurate vehicle trajectory prediction is essential for ensuring safety and
efficiency in fully autonomous driving systems. While existing methods
primarily focus on modeling observed motion patterns and interactions with
other vehicles, they often neglect the potential risks posed by the uncertain
or aggressive behaviors of surrounding vehicles. In this paper, we propose a
novel spatial-temporal risk-attentive trajectory prediction framework that
incorporates a risk potential field to assess perceived risks arising from
behaviors of nearby vehicles. The framework leverages a spatial-temporal
encoder and a risk-attentive feature fusion decoder to embed the risk potential
field into the extracted spatial-temporal feature representations for
trajectory prediction. A risk-scaled loss function is further designed to
improve the prediction accuracy of high-risk scenarios, such as short relative
spacing. Experiments on the widely used NGSIM and HighD datasets demonstrate
that our method reduces average prediction errors by 4.8% and 31.2%
respectively compared to state-of-the-art approaches, especially in high-risk
scenarios. The proposed framework provides interpretable, risk-aware
predictions, contributing to more robust decision-making for autonomous driving
systems.

</details>


### [51] [AbbIE: Autoregressive Block-Based Iterative Encoder for Efficient Sequence Modeling](https://arxiv.org/abs/2507.08567)
*Preslav Aleksandrov, Meghdad Kurmanji, Fernando Garcia Redondo, David O'Shea, William Shen, Alex Iacob, Lorenzo Sani, Xinchi Qiu, Nicola Cancedda, Nicholas D. Lane*

**主要类别:** cs.LG

**AI概要:** This paper introduces AbbIE, a novel recursive generalization of the encoder-only Transformer architecture that achieves better perplexity and allows for dynamic scaling of compute resources.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind AbbIE is to achieve better perplexity than a standard Transformer and allow for the dynamic scaling of compute resources at test time, complementing the scaling of large language model (LLM) performance through parameter and token counts.

**方法:** The Autoregressive Block-Based Iterative Encoder (AbbIE) is introduced as a novel recursive generalization of the encoder-only Transformer architecture.

**结果:** AbbIE shows up to 12% improvement in zero-shot in-context learning tasks and up to 5% improvement in language perplexity compared to other iterative and standard methods.

**结论:** AbbIE opens a new avenue to Transformer performance scaling and shows significant improvements in zero-shot in-context learning tasks and language perplexity.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AbbIE%3A+Autoregressive+Block-Based+Iterative+Encoder+for+Efficient+Sequence+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08567，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08567&send_immediately=true&force_search=false)

**原文摘要:** We introduce the Autoregressive Block-Based Iterative Encoder (AbbIE), a
novel recursive generalization of the encoder-only Transformer architecture,
which achieves better perplexity than a standard Transformer and allows for the
dynamic scaling of compute resources at test time. This simple, recursive
approach is a complement to scaling large language model (LLM) performance
through parameter and token counts. AbbIE performs its iterations in latent
space, but unlike latent reasoning models, does not require a specialized
dataset or training protocol. We show that AbbIE upward generalizes (ability to
generalize to arbitrary iteration lengths) at test time by only using 2
iterations during train time, far outperforming alternative iterative methods.
AbbIE's ability to scale its computational expenditure based on the complexity
of the task gives it an up to \textbf{12\%} improvement in zero-shot in-context
learning tasks versus other iterative and standard methods and up to 5\%
improvement in language perplexity. The results from this study open a new
avenue to Transformer performance scaling. We perform all of our evaluations on
model sizes up to 350M parameters.

</details>


### [52] [Remote Sensing Reveals Adoption of Sustainable Rice Farming Practices Across Punjab, India](https://arxiv.org/abs/2507.08605)
*Ando Shah, Rajveer Singh, Akram Zaytar, Girmaw Abebe Tadesse, Caleb Robinson, Negar Tafti, Stephen A. Wood, Rahul Dodhia, Juan M. Lavista Ferres*

**主要类别:** cs.LG

**AI概要:** This paper presents a remote sensing framework to monitor sustainable water management practices in Punjab, India, achieving a 78% F1-score in distinguishing direct seeded rice from traditional methods and correlating well with government records.


<details>
  <summary>更多</summary>
  
**动机:** Limited data on the adoption rates of sustainable irrigation practices like direct seeded rice (DSR) and alternate wetting and drying (AWD) prevents evidence-based policymaking and targeted resource allocation.

**方法:** A novel remote sensing framework was developed using Sentinel-1 satellite imagery to monitor sustainable water management practices in Punjab, India.

**结果:** The approach achieved a 78% F1-score in distinguishing DSR from traditional puddled transplanted rice and showed strong correlation (Pearson=0.77, RBO= 0.77) with government records.

**结论:** The study provides a powerful tool for policymakers to track sustainable water management adoption, target interventions, and measure program impacts at scale.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Remote+Sensing+Reveals+Adoption+of+Sustainable+Rice+Farming+Practices+Across+Punjab%2C+India，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08605，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08605&send_immediately=true&force_search=false)

**原文摘要:** Rice cultivation consumes 24-30% of global freshwater, creating critical
water management challenges in major rice-producing regions. Sustainable
irrigation practices like direct seeded rice (DSR) and alternate wetting and
drying (AWD) can reduce water use by 20-40% while maintaining yields, helping
secure long-term agricultural productivity as water scarcity intensifies - a
key component of the Zero Hunger Sustainable Development Goal. However, limited
data on adoption rates of these practices prevents evidence-based policymaking
and targeted resource allocation. We developed a novel remote sensing framework
to monitor sustainable water management practices at scale in Punjab, India - a
region facing severe groundwater depletion of 41.6 cm/year. To collect
essential ground truth data, we partnered with the Nature Conservancy's
Promoting Regenerative and No-burn Agriculture (PRANA) program, which trained
approximately 1,400 farmers on water-saving techniques while documenting their
field-level practices. Using this data, we created a classification system with
Sentinel-1 satellite imagery that separates water management along sowing and
irrigation dimensions. Our approach achieved a 78% F1-score in distinguishing
DSR from traditional puddled transplanted rice without requiring prior
knowledge of planting dates. We demonstrated scalability by mapping DSR
adoption across approximately 3 million agricultural plots in Punjab, with
district-level predictions showing strong correlation (Pearson=0.77, RBO= 0.77)
with government records. This study provides policymakers with a powerful tool
to track sustainable water management adoption, target interventions, and
measure program impacts at scale.

</details>


### [53] [Emergent Natural Language with Communication Games for Improving Image Captioning Capabilities without Additional Data](https://arxiv.org/abs/2507.08610)
*Parag Dutta, Ambedkar Dukkipati*

**主要类别:** cs.LG

**AI概要:** This paper proposes LoGIC, a Multi-agent Reinforcement Learning game for unsupervised image captioning, showing improvements over existing methods.


<details>
  <summary>更多</summary>
  
**动机:** Considering the challenge of improving performance in existing labelled datasets for Vision Language Models (VLMs), it is essential to explore unsupervised image captioning performance.

**方法:** The proposed method, LoGIC (Lewis Communication Game for Image Captioning), consists of two agents, a 'speaker' and a 'listener', trained in a cooperative common-reward setting using the GRPO algorithm.

**结果:** Using pre-trained VLMs as the 'speaker' and a Large Language Model (LLM) for language understanding in the 'listener', a 46 BLEU score was achieved after fine-tuning using LoGIC without additional labels. Replacing the VLM from the 'speaker' with lightweight components obtained a 31 BLEU score in the unsupervised setting.

**结论:** The proposed LoGIC method shows significant improvements in unsupervised image captioning performance and opens up new possibilities for lightweight components in the 'speaker' role.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Emergent+Natural+Language+with+Communication+Games+for+Improving+Image+Captioning+Capabilities+without+Additional+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08610，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08610&send_immediately=true&force_search=false)

**原文摘要:** Image captioning is an important problem in developing various AI systems,
and these tasks require large volumes of annotated images to train the models.
Since all existing labelled datasets are already used for training the large
Vision Language Models (VLMs), it becomes challenging to improve the
performance of the same. Considering this, it is essential to consider the
unsupervised image captioning performance, which remains relatively
under-explored. To that end, we propose LoGIC (Lewis Communication Game for
Image Captioning), a Multi-agent Reinforcement Learning game. The proposed
method consists of two agents, a 'speaker' and a 'listener', with the objective
of learning a strategy for communicating in natural language. We train agents
in the cooperative common-reward setting using the GRPO algorithm and show that
improvement in image captioning performance emerges as a consequence of the
agents learning to play the game. We show that using pre-trained VLMs as the
'speaker' and Large Language Model (LLM) for language understanding in the
'listener', we achieved a $46$ BLEU score after fine-tuning using LoGIC without
additional labels, a $2$ units advantage in absolute metrics compared to the
$44$ BLEU score of the vanilla VLM. Additionally, we replace the VLM from the
'speaker' with lightweight components: (i) a ViT for image perception and (ii)
a GPT2 language generation, and train them from scratch using LoGIC, obtaining
a $31$ BLEU score in the unsupervised setting, a $10$ points advantage over
existing unsupervised image-captioning methods.

</details>


### [54] [Forget Me Not: Fighting Local Overfitting with Knowledge Fusion and Distillation](https://arxiv.org/abs/2507.08686)
*Uri Stern, Eli Corn, Daphna Weinshall*

**主要类别:** cs.LG

**AI概要:** This paper investigates local overfitting in deep learning models and proposes a two-step solution that improves performance without extra inference cost.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this research is the puzzling observation that deep neural networks do not overfit as frequently as expected despite their large capacity. This work explores whether overfitting might instead manifest locally within specific areas of the data space.

**方法:** The researchers introduce a novel score to measure the forgetting rate on validation data, capturing 'local overfitting.' They then propose a two-stage method involving aggregating model checkpoints into an ensemble and distilling this ensemble into a single model.

**结果:** Results show that the proposed method performs well across various datasets and architectures, particularly excelling when dealing with label noise, where it surpasses both the original model and independently trained ensembles.

**结论:** The study concludes that local overfitting can occur in deep neural networks, which isn't typically captured by conventional metrics. The introduced two-stage approach of Knowledge Fusion and Distillation effectively combats this issue, enhancing model performance without increasing inference costs.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Forget+Me+Not%3A+Fighting+Local+Overfitting+with+Knowledge+Fusion+and+Distillation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08686，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08686&send_immediately=true&force_search=false)

**原文摘要:** Overfitting in deep neural networks occurs less frequently than expected.
This is a puzzling observation, as theory predicts that greater model capacity
should eventually lead to overfitting -- yet this is rarely seen in practice.
But what if overfitting does occur, not globally, but in specific sub-regions
of the data space? In this work, we introduce a novel score that measures the
forgetting rate of deep models on validation data, capturing what we term local
overfitting: a performance degradation confined to certain regions of the input
space. We demonstrate that local overfitting can arise even without
conventional overfitting, and is closely linked to the double descent
phenomenon.
  Building on these insights, we introduce a two-stage approach that leverages
the training history of a single model to recover and retain forgotten
knowledge: first, by aggregating checkpoints into an ensemble, and then by
distilling it into a single model of the original size, thus enhancing
performance without added inference cost.
  Extensive experiments across multiple datasets, modern architectures, and
training regimes validate the effectiveness of our approach. Notably, in the
presence of label noise, our method -- Knowledge Fusion followed by Knowledge
Distillation -- outperforms both the original model and independently trained
ensembles, achieving a rare win-win scenario: reduced training and inference
complexity.

</details>


### [55] [Domain-Informed Operation Excellence of Gas Turbine System with Machine Learning](https://arxiv.org/abs/2507.08697)
*Waqar Muhammad Ashraf, Amir H. Keshavarzzadeh, Abdulelah S. Alshehri, Abdulrahman bin Jumah, Ramit Debnath, Vivek Dua*

**主要类别:** cs.LG

**AI概要:** Developed a MAD-OPT framework to incorporate domain knowledge into data-centric analytics for thermal power plants, demonstrating its effectiveness in optimizing process conditions.


<details>
  <summary>更多</summary>
  
**动机:** The domain-consistent adoption of artificial intelligence (AI) remains low in thermal power plants due to the black-box nature of AI algorithms and low representation of domain knowledge in conventional data-centric analytics.

**方法:** Developed a MAhalanobis Distance-based OPTimization (MAD-OPT) framework that incorporates Mahalanobis distance-based constraint to introduce domain knowledge into data-centric analytics.

**结果:** The MAD-OPT framework can estimate domain-informed optimal process conditions under different ambient conditions, with robust solutions as evaluated by Monte Carlo simulations. Comparable results were found when estimating optimal conditions beyond the design limit.

**结论:** This research advances the integration of data-driven domain knowledge into machine learning-powered analytics, paving the way for safe AI adoption in thermal power systems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Domain-Informed+Operation+Excellence+of+Gas+Turbine+System+with+Machine+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08697，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08697&send_immediately=true&force_search=false)

**原文摘要:** The domain-consistent adoption of artificial intelligence (AI) remains low in
thermal power plants due to the black-box nature of AI algorithms and low
representation of domain knowledge in conventional data-centric analytics. In
this paper, we develop a MAhalanobis Distance-based OPTimization (MAD-OPT)
framework that incorporates the Mahalanobis distance-based constraint to
introduce domain knowledge into data-centric analytics. The developed MAD-OPT
framework is applied to maximize thermal efficiency and minimize turbine heat
rate for a 395 MW capacity gas turbine system. We demonstrate that the MAD-OPT
framework can estimate domain-informed optimal process conditions under
different ambient conditions, and the optimal solutions are found to be robust
as evaluated by Monte Carlo simulations. We also apply the MAD-OPT framework to
estimate optimal process conditions beyond the design power generation limit of
the gas turbine system, and have found comparable results with the actual data
of the power plant. We demonstrate that implementing data-centric optimization
analytics without incorporating domain-informed constraints may provide
ineffective solutions that may not be implementable in the real operation of
the gas turbine system. This research advances the integration of the
data-driven domain knowledge into machine learning-powered analytics that
enhances the domain-informed operation excellence and paves the way for safe AI
adoption in thermal power systems.

</details>


### [56] [SPLASH! Sample-efficient Preference-based inverse reinforcement learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical demonstrations](https://arxiv.org/abs/2507.08707)
*Peter Crowley, Zachary Serlin, Tyler Paine, Makai Mann, Michael Benjamin, Calin Belta*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的逆向强化学习方法SPLASH，它弥补了IRL在长时段和对抗性任务上的不足，并通过实验验证了其优越性。


<details>
  <summary>更多</summary>
  
**动机:** 许多机器人能力属于长时段和/或对抗性任务类别中的一个或两个，这突显了IRL产生现场准备好的机器人代理的能力的一个关键缺陷。

**方法:** 引入了SPLASH，这是一种从次优的层级演示中学习长时段对抗任务的高效偏好逆向强化学习方法。

**结果:** 在模拟的海上捕获旗任务中对SPLASH进行了实证验证，并通过自主无人水面车辆的仿真到现实转换实验展示了其现实世界的适用性。

**结论:** SPLASH在从次优演示中学习奖励方面显著优于现有技术。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SPLASH%21+Sample-efficient+Preference-based+inverse+reinforcement+learning+for+Long-horizon+Adversarial+tasks+from+Suboptimal+Hierarchical+demonstrations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08707，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08707&send_immediately=true&force_search=false)

**原文摘要:** Inverse Reinforcement Learning (IRL) presents a powerful paradigm for
learning complex robotic tasks from human demonstrations. However, most
approaches make the assumption that expert demonstrations are available, which
is often not the case. Those that allow for suboptimality in the demonstrations
are not designed for long-horizon goals or adversarial tasks. Many desirable
robot capabilities fall into one or both of these categories, thus highlighting
a critical shortcoming in the ability of IRL to produce field-ready robotic
agents. We introduce Sample-efficient Preference-based inverse reinforcement
learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical
demonstrations (SPLASH), which advances the state-of-the-art in learning from
suboptimal demonstrations to long-horizon and adversarial settings. We
empirically validate SPLASH on a maritime capture-the-flag task in simulation,
and demonstrate real-world applicability with sim-to-real translation
experiments on autonomous unmanned surface vehicles. We show that our proposed
methods allow SPLASH to significantly outperform the state-of-the-art in reward
learning from suboptimal demonstrations.

</details>


### [57] [On the Effect of Regularization in Policy Mirror Descent](https://arxiv.org/abs/2507.08718)
*Jan Felix Kleuker, Aske Plaat, Thomas Moerland*

**主要类别:** cs.LG

**AI概要:** 提供策略镜像下降（PMD）中两种关键正则化组件之间相互作用的大规模实证分析，强调了在强化学习中开发更鲁棒算法的潜力，特别是在超参数敏感性方面。


<details>
  <summary>更多</summary>
  
**动机:** 尽管策略镜像下降（PMD）在理论上已被广泛研究，但实证调查仍然稀缺。因此，本文旨在填补这一空白。

**方法:** 通过在小型RL环境上运行超过500k个训练种子，对这两种正则化技术之间的相互作用进行大规模实证分析。

**结果:** 结果表明，尽管两个正则化器可以部分替代彼此，但它们的精确组合对于实现鲁棒性能至关重要。

**结论:** 两种正则化技术的精确组合对于实现鲁棒性能至关重要，强调了在强化学习中开发更鲁棒算法的潜力，特别是在超参数敏感性方面。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Effect+of+Regularization+in+Policy+Mirror+Descent，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08718，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08718&send_immediately=true&force_search=false)

**原文摘要:** Policy Mirror Descent (PMD) has emerged as a unifying framework in
reinforcement learning (RL) by linking policy gradient methods with a
first-order optimization method known as mirror descent. At its core, PMD
incorporates two key regularization components: (i) a distance term that
enforces a trust region for stable policy updates and (ii) an MDP regularizer
that augments the reward function to promote structure and robustness. While
PMD has been extensively studied in theory, empirical investigations remain
scarce. This work provides a large-scale empirical analysis of the interplay
between these two regularization techniques, running over 500k training seeds
on small RL environments. Our results demonstrate that, although the two
regularizers can partially substitute each other, their precise combination is
critical for achieving robust performance. These findings highlight the
potential for advancing research on more robust algorithms in RL, particularly
with respect to hyperparameter sensitivity.

</details>


### [58] [Partitioned Hybrid Quantum Fourier Neural Operators for Scientific Quantum Machine Learning](https://arxiv.org/abs/2507.08746)
*Paolo Marcandelli, Yuanchun He, Stefano Mariani, Martina Siena, Stefano Markidis*

**主要类别:** cs.LG

**AI概要:** This paper introduces PHQFNO, a generalization of QFNO that enables tunable quantum-classical hybridization. The method is evaluated on various equations and shows higher accuracy and stability compared to classical counterparts.


<details>
  <summary>更多</summary>
  
**动机:** To generalize the Quantum Fourier Neural Operator (QFNO) for scientific machine learning and enable tunable quantum-classical hybridization and distributed execution across quantum and classical devices.

**方法:** The method extends QFNOs to higher dimensions, incorporates a message-passing framework to distribute data across different partitions, encodes input data into quantum states using unary encoding, and optimizes quantum circuit parameters using a variational scheme.

**结果:** PHQFNO achieves higher accuracy than its classical counterparts on incompressible Navier-Stokes equations and shows improved stability over classical baselines under input noise.

**结论:** PHQFNO recovers classical FNO accuracy and shows improved stability over classical baselines under input noise.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Partitioned+Hybrid+Quantum+Fourier+Neural+Operators+for+Scientific+Quantum+Machine+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08746，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08746&send_immediately=true&force_search=false)

**原文摘要:** We introduce the Partitioned Hybrid Quantum Fourier Neural Operator (PHQFNO),
a generalization of the Quantum Fourier Neural Operator (QFNO) for scientific
machine learning. PHQFNO partitions the Fourier operator computation across
classical and quantum resources, enabling tunable quantum-classical
hybridization and distributed execution across quantum and classical devices.
The method extends QFNOs to higher dimensions and incorporates a
message-passing framework to distribute data across different partitions. Input
data are encoded into quantum states using unary encoding, and quantum circuit
parameters are optimized using a variational scheme. We implement PHQFNO using
PennyLane with PyTorch integration and evaluate it on Burgers' equation,
incompressible and compressible Navier-Stokes equations. We show that PHQFNO
recovers classical FNO accuracy. On incompressible Navier-Stokes, PHQFNO
achieves higher accuracy than its classical counterparts. Finally, we perform a
sensitivity analysis under input noise, confirming improved stability of PHQFNO
over classical baselines.

</details>


### [59] [Modeling Partially Observed Nonlinear Dynamical Systems and Efficient Data Assimilation via Discrete-Time Conditional Gaussian Koopman Network](https://arxiv.org/abs/2507.08749)
*Chuanqi Chen, Zhongrui Wang, Nan Chen, Jin-Long Wu*

**主要类别:** cs.LG

**AI概要:** Developed a discrete-time conditional Gaussian Koopman network (CGKN) to learn surrogate models that can perform efficient state forecast and data assimilation (DA) for high-dimensional complex dynamical systems.


<details>
  <summary>更多</summary>
  
**动机:** To perform efficient state forecast and data assimilation (DA) for high-dimensional complex dynamical systems

**方法:** developed a discrete-time conditional Gaussian Koopman network (CGKN) to learn surrogate models

**结果:** Demonstrated the performance of discrete-time CGKN on several canonical problems governed by nonlinear PDEs with intermittency and turbulent features

**结论:** The discrete-time CGKN framework achieves comparable performance as the state-of-the-art SciML methods in state forecast and provides efficient and accurate DA results.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Modeling+Partially+Observed+Nonlinear+Dynamical+Systems+and+Efficient+Data+Assimilation+via+Discrete-Time+Conditional+Gaussian+Koopman+Network，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08749，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08749&send_immediately=true&force_search=false)

**原文摘要:** A discrete-time conditional Gaussian Koopman network (CGKN) is developed in
this work to learn surrogate models that can perform efficient state forecast
and data assimilation (DA) for high-dimensional complex dynamical systems,
e.g., systems governed by nonlinear partial differential equations (PDEs).
Focusing on nonlinear partially observed systems that are common in many
engineering and earth science applications, this work exploits Koopman
embedding to discover a proper latent representation of the unobserved system
states, such that the dynamics of the latent states are conditional linear,
i.e., linear with the given observed system states. The modeled system of the
observed and latent states then becomes a conditional Gaussian system, for
which the posterior distribution of the latent states is Gaussian and can be
efficiently evaluated via analytical formulae. The analytical formulae of DA
facilitate the incorporation of DA performance into the learning process of the
modeled system, which leads to a framework that unifies scientific machine
learning (SciML) and data assimilation. The performance of discrete-time CGKN
is demonstrated on several canonical problems governed by nonlinear PDEs with
intermittency and turbulent features, including the viscous Burgers' equation,
the Kuramoto-Sivashinsky equation, and the 2-D Navier-Stokes equations, with
which we show that the discrete-time CGKN framework achieves comparable
performance as the state-of-the-art SciML methods in state forecast and
provides efficient and accurate DA results. The discrete-time CGKN framework
also serves as an example to illustrate unifying the development of SciML
models and their other outer-loop applications such as design optimization,
inverse problems, and optimal control.

</details>


### [60] [ML-Based Automata Simplification for Symbolic Accelerators](https://arxiv.org/abs/2507.08751)
*Tiffany Yu, Rye Stahle-Smith, Darssan Eswaramoorthi, Rasha Karakchi*

**主要类别:** cs.LG

**AI概要:** This paper introduces AutoSlim, a machine learning framework that simplifies automata graphs for symbolic accelerators, reducing resource usage while maintaining semantic correctness.


<details>
  <summary>更多</summary>
  
**动机:** Symbolic accelerators face scalability issues due to high memory use and routing complexity, especially when targeting large data sets.

**方法:** AutoSlim uses machine learning (Random Forest classification) to perform graph simplification by pruning low-impact transitions based on edge scores and structural features.

**结果:** AutoSlim achieves up to 40% reduction in FPGA LUTs and over 30% pruning in transitions, scaling to graphs an order of magnitude larger than existing benchmarks.

**结论:** AutoSlim effectively reduces the complexity of symbolic accelerators on FPGA-based overlays, providing a scalable solution for large graphs.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ML-Based+Automata+Simplification+for+Symbolic+Accelerators，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08751，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08751&send_immediately=true&force_search=false)

**原文摘要:** Symbolic accelerators are increasingly used for symbolic data processing in
domains such as genomics, NLP, and cybersecurity. However, these accelerators
face scalability issues due to excessive memory use and routing complexity,
especially when targeting a large set. We present AutoSlim, a machine
learning-based graph simplification framework designed to reduce the complexity
of symbolic accelerators built on Non-deterministic Finite Automata (NFA)
deployed on FPGA-based overlays such as NAPOLY+. AutoSlim uses Random Forest
classification to prune low-impact transitions based on edge scores and
structural features, significantly reducing automata graph density while
preserving semantic correctness. Unlike prior tools, AutoSlim targets automated
score-aware simplification with weighted transitions, enabling efficient
ranking-based sequence analysis. We evaluated data sets (1K to 64K nodes) in
NAPOLY+ and conducted performance measurements including latency, throughput,
and resource usage. AutoSlim achieves up to 40 percent reduction in FPGA LUTs
and over 30 percent pruning in transitions, while scaling to graphs an order of
magnitude larger than existing benchmarks. Our results also demonstrate how
hardware interconnection (fanout) heavily influences hardware cost and that
AutoSlim's pruning mitigates resource blowup.

</details>


### [61] [BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity](https://arxiv.org/abs/2507.08771)
*Chenyang Song, Weilin Zhao, Xu Han, Chaojun Xiao, Yingfa Chen, Yuxuan Li, Zhiyuan Liu, Maosong Sun*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种名为BlockFFN的新MoE架构及其有效训练和部署技术，解决了大型语言模型的计算负担问题。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决大型语言模型（LLMs）的计算负担问题，以混合专家（MoE）为代表的激活稀疏性架构受到了越来越多的关注。然而，vanilla MoE的不可微分和不灵活的路由损害了模型性能。此外，尽管每个标记只激活少量参数，这些稀疏激活的架构表现出较低的块级稀疏性，这意味着多个连续标记的联合激活了大比例的参数。这种稀疏模式在资源条件低下（例如，终端设备）不利于加速，并且与主流加速技术（例如，推测解码）不兼容。

**方法:** 研究提出了一种新的MoE架构BlockFFN及其有效的训练和部署技术。具体来说，使用结合ReLU激活和RMSNorm的路由进行可微分且灵活的路由。为了促进令牌级稀疏性（TLS）和块级稀疏性（CLS），设计了对CLS有感知的训练目标，使BlockFFN更加友好加速。最后，实现了高效的加速核，首次结合了激活稀疏性和推测解码。

**结果:** BlockFFN在实验结果中优于其他MoE基线，实现了超过80%的TLS和70%的8-token CLS。我们的内核在实际端侧设备上比密集模型加速高达3.67倍。所有代码和检查点均公开可用。

**结论:** BlockFFN在实验中表现出比其他MoE基线更优越的性能，在实际终端设备上比密集模型快3.67倍。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BlockFFN%3A+Towards+End-Side+Acceleration-Friendly+Mixture-of-Experts+with+Chunk-Level+Activation+Sparsity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08771，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08771&send_immediately=true&force_search=false)

**原文摘要:** To alleviate the computational burden of large language models (LLMs),
architectures with activation sparsity, represented by mixture-of-experts
(MoE), have attracted increasing attention. However, the non-differentiable and
inflexible routing of vanilla MoE hurts model performance. Moreover, while each
token activates only a few parameters, these sparsely-activated architectures
exhibit low chunk-level sparsity, indicating that the union of multiple
consecutive tokens activates a large ratio of parameters. Such a sparsity
pattern is unfriendly for acceleration under low-resource conditions (e.g.,
end-side devices) and incompatible with mainstream acceleration techniques
(e.g., speculative decoding). To address these challenges, we introduce a novel
MoE architecture, BlockFFN, as well as its efficient training and deployment
techniques. Specifically, we use a router integrating ReLU activation and
RMSNorm for differentiable and flexible routing. Next, to promote both
token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training
objectives are designed, making BlockFFN more acceleration-friendly. Finally,
we implement efficient acceleration kernels, combining activation sparsity and
speculative decoding for the first time. The experimental results demonstrate
the superior performance of BlockFFN over other MoE baselines, achieving over
80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\times$ speedup on
real end-side devices than dense models. All codes and checkpoints are
available publicly (https://github.com/thunlp/BlockFFN).

</details>


### [62] [Greedy Low-Rank Gradient Compression for Distributed Learning with Convergence Guarantees](https://arxiv.org/abs/2507.08784)
*Chuyan Chen, Yutong He, Pengrui Li, Weichen Jia, Kun Yuan*

**主要类别:** cs.LG

**AI概要:** This paper introduces GreedyLore, a new algorithm for low-rank gradient compression in distributed learning that combines the strengths of both randomized and greedy methods while addressing their weaknesses.


<details>
  <summary>更多</summary>
  
**动机:** To address the gap between randomized and greedy compression strategies in low-rank gradient compression, which either introduce high variance or lack convergence guarantees.

**方法:** The paper proposes GreedyLore - the first Greedy Low-Rank gradient compression algorithm for distributed learning with rigorous convergence guarantees.

**结果:** With error feedback and semi-lazy subspace update techniques, GreedyLore achieves a convergence rate of $\mathcal{O}(\sigma/\sqrt{NT} + 1/T)$ under standard optimizers such as MSGD and Adam.

**结论:** GreedyLore provides a new approach to low-rank gradient compression with guaranteed convergence, making it a promising tool for distributed learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Greedy+Low-Rank+Gradient+Compression+for+Distributed+Learning+with+Convergence+Guarantees，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08784，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08784&send_immediately=true&force_search=false)

**原文摘要:** Distributed optimization is pivotal for large-scale signal processing and
machine learning, yet communication overhead remains a major bottleneck.
Low-rank gradient compression, in which the transmitted gradients are
approximated by low-rank matrices to reduce communication, offers a promising
remedy. Existing methods typically adopt either randomized or greedy
compression strategies: randomized approaches project gradients onto randomly
chosen subspaces, introducing high variance and degrading empirical
performance; greedy methods select the most informative subspaces, achieving
strong empirical results but lacking convergence guarantees. To address this
gap, we propose GreedyLore--the first Greedy Low-Rank gradient compression
algorithm for distributed learning with rigorous convergence guarantees.
GreedyLore incorporates error feedback to correct the bias introduced by greedy
compression and introduces a semi-lazy subspace update that ensures the
compression operator remains contractive throughout all iterations. With these
techniques, we prove that GreedyLore achieves a convergence rate of
$\mathcal{O}(\sigma/\sqrt{NT} + 1/T)$ under standard optimizers such as MSGD
and Adam--marking the first linear speedup convergence rate for low-rank
gradient compression. Extensive experiments are conducted to validate our
theoretical findings.

</details>


### [63] [One Token to Fool LLM-as-a-Judge](https://arxiv.org/abs/2507.08794)
*Yulai Zhao, Haolin Liu, Dian Yu, S. Y. Kung, Haitao Mi, Dong Yu*

**主要类别:** cs.LG

**AI概要:** Generative reward models are found to be vulnerable to superficial manipulations. A new robust generative reward model is trained to address this issue.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to address the vulnerabilities of generative reward models to superficial manipulations and improve their reliability for complex reasoning tasks.

**方法:** The method involves using large language models (LLMs) as judges in reinforcement learning with verifiable rewards (RLVR), and introducing a data augmentation strategy to train a new generative reward model.

**结果:** The result is the development of a new generative reward model with substantially improved robustness.

**结论:** The paper concludes that generative reward models are vulnerable to superficial manipulations, and to mitigate this issue, a new robust generative reward model is trained with improved reliability.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是One+Token+to+Fool+LLM-as-a-Judge，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08794，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08794&send_immediately=true&force_search=false)

**原文摘要:** Generative reward models (also known as LLMs-as-judges), which use large
language models (LLMs) to evaluate answer quality, are increasingly adopted in
reinforcement learning with verifiable rewards (RLVR). They are often preferred
over rigid rule-based metrics, especially for complex reasoning tasks involving
free-form outputs. In this paradigm, an LLM is typically prompted to compare a
candidate answer against a ground-truth reference and assign a binary reward
indicating correctness. Despite the seeming simplicity of this comparison task,
we find that generative reward models exhibit surprising vulnerabilities to
superficial manipulations: non-word symbols (e.g., ":" or ".") or reasoning
openers like "Thought process:" and "Let's solve this problem step by step."
can often lead to false positive rewards. We demonstrate that this weakness is
widespread across LLMs, datasets, and prompt formats, posing a serious threat
for core algorithmic paradigms that rely on generative reward models, such as
rejection sampling, preference optimization, and RLVR. To mitigate this issue,
we introduce a simple yet effective data augmentation strategy and train a new
generative reward model with substantially improved robustness. Our findings
highlight the urgent need for more reliable LLM-based evaluation methods. We
release our robust, general-domain reward model and its synthetic training data
at https://huggingface.co/sarosavo/Master-RM and
https://huggingface.co/datasets/sarosavo/Master-RM.

</details>


### [64] [The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability?](https://arxiv.org/abs/2507.08802)
*Denis Sutter, Julian Minder, Thomas Hofmann, Tiago Pimentel*

**主要类别:** cs.LG

**AI概要:** This paper critically examines the concept of causal abstraction and proves that it becomes vacuous without assumptions about how models encode information.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this paper is to demystify the opaque decision-making processes of machine learning models through the concept of causal abstraction.

**方法:** The authors critically examine the concept of causal abstraction by considering arbitrarily powerful alignment maps and prove that under reasonable assumptions, any neural network can be mapped to any algorithm. They also provide empirical evidence demonstrating that it is possible to perfectly map models to algorithms even when these models are incapable of solving the actual task.

**结果:** Under reasonable assumptions, any neural network can be mapped to any algorithm, rendering the unrestricted notion of causal abstraction trivial and uninformative. The empirical evidence shows that it is possible to perfectly map models to algorithms even when these models are incapable of solving the actual task.

**结论:** Causal abstraction is not enough for mechanistic interpretability, as it becomes vacuous without assumptions about how models encode information.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Non-Linear+Representation+Dilemma%3A+Is+Causal+Abstraction+Enough+for+Mechanistic+Interpretability%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08802，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08802&send_immediately=true&force_search=false)

**原文摘要:** The concept of causal abstraction got recently popularised to demystify the
opaque decision-making processes of machine learning models; in short, a neural
network can be abstracted as a higher-level algorithm if there exists a
function which allows us to map between them. Notably, most interpretability
papers implement these maps as linear functions, motivated by the linear
representation hypothesis: the idea that features are encoded linearly in a
model's representations. However, this linearity constraint is not required by
the definition of causal abstraction. In this work, we critically examine the
concept of causal abstraction by considering arbitrarily powerful alignment
maps. In particular, we prove that under reasonable assumptions, any neural
network can be mapped to any algorithm, rendering this unrestricted notion of
causal abstraction trivial and uninformative. We complement these theoretical
findings with empirical evidence, demonstrating that it is possible to
perfectly map models to algorithms even when these models are incapable of
solving the actual task; e.g., on an experiment using randomly initialised
language models, our alignment maps reach 100% interchange-intervention
accuracy on the indirect object identification task. This raises the non-linear
representation dilemma: if we lift the linearity constraint imposed to
alignment maps in causal abstraction analyses, we are left with no principled
way to balance the inherent trade-off between these maps' complexity and
accuracy. Together, these results suggest an answer to our title's question:
causal abstraction is not enough for mechanistic interpretability, as it
becomes vacuous without assumptions about how models encode information.
Studying the connection between this information-encoding assumption and causal
abstraction should lead to exciting future work.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [65] [Human Creativity and AI](https://arxiv.org/abs/2507.08001)
*Shengyi Xie*

**主要类别:** cs.AI

**AI概要:** This paper investigates the philosophy of creativity in the context of AI development.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this paper is to address the central question of whether AI can exhibit creativity in light of contemporary research in the fields of psychology, cognitive neuroscience, and the philosophy of creativity.

**方法:** The paper reviews historical perspectives on the philosophy of creativity, explores the influence of psychological advancements on the study of creativity, analyzes various definitions of creativity, and examines the responses of naturalism and cognitive neuroscience to the concept of creativity.

**结果:** The result of this paper is a comprehensive review of the historical and contemporary perspectives on the philosophy of creativity, as well as an analysis of the influence of psychological advancements and the responses of naturalism and cognitive neuroscience to the concept of creativity.

**结论:** The paper concludes that the question of whether AI can exhibit creativity is complex and requires further investigation from multiple perspectives.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Human+Creativity+and+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08001，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08001&send_immediately=true&force_search=false)

**原文摘要:** With the advancement of science and technology, the philosophy of creativity
has undergone significant reinterpretation. This paper investigates
contemporary research in the fields of psychology, cognitive neuroscience, and
the philosophy of creativity, particularly in the context of the development of
artificial intelligence (AI) techniques. It aims to address the central
question: Can AI exhibit creativity? The paper reviews the historical
perspectives on the philosophy of creativity and explores the influence of
psychological advancements on the study of creativity. Furthermore, it analyzes
various definitions of creativity and examines the responses of naturalism and
cognitive neuroscience to the concept of creativity.

</details>


### [66] [TableReasoner: Advancing Table Reasoning Framework with Large Language Models](https://arxiv.org/abs/2507.08046)
*Sishi Xiong, Dakai Wang, Yu Zhao, Jie Zhang, Changzai Pan, Haowei He, Xiangyu Li, Wenhan Chang, Zhongjiang He, Shuangyong Song, Yongxiang Li*

**主要类别:** cs.AI

**AI概要:** This paper presents a system developed for table question answering (TQA) using a large language model (LLM)-powered and programming-based table reasoning framework, named TableReasoner. It addresses the issues of large size, incomplete column semantics, and entity ambiguity by modeling a table using the schema that combines structural and semantic representations.


<details>
  <summary>更多</summary>
  
**动机:** TQA tasks face challenges due to the characteristics of real-world tabular data, such as large size, incomplete column semantics, and entity ambiguity.

**方法:** We propose a large language model (LLM)-powered and programming-based table reasoning framework, named TableReasoner.

**结果:** Our system achieves first place in both subtasks of SemEval-2025 Task 8.

**结论:** Our system achieves first place in both subtasks of SemEval-2025 Task 8.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TableReasoner%3A+Advancing+Table+Reasoning+Framework+with+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08046，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08046&send_immediately=true&force_search=false)

**原文摘要:** The paper presents our system developed for table question answering (TQA).
TQA tasks face challenges due to the characteristics of real-world tabular
data, such as large size, incomplete column semantics, and entity ambiguity. To
address these issues, we propose a large language model (LLM)-powered and
programming-based table reasoning framework, named TableReasoner. It models a
table using the schema that combines structural and semantic representations,
enabling holistic understanding and efficient processing of large tables. We
design a multi-step schema linking plan to derive a focused table schema that
retains only query-relevant information, eliminating ambiguity and alleviating
hallucinations. This focused table schema provides precise and sufficient table
details for query refinement and programming. Furthermore, we integrate the
reasoning workflow into an iterative thinking architecture, allowing
incremental cycles of thinking, reasoning and reflection. Our system achieves
first place in both subtasks of SemEval-2025 Task 8.

</details>


### [67] [A Dynamic Stackelberg Game Framework for Agentic AI Defense Against LLM Jailbreaking](https://arxiv.org/abs/2507.08207)
*Zhengye Han, Quanyan Zhu*

**主要类别:** cs.AI

**AI概要:** This paper presents a dynamic Stackelberg game framework to model the interactions between attackers and defenders in the context of LLM jailbreaking.


<details>
  <summary>更多</summary>
  
**动机:** As LLMs are increasingly deployed in critical applications, the challenge of jailbreaking has become a significant concern.

**方法:** A dynamic Stackelberg game framework is used to model the interactions between attackers and defenders. The novel agentic AI solution, the 'Purple Agent', integrates adversarial exploration and defensive strategies using Rapidly-exploring Random Trees (RRT).

**结果:** The approach offers a principled method for analyzing adversarial dynamics and provides a foundation for mitigating the risk of jailbreaking.

**结论:** The Purple Agent provides a foundation for mitigating the risk of jailbreaking in LLMs.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Dynamic+Stackelberg+Game+Framework+for+Agentic+AI+Defense+Against+LLM+Jailbreaking，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08207，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08207&send_immediately=true&force_search=false)

**原文摘要:** As large language models (LLMs) are increasingly deployed in critical
applications, the challenge of jailbreaking, where adversaries manipulate the
models to bypass safety mechanisms, has become a significant concern. This
paper presents a dynamic Stackelberg game framework to model the interactions
between attackers and defenders in the context of LLM jailbreaking. The
framework treats the prompt-response dynamics as a sequential extensive-form
game, where the defender, as the leader, commits to a strategy while
anticipating the attacker's optimal responses. We propose a novel agentic AI
solution, the "Purple Agent," which integrates adversarial exploration and
defensive strategies using Rapidly-exploring Random Trees (RRT). The Purple
Agent actively simulates potential attack trajectories and intervenes
proactively to prevent harmful outputs. This approach offers a principled
method for analyzing adversarial dynamics and provides a foundation for
mitigating the risk of jailbreaking.

</details>


### [68] [Reasoning and Behavioral Equilibria in LLM-Nash Games: From Mindsets to Actions](https://arxiv.org/abs/2507.08208)
*Quanyan Zhu*

**主要类别:** cs.AI

**AI概要:** This paper presents the LLM-Nash framework, which models how agents choose reasoning prompts for decision-making with LLMs, capturing bounded rationality and differing from traditional game theory.


<details>
  <summary>更多</summary>
  
**动机:** To create a model that captures bounded rationality in agent decision-making processes when using Large Language Models (LLMs), as opposed to assuming full rationality in classical game theory.

**方法:** The paper proposes the LLM-Nash framework, a game-theoretic model where agents select reasoning prompts to guide decision-making via LLMs. Equilibrium is defined over the prompt space, with actions emerging as the behavioral output of LLM inference.

**结果:** Reasoning equilibria within the LLM-Nash framework can diverge from classical Nash outcomes, allowing the study of cognitive constraints, mindset expressiveness, and epistemic learning.

**结论:** The LLM-Nash framework provides a new foundation for strategic interaction in systems enabled by Large Language Models, taking into account cognitive constraints and diverging from classical Nash outcomes.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reasoning+and+Behavioral+Equilibria+in+LLM-Nash+Games%3A+From+Mindsets+to+Actions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08208，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08208&send_immediately=true&force_search=false)

**原文摘要:** We introduce the LLM-Nash framework, a game-theoretic model where agents
select reasoning prompts to guide decision-making via Large Language Models
(LLMs). Unlike classical games that assume utility-maximizing agents with full
rationality, this framework captures bounded rationality by modeling the
reasoning process explicitly. Equilibrium is defined over the prompt space,
with actions emerging as the behavioral output of LLM inference. This approach
enables the study of cognitive constraints, mindset expressiveness, and
epistemic learning. Through illustrative examples, we show how reasoning
equilibria can diverge from classical Nash outcomes, offering a new foundation
for strategic interaction in LLM-enabled systems.

</details>


### [69] [From Curiosity to Competence: How World Models Interact with the Dynamics of Exploration](https://arxiv.org/abs/2507.08210)
*Fryderyk Mantiuk, Hanqi Zhou, Charley M. Wu*

**主要类别:** cs.AI

**AI概要:** This paper explores how intelligent agents balance curiosity and competence during exploration.


<details>
  <summary>更多</summary>
  
**动机:** To understand what drives an agent to explore the world while also maintaining control over the environment.

**方法:** Two model-based agents are compared using handcrafted state abstractions (Tabular) or learning an internal world model (Dreamer).

**结果:** The Tabular agent shows curiosity and competence guide exploration in distinct patterns, while prioritizing both improves exploration. The Dreamer agent reveals a two-way interaction between exploration and representation learning.

**结论:** The findings formalize adaptive exploration as a balance between pursuing the unknown and the controllable, offering insights for cognitive theories and efficient reinforcement learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Curiosity+to+Competence%3A+How+World+Models+Interact+with+the+Dynamics+of+Exploration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08210，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08210&send_immediately=true&force_search=false)

**原文摘要:** What drives an agent to explore the world while also maintaining control over
the environment? From a child at play to scientists in the lab, intelligent
agents must balance curiosity (the drive to seek knowledge) with competence
(the drive to master and control the environment). Bridging cognitive theories
of intrinsic motivation with reinforcement learning, we ask how evolving
internal representations mediate the trade-off between curiosity (novelty or
information gain) and competence (empowerment). We compare two model-based
agents using handcrafted state abstractions (Tabular) or learning an internal
world model (Dreamer). The Tabular agent shows curiosity and competence guide
exploration in distinct patterns, while prioritizing both improves exploration.
The Dreamer agent reveals a two-way interaction between exploration and
representation learning, mirroring the developmental co-evolution of curiosity
and competence. Our findings formalize adaptive exploration as a balance
between pursuing the unknown and the controllable, offering insights for
cognitive theories and efficient reinforcement learning.

</details>


### [70] [Grounding Methods for Neural-Symbolic AI](https://arxiv.org/abs/2507.08216)
*Rodrigo Castellano Ontiveros, Francesco Giannini, Marco Gori, Giuseppe Marra, Michelangelo Diligenti*

**主要类别:** cs.AI

**AI概要:** This paper proposes a parametrized family of grounding methods generalizing classic Backward Chaining inspired by multi-hop symbolic reasoning.


<details>
  <summary>更多</summary>
  
**动机:** Taking inspiration from multi-hop symbolic reasoning, this paper aims to propose a parametrized family of grounding methods that allow us to obtain commonly employed grounding methods as special cases, and to control the trade-off between expressiveness and scalability of the reasoner.

**方法:** This paper proposes a parametrized family of grounding methods generalizing classic Backward Chaining.

**结果:** Different selections within this family allow us to obtain commonly employed grounding methods as special cases, and to control the trade-off between expressiveness and scalability of the reasoner. The experimental results show that the selection of the grounding criterion is often as important as the NeSy method itself.

**结论:** The selection of the grounding criterion is often as important as the NeSy method itself.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Grounding+Methods+for+Neural-Symbolic+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08216，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08216&send_immediately=true&force_search=false)

**原文摘要:** A large class of Neural-Symbolic (NeSy) methods employs a machine learner to
process the input entities, while relying on a reasoner based on First-Order
Logic to represent and process more complex relationships among the entities. A
fundamental role for these methods is played by the process of logic grounding,
which determines the relevant substitutions for the logic rules using a
(sub)set of entities. Some NeSy methods use an exhaustive derivation of all
possible substitutions, preserving the full expressive power of the logic
knowledge. This leads to a combinatorial explosion in the number of ground
formulas to consider and, therefore, strongly limits their scalability. Other
methods rely on heuristic-based selective derivations, which are generally more
computationally efficient, but lack a justification and provide no guarantees
of preserving the information provided to and returned by the reasoner. Taking
inspiration from multi-hop symbolic reasoning, this paper proposes a
parametrized family of grounding methods generalizing classic Backward
Chaining. Different selections within this family allow us to obtain commonly
employed grounding methods as special cases, and to control the trade-off
between expressiveness and scalability of the reasoner. The experimental
results show that the selection of the grounding criterion is often as
important as the NeSy method itself.

</details>


### [71] [Quantum Federated Learning for Multimodal Data: A Modality-Agnostic Approach](https://arxiv.org/abs/2507.08217)
*Atit Pokharel, Ratun Rahman, Thomas Morris, Dinh C. Nguyen*

**主要类别:** cs.AI

**AI概要:** This paper introduces a novel multimodal approach for quantum federated learning with a mechanism to handle missing modalities, improving accuracy in simulations.


<details>
  <summary>更多</summary>
  
**动机:** Existing QFL frameworks focus on unimodal systems, limiting their applicability to real-world tasks involving multiple modalities. There is also a bottleneck in multimodal QFL when certain modalities are absent during training.

**方法:** A novel multimodal approach for QFL using quantum entanglement with an intermediate fusion and a Missing Modality Agnostic (MMA) mechanism.

**结果:** Simulation results show an improvement in accuracy of 6.84% in IID and 7.25% in non-IID data distributions.

**结论:** The proposed multimodal QFL method with MMA improves accuracy in both IID and non-IID data distributions.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quantum+Federated+Learning+for+Multimodal+Data%3A+A+Modality-Agnostic+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08217，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08217&send_immediately=true&force_search=false)

**原文摘要:** Quantum federated learning (QFL) has been recently introduced to enable a
distributed privacy-preserving quantum machine learning (QML) model training
across quantum processors (clients). Despite recent research efforts, existing
QFL frameworks predominantly focus on unimodal systems, limiting their
applicability to real-world tasks that often naturally involve multiple
modalities. To fill this significant gap, we present for the first time a novel
multimodal approach specifically tailored for the QFL setting with the
intermediate fusion using quantum entanglement. Furthermore, to address a major
bottleneck in multimodal QFL, where the absence of certain modalities during
training can degrade model performance, we introduce a Missing Modality
Agnostic (MMA) mechanism that isolates untrained quantum circuits, ensuring
stable training without corrupted states. Simulation results demonstrate that
the proposed multimodal QFL method with MMA yields an improvement in accuracy
of 6.84% in independent and identically distributed (IID) and 7.25% in non-IID
data distributions compared to the state-of-the-art methods.

</details>


### [72] [Giving AI Agents Access to Cryptocurrency and Smart Contracts Creates New Vectors of AI Harm](https://arxiv.org/abs/2507.08249)
*Bill Marino, Ari Juels*

**主要类别:** cs.AI

**AI概要:** 本文讨论了给予AI代理加密货币和智能合约访问权限可能会导致新的AI危害向量，并呼吁进行更多的技术研究来防止和减轻这些危害。


<details>
  <summary>更多</summary>
  
**动机:** 给予AI代理访问加密货币以及交易它们的智能合约的兴趣日益增长，但这可能会导致新的AI危害向量。

**方法:** 首先检查了加密货币和智能合约的独特属性，这些属性可能导致新的危害向量。然后详细描述了每个新的危害向量。

**结果:** 详细描述了新的AI危害向量。

**结论:** 呼吁进行更多的技术研究，以防止和减轻这些危害，从而更安全地赋予AI代理加密货币和智能合约。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Giving+AI+Agents+Access+to+Cryptocurrency+and+Smart+Contracts+Creates+New+Vectors+of+AI+Harm，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08249，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08249&send_immediately=true&force_search=false)

**原文摘要:** There is growing interest in giving AI agents access to cryptocurrencies as
well as to the smart contracts that transact them. But doing so, this position
paper argues, could lead to formidable new vectors of AI harm. To support this
argument, we first examine the unique properties of cryptocurrencies and smart
contracts that could lead to these new vectors of harm. Next, we describe each
of these new vectors of harm in detail. Finally, we conclude with a call for
more technical research aimed at preventing and mitigating these harms and,
thereby making it safer to endow AI agents with cryptocurrencies and smart
contracts.

</details>


### [73] [Abductive Computational Systems: Creative Abduction and Future Directions](https://arxiv.org/abs/2507.08264)
*Abhinav Sood, Kazjon Grace, Stephen Wan, Cecile Paris*

**主要类别:** cs.AI

**AI概要:** this paper examines abductive reasoning across domains, highlighting its potential for creativity in computational systems but finds current implementations lacking


<details>
  <summary>更多</summary>
  
**动机:** understanding the role of abductive reasoning across domains and its potential for creativity in computational systems

**方法:** review of theoretical accounts and analysis of computational systems

**结果:** theoretical frameworks and computational implementations do not adequately address creative abductive hypothesis generation

**结论:** abductive reasoning in computational systems requires further research for creative hypothesis generation

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Abductive+Computational+Systems%3A+Creative+Abduction+and+Future+Directions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08264，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08264&send_immediately=true&force_search=false)

**原文摘要:** Abductive reasoning, reasoning for inferring explanations for observations,
is often mentioned in scientific, design-related and artistic contexts, but its
understanding varies across these domains. This paper reviews how abductive
reasoning is discussed in epistemology, science and design, and then analyses
how various computational systems use abductive reasoning. Our analysis shows
that neither theoretical accounts nor computational implementations of
abductive reasoning adequately address generating creative hypotheses.
Theoretical frameworks do not provide a straightforward model for generating
creative abductive hypotheses, computational systems largely implement
syllogistic forms of abductive reasoning. We break down abductive computational
systems into components and conclude by identifying specific directions for
future research that could advance the state of creative abductive reasoning in
computational systems.

</details>


### [74] [Agent Safety Alignment via Reinforcement Learning](https://arxiv.org/abs/2507.08270)
*Zeyang Sha, Hanling Tian, Zhuoer Xu, Shiwen Cui, Changhua Meng, Weiqiang Wang*

**主要类别:** cs.AI

**AI概要:** This paper proposes a safety-alignment framework for autonomous Large Language Model (LLM) agents that enables them to handle both user-initiated and tool-initiated threats. The framework uses structured reasoning, sandboxed reinforcement learning, and a custom-designed sandbox environment. Evaluations show improved resistance to security threats while maintaining utility on benign tasks.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to address the new safety risks posed by autonomous Large Language Model (LLM) agents capable of tool usage, which are vulnerable to both user-initiated and tool-initiated threats.

**方法:** The paper proposes a unified safety-alignment framework for tool-using agents, employing structured reasoning, sandboxed reinforcement learning, a tri-modal taxonomy, and a custom-designed sandbox environment.

**结果:** The proposed safety-aligned agents significantly improve resistance to security threats while preserving strong utility on benign tasks.

**结论:** Safety and effectiveness can be jointly optimized for autonomous LLM agents, enabling their trustworthy deployment.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Agent+Safety+Alignment+via+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08270，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08270&send_immediately=true&force_search=false)

**原文摘要:** The emergence of autonomous Large Language Model (LLM) agents capable of tool
usage has introduced new safety risks that go beyond traditional conversational
misuse. These agents, empowered to execute external functions, are vulnerable
to both user-initiated threats (e.g., adversarial prompts) and tool-initiated
threats (e.g., malicious outputs from compromised tools). In this paper, we
propose the first unified safety-alignment framework for tool-using agents,
enabling models to handle both channels of threat via structured reasoning and
sandboxed reinforcement learning. We introduce a tri-modal taxonomy, including
benign, malicious, and sensitive for both user prompts and tool responses, and
define a policy-driven decision model. Our framework employs a custom-designed
sandbox environment that simulates real-world tool execution and allows
fine-grained reward shaping. Through extensive evaluations on public and
self-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we
demonstrate that our safety-aligned agents significantly improve resistance to
security threats while preserving strong utility on benign tasks. Our results
show that safety and effectiveness can be jointly optimized, laying the
groundwork for trustworthy deployment of autonomous LLM agents.

</details>


### [75] [M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning](https://arxiv.org/abs/2507.08306)
*Inclusion AI, :, Fudong Wang, Jiajia Liu, Jingdong Chen, Jun Zhou, Kaixiang Ji, Lixiang Ru, Qingpei Guo, Ruobing Zheng, Tianqi Li, Yi Yuan, Yifan Mao, Yuting Xiao, Ziping Ma*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种新的模型M2-Reasoning-7B，它在一般和空间推理上都有出色表现，并通过创新的数据管道和训练策略达到了最先进水平。


<details>
  <summary>更多</summary>
  
**动机:** 尽管在多模态大型语言模型（MLLMs）领域取得了进步，但这些模型在动态空间交互方面仍存在困难，这是实际应用中的关键能力。为了弥补这个差距，我们引入了M2-Reasoning-7B模型。

**方法:** 该方法集成了两个关键创新：一个新颖的数据管道，生成了294.2K高质量的数据样本；以及一种动态的多任务训练策略，通过逐步优化来缓解数据和任务特定奖励之间的冲突。

**结果:** M2-Reasoning-7B在一般推理和空间推理上都表现出色，成为新的最先进模型。

**结论:** M2-Reasoning-7B在8个基准测试中展示了优越的性能，成为了新的最先进模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是M2-Reasoning%3A+Empowering+MLLMs+with+Unified+General+and+Spatial+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08306，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08306&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in Multimodal Large Language Models (MLLMs), particularly
through Reinforcement Learning with Verifiable Rewards (RLVR), have
significantly enhanced their reasoning abilities. However, a critical gap
persists: these models struggle with dynamic spatial interactions, a capability
essential for real-world applications. To bridge this gap, we introduce
M2-Reasoning-7B, a model designed to excel in both general and spatial
reasoning. Our approach integrates two key innovations: (1) a novel data
pipeline that generates 294.2K high-quality data samples (168K for cold-start
fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning
trajectories and have undergone comprehensive assessment; and (2) a dynamic
multi-task training strategy with step-wise optimization to mitigate conflicts
between data, and task-specific rewards for delivering tailored incentive
signals. This combination of curated data and advanced training allows
M2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks,
showcasing superior performance in both general and spatial reasoning domains.

</details>


### [76] [Multi-Agent LLMs as Ethics Advocates in AI-Based Systems](https://arxiv.org/abs/2507.08392)
*Asma Yamani, Malak Baslyman, Moataz Ahmed*

**主要类别:** cs.AI

**AI概要:** Proposes a framework for generating ethics requirements drafts using an ethics advocate agent in a multi-agent LLM setting. Evaluation through two case studies shows that the framework captures most ethics requirements identified by researchers but also highlights reliability issues.


<details>
  <summary>更多</summary>
  
**动机:** Incorporating ethics into the requirement elicitation process is essential for creating ethically aligned systems. Although eliciting manual ethics requirements is effective, it requires diverse input from multiple stakeholders, which can be challenging due to time and resource constraints. Moreover, it is often given a low priority in the requirements elicitation process.

**方法:** The study proposes a framework for generating ethics requirements drafts by introducing an ethics advocate agent in a multi-agent LLM setting. The proposed framework is evaluated through two case studies from different contexts.

**结果:** The proposed framework captures the majority of ethics requirements identified by researchers during 30-minute interviews and introduces several additional relevant requirements. However, it also highlights reliability issues in generating ethics requirements, emphasizing the need for human feedback in this sensitive domain.

**结论:** This work can facilitate the broader adoption of ethics in the requirements engineering process, ultimately leading to more ethically aligned products.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Agent+LLMs+as+Ethics+Advocates+in+AI-Based+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08392，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08392&send_immediately=true&force_search=false)

**原文摘要:** Incorporating ethics into the requirement elicitation process is essential
for creating ethically aligned systems. Although eliciting manual ethics
requirements is effective, it requires diverse input from multiple
stakeholders, which can be challenging due to time and resource constraints.
Moreover, it is often given a low priority in the requirements elicitation
process. This study proposes a framework for generating ethics requirements
drafts by introducing an ethics advocate agent in a multi-agent LLM setting.
This agent critiques and provides input on ethical issues based on the system
description. The proposed framework is evaluated through two case studies from
different contexts, demonstrating that it captures the majority of ethics
requirements identified by researchers during 30-minute interviews and
introduces several additional relevant requirements. However, it also
highlights reliability issues in generating ethics requirements, emphasizing
the need for human feedback in this sensitive domain. We believe this work can
facilitate the broader adoption of ethics in the requirements engineering
process, ultimately leading to more ethically aligned products.

</details>


### [77] [Why this and not that? A Logic-based Framework for Contrastive Explanations](https://arxiv.org/abs/2507.08454)
*Tobias Geibinger, Reijo Jaakkola, Antti Kuusisto, Xinghan Liu, Miikka Vilander*

**主要类别:** cs.AI

**AI概要:** This paper defines canonical problems related to contrastive explanations in propositional logic, providing an extensive analysis of their computational complexities and demonstrating how they work in practice.


<details>
  <summary>更多</summary>
  
**动机:** To compute causes for both P and Q, explicitly comparing their differences, and answer questions of the form 'Why P but not Q?'.

**方法:** Defining canonical problems related to contrastive explanations in propositional logic, investigating their basic properties, and implementing them for CNF-formulas using answer set programming.

**结果:** The framework captures a cardinality-minimal version of existing contrastive explanations and the problems are implemented for CNF-formulas using answer set programming.

**结论:** The framework captures a cardinality-minimal version of existing contrastive explanations and provides an extensive analysis of the computational complexities of the problems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Why+this+and+not+that%3F+A+Logic-based+Framework+for+Contrastive+Explanations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08454，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08454&send_immediately=true&force_search=false)

**原文摘要:** We define several canonical problems related to contrastive explanations,
each answering a question of the form ''Why P but not Q?''. The problems
compute causes for both P and Q, explicitly comparing their differences. We
investigate the basic properties of our definitions in the setting of
propositional logic. We show, inter alia, that our framework captures a
cardinality-minimal version of existing contrastive explanations in the
literature. Furthermore, we provide an extensive analysis of the computational
complexities of the problems. We also implement the problems for CNF-formulas
using answer set programming and present several examples demonstrating how
they work in practice.

</details>


### [78] [From Language to Logic: A Bi-Level Framework for Structured Reasoning](https://arxiv.org/abs/2507.08501)
*Keying Yang, Hao Wang, Kai Yang*

**主要类别:** cs.AI

**AI概要:** This paper proposes a bi-level framework for structured reasoning over natural language inputs. The framework maps language to logic through a two-stage process: high-level task abstraction and low-level logic generation. Experiments show that the approach significantly outperforms existing baselines in accuracy.


<details>
  <summary>更多</summary>
  
**动机:** Structured reasoning over natural language inputs remains a core challenge in artificial intelligence, as it requires bridging the gap between unstructured linguistic expressions and formal logical representations.

**方法:** A novel bi-level framework that maps language to logic through a two-stage process: high-level task abstraction and low-level logic generation.

**结果:** Experiments on multiple realistic reasoning benchmarks demonstrate that the approach significantly outperforms existing baselines in accuracy, with accuracy gains reaching as high as 40%.

**结论:** The bi-level framework proposed in this paper significantly outperforms existing baselines in accuracy, and enhances transparency and error traceability, offering a promising step toward trustworthy and systematic reasoning with LLMs.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Language+to+Logic%3A+A+Bi-Level+Framework+for+Structured+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08501，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08501&send_immediately=true&force_search=false)

**原文摘要:** Structured reasoning over natural language inputs remains a core challenge in
artificial intelligence, as it requires bridging the gap between unstructured
linguistic expressions and formal logical representations. In this paper, we
propose a novel \textbf{bi-level framework} that maps language to logic through
a two-stage process: high-level task abstraction and low-level logic
generation. At the upper level, a large language model (LLM) parses natural
language queries into intermediate structured representations specifying the
problem type, objectives, decision variables, and symbolic constraints. At the
lower level, the LLM uses these representations to generate symbolic workflows
or executable reasoning programs for accurate and interpretable decision
making. The framework supports modular reasoning, enforces explicit
constraints, and generalizes across domains such as mathematical problem
solving, question answering, and logical inference. We further optimize the
framework with an end-to-end {bi-level} optimization approach that jointly
refines both the high-level abstraction and low-level logic generation stages.
Experiments on multiple realistic reasoning benchmarks demonstrate that our
approach significantly outperforms existing baselines in accuracy, with
accuracy gains reaching as high as 40\%. Moreover, the bi-level design enhances
transparency and error traceability, offering a promising step toward
trustworthy and systematic reasoning with LLMs.

</details>


### [79] [A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge Graph Fusion Framework for Rare Disease Diagnosis](https://arxiv.org/abs/2507.08529)
*Mingda Zhang, Na Zhao, Jianglong Qin, Guoyu Ye, Ruixiang Tang*

**主要类别:** cs.AI

**AI概要:** A novel framework for improving rare-disease diagnosis using medical large language models.


<details>
  <summary>更多</summary>
  
**动机:** Rare-disease diagnosis remains hampered by insufficient knowledge-representation depth, limited concept understanding, and constrained clinical reasoning.

**方法:** A framework that couples multi-granularity sparse activation of medical concepts with a hierarchical knowledge graph is proposed. Four complementary matching algorithms, diversity control, and a five-level fallback strategy enable precise concept activation. A three-layer knowledge graph (taxonomy, clinical features, instances) provides structured, up-to-date context.

**结果:** Experiments on the BioASQ rare-disease QA set show BLEU gains of 0.09, ROUGE gains of 0.05, and accuracy gains of 0.12, with peak accuracy of 0.89 approaching the 0.90 clinical threshold.

**结论:** The proposed framework improves the performance of medical large language models in rare-disease diagnosis, as evidenced by increased accuracy and expert evaluation.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Multi-granularity+Concept+Sparse+Activation+and+Hierarchical+Knowledge+Graph+Fusion+Framework+for+Rare+Disease+Diagnosis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08529，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08529&send_immediately=true&force_search=false)

**原文摘要:** Despite advances from medical large language models in healthcare,
rare-disease diagnosis remains hampered by insufficient
knowledge-representation depth, limited concept understanding, and constrained
clinical reasoning. We propose a framework that couples multi-granularity
sparse activation of medical concepts with a hierarchical knowledge graph. Four
complementary matching algorithms, diversity control, and a five-level fallback
strategy enable precise concept activation, while a three-layer knowledge graph
(taxonomy, clinical features, instances) provides structured, up-to-date
context. Experiments on the BioASQ rare-disease QA set show BLEU gains of 0.09,
ROUGE gains of 0.05, and accuracy gains of 0.12, with peak accuracy of 0.89
approaching the 0.90 clinical threshold. Expert evaluation confirms
improvements in information quality, reasoning, and professional expression,
suggesting our approach shortens the "diagnostic odyssey" for rare-disease
patients.

</details>


### [80] [Large Multi-modal Model Cartographic Map Comprehension for Textual Locality Georeferencing](https://arxiv.org/abs/2507.08575)
*Kalana Wijegunarathna, Kristin Stock, Christopher B. Jones*

**主要类别:** cs.AI

**AI概要:** This paper presents a novel method that uses multi-modal capabilities of recent Large Multi-Modal Models (LMM) for georeferencing complex locality descriptions associated with biological sample records.


<details>
  <summary>更多</summary>
  
**动机:** Millions of biological sample records are un-georeferenced and existing automated methods do not exploit maps which are essential for georeferencing complex relations.

**方法:** A novel method that exploits multi-modal capabilities of recent Large Multi-Modal Models (LMM) using a grid-based approach in a zero-shot setting.

**结果:** Experiments on a small manually annotated dataset show impressive results with an average distance error of ~1 km.

**结论:** The proposed method shows potential for improving georeferencing accuracy by exploiting multi-modal models and a practical framework is proposed to integrate this method into a georeferencing workflow.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Large+Multi-modal+Model+Cartographic+Map+Comprehension+for+Textual+Locality+Georeferencing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08575，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08575&send_immediately=true&force_search=false)

**原文摘要:** Millions of biological sample records collected in the last few centuries
archived in natural history collections are un-georeferenced. Georeferencing
complex locality descriptions associated with these collection samples is a
highly labour-intensive task collection agencies struggle with. None of the
existing automated methods exploit maps that are an essential tool for
georeferencing complex relations. We present preliminary experiments and
results of a novel method that exploits multi-modal capabilities of recent
Large Multi-Modal Models (LMM). This method enables the model to visually
contextualize spatial relations it reads in the locality description. We use a
grid-based approach to adapt these auto-regressive models for this task in a
zero-shot setting. Our experiments conducted on a small manually annotated
dataset show impressive results for our approach ($\sim$1 km Average distance
error) compared to uni-modal georeferencing with Large Language Models and
existing georeferencing tools. The paper also discusses the findings of the
experiments in light of an LMM's ability to comprehend fine-grained maps.
Motivated by these results, a practical framework is proposed to integrate this
method into a georeferencing workflow.

</details>


### [81] [Unlocking Speech Instruction Data Potential with Query Rewriting](https://arxiv.org/abs/2507.08603)
*Yonghua Hei, Yibo Yan, Shuliang Liu, Huiyu Zhou, Linfeng Zhang, Xuming Hu*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种使用多LLM知识融合的查询重写框架，以解决将分布外文本指令适当地转换为语音的挑战，从而实现构建高质量语音指令数据集的目标。


<details>
  <summary>更多</summary>
  
**动机:** 端到端大型语音语言模型在遵循语音指令方面的能力尚未完全实现，由于缺乏数据集和训练任务的严重偏差，需要一种方法来构建高质量的语音指令数据集而不依赖人工注释。

**方法:** 提出了一种使用多LLM知识融合的查询重写框架，采用多个代理对合成语音进行注释和验证，以零样本重写将文本指令转换为更适合TTS模型分布的形式。

**结果:** 实验表明，该方法可以通过零样本重写将文本指令转换为更适合TTS模型进行语音合成的分布，将数据可用性从72%增加到93%。

**结论:** 使用多LLM知识融合的查询重写框架能够构建高质量的语音指令数据集，提高数据可用性，并在需要复杂知识和上下文相关能力的重写任务中显示出独特优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unlocking+Speech+Instruction+Data+Potential+with+Query+Rewriting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08603，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08603&send_immediately=true&force_search=false)

**原文摘要:** End-to-end Large Speech Language Models~(\textbf{LSLMs}) demonstrate strong
potential in response latency and speech comprehension capabilities, showcasing
general intelligence across speech understanding tasks. However, the ability to
follow speech instructions has not been fully realized due to the lack of
datasets and heavily biased training tasks. Leveraging the rich ASR datasets,
previous approaches have used Large Language Models~(\textbf{LLMs}) to continue
the linguistic information of speech to construct speech instruction datasets.
Yet, due to the gap between LLM-generated results and real human responses, the
continuation methods further amplify these shortcomings. Given the high costs
of collecting and annotating speech instruction datasets by humans, using
speech synthesis to construct large-scale speech instruction datasets has
become a balanced and robust alternative. Although modern
Text-To-Speech~(\textbf{TTS}) models have achieved near-human-level synthesis
quality, it is challenging to appropriately convert out-of-distribution text
instruction to speech due to the limitations of the training data distribution
in TTS models. To address this issue, we propose a query rewriting framework
with multi-LLM knowledge fusion, employing multiple agents to annotate and
validate the synthesized speech, making it possible to construct high-quality
speech instruction datasets without relying on human annotation. Experiments
show that this method can transform text instructions into distributions more
suitable for TTS models for speech synthesis through zero-shot rewriting,
increasing data usability from 72\% to 93\%. It also demonstrates unique
advantages in rewriting tasks that require complex knowledge and
context-related abilities.

</details>


### [82] [Agentic Large Language Models for Conceptual Systems Engineering and Design](https://arxiv.org/abs/2507.08619)
*Soheyl Massoudi, Mark Fuge*

**主要类别:** cs.AI

**AI概要:** This paper evaluates whether a structured multi-agent system can manage engineering design tasks more effectively than a two-agent system, focusing on a solar-powered water filtration system.


<details>
  <summary>更多</summary>
  
**动机:** Early-stage engineering design involves complex, iterative reasoning that existing LLM workflows struggle to handle effectively.

**方法:** Evaluate a structured multi-agent system (MAS) against a simpler two-agent system (2AS) for managing engineering design tasks.

**结果:** The MAS generated more granular DSGs, while 2AS mode-collapsed. Reasoning-distilled LLM improved completion rates.

**结论:** Structured multi-agent orchestration enhances design detail, but low requirements and fidelity gaps in coding persist.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Agentic+Large+Language+Models+for+Conceptual+Systems+Engineering+and+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08619，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08619&send_immediately=true&force_search=false)

**原文摘要:** Early-stage engineering design involves complex, iterative reasoning, yet
existing large language model (LLM) workflows struggle to maintain task
continuity and generate executable models. We evaluate whether a structured
multi-agent system (MAS) can more effectively manage requirements extraction,
functional decomposition, and simulator code generation than a simpler
two-agent system (2AS). The target application is a solar-powered water
filtration system as described in a cahier des charges. We introduce the
Design-State Graph (DSG), a JSON-serializable representation that bundles
requirements, physical embodiments, and Python-based physics models into graph
nodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS
collapses the process to a Generator-Reflector loop. Both systems run a total
of 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1
70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON
validity, requirement coverage, embodiment presence, code compatibility,
workflow completion, runtime, and graph size. Across all runs, both MAS and 2AS
maintained perfect JSON integrity and embodiment tagging. Requirement coverage
remained minimal (less than 20\%). Code compatibility peaked at 100\% under
specific 2AS settings but averaged below 50\% for MAS. Only the
reasoning-distilled model reliably flagged workflow completion. Powered by
DeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)
whereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced
design detail. Reasoning-distilled LLM improved completion rates, yet low
requirements and fidelity gaps in coding persisted.

</details>


### [83] [Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem Proving via Reinforcement Learning](https://arxiv.org/abs/2507.08649)
*Xingguang Ji, Yahui Liu, Qi Wang, Jingyuan Zhang, Yang Yue, Rui Shi, Chenxi Sun, Fuzheng Zhang, Guorui Zhou, Kun Gai*

**主要类别:** cs.AI

**AI概要:** 本文介绍了Leanabell-Prover-V2，一个可以生成Lean 4中正式定理证明的大规模语言模型，并详细阐述了其采用的技术手段与实验结果。


<details>
  <summary>更多</summary>
  
**动机:** 基于之前的工作Leanabell-Prover-V1，选择对现有的强大证明模型进行后训练以进一步提高性能。

**方法:** 通过升级强化学习（RL）与Lean 4验证器提供的反馈，主要改进了模型的自我意识和错误纠正能力。此外，还采用了多轮验证器交互、反馈令牌屏蔽和简单的奖励策略来优化LLM推理轨迹。

**结果:** 实验表明，Leanabell-Prover-V2在MiniF2F测试集上提高了性能。

**结论:** Leanabell-Prover-V2提高了在MiniF2F测试集上的性能，使用Kimina-Prover-Preview-Distill-7B提高了3.2%（pass@128），使用DeepSeek-Prover-V2-7B提高了2.0%（pass@128）。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leanabell-Prover-V2%3A+Verifier-integrated+Reasoning+for+Formal+Theorem+Proving+via+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08649，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08649&send_immediately=true&force_search=false)

**原文摘要:** We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that
can produce formal theorem proofs in Lean 4, with verifier-integrated Long
Chain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we
continual to choose to posttrain existing strong prover models for further
performance improvement. In our V2 version, we mainly upgrade the Reinforcement
Learning (RL) with feedback provided by the Lean 4 verifier. Crucially,
verifier feedback, such as indicating success or detailing specific errors,
allows the LLM to become ``self-aware'' of the correctness of its own reasoning
process and learn to reflexively correct errors. Leanabell-Prover-V2 directly
optimizes LLM reasoning trajectories with multi-turn verifier interactions,
together with feedback token masking for stable RL training and a simple reward
strategy. Experiments show that Leanabell-Prover-V2 improves performance by
3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with
DeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data
and models are available at:
https://github.com/Leanabell-LM/Leanabell-Prover-V2.

</details>


### [84] [Introspection of Thought Helps AI Agents](https://arxiv.org/abs/2507.08664)
*Haoran Sun, Shaoning Zeng*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种新的AI代理推理框架INoT，通过设计一个新的LLM-Read代码来实现程序对话推理过程，从而减少了令牌成本并提高了性能。


<details>
  <summary>更多</summary>
  
**动机:** 目前的AI代理受限于LLM在理解自然语言方面的固有局限性，迭代推理过程会产生大量的推理成本。

**方法:** 提出了一种新的AI代理推理框架INoT，通过设计一个新的LLM-Read代码来实现程序对话推理过程。

**结果:** 在六个基准测试中的三个不同任务上，INoT的有效性得到了验证，性能平均提高了7.95%，超过了基线。此外，INoT的令牌成本平均比基线的最佳表现方法低58.3%。

**结论:** INoT是一个有效的AI代理推理框架，可以减少令牌成本，并在不同的任务中表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Introspection+of+Thought+Helps+AI+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08664，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08664&send_immediately=true&force_search=false)

**原文摘要:** AI Agents rely on Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to
perform interpretation and inference in text and image tasks without
post-training, where LLMs and MLLMs play the most critical role and determine
the initial ability and limitations of AI Agents. Usually, AI Agents utilize
sophisticated prompt engineering and external reasoning framework to obtain a
promising interaction with LLMs, e.g., Chain-of-Thought, Iteration of Thought
and Image-of-Thought. However, they are still constrained by the inherent
limitations of LLM in understanding natural language, and the iterative
reasoning process will generate a large amount of inference cost. To this end,
we propose a novel AI Agent Reasoning Framework with Introspection of Thought
(INoT) by designing a new LLM-Read code in prompt. It enables LLM to execute
programmatic dialogue reasoning processes following the code in prompt.
Therefore, self-denial and reflection occur within LLM instead of outside LLM,
which can reduce token cost effectively. Through our experiments on six
benchmarks for three different tasks, the effectiveness of INoT is verified,
with an average improvement of 7.95\% in performance, exceeding the baselines.
Furthermore, the token cost of INoT is lower on average than the best
performing method at baseline by 58.3\%. In addition, we demonstrate the
versatility of INoT in image interpretation and inference through verification
experiments.

</details>


### [85] [elsciRL: Integrating Language Solutions into Reinforcement Learning Problem Settings](https://arxiv.org/abs/2507.08705)
*Philip Osborne, Danilo S. Carvalho, André Freitas*

**主要类别:** cs.AI

**AI概要:** This paper presents elsciRL, an open-source Python library that facilitates the application of language solutions on reinforcement learning problems. The library extends the Language Adapter with Self-Completing Instruction framework and provides a novel GUI that allows a user to provide text input for an LLM to generate instructions which it can then self-complete. Empirical results indicate that these instructions can improve a reinforcement learning agent's performance.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this paper is to extend the Language Adapter with Self-Completing Instruction framework defined in (Osborne, 2024) with the use of LLMs and provide a novel GUI that allows a user to provide text input for an LLM to generate instructions which it can then self-complete.

**方法:** The paper presents an open-source Python library called elsciRL, which facilitates the application of language solutions on reinforcement learning problems.

**结果:** Empirical results indicate that these instructions can improve a reinforcement learning agent's performance.

**结论:** The elsciRL library can accelerate the evaluation of language solutions on reward based environments, enabling new opportunities for scientific discovery.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是elsciRL%3A+Integrating+Language+Solutions+into+Reinforcement+Learning+Problem+Settings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08705，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08705&send_immediately=true&force_search=false)

**原文摘要:** We present elsciRL, an open-source Python library to facilitate the
application of language solutions on reinforcement learning problems. We
demonstrate the potential of our software by extending the Language Adapter
with Self-Completing Instruction framework defined in (Osborne, 2024) with the
use of LLMs. Our approach can be re-applied to new applications with minimal
setup requirements. We provide a novel GUI that allows a user to provide text
input for an LLM to generate instructions which it can then self-complete.
Empirical results indicate that these instructions \textit{can} improve a
reinforcement learning agent's performance. Therefore, we present this work to
accelerate the evaluation of language solutions on reward based environments to
enable new opportunities for scientific discovery.

</details>


### [86] [System-of-systems Modeling and Optimization: An Integrated Framework for Intermodal Mobility](https://arxiv.org/abs/2507.08715)
*Paul Saves, Jasper Bussemaker, Rémi Lafage, Thierry Lefebvre, Nathalie Bartoli, Youssef Diouane, Joseph Morlier*

**主要类别:** cs.AI

**AI概要:** This paper discusses the use of surrogate-based optimization algorithms, specifically Bayesian optimization utilizing Gaussian process models, to address the challenges posed by dedicated approaches when exploring novel architectures in system-of-systems.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind the paper is to address the challenges posed by the use of efficient dedicated approaches (often physics-based simulations) for exploring novel architectures in system-of-systems.

**方法:** The paper discusses the use of surrogate-based optimization algorithms, specifically Bayesian optimization utilizing Gaussian process models.

**结果:** The result of the paper is the emergence of surrogate-based optimization algorithms, such as Bayesian optimization utilizing Gaussian process models.

**结论:** Surrogate-based optimization algorithms, such as Bayesian optimization utilizing Gaussian process models, have emerged to address the challenges posed by dedicated approaches.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是System-of-systems+Modeling+and+Optimization%3A+An+Integrated+Framework+for+Intermodal+Mobility，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08715，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08715&send_immediately=true&force_search=false)

**原文摘要:** For developing innovative systems architectures, modeling and optimization
techniques have been central to frame the architecting process and define the
optimization and modeling problems. In this context, for system-of-systems the
use of efficient dedicated approaches (often physics-based simulations) is
highly recommended to reduce the computational complexity of the targeted
applications. However, exploring novel architectures using such dedicated
approaches might pose challenges for optimization algorithms, including
increased evaluation costs and potential failures. To address these challenges,
surrogate-based optimization algorithms, such as Bayesian optimization
utilizing Gaussian process models have emerged.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [87] [Beyond the Worst Case: Extending Differential Privacy Guarantees to Realistic Adversaries](https://arxiv.org/abs/2507.08158)
*Marika Swanberg, Meenatchi Sundaram Muthu Selva Annamalai, Jamie Hayes, Borja Balle, Adam Smith*

**主要类别:** cs.CR

**AI概要:** 本文旨在通过一个灵活的框架，对差分隐私机制在多种自然攻击设定下的高概率保障进行计算，以理解其在以前未被充分研究的攻击设定中的隐私泄露问题。


<details>
  <summary>更多</summary>
  
**动机:** 差分隐私最坏情况下的保障与所提供的隐私保护之间的分析性权衡目前还不太清楚。这项工作阐明了DP的最坏情况保证意味着什么，对于更具现实世界隐私风险代表性的攻击者来说意味着成功的机会。

**方法:** 提出了一种灵活的框架，可以计算出在大量自然攻击设置下DP机制的高概率保证。

**结果:** 进行了两个实证案例研究来说明我们界限的多功能性，并与最先进的攻击的成功进行比较。发现攻击非均匀数据的绝对隐私风险高度依赖于对手先前成功的概率。

**结论:** 在以前未被充分研究的攻击设置中，差分隐私机制的隐私泄露有了更细致的理解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+the+Worst+Case%3A+Extending+Differential+Privacy+Guarantees+to+Realistic+Adversaries，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08158，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08158&send_immediately=true&force_search=false)

**原文摘要:** Differential Privacy (DP) is a family of definitions that bound the
worst-case privacy leakage of a mechanism. One important feature of the
worst-case DP guarantee is it naturally implies protections against adversaries
with less prior information, more sophisticated attack goals, and complex
measures of a successful attack. However, the analytical tradeoffs between the
adversarial model and the privacy protections conferred by DP are not well
understood thus far. To that end, this work sheds light on what the worst-case
guarantee of DP implies about the success of attackers that are more
representative of real-world privacy risks.
  In this paper, we present a single flexible framework that generalizes and
extends the patchwork of bounds on DP mechanisms found in prior work. Our
framework allows us to compute high-probability guarantees for DP mechanisms on
a large family of natural attack settings that previous bounds do not capture.
One class of such settings is the approximate reconstruction of multiple
individuals' data, such as inferring nearly entire columns of a tabular data
set from noisy marginals and extracting sensitive information from DP-trained
language models.
  We conduct two empirical case studies to illustrate the versatility of our
bounds and compare them to the success of state-of-the-art attacks.
Specifically, we study attacks that extract non-uniform PII from a DP-trained
language model, as well as multi-column reconstruction attacks where the
adversary has access to some columns in the clear and attempts to reconstruct
the remaining columns for each person's record. We find that the absolute
privacy risk of attacking non-uniform data is highly dependent on the
adversary's prior probability of success. Our high probability bounds give us a
nuanced understanding of the privacy leakage of DP mechanisms in a variety of
previously understudied attack settings.

</details>


### [88] [GPUHammer: Rowhammer Attacks on GPU Memories are Practical](https://arxiv.org/abs/2507.08166)
*Chris S. Lin, Joyce Qu, Gururaj Saileshwar*

**主要类别:** cs.CR

**AI概要:** 这项工作介绍了GPUHammer，这是针对NVIDIA GPU与GDDR6 DRAM的第一个Rowhammer攻击。


<details>
  <summary>更多</summary>
  
**动机:** 尽管Rowhammer漏洞在Intel和AMD CPU上已经得到了广泛的研究，但其在使用GDDR内存的GPU上的影响，尤其是在新兴的机器学习应用中，仍未被探索。此外，GPU上的Rowhammer攻击面临着独特的挑战。

**方法:** 本研究提出了一种新的技术来反向工程GDDR DRAM行映射，并采用特定于GPU的内存访问优化来增强锤击强度并绕过缓解措施。

**结果:** 通过NVIDIA A6000 GPU和GDDR6内存，研究成功地展示了首个离散GPU上的Rowhammer攻击，能够跨4个DRAM库注入多达8个位翻转，并可能导致机器学习模型的准确性大幅下降（高达80%）。

**结论:** 总之，GPUHammer展示了对离散GPU的首次Rowhammer攻击，并强调了在机器学习应用中考虑此类安全风险的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GPUHammer%3A+Rowhammer+Attacks+on+GPU+Memories+are+Practical，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08166，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08166&send_immediately=true&force_search=false)

**原文摘要:** Rowhammer is a read disturbance vulnerability in modern DRAM that causes
bit-flips, compromising security and reliability. While extensively studied on
Intel and AMD CPUs with DDR and LPDDR memories, its impact on GPUs using GDDR
memories, critical for emerging machine learning applications, remains
unexplored. Rowhammer attacks on GPUs face unique challenges: (1) proprietary
mapping of physical memory to GDDR banks and rows, (2) high memory latency and
faster refresh rates that hinder effective hammering, and (3) proprietary
mitigations in GDDR memories, difficult to reverse-engineer without FPGA-based
test platforms. We introduce GPUHammer, the first Rowhammer attack on NVIDIA
GPUs with GDDR6 DRAM. GPUHammer proposes novel techniques to reverse-engineer
GDDR DRAM row mappings, and employs GPU-specific memory access optimizations to
amplify hammering intensity and bypass mitigations. Thus, we demonstrate the
first successful Rowhammer attack on a discrete GPU, injecting up to 8
bit-flips across 4 DRAM banks on an NVIDIA A6000 with GDDR6 memory. We also
show how an attacker can use these to tamper with ML models, causing
significant accuracy drops (up to 80%).

</details>


### [89] [TruChain: A Multi-Layer Architecture for Trusted, Verifiable, and Immutable Open Banking Data](https://arxiv.org/abs/2507.08286)
*Aufa Nasywa Rahman, Bimo Sunarfri Hantono, Guntur Dharma Putra*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种三层架构，用于解决开放银行标准中存在的科技风险，包括未经验证的数据来源、不一致的数据完整性和缺乏不可变性。


<details>
  <summary>更多</summary>
  
**动机:** 开放银行框架使第三方提供商能够跨银行机构访问金融数据，导致金融部门出现了前所未有的创新。然而，一些开放银行标准仍然容易受到严重的科技风险的影响，包括未经验证的数据来源、不一致的数据完整性和缺乏不可变性。

**方法:** 提出了一种分层架构，该架构通过三个不同层次的信任（包括源验证、数据级身份验证和防篡改存储）来确保数据的可信度。第一层使用去中心化身份和可验证展示来保证源合法性，第二层使用加密签名验证数据的真实性和一致性，第三层通过有向无环图分布式账本Tangle保证数据不可变性。

**结果:** 实施了概念验证实现以评估其性能，结果表明系统具有稳定的吞吐量，100% 的验证率，并且CPU利用率低于35%，内存使用量低于350MiB。与现实世界的开放银行实现相比，我们的解决方案提供了显著降低的延迟和更强的数据完整性保障。

**结论:** 提出的解决方案为金融生态系统中的安全数据共享提供了一个实用且高效的系统，同时保持了法规遵从性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TruChain%3A+A+Multi-Layer+Architecture+for+Trusted%2C+Verifiable%2C+and+Immutable+Open+Banking+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08286，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08286&send_immediately=true&force_search=false)

**原文摘要:** Open banking framework enables third party providers to access financial data
across banking institutions, leading to unprecedented innovations in the
financial sector. However, some open banking standards remain susceptible to
severe technological risks, including unverified data sources, inconsistent
data integrity, and lack of immutability. In this paper, we propose a layered
architecture that provides assurance in data trustworthiness with three
distinct levels of trust, covering source validation, data-level
authentication, and tamper-proof storage. The first layer guarantees the source
legitimacy using decentralized identity and verifiable presentation, while the
second layer verifies data authenticity and consistency using cryptographic
signing. Lastly, the third layer guarantees data immutability through the
Tangle, a directed acyclic graph distributed ledger. We implemented a
proof-of-concept implementation of our solution to evaluate its performance,
where the results demonstrate that the system scales linearly with a stable
throughput, exhibits a 100% validation rate, and utilizes under 35% of CPU and
350 MiB memory. Compared to a real-world open banking implementation, our
solution offers significantly reduced latency and stronger data integrity
assurance. Overall, our solution offers a practical and efficient system for
secure data sharing in financial ecosystems while maintaining regulatory
compliance.

</details>


### [90] [Invariant-based Robust Weights Watermark for Large Language Models](https://arxiv.org/abs/2507.08288)
*Qingxiao Guo, Xinjie Zhu, Yilong Ma, Hui Jin, Yunhao Wang, Weifeng Zhang, Xiaobing Guo*

**主要类别:** cs.CR

**AI概要:** This paper introduces a robust watermarking scheme for transformer models without retraining or fine-tuning, using unique keys and a noise mechanism to protect against IP theft.


<details>
  <summary>更多</summary>
  
**动机:** Watermarking technology is important for protecting intellectual property rights, especially with the increasing use of large language models on edge devices.

**方法:** A unique key is generated for each user, and a stable watermark value is derived by solving linear constraints from model invariants. A noise mechanism is used to hide watermark locations in multi-user scenarios.

**结果:** The approach was evaluated on three popular models (Llama3, Phi3, Gemma) and showed strong robustness against various attack methods.

**结论:** The watermarking scheme is robust and effective in protecting IP rights for transformer models.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Invariant-based+Robust+Weights+Watermark+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08288，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08288&send_immediately=true&force_search=false)

**原文摘要:** Watermarking technology has gained significant attention due to the
increasing importance of intellectual property (IP) rights, particularly with
the growing deployment of large language models (LLMs) on billions
resource-constrained edge devices. To counter the potential threats of IP theft
by malicious users, this paper introduces a robust watermarking scheme without
retraining or fine-tuning for transformer models. The scheme generates a unique
key for each user and derives a stable watermark value by solving linear
constraints constructed from model invariants. Moreover, this technology
utilizes noise mechanism to hide watermark locations in multi-user scenarios
against collusion attack. This paper evaluates the approach on three popular
models (Llama3, Phi3, Gemma), and the experimental results confirm the strong
robustness across a range of attack methods (fine-tuning, pruning,
quantization, permutation, scaling, reversible matrix and collusion attacks).

</details>


### [91] [Evaluating Post-Quantum Cryptographic Algorithms on Resource-Constrained Devices](https://arxiv.org/abs/2507.08312)
*Jesus Lopez, Viviana Cadena, Mohammad Saidur Rahman*

**主要类别:** cs.CR

**AI概要:** This paper investigates the feasibility of deploying post-quantum cryptography (PQC) algorithms on resource-constrained devices.


<details>
  <summary>更多</summary>
  
**动机:** The rapid advancement of quantum computing poses a critical threat to classical cryptographic algorithms such as RSA and ECC, particularly in IoT devices.

**方法:** Implementing three PQC algorithms (BIKE, CRYSTALS-Kyber, and HQC) on a lightweight IoT platform built with Raspberry Pi devices. Leveraging the Open Quantum Safe (liboqs) library in conjunction with mbedTLS to develop quantum-secure key exchange protocols.

**结果:** Experimental results demonstrate that the integration of PQC algorithms on constrained hardware is practical.

**结论:** The integration of PQC algorithms on constrained hardware is practical and there is an urgent need for quantum-resilient cryptographic frameworks in next-generation IoT devices.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+Post-Quantum+Cryptographic+Algorithms+on+Resource-Constrained+Devices，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08312，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08312&send_immediately=true&force_search=false)

**原文摘要:** The rapid advancement of quantum computing poses a critical threat to
classical cryptographic algorithms such as RSA and ECC, particularly in
Internet of Things (IoT) devices, where secure communication is essential but
often constrained by limited computational resources. This paper investigates
the feasibility of deploying post-quantum cryptography (PQC) algorithms on
resource-constrained devices. In particular, we implement three PQC algorithms
-- BIKE, CRYSTALS-Kyber, and HQC -- on a lightweight IoT platform built with
Raspberry Pi devices. Leveraging the Open Quantum Safe (\texttt{liboqs})
library in conjunction with \texttt{mbedTLS}, we develop quantum-secure key
exchange protocols, and evaluate their performance in terms of computational
overhead, memory usage, and energy consumption for quantum secure
communication. Experimental results demonstrate that the integration of PQC
algorithms on constrained hardware is practical, reinforcing the urgent need
for quantum-resilient cryptographic frameworks in next-generation IoT devices.
The implementation of this paper is available at
https://iqsec-lab.github.io/PQC-IoT/.

</details>


### [92] [Qualcomm Trusted Application Emulation for Fuzzing Testing](https://arxiv.org/abs/2507.08331)
*Chun-I Fan, Li-En Chang, Cheng-Han Shie*

**主要类别:** cs.CR

**AI概要:** This research focuses on trusted applications within the Qualcomm TEE and introduces a novel emulator specifically designed for these applications. The research makes a significant contribution by providing both the implementation methods and source codes for the emulator.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this research is the increasing awareness of cybersecurity and the need to safeguard sensitive user information within hardware devices and products. Vulnerabilities within Trusted Execution Environments (TEEs) present significant risks, which could lead to the leakage of sensitive data and compromise user privacy and security.

**方法:** The research involves reverse engineering techniques to thoroughly analyze Qualcomm TAs and develop a partial emulation environment. Additionally, fuzzing testing techniques are integrated into the emulator to systematically uncover potential vulnerabilities.

**结果:** The research introduces a novel emulator specifically designed for trusted applications (TAs) within the Qualcomm TEE. It also provides both the implementation methods and source codes for the emulator, offering a valuable reference for future research efforts.

**结论:** This research provides a valuable reference for future research efforts in the field of trusted applications within Qualcomm TEE.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Qualcomm+Trusted+Application+Emulation+for+Fuzzing+Testing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08331，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08331&send_immediately=true&force_search=false)

**原文摘要:** In recent years, the increasing awareness of cybersecurity has led to a
heightened focus on information security within hardware devices and products.
Incorporating Trusted Execution Environments (TEEs) into product designs has
become a standard practice for safeguarding sensitive user information.
However, vulnerabilities within these components present significant risks, if
exploited by attackers, these vulnerabilities could lead to the leakage of
sensitive data, thereby compromising user privacy and security. This research
centers on trusted applications (TAs) within the Qualcomm TEE and introduces a
novel emulator specifically designed for these applications. Through reverse
engineering techniques, we thoroughly analyze Qualcomm TAs and develop a
partial emulation environment that accurately emulates their behavior.
Additionally, we integrate fuzzing testing techniques into the emulator to
systematically uncover potential vulnerabilities within Qualcomm TAs,
demonstrating its practical effectiveness in identifying real-world security
flaws. This research makes a significant contribution by being the first to
provide both the implementation methods and source codes for a Qualcomm TAs
emulator, offering a valuable reference for future research efforts. Unlike
previous approaches that relied on complex and resource-intensive full-system
simulations, our approach is lightweight and effective, making security testing
of TA more convenient.

</details>


### [93] [White-Basilisk: A Hybrid Model for Code Vulnerability Detection](https://arxiv.org/abs/2507.08540)
*Ioannis Lamprou, Alexander Shevtsov, Ioannis Arapakis, Sotiris Ioannidis*

**主要类别:** cs.CR

**AI概要:** White-Basilisk is a new method for detecting software vulnerabilities that beats current standards, using fewer resources and challenging AI model scaling assumptions.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to address the challenge of software vulnerability detection with more effective methodologies.

**方法:** White-Basilisk utilizes an architecture that integrates Mamba layers, linear self-attention, and a Mixture of Experts framework.

**结果:** White-Basilisk achieves state-of-the-art results in vulnerability detection with only 200M parameters, processing extensive codebases in a single pass, and performs robustly on imbalanced datasets while maintaining computational efficiency.

**结论:** White-Basilisk establishes new benchmarks in code security and suggests that compact models can outperform larger ones in specialized tasks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是White-Basilisk%3A+A+Hybrid+Model+for+Code+Vulnerability+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.08540，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.08540&send_immediately=true&force_search=false)

**原文摘要:** The proliferation of software vulnerabilities presents a significant
challenge to cybersecurity, necessitating more effective detection
methodologies. We introduce White-Basilisk, a novel approach to vulnerability
detection that demonstrates superior performance while challenging prevailing
assumptions in AI model scaling. Utilizing an innovative architecture that
integrates Mamba layers, linear self-attention, and a Mixture of Experts
framework, White-Basilisk achieves state-of-the-art results in vulnerability
detection tasks with a parameter count of only 200M. The model's capacity to
process sequences of unprecedented length enables comprehensive analysis of
extensive codebases in a single pass, surpassing the context limitations of
current Large Language Models (LLMs). White-Basilisk exhibits robust
performance on imbalanced, real-world datasets, while maintaining computational
efficiency that facilitates deployment across diverse organizational scales.
This research not only establishes new benchmarks in code security but also
provides empirical evidence that compact, efficiently designed models can
outperform larger counterparts in specialized tasks, potentially redefining
optimization strategies in AI development for domain-specific applications.

</details>
