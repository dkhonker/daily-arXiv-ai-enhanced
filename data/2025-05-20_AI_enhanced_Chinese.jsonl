{"id": "2505.10605", "pdf": "https://arxiv.org/pdf/2505.10605", "abs": "https://arxiv.org/abs/2505.10605", "authors": ["Frederik K\u00f6hne", "Anton Schiela"], "title": "An Exponential Averaging Process with Strong Convergence Properties", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST", "stat.TH", "60F15 (Primary) 60G10, 60J20, 68T05, 90C15 (Secondary)"], "comment": null, "summary": "Averaging, or smoothing, is a fundamental approach to obtain stable,\nde-noised estimates from noisy observations. In certain scenarios, observations\nmade along trajectories of random dynamical systems are of particular interest.\nOne popular smoothing technique for such a scenario is exponential moving\naveraging (EMA), which assigns observations a weight that decreases\nexponentially in their age, thus giving younger observations a larger weight.\nHowever, EMA fails to enjoy strong stochastic convergence properties, which\nstems from the fact that the weight assigned to the youngest observation is\nconstant over time, preventing the noise in the averaged quantity from\ndecreasing to zero. In this work, we consider an adaptation to EMA, which we\ncall $p$-EMA, where the weights assigned to the last observations decrease to\nzero at a subharmonic rate. We provide stochastic convergence guarantees for\nthis kind of averaging under mild assumptions on the autocorrelations of the\nunderlying random dynamical system. We further discuss the implications of our\nresults for a recently introduced adaptive step size control for Stochastic\nGradient Descent (SGD), which uses $p$-EMA for averaging noisy observations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6307\u6570\u79fb\u52a8\u5e73\u5747\u6cd5(p-EMA)\uff0c\u5728\u968f\u673a\u52a8\u529b\u7cfb\u7edf\u8f68\u8ff9\u89c2\u6d4b\u7684\u5e73\u6ed1\u95ee\u9898\u4e0a\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u6536\u655b\u4fdd\u8bc1\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5bf9\u81ea\u9002\u5e94\u6b65\u957f\u63a7\u5236\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7684\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u6307\u6570\u79fb\u52a8\u5e73\u5747\u6cd5(EMA)\u5728\u5904\u7406\u968f\u673a\u52a8\u529b\u7cfb\u7edf\u7684\u89c2\u6d4b\u65f6\uff0c\u7531\u4e8e\u5bf9\u6700\u65b0\u89c2\u6d4b\u503c\u6743\u91cd\u6052\u5b9a\uff0c\u65e0\u6cd5\u4f7f\u5e73\u5747\u91cf\u4e2d\u7684\u566a\u58f0\u51cf\u5c0f\u5230\u96f6\uff0c\u7f3a\u4e4f\u5f3a\u7684\u968f\u673a\u6536\u655b\u6027\u8d28\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5p-EMA\uff0c\u5176\u6700\u8fd1\u89c2\u6d4b\u503c\u7684\u6743\u91cd\u4ee5\u6b21\u8c10\u6ce2\u901f\u7387\u51cf\u5c0f\u3002", "result": "\u5728\u968f\u673a\u52a8\u529b\u7cfb\u7edf\u81ea\u76f8\u5173\u6027\u5047\u8bbe\u4e0b\uff0c\u4e3a\u8fd9\u79cd\u5e73\u6ed1\u65b9\u6cd5\u63d0\u4f9b\u4e86\u968f\u673a\u6536\u655b\u4fdd\u8bc1\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684p-EMA\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u589e\u5f3a\u4e86\u5bf9\u968f\u673a\u52a8\u529b\u7cfb\u7edf\u8f68\u8ff9\u89c2\u6d4b\u7684\u5e73\u6ed1\u80fd\u529b\uff0c\u5e76\u4e14\u5728\u5b9e\u8df5\u4e2d\u5bf9\u81ea\u9002\u5e94\u6b65\u957f\u63a7\u5236\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u6709\u6f5c\u5728\u5f71\u54cd\u3002"}}
{"id": "2505.10628", "pdf": "https://arxiv.org/pdf/2505.10628", "abs": "https://arxiv.org/abs/2505.10628", "authors": ["Jonathan Garc\u00eda", "Philipp Petersen"], "title": "Minimax learning rates for estimating binary classifiers under margin conditions", "categories": ["stat.ML", "cs.LG", "math.PR", "68T05, 62C20, 41A25, 41A46"], "comment": null, "summary": "We study classification problems using binary estimators where the decision\nboundary is described by horizon functions and where the data distribution\nsatisfies a geometric margin condition. We establish upper and lower bounds for\nthe minimax learning rate over broad function classes with bounded Kolmogorov\nentropy in Lebesgue norms. A key novelty of our work is the derivation of lower\nbounds on the worst-case learning rates under a geometric margin condition -- a\nsetting that is almost universally satisfied in practice but remains\ntheoretically challenging. Moreover, our results deal with the noiseless\nsetting, where lower bounds are particularly hard to establish. We apply our\ngeneral results to classification problems with decision boundaries belonging\nto several function classes: for Barron-regular functions, and for\nH\\\"older-continuous functions with strong margins, we identify optimal rates\nclose to the fast learning rates of $\\mathcal{O}(n^{-1})$ for $n \\in\n\\mathbb{N}$ samples. Also for merely convex decision boundaries, in a strong\nmargin case optimal rates near $\\mathcal{O}(n^{-1/2})$ can be achieved.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4f7f\u7528\u4e8c\u5143\u4f30\u8ba1\u5668\u7684\u5206\u7c7b\u95ee\u9898\uff0c\u7ed9\u51fa\u4e86\u5728\u6ee1\u8db3\u51e0\u4f55\u8fb9\u8ddd\u6761\u4ef6\u7684\u6570\u636e\u5206\u5e03\u4e0b\u7684\u5b66\u4e60\u7387\u4e0a\u4e0b\u754c\u3002\u7279\u522b\u662f\uff0c\u5728\u65e0\u566a\u58f0\u8bbe\u5b9a\u4e0b\uff0c\u5bf9\u4e8eBarron-\u6b63\u5219\u51fd\u6570\u548c\u5f3a\u8fb9\u8ddd\u7684H\u00f6lder\u8fde\u7eed\u51fd\u6570\uff0c\u8bc6\u522b\u51fa\u63a5\u8fd1\u5feb\u5b66\u4e60\u7387\u7684\u7ed3\u679c\u3002", "motivation": "\u7814\u7a76\u6ee1\u8db3\u666e\u904d\u5b9e\u8df5\u4e2d\u7684\u51e0\u4f55\u8fb9\u8ddd\u6761\u4ef6\u4f46\u7406\u8bba\u4e0a\u6709\u6311\u6218\u6027\u7684\u5206\u7c7b\u95ee\u9898\u3002", "method": "\u5206\u6790\u5177\u6709\u51e0\u4f55\u8fb9\u8ddd\u6761\u4ef6\u7684\u51b3\u7b56\u8fb9\u754c\u7684\u5b66\u4e60\u7387\uff0c\u5e76\u7ed9\u51fa\u4e0a\u4e0b\u754c\u3002", "result": "\u5bf9\u4e8eBarron-\u6b63\u5219\u51fd\u6570\u548c\u5f3a\u8fb9\u8ddd\u7684H\u00f6lder\u8fde\u7eed\u51fd\u6570\uff0c\u63a5\u8fd1\u5feb\u5b66\u4e60\u7387\uff1b\u5bf9\u4e8e\u4ec5\u51f8\u7684\u51b3\u7b56\u8fb9\u754c\uff0c\u5728\u5f3a\u8fb9\u8ddd\u60c5\u51b5\u4e0b\u53ef\u8fbe\u5230\u63a5\u8fd1\u6839\u53f7\u5012\u6570\u7684\u5b66\u4e60\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u5173\u4e8e\u5206\u7c7b\u95ee\u9898\u5b66\u4e60\u7387\u7684\u91cd\u8981\u7406\u8bba\u8d21\u732e\uff0c\u7279\u522b\u662f\u5728\u65e0\u566a\u58f0\u6761\u4ef6\u4e0b\u3002"}}
{"id": "2505.11006", "pdf": "https://arxiv.org/pdf/2505.11006", "abs": "https://arxiv.org/abs/2505.11006", "authors": ["Oskar Allerbo", "Thomas B. Sch\u00f6n"], "title": "Supervised Models Can Generalize Also When Trained on Random Label", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "The success of unsupervised learning raises the question of whether also\nsupervised models can be trained without using the information in the output\n$y$. In this paper, we demonstrate that this is indeed possible. The key step\nis to formulate the model as a smoother, i.e. on the form $\\hat{f}=Sy$, and to\nconstruct the smoother matrix $S$ independently of $y$, e.g. by training on\nrandom labels. We present a simple model selection criterion based on the\ndistribution of the out-of-sample predictions and show that, in contrast to\ncross-validation, this criterion can be used also without access to $y$. We\ndemonstrate on real and synthetic data that $y$-free trained versions of linear\nand kernel ridge regression, smoothing splines, and neural networks perform\nsimilarly to their standard, $y$-based, versions and, most importantly,\nsignificantly better than random guessing.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8f93\u51fa\u6807\u7b7ey\u7684\u76d1\u7763\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u5176\u6027\u80fd\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\u3002", "motivation": "\u63a2\u8ba8\u662f\u5426\u53ef\u4ee5\u65e0\u9700\u4f7f\u7528\u8f93\u51fa\u4fe1\u606fy\u6765\u8bad\u7ec3\u76d1\u7763\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u5c06\u6a21\u578b\u8868\u8ff0\u4e3a\u5e73\u6ed1\u5668\u5e76\u6784\u5efa\u72ec\u7acb\u4e8ey\u7684\u5e73\u6ed1\u77e9\u9635S\uff0c\u4f8b\u5982\u901a\u8fc7\u968f\u673a\u6807\u7b7e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u63d0\u51fa\u7684y-free\u8bad\u7ec3\u65b9\u6cd5\u5728\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4e14\u65e0\u9700\u8bbf\u95eey\u3002", "conclusion": "\u6210\u529f\u5c55\u793a\u4e86\u65e0\u9700\u8f93\u51fa\u6807\u7b7ey\u4e5f\u80fd\u8bad\u7ec3\u76d1\u7763\u6a21\u578b\uff0c\u4e14\u6027\u80fd\u4e0e\u6807\u51c6\u7248\u672c\u76f8\u5f53\u3002"}}
{"id": "2505.11089", "pdf": "https://arxiv.org/pdf/2505.11089", "abs": "https://arxiv.org/abs/2505.11089", "authors": ["Yiran Yang", "Rui Chen"], "title": "Inexact Column Generation for Bayesian Network Structure Learning via Difference-of-Submodular Optimization", "categories": ["stat.ML", "cs.LG", "math.OC"], "comment": null, "summary": "In this paper, we consider a score-based Integer Programming (IP) approach\nfor solving the Bayesian Network Structure Learning (BNSL) problem.\nState-of-the-art BNSL IP formulations suffer from the exponentially large\nnumber of variables and constraints. A standard approach in IP to address such\nchallenges is to employ row and column generation techniques, which dynamically\ngenerate rows and columns, while the complex pricing problem remains a\ncomputational bottleneck for BNSL. For the general class of $\\ell_0$-penalized\nlikelihood scores, we show how the pricing problem can be reformulated as a\ndifference of submodular optimization problem, and how the Difference of Convex\nAlgorithm (DCA) can be applied as an inexact method to efficiently solve the\npricing problems. Empirically, we show that, for continuous Gaussian data, our\nrow and column generation approach yields solutions with higher quality than\nstate-of-the-art score-based approaches, especially when the graph density\nincreases, and achieves comparable performance against benchmark\nconstraint-based and hybrid approaches, even when the graph size increases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5206\u6570\u7684\u6574\u6570\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u884c\u548c\u5217\u6765\u89e3\u51b3BNSL\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u9ad8\u5bc6\u5ea6\u548c\u5927\u89c4\u6a21\u56fe\u65f6\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u8d1d\u53f6\u65af\u7f51\u7edc\u7ed3\u6784\u5b66\u4e60\uff08BNSL\uff09\u6574\u6570\u89c4\u5212\u516c\u5f0f\u53d7\u5230\u53d8\u91cf\u548c\u7ea6\u675f\u6570\u91cf\u5448\u6307\u6570\u589e\u957f\u7684\u6311\u6218\uff0c\u800c\u590d\u6742\u7684\u5b9a\u4ef7\u95ee\u9898\u6210\u4e3a\u8ba1\u7b97\u74f6\u9888\u3002", "method": "\u901a\u8fc7\u5c06\u5b9a\u4ef7\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u5dee\u5206\u6b21\u6a21\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5e94\u7528\u5dee\u5f02\u51f8\u7b97\u6cd5\uff08DCA\uff09\u4f5c\u4e3a\u4e0d\u7cbe\u786e\u65b9\u6cd5\u6765\u9ad8\u6548\u89e3\u51b3\u5b9a\u4ef7\u95ee\u9898\u3002", "result": "\u5bf9\u4e8e\u8fde\u7eed\u9ad8\u65af\u6570\u636e\uff0c\u63d0\u51fa\u7684\u884c\u548c\u5217\u751f\u6210\u65b9\u6cd5\u5f97\u5230\u7684\u89e3\u8d28\u91cf\u9ad8\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u5206\u6570\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u56fe\u5bc6\u5ea6\u589e\u52a0\u65f6\uff0c\u5e76\u4e14\u5728\u56fe\u89c4\u6a21\u589e\u52a0\u65f6\uff0c\u5176\u8868\u73b0\u4e0e\u57fa\u51c6\u7684\u7ea6\u675f\u548c\u6df7\u5408\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u91c7\u7528\u884c\u548c\u5217\u751f\u6210\u65b9\u6cd5\u7684\u57fa\u4e8e\u5206\u6570\u7684\u6574\u6570\u89c4\u5212\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u5bc6\u5ea6\u56fe\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u5728\u5904\u7406\u5927\u89c4\u6a21\u56fe\u65f6\u4e5f\u8fbe\u5230\u4e86\u4e0e\u57fa\u51c6\u7ea6\u675f\u548c\u6df7\u5408\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002"}}
{"id": "2505.10597", "pdf": "https://arxiv.org/pdf/2505.10597", "abs": "https://arxiv.org/abs/2505.10597", "authors": ["Jiazheng Zhang", "Wenqing Jing", "Zizhuo Zhang", "Zhiheng Xi", "Shihan Dou", "Rongxiang Weng", "Jiahuan Li", "Jingang Wang", "Mingxu Chai", "Shibo Hong", "Tao Gui", "Qi Zhang"], "title": "Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reward models (RMs) play a pivotal role in aligning large language models\n(LLMs) with human values. However, noisy preferences in human feedback can lead\nto reward misgeneralization - a phenomenon where reward models learn spurious\ncorrelations or overfit to noisy preferences, which poses important challenges\nto the generalization of RMs. This paper systematically analyzes the\ncharacteristics of preference pairs and aims to identify how noisy preferences\ndiffer from human-aligned preferences in reward modeling. Our analysis reveals\nthat noisy preferences are difficult for RMs to fit, as they cause sharp\ntraining fluctuations and irregular gradient updates. These distinctive\ndynamics suggest the feasibility of identifying and excluding such noisy\npreferences. Empirical studies demonstrate that policy LLM optimized with a\nreward model trained on the full preference dataset, which includes substantial\nnoise, performs worse than the one trained on a subset of exclusively high\nquality preferences. To address this challenge, we propose an online\nCollaborative Reward Modeling (CRM) framework to achieve robust preference\nlearning through peer review and curriculum learning. In particular, CRM\nmaintains two RMs that collaboratively filter potential noisy preferences by\npeer-reviewing each other's data selections. Curriculum learning synchronizes\nthe capabilities of two models, mitigating excessive disparities to promote the\nutility of peer review. Extensive experiments demonstrate that CRM\nsignificantly enhances RM generalization, with up to 9.94 points improvement on\nRewardBench under an extreme 40\\% noise. Moreover, CRM can seamlessly extend to\nimplicit-reward alignment methods, offering a robust and versatile alignment\nstrategy.", "AI": {"tldr": "This paper analyzes the characteristics of preference pairs in reward models and identifies noisy preferences that affect the generalization of reward models. It proposes an online Collaborative Reward Modeling (CRM) framework to improve the robustness of preference learning.", "motivation": "To address the issue of reward misgeneralization caused by noisy preferences in human feedback.", "method": "Systematically analyzing the characteristics of preference pairs and proposing an online Collaborative Reward Modeling (CRM) framework.", "result": "The proposed CRM framework significantly enhances RM generalization, with up to 9.94 points improvement on RewardBench under an extreme 40% noise.", "conclusion": "CRM offers a robust and versatile alignment strategy that can be extended to implicit-reward alignment methods."}}
{"id": "2505.10653", "pdf": "https://arxiv.org/pdf/2505.10653", "abs": "https://arxiv.org/abs/2505.10653", "authors": ["Sandeep Neema", "Susmit Jha", "Adam Nagel", "Ethan Lew", "Chandrasekar Sureshkumar", "Aleksa Gordic", "Chase Shimmin", "Hieu Nguygen", "Paul Eremenko"], "title": "On the Evaluation of Engineering Artificial General Intelligence", "categories": ["cs.AI", "I.2; J.2; J.6"], "comment": "21 pages", "summary": "We discuss the challenges and propose a framework for evaluating engineering\nartificial general intelligence (eAGI) agents. We consider eAGI as a\nspecialization of artificial general intelligence (AGI), deemed capable of\naddressing a broad range of problems in the engineering of physical systems and\nassociated controllers. We exclude software engineering for a tractable scoping\nof eAGI and expect dedicated software engineering AI agents to address the\nsoftware implementation challenges. Similar to human engineers, eAGI agents\nshould possess a unique blend of background knowledge (recall and retrieve) of\nfacts and methods, demonstrate familiarity with tools and processes, exhibit\ndeep understanding of industrial components and well-known design families, and\nbe able to engage in creative problem solving (analyze and synthesize),\ntransferring ideas acquired in one context to another. Given this broad\nmandate, evaluating and qualifying the performance of eAGI agents is a\nchallenge in itself and, arguably, a critical enabler to developing eAGI\nagents. In this paper, we address this challenge by proposing an extensible\nevaluation framework that specializes and grounds Bloom's taxonomy - a\nframework for evaluating human learning that has also been recently used for\nevaluating LLMs - in an engineering design context. Our proposed framework\nadvances the state of the art in benchmarking and evaluation of AI agents in\nterms of the following: (a) developing a rich taxonomy of evaluation questions\nspanning from methodological knowledge to real-world design problems; (b)\nmotivating a pluggable evaluation framework that can evaluate not only textual\nresponses but also evaluate structured design artifacts such as CAD models and\nSysML models; and (c) outlining an automatable procedure to customize the\nevaluation benchmark to different engineering contexts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u5de5\u7a0b\u4eba\u5de5\u901a\u7528\u667a\u80fd\uff08eAGI\uff09\u4ee3\u7406\u7684\u6846\u67b6\u3002\u8be5\u6846\u67b6\u57fa\u4e8eBloom\u7684\u5b66\u4e60\u8bc4\u4f30\u5206\u7c7b\u6cd5\uff0c\u5e76\u5728\u5de5\u7a0b\u8bbe\u8ba1\u80cc\u666f\u4e0b\u8fdb\u884c\u4e86\u6269\u5c55\u548c\u5177\u4f53\u5316\u3002", "motivation": "\u7814\u7a76eAGI\u4ee3\u7406\u7684\u8bc4\u4f30\u95ee\u9898\uff0c\u56e0\u4e3a\u5176\u6027\u80fd\u8bc4\u4f30\u5177\u6709\u6311\u6218\u6027\uff0c\u4f46\u5bf9\u5f00\u53d1eAGI\u4ee3\u7406\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBloom\u5206\u7c7b\u6cd5\u6269\u5c55\u7684\u53ef\u6269\u5c55\u8bc4\u4f30\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5de5\u7a0b\u8bbe\u8ba1\u80cc\u666f\u3002", "result": "\u6846\u67b6\u53ef\u4ee5\u8bc4\u4f30\u4ece\u65b9\u6cd5\u5b66\u5230\u5b9e\u9645\u8bbe\u8ba1\u95ee\u9898\u7684\u4e30\u5bcc\u8bc4\u4f30\u95ee\u9898\u96c6\uff0c\u5e76\u80fd\u8bc4\u4ef7CAD\u6a21\u578b\u548cSysML\u6a21\u578b\u7b49\u7ed3\u6784\u5316\u8bbe\u8ba1\u5de5\u4ef6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u63a8\u8fdb\u4e86AI\u4ee3\u7406\u5728\u5de5\u7a0b\u9886\u57df\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u7684\u524d\u6cbf\u3002"}}
{"id": "2505.11143", "pdf": "https://arxiv.org/pdf/2505.11143", "abs": "https://arxiv.org/abs/2505.11143", "authors": ["William R. P. Denault"], "title": "Nash: Neural Adaptive Shrinkage for Structured High-Dimensional Regression", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Sparse linear regression is a fundamental tool in data analysis. However,\ntraditional approaches often fall short when covariates exhibit structure or\narise from heterogeneous sources. In biomedical applications, covariates may\nstem from distinct modalities or be structured according to an underlying\ngraph. We introduce Neural Adaptive Shrinkage (Nash), a unified framework that\nintegrates covariate-specific side information into sparse regression via\nneural networks. Nash adaptively modulates penalties on a per-covariate basis,\nlearning to tailor regularization without cross-validation. We develop a\nvariational inference algorithm for efficient training and establish\nconnections to empirical Bayes regression. Experiments on real data demonstrate\nthat Nash can improve accuracy and adaptability over existing methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNeural Adaptive Shrinkage (Nash) \u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u7ed3\u6784\u5316\u7a00\u758f\u56de\u5f52\u95ee\u9898\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u6574\u5408\u5148\u9a8c\u4fe1\u606f\uff0c\u65e0\u9700\u4ea4\u53c9\u9a8c\u8bc1\u5373\u53ef\u81ea\u9002\u5e94\u8c03\u6574\u6b63\u5219\u5316\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u51c6\u786e\u6027\u4e0e\u9002\u5e94\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7a00\u758f\u7ebf\u6027\u56de\u5f52\u65b9\u6cd5\u5728\u5904\u7406\u5177\u6709\u7ed3\u6784\u6216\u6765\u81ea\u5f02\u6784\u6765\u6e90\u7684\u534f\u53d8\u91cf\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u751f\u7269\u533b\u5b66\u5e94\u7528\u4e2d\uff0c\u534f\u53d8\u91cf\u53ef\u80fd\u6765\u6e90\u4e8e\u4e0d\u540c\u7684\u6a21\u6001\u6216\u9075\u5faa\u6f5c\u5728\u56fe\u7ed3\u6784\u3002", "method": "\u5f15\u5165\u4e86Neural Adaptive Shrinkage (Nash) \u6846\u67b6\uff0c\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u6574\u5408\u534f\u53d8\u91cf\u7279\u5b9a\u7684\u4fa7\u4fe1\u606f\u5230\u7a00\u758f\u56de\u5f52\u4e2d\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u53d8\u5206\u63a8\u7406\u7b97\u6cd5\u7528\u4e8e\u9ad8\u6548\u8bad\u7ec3\uff0c\u540c\u65f6\u5efa\u7acb\u4e86\u4e0e\u7ecf\u9a8c\u8d1d\u53f6\u65af\u56de\u5f52\u7684\u8054\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eNash\u65b9\u6cd5\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Nash\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u4e14\u7075\u6d3b\u7684\u6846\u67b6\u6765\u89e3\u51b3\u5e26\u6709\u7ed3\u6784\u5316\u534f\u53d8\u91cf\u7684\u7a00\u758f\u56de\u5f52\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u826f\u597d\u7684\u6027\u80fd\u548c\u6f5c\u529b\u3002"}}
{"id": "2505.10599", "pdf": "https://arxiv.org/pdf/2505.10599", "abs": "https://arxiv.org/abs/2505.10599", "authors": ["Jiaxuan Liu", "Zhenhua Ling"], "title": "UDDETTS: Unifying Discrete and Dimensional Emotions for Controllable Emotional Text-to-Speech", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Under review", "summary": "Recent neural codec language models have made great progress in the field of\ntext-to-speech (TTS), but controllable emotional TTS still faces many\nchallenges. Traditional methods rely on predefined discrete emotion labels to\ncontrol emotion categories and intensities, which can't capture the complexity\nand continuity of human emotional perception and expression. The lack of\nlarge-scale emotional speech datasets with balanced emotion distributions and\nfine-grained emotion annotations often causes overfitting in synthesis models\nand impedes effective emotion control. To address these issues, we propose\nUDDETTS, a neural codec language model unifying discrete and dimensional\nemotions for controllable emotional TTS. This model introduces the\ninterpretable Arousal-Dominance-Valence (ADV) space for dimensional emotion\ndescription and supports emotion control driven by either discrete emotion\nlabels or nonlinearly quantified ADV values. Furthermore, a semi-supervised\ntraining strategy is designed to comprehensively utilize diverse speech\ndatasets with different types of emotion annotations to train the UDDETTS.\nExperiments show that UDDETTS achieves linear emotion control along the three\ndimensions of ADV space, and exhibits superior end-to-end emotional speech\nsynthesis capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u7f16\u89e3\u7801\u8bed\u8a00\u6a21\u578bUDDETTS\uff0c\u7528\u4e8e\u53ef\u63a7\u60c5\u611f\u6587\u672c\u5230\u8bed\u97f3\u5408\u6210\uff0c\u8be5\u6a21\u578b\u7ed3\u5408\u79bb\u6563\u548c\u7ef4\u5ea6\u60c5\u611f\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u89e3\u91ca\u7684\u5524\u9192-\u652f\u914d-\u6548\u4ef7\uff08ADV\uff09\u7a7a\u95f4\u5b9e\u73b0\u60c5\u611f\u63a7\u5236\uff0c\u5e76\u8bbe\u8ba1\u534a\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\u5229\u7528\u4e0d\u540c\u60c5\u611f\u6807\u6ce8\u7684\u591a\u6837\u5316\u8bed\u97f3\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002\u5b9e\u9a8c\u8868\u660eUDDETTS\u5728ADV\u7a7a\u95f4\u7684\u4e09\u4e2a\u7ef4\u5ea6\u4e0a\u5b9e\u73b0\u4e86\u7ebf\u6027\u60c5\u611f\u63a7\u5236\uff0c\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u60c5\u611f\u8bed\u97f3\u5408\u6210\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u79bb\u6563\u60c5\u611f\u6807\u7b7e\u63a7\u5236\u60c5\u611f\u7c7b\u522b\u548c\u5f3a\u5ea6\uff0c\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u60c5\u611f\u611f\u77e5\u548c\u8868\u8fbe\u7684\u590d\u6742\u6027\u548c\u8fde\u7eed\u6027\uff1b\u7f3a\u4e4f\u5927\u89c4\u6a21\u60c5\u611f\u8bed\u97f3\u6570\u636e\u96c6\u5bfc\u81f4\u5408\u6210\u6a21\u578b\u8fc7\u62df\u5408\u5e76\u963b\u788d\u6709\u6548\u60c5\u611f\u63a7\u5236\u3002", "method": "\u63d0\u51faUDDETTS\u6a21\u578b\uff0c\u5f15\u5165ADV\u7a7a\u95f4\u63cf\u8ff0\u7ef4\u5ea6\u60c5\u611f\uff0c\u652f\u6301\u7531\u79bb\u6563\u60c5\u611f\u6807\u7b7e\u6216\u975e\u7ebf\u6027\u91cf\u5316ADV\u503c\u9a71\u52a8\u7684\u60c5\u611f\u63a7\u5236\uff0c\u8bbe\u8ba1\u534a\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\u5229\u7528\u591a\u6837\u5316\u8bed\u97f3\u6570\u636e\u96c6\u3002", "result": "UDDETTS\u5728ADV\u7a7a\u95f4\u7684\u4e09\u4e2a\u7ef4\u5ea6\u4e0a\u5b9e\u73b0\u4e86\u7ebf\u6027\u60c5\u611f\u63a7\u5236\uff0c\u5177\u6709\u5353\u8d8a\u7684\u7aef\u5230\u7aef\u60c5\u611f\u8bed\u97f3\u5408\u6210\u80fd\u529b\u3002", "conclusion": "UDDETTS\u89e3\u51b3\u4e86\u53ef\u63a7\u60c5\u611fTTS\u4e2d\u7684\u8bb8\u591a\u6311\u6218\uff0c\u901a\u8fc7\u7ed3\u5408\u79bb\u6563\u548c\u7ef4\u5ea6\u60c5\u611f\uff0c\u4ee5\u53ca\u91c7\u7528\u534a\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u60c5\u611f\u63a7\u5236\u7684\u6548\u679c\u548c\u8bed\u97f3\u5408\u6210\u8d28\u91cf\u3002"}}
{"id": "2505.10670", "pdf": "https://arxiv.org/pdf/2505.10670", "abs": "https://arxiv.org/abs/2505.10670", "authors": ["Jan Chojnacki"], "title": "Interpretable Risk Mitigation in LLM Agent Systems", "categories": ["cs.AI", "cs.CY", "cs.GT"], "comment": null, "summary": "Autonomous agents powered by large language models (LLMs) enable novel use\ncases in domains where responsible action is increasingly important. Yet the\ninherent unpredictability of LLMs raises safety concerns about agent\nreliability. In this work, we explore agent behaviour in a toy, game-theoretic\nenvironment based on a variation of the Iterated Prisoner's Dilemma. We\nintroduce a strategy-modification method-independent of both the game and the\nprompt-by steering the residual stream with interpretable features extracted\nfrom a sparse autoencoder latent space. Steering with the good-faith\nnegotiation feature lowers the average defection probability by 28 percentage\npoints. We also identify feasible steering ranges for several open-source LLM\nagents. Finally, we hypothesise that game-theoretic evaluation of LLM agents,\ncombined with representation-steering alignment, can generalise to real-world\napplications on end-user devices and embodied platforms.", "AI": {"tldr": "This study investigates agent behavior in a game-theoretic environment using a strategy-modification method based on interpretable features from a sparse autoencoder latent space to improve reliability and safety of autonomous agents powered by large language models.", "motivation": "To address safety concerns and improve reliability of autonomous agents driven by large language models in responsible action domains.", "method": "Introduces a strategy-modification method independent of the game and prompt by steering the residual stream with interpretable features extracted from a sparse autoencoder latent space.", "result": "Steering with the good-faith negotiation feature reduces the average defection probability by 28 percentage points and identifies feasible steering ranges for several open-source LLM agents.", "conclusion": "Hypothesizes that game-theoretic evaluation of LLM agents combined with representation-steering alignment can generalize to real-world applications on end-user devices and embodied platforms."}}
{"id": "2505.11183", "pdf": "https://arxiv.org/pdf/2505.11183", "abs": "https://arxiv.org/abs/2505.11183", "authors": ["Jacob Trauger", "Ambuj Tewari"], "title": "On Next-Token Prediction in LLMs: How End Goals Determine the Consistency of Decoding Algorithms", "categories": ["stat.ML", "cs.CL", "cs.LG"], "comment": "23 pages", "summary": "Probabilistic next-token prediction trained using cross-entropy loss is the\nbasis of most large language models. Given a sequence of previous values,\nnext-token prediction assigns a probability to each possible next value in the\nvocabulary. There are many ways to use next-token prediction to output token\nsequences. This paper examines a few of these algorithms (greedy, lookahead,\nrandom sampling, and temperature-scaled random sampling) and studies their\nconsistency with respect to various goals encoded as loss functions. Although\nconsistency of surrogate losses with respect to a target loss function is a\nwell researched topic, we are the first to study it in the context of LLMs (to\nthe best of our knowledge). We find that, so long as next-token prediction\nconverges to its true probability distribution, random sampling is consistent\nwith outputting sequences that mimic sampling from the true probability\ndistribution. For the other goals, such as minimizing the 0-1 loss on the\nentire sequence, we show no polynomial-time algorithm is optimal for all\nprobability distributions and all decoding algorithms studied are only optimal\nfor a subset of probability distributions. When analyzing these results, we see\nthat there is a dichotomy created between the goals of information retrieval\nand creative generation for the decoding algorithms. This shows that choosing\nthe correct decoding algorithm based on the desired goal is extremely important\nand many of the ones used are lacking theoretical grounding in numerous\nscenarios.", "AI": {"tldr": "This paper examines different algorithms for next-token prediction in large language models and their consistency with respect to various goals.", "motivation": "To study the consistency of surrogate losses with respect to a target loss function in the context of LLMs.", "method": "Examining greedy, lookahead, random sampling, and temperature-scaled random sampling algorithms.", "result": "Random sampling is consistent with outputting sequences that mimic sampling from the true probability distribution when next-token prediction converges to its true probability distribution. For other goals, no polynomial-time algorithm is optimal for all probability distributions.", "conclusion": "Choosing the correct decoding algorithm based on the desired goal is crucial, and many of the ones used lack theoretical grounding in numerous scenarios."}}
{"id": "2505.10600", "pdf": "https://arxiv.org/pdf/2505.10600", "abs": "https://arxiv.org/abs/2505.10600", "authors": ["Md. Ehsanul Haque", "Md. Saymon Hosen Polash", "Md Al-Imran Sanjida Simla", "Md Alomgir Hossain", "Sarwar Jahan"], "title": "Enhancing IoT Cyber Attack Detection in the Presence of Highly Imbalanced Data", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "Published paper of CSNT2025", "summary": "Due to the rapid growth in the number of Internet of Things (IoT) networks,\nthe cyber risk has increased exponentially, and therefore, we have to develop\neffective IDS that can work well with highly imbalanced datasets. A high rate\nof missed threats can be the result, as traditional machine learning models\ntend to struggle in identifying attacks when normal data volume is much higher\nthan the volume of attacks. For example, the dataset used in this study reveals\na strong class imbalance with 94,659 instances of the majority class and only\n28 instances of the minority class, making it quite challenging to determine\nrare attacks accurately. The challenges presented in this research are\naddressed by hybrid sampling techniques designed to improve data imbalance\ndetection accuracy in IoT domains. After applying these techniques, we evaluate\nthe performance of several machine learning models such as Random Forest, Soft\nVoting, Support Vector Classifier (SVC), K-Nearest Neighbors (KNN), Multi-Layer\nPerceptron (MLP), and Logistic Regression with respect to the classification of\ncyber-attacks. The obtained results indicate that the Random Forest model\nachieved the best performance with a Kappa score of 0.9903, test accuracy of\n0.9961, and AUC of 0.9994. Strong performance is also shown by the Soft Voting\nmodel, with an accuracy of 0.9952 and AUC of 0.9997, indicating the benefits of\ncombining model predictions. Overall, this work demonstrates the value of\nhybrid sampling combined with robust model and feature selection for\nsignificantly improving IoT security against cyber-attacks, especially in\nhighly imbalanced data environments.", "AI": {"tldr": "This study addresses the challenge of detecting rare cyber-attacks in IoT networks with highly imbalanced datasets using hybrid sampling techniques. It evaluates various machine learning models and finds that the Random Forest model performs best.", "motivation": "To develop effective Intrusion Detection Systems (IDS) for IoT networks with imbalanced datasets due to rapidly growing cyber risks.", "method": "Hybrid sampling techniques were applied to improve data imbalance detection accuracy, followed by evaluating multiple machine learning models including Random Forest, Soft Voting, SVC, KNN, MLP, and Logistic Regression.", "result": "Random Forest model achieved the best performance with a Kappa score of 0.9903, test accuracy of 0.9961, and AUC of 0.9994. Soft Voting model also performed strongly.", "conclusion": "Hybrid sampling combined with robust model and feature selection significantly improves IoT security against cyber-attacks, particularly in imbalanced data environments."}}
{"id": "2505.10705", "pdf": "https://arxiv.org/pdf/2505.10705", "abs": "https://arxiv.org/abs/2505.10705", "authors": ["Matej Hoffmann", "Shubhan Parag Patni"], "title": "Embodied AI in Machine Learning -- is it Really Embodied?", "categories": ["cs.AI", "cs.NE", "cs.RO", "68T40", "I.2.9"], "comment": "16 pages, 3 figures", "summary": "Embodied Artificial Intelligence (Embodied AI) is gaining momentum in the\nmachine learning communities with the goal of leveraging current progress in AI\n(deep learning, transformers, large language and visual-language models) to\nempower robots. In this chapter we put this work in the context of \"Good\nOld-Fashioned Artificial Intelligence\" (GOFAI) (Haugeland, 1989) and the\nbehavior-based or embodied alternatives (R. A. Brooks 1991; Pfeifer and Scheier\n2001). We claim that the AI-powered robots are only weakly embodied and inherit\nsome of the problems of GOFAI. Moreover, we review and critically discuss the\npossibility of cross-embodiment learning (Padalkar et al. 2024). We identify\nfundamental roadblocks and propose directions on how to make progress.", "AI": {"tldr": "This paper discusses the current state of Embodied Artificial Intelligence, comparing it to Good Old-Fashioned Artificial Intelligence and exploring the potential for cross-embodiment learning.", "motivation": "To explore the relationship between Embodied AI and traditional AI approaches, and to address limitations by proposing new directions in cross-embodiment learning.", "method": "Literature review and critical discussion of existing methodologies and theories in AI and robotics.", "result": "Identified fundamental challenges in achieving strong embodiment in AI-powered robots and suggested possible ways forward.", "conclusion": "AI-powered robots exhibit weak embodiment and inherit some issues from GOFAI; there is a need for innovation in cross-embodiment learning to advance the field."}}
{"id": "2505.11278", "pdf": "https://arxiv.org/pdf/2505.11278", "abs": "https://arxiv.org/abs/2505.11278", "authors": ["Fabian Falck", "Teodora Pandeva", "Kiarash Zahirnia", "Rachel Lawrence", "Richard Turner", "Edward Meeds", "Javier Zazo", "Sushrut Karmalkar"], "title": "A Fourier Space Perspective on Diffusion Models", "categories": ["stat.ML", "cs.CV", "cs.LG", "stat.ME"], "comment": null, "summary": "Diffusion models are state-of-the-art generative models on data modalities\nsuch as images, audio, proteins and materials. These modalities share the\nproperty of exponentially decaying variance and magnitude in the Fourier\ndomain. Under the standard Denoising Diffusion Probabilistic Models (DDPM)\nforward process of additive white noise, this property results in\nhigh-frequency components being corrupted faster and earlier in terms of their\nSignal-to-Noise Ratio (SNR) than low-frequency ones. The reverse process then\ngenerates low-frequency information before high-frequency details. In this\nwork, we study the inductive bias of the forward process of diffusion models in\nFourier space. We theoretically analyse and empirically demonstrate that the\nfaster noising of high-frequency components in DDPM results in violations of\nthe normality assumption in the reverse process. Our experiments show that this\nleads to degraded generation quality of high-frequency components. We then\nstudy an alternate forward process in Fourier space which corrupts all\nfrequencies at the same rate, removing the typical frequency hierarchy during\ngeneration, and demonstrate marked performance improvements on datasets where\nhigh frequencies are primary, while performing on par with DDPM on standard\nimaging benchmarks.", "AI": {"tldr": "This paper explores the inductive bias of the forward process in diffusion models within the Fourier space. It identifies issues with high-frequency component degradation in the standard Denoising Diffusion Probabilistic Models (DDPM) and proposes an alternative forward process that improves generation quality for high-frequency components.", "motivation": "The motivation is to address the issue of degraded high-frequency component generation in DDPM due to the faster corruption of high-frequency components in the Fourier space.", "method": "The authors theoretically analyze and empirically demonstrate the problem in DDPM and propose an alternate forward process that corrupts all frequencies at the same rate.", "result": "Experiments show performance improvements on datasets with significant high frequencies, while maintaining comparable performance on standard imaging benchmarks.", "conclusion": "The study concludes that the proposed alternate forward process can enhance the quality of high-frequency component generation in diffusion models."}}
{"id": "2505.10606", "pdf": "https://arxiv.org/pdf/2505.10606", "abs": "https://arxiv.org/abs/2505.10606", "authors": ["Hector Pasten", "Felipe Urrutia", "Hector Jimenez", "Cristian B. Calderon", "Crist\u00f3bal Rojas", "Alexander Kozachinskiy"], "title": "Continuity and Isolation Lead to Doubts or Dilemmas in Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Understanding how Transformers work and how they process information is key\nto the theoretical and empirical advancement of these machines. In this work,\nwe demonstrate the existence of two phenomena in Transformers, namely isolation\nand continuity. Both of these phenomena hinder Transformers to learn even\nsimple pattern sequences. Isolation expresses that any learnable sequence must\nbe isolated from another learnable sequence, and hence some sequences cannot be\nlearned by a single Transformer at the same time. Continuity entails that an\nattractor basin forms around a learned sequence, such that any sequence falling\nin that basin will collapse towards the learned sequence. Here, we\nmathematically prove these phenomena emerge in all Transformers that use\ncompact positional encoding, and design rigorous experiments, demonstrating\nthat the theoretical limitations we shed light on occur on the practical scale.", "AI": {"tldr": "This paper explores two phenomena, isolation and continuity, in Transformers, which hinder their ability to learn simple pattern sequences.", "motivation": "To understand how Transformers work and process information for theoretical and empirical advancement.", "method": "Mathematical proof and rigorous experiments.", "result": "Isolation and continuity phenomena exist in all Transformers using compact positional encoding, causing practical limitations.", "conclusion": "The study reveals theoretical limitations of Transformers, providing insights into their learning capabilities."}}
{"id": "2505.10742", "pdf": "https://arxiv.org/pdf/2505.10742", "abs": "https://arxiv.org/abs/2505.10742", "authors": ["Brandon Lepine", "Gawesha Weerantunga", "Juho Kim", "Pamela Mishkin", "Matthew Beane"], "title": "Evaluations at Work: Measuring the Capabilities of GenAI in Use", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Current AI benchmarks miss the messy, multi-turn nature of human-AI\ncollaboration. We present an evaluation framework that decomposes real-world\ntasks into interdependent subtasks, letting us track both LLM performance and\nusers' strategies across a dialogue. Complementing this framework, we develop a\nsuite of metrics, including a composite usage derived from semantic similarity,\nword overlap, and numerical matches; structural coherence; intra-turn\ndiversity; and a novel measure of the \"information frontier\" reflecting the\nalignment between AI outputs and users' working knowledge. We demonstrate our\nmethodology in a financial valuation task that mirrors real-world complexity.\nOur empirical findings reveal that while greater integration of LLM-generated\ncontent generally enhances output quality, its benefits are moderated by\nfactors such as response incoherence, excessive subtask diversity, and the\ndistance of provided information from users' existing knowledge. These results\nsuggest that proactive dialogue strategies designed to inject novelty may\ninadvertently undermine task performance. Our work thus advances a more\nholistic evaluation of human-AI collaboration, offering both a robust\nmethodological framework and actionable insights for developing more effective\nAI-augmented work processes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u4eba\u673a\u534f\u4f5c\u7684\u65b0\u6846\u67b6\u548c\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5e76\u5728\u91d1\u878d\u4f30\u503c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002\u53d1\u73b0LLM\u751f\u6210\u5185\u5bb9\u7684\u6574\u5408\u867d\u80fd\u63d0\u5347\u8f93\u51fa\u8d28\u91cf\uff0c\u4f46\u53d7\u5bf9\u8bdd\u8fde\u8d2f\u6027\u3001\u5b50\u4efb\u52a1\u591a\u6837\u6027\u548c\u4fe1\u606f\u4e0e\u7528\u6237\u5df2\u6709\u77e5\u8bc6\u7684\u5339\u914d\u7a0b\u5ea6\u7b49\u56e0\u7d20\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709AI\u57fa\u51c6\u672a\u80fd\u6355\u6349\u5230\u4eba\u7c7b\u4e0eAI\u534f\u4f5c\u4e2d\u7684\u590d\u6742\u3001\u591a\u8f6e\u4ea4\u4e92\u7279\u6027\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5206\u89e3\u73b0\u5b9e\u4efb\u52a1\u4e3a\u76f8\u4e92\u4f9d\u8d56\u5b50\u4efb\u52a1\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u7cfb\u5217\u5ea6\u91cf\u6307\u6807\u3002", "result": "LLM\u751f\u6210\u5185\u5bb9\u7684\u6574\u5408\u901a\u5e38\u80fd\u63d0\u9ad8\u8f93\u51fa\u8d28\u91cf\uff0c\u4f46\u53d7\u5230\u54cd\u5e94\u8fde\u8d2f\u6027\u3001\u5b50\u4efb\u52a1\u591a\u6837\u6027\u53ca\u6240\u63d0\u4f9b\u4fe1\u606f\u8ddd\u79bb\u7528\u6237\u5df2\u6709\u77e5\u8bc6\u8fdc\u8fd1\u7b49\u56e0\u7d20\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u63a8\u8fdb\u4e86\u5bf9\u4eba\u673a\u534f\u4f5c\u66f4\u5168\u9762\u7684\u8bc4\u4f30\uff0c\u4e3a\u5f00\u53d1\u66f4\u6709\u6548\u7684AI\u589e\u5f3a\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u6846\u67b6\u548c\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2505.11281", "pdf": "https://arxiv.org/pdf/2505.11281", "abs": "https://arxiv.org/abs/2505.11281", "authors": ["Yuejiang Wen", "Paul D. Franzon"], "title": "Adaptive Linear Embedding for Nonstationary High-Dimensional Optimization", "categories": ["stat.ML", "cs.LG"], "comment": "working, to be submitted", "summary": "Bayesian Optimization (BO) in high-dimensional spaces remains fundamentally\nlimited by the curse of dimensionality and the rigidity of global\nlow-dimensional assumptions. While Random EMbedding Bayesian Optimization\n(REMBO) mitigates this via linear projections into low-dimensional subspaces,\nit typically assumes a single global embedding and a stationary objective. In\nthis work, we introduce Self-Adaptive embedding REMBO (SA-REMBO), a novel\nframework that generalizes REMBO to support multiple random Gaussian\nembeddings, each capturing a different local subspace structure of the\nhigh-dimensional objective. An index variable governs the embedding choice and\nis jointly modeled with the latent optimization variable via a product kernel\nin a Gaussian Process surrogate. This enables the optimizer to adaptively\nselect embeddings conditioned on location, effectively capturing locally\nvarying effective dimensionality, nonstationarity, and heteroscedasticity in\nthe objective landscape. We theoretically analyze the expressiveness and\nstability of the index-conditioned product kernel and empirically demonstrate\nthe advantage of our method across synthetic and real-world high-dimensional\nbenchmarks, where traditional REMBO and other low-rank BO methods fail. Our\nresults establish SA-REMBO as a powerful and flexible extension for scalable BO\nin complex, structured design spaces.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6SA-REMBO\uff0c\u901a\u8fc7\u652f\u6301\u591a\u4e2a\u968f\u673a\u9ad8\u65af\u5d4c\u5165\u6765\u6269\u5c55REMBO\uff0c\u9002\u5e94\u4e0d\u540c\u5c40\u90e8\u5b50\u7a7a\u95f4\u7ed3\u6784\u7684\u9ad8\u7ef4\u76ee\u6807\u3002", "motivation": "\u89e3\u51b3\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u8d1d\u53f6\u65af\u4f18\u5316\u53d7\u9650\u4e8e\u7ef4\u5ea6\u707e\u96be\u548c\u5168\u5c40\u4f4e\u7ef4\u5047\u8bbe\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u81ea\u9002\u5e94\u5d4c\u5165REMBO\uff08SA-REMBO\uff09\uff0c\u4f7f\u7528\u4ea7\u54c1\u6838\u5728\u9ad8\u65af\u8fc7\u7a0b\u4ee3\u7406\u4e2d\u8054\u5408\u5efa\u6a21\u7d22\u5f15\u53d8\u91cf\u548c\u6f5c\u5728\u4f18\u5316\u53d8\u91cf\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u9ad8\u7ef4\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u4f20\u7edfREMBO\u548c\u5176\u4ed6\u4f4e\u79e9BO\u65b9\u6cd5\u3002", "conclusion": "\u8bc1\u660e\u4e86SA-REMBO\u4f5c\u4e3a\u53ef\u6269\u5c55\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u5f3a\u5927\u4e14\u7075\u6d3b\u7684\u6269\u5c55\u3002"}}
{"id": "2505.10607", "pdf": "https://arxiv.org/pdf/2505.10607", "abs": "https://arxiv.org/abs/2505.10607", "authors": ["Patara Trirat", "Jae-Gil Lee"], "title": "MONAQ: Multi-Objective Neural Architecture Querying for Time-Series Analysis on Resource-Constrained Devices", "categories": ["cs.LG", "cs.AI"], "comment": "Code will be available at https://github.com/kaist-dmlab/MONAQ", "summary": "The growing use of smartphones and IoT devices necessitates efficient\ntime-series analysis on resource-constrained hardware, which is critical for\nsensing applications such as human activity recognition and air quality\nprediction. Recent efforts in hardware-aware neural architecture search (NAS)\nautomate architecture discovery for specific platforms; however, none focus on\ngeneral time-series analysis with edge deployment. Leveraging the\nproblem-solving and reasoning capabilities of large language models (LLM), we\npropose MONAQ, a novel framework that reformulates NAS into Multi-Objective\nNeural Architecture Querying tasks. MONAQ is equipped with multimodal query\ngeneration for processing multimodal time-series inputs and hardware\nconstraints, alongside an LLM agent-based multi-objective search to achieve\ndeployment-ready models via code generation. By integrating numerical data,\ntime-series images, and textual descriptions, MONAQ improves an LLM's\nunderstanding of time-series data. Experiments on fifteen datasets demonstrate\nthat MONAQ-discovered models outperform both handcrafted models and NAS\nbaselines while being more efficient.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMONAQ\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u786c\u4ef6\u611f\u77e5\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u91cd\u65b0\u8868\u8ff0\u4e3a\u591a\u76ee\u6807\u795e\u7ecf\u67b6\u6784\u67e5\u8be2\u4efb\u52a1\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u9002\u7528\u4e8e\u8fb9\u7f18\u90e8\u7f72\u7684\u4e00\u822c\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u624b\u673a\u548c\u7269\u8054\u7f51\u8bbe\u5907\u7684\u666e\u53ca\uff0c\u9700\u8981\u5728\u8d44\u6e90\u53d7\u9650\u7684\u786c\u4ef6\u4e0a\u8fdb\u884c\u9ad8\u6548\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u6790\uff0c\u7279\u522b\u662f\u5728\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u548c\u7a7a\u6c14\u8d28\u91cf\u9884\u6d4b\u7b49\u4f20\u611f\u5e94\u7528\u4e2d\u3002\u73b0\u6709\u7684\u786c\u4ef6\u611f\u77e5NAS\u65b9\u6cd5\u4e13\u6ce8\u4e8e\u7279\u5b9a\u5e73\u53f0\uff0c\u7f3a\u4e4f\u9488\u5bf9\u8fb9\u7f18\u90e8\u7f72\u7684\u4e00\u822c\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u3002", "method": "MONAQ\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u67e5\u8be2\u751f\u6210\u5904\u7406\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u8f93\u5165\u548c\u786c\u4ef6\u7ea6\u675f\uff0c\u5e76\u91c7\u7528\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u591a\u76ee\u6807\u641c\u7d22\u6765\u5b9e\u73b0\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u7684\u90e8\u7f72\u5c31\u7eea\u6a21\u578b\u3002\u96c6\u6210\u6570\u503c\u6570\u636e\u3001\u65f6\u95f4\u5e8f\u5217\u56fe\u50cf\u548c\u6587\u672c\u63cf\u8ff0\u4ee5\u63d0\u9ad8LLM\u5bf9\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u7406\u89e3\u3002", "result": "MONAQ\u53d1\u73b0\u7684\u6a21\u578b\u572815\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u624b\u5de5\u8bbe\u8ba1\u7684\u6a21\u578b\u548cNAS\u57fa\u7ebf\uff0c\u540c\u65f6\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "MONAQ\u5c55\u793a\u4e86\u5229\u7528LLM\u8fdb\u884c\u591a\u76ee\u6807\u795e\u7ecf\u67b6\u6784\u67e5\u8be2\u5728\u4e00\u822c\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e0b\u7684\u5e94\u7528\u3002"}}
{"id": "2505.10749", "pdf": "https://arxiv.org/pdf/2505.10749", "abs": "https://arxiv.org/abs/2505.10749", "authors": ["Ashwath Vaithinathan Aravindan", "Zhisheng Tang", "Mayank Kejriwal"], "title": "Code-Driven Planning in Grid Worlds with Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "We propose an iterative programmatic planning (IPP) framework for solving\ngrid-based tasks by synthesizing interpretable agent policies expressed in code\nusing large language models (LLMs). Instead of relying on traditional search or\nreinforcement learning, our approach uses code generation as policy synthesis,\nwhere the LLM outputs executable programs that map environment states to action\nsequences. Our proposed architecture incorporates several prompting strategies,\nincluding direct code generation, pseudocode-conditioned refinement, and\ncurriculum-based prompting, but also includes an iterative refinement mechanism\nthat updates code based on task performance feedback. We evaluate our approach\nusing six leading LLMs and two challenging grid-based benchmarks (GRASP and\nMiniGrid). Our IPP framework demonstrates improvements over direct code\ngeneration ranging from 10\\% to as much as 10x across five of the six models\nand establishes a new state-of-the-art result for GRASP. IPP is found to\nsignificantly outperform direct elicitation of a solution from GPT-o3-mini (by\n63\\% on MiniGrid to 116\\% on GRASP), demonstrating the viability of the overall\napproach. Computational costs of all code generation approaches are similar.\nWhile code generation has a higher initial prompting cost compared to direct\nsolution elicitation (\\$0.08 per task vs. \\$0.002 per instance for\nGPT-o3-mini), the code can be reused for any number of instances, making the\namortized cost significantly lower (by 400x on GPT-o3-mini across the complete\nGRASP benchmark).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fed\u4ee3\u7a0b\u5e8f\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5408\u6210\u53ef\u89e3\u91ca\u7684\u667a\u80fd\u4f53\u7b56\u7565\u6765\u89e3\u51b3\u57fa\u4e8e\u7f51\u683c\u7684\u4efb\u52a1\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u591a\u79cd\u63d0\u793a\u7b56\u7565\u5e76\u5f15\u5165\u4e86\u8fed\u4ee3\u7cbe\u5316\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u5e76\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u65b0\u7684\u6280\u672f\u6c34\u5e73\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5982\u641c\u7d22\u548c\u5f3a\u5316\u5b66\u4e60\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u63a2\u7d22\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u751f\u6210\u53ef\u89e3\u91ca\u7684\u667a\u80fd\u4f53\u7b56\u7565\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8fed\u4ee3\u7a0b\u5e8f\u89c4\u5212\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4ee3\u7801\u751f\u6210\u548c\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u6846\u67b6\u5728\u4e94\u4e2a\u6a21\u578b\u4e0a\u6bd4\u76f4\u63a5\u4ee3\u7801\u751f\u6210\u63d0\u9ad8\u4e8610%\u523010\u500d\uff0c\u5728GRASP\u57fa\u51c6\u4e0a\u8fbe\u5230\u65b0SOTA\uff0c\u663e\u8457\u4f18\u4e8e\u76f4\u63a5\u89e3\u6cd5\u83b7\u53d6\u3002", "conclusion": "\u8bc1\u660e\u4e86\u8fed\u4ee3\u7a0b\u5e8f\u89c4\u5212\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u53ef\u884c\u6027\uff0c\u5c3d\u7ba1\u521d\u59cb\u63d0\u793a\u6210\u672c\u8f83\u9ad8\uff0c\u4f46\u603b\u4f53\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u4e14\u4ee3\u7801\u53ef\u590d\u7528\u3002"}}
{"id": "2505.11323", "pdf": "https://arxiv.org/pdf/2505.11323", "abs": "https://arxiv.org/abs/2505.11323", "authors": ["Haowei Wang", "Jingyi Wang", "Zhongxiang Dai", "Nai-Yuan Chiang", "Szu Hui Ng", "Cosmin G. Petra"], "title": "Convergence Rates of Constrained Expected Improvement", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Constrained Bayesian optimization (CBO) methods have seen significant success\nin black-box optimization with constraints, and one of the most commonly used\nCBO methods is the constrained expected improvement (CEI) algorithm. CEI is a\nnatural extension of the expected improvement (EI) when constraints are\nincorporated. However, the theoretical convergence rate of CEI has not been\nestablished. In this work, we study the convergence rate of CEI by analyzing\nits simple regret upper bound. First, we show that when the objective function\n$f$ and constraint function $c$ are assumed to each lie in a reproducing kernel\nHilbert space (RKHS), CEI achieves the convergence rates of $\\mathcal{O}\n\\left(t^{-\\frac{1}{2}}\\log^{\\frac{d+1}{2}}(t) \\right) \\ \\text{and }\\\n\\mathcal{O}\\left(t^{\\frac{-\\nu}{2\\nu+d}} \\log^{\\frac{\\nu}{2\\nu+d}}(t)\\right)$\nfor the commonly used squared exponential and Mat\\'{e}rn kernels, respectively.\nSecond, we show that when $f$ and $c$ are assumed to be sampled from Gaussian\nprocesses (GPs), CEI achieves the same convergence rates with a high\nprobability. Numerical experiments are performed to validate the theoretical\nanalysis.", "AI": {"tldr": "\u7814\u7a76\u4e86\u7ea6\u675f\u671f\u671b\u6539\u8fdb(CEI)\u7b97\u6cd5\u5728\u7ea6\u675f\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\u7684\u6536\u655b\u7387\u3002", "motivation": "CEI\u662f\u5904\u7406\u7ea6\u675f\u95ee\u9898\u7684\u81ea\u7136\u6269\u5c55\uff0c\u4f46\u5176\u7406\u8bba\u6536\u655b\u7387\u5c1a\u672a\u5efa\u7acb\u3002", "method": "\u901a\u8fc7\u5206\u6790\u7b80\u5355\u9057\u61be\u4e0a\u754c\u6765\u7814\u7a76CEI\u7684\u6536\u655b\u7387\u3002", "result": "\u5bf9\u4e8e\u5e38\u7528\u7684\u5e73\u65b9\u6307\u6570\u548cMat\u00e9rn\u6838\uff0c\u5206\u522b\u5f97\u5230\u4e86\u7279\u5b9a\u7684\u6536\u655b\u7387\u3002", "conclusion": "\u5f53\u76ee\u6807\u51fd\u6570\u548c\u7ea6\u675f\u51fd\u6570\u4ece\u9ad8\u65af\u8fc7\u7a0b\u4e2d\u91c7\u6837\u65f6\uff0cCEI\u8fbe\u5230\u76f8\u540c\u7684\u6536\u655b\u7387\u3002"}}
{"id": "2505.10630", "pdf": "https://arxiv.org/pdf/2505.10630", "abs": "https://arxiv.org/abs/2505.10630", "authors": ["Ben Adcock", "Nick Huang"], "title": "How many measurements are enough? Bayesian recovery in inverse problems with general distributions", "categories": ["cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "We study the sample complexity of Bayesian recovery for solving inverse\nproblems with general prior, forward operator and noise distributions. We\nconsider posterior sampling according to an approximate prior $\\mathcal{P}$,\nand establish sufficient conditions for stable and accurate recovery with high\nprobability. Our main result is a non-asymptotic bound that shows that the\nsample complexity depends on (i) the intrinsic complexity of $\\mathcal{P}$,\nquantified by its so-called approximate covering number, and (ii) concentration\nbounds for the forward operator and noise distributions. As a key application,\nwe specialize to generative priors, where $\\mathcal{P}$ is the pushforward of a\nlatent distribution via a Deep Neural Network (DNN). We show that the sample\ncomplexity scales log-linearly with the latent dimension $k$, thus establishing\nthe efficacy of DNN-based priors. Generalizing existing results on\ndeterministic (i.e., non-Bayesian) recovery for the important problem of random\nsampling with an orthogonal matrix $U$, we show how the sample complexity is\ndetermined by the coherence of $U$ with respect to the support of\n$\\mathcal{P}$. Hence, we establish that coherence plays a fundamental role in\nBayesian recovery as well. Overall, our framework unifies and extends prior\nwork, providing rigorous guarantees for the sample complexity of solving\nBayesian inverse problems with arbitrary distributions.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5177\u6709\u901a\u7528\u5148\u9a8c\u3001\u524d\u5411\u7b97\u5b50\u548c\u566a\u58f0\u5206\u5e03\u7684\u8d1d\u53f6\u65af\u6062\u590d\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002\u4e3b\u8981\u7ed3\u679c\u662f\u975e\u6e10\u8fd1\u754c\uff0c\u8868\u660e\u6837\u672c\u590d\u6742\u5ea6\u53d6\u51b3\u4e8e\u5148\u9a8c\u7684\u8fd1\u4f3c\u8986\u76d6\u6570\u53ca\u5176\u524d\u5411\u7b97\u5b50\u548c\u566a\u58f0\u5206\u5e03\u7684\u96c6\u4e2d\u6027\u3002\u7279\u522b\u5730\uff0c\u5bf9\u4e8e\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u5148\u9a8c\uff0c\u8bc1\u660e\u4e86\u6837\u672c\u590d\u6742\u5ea6\u4e0e\u6f5c\u5728\u7ef4\u5ea6\u5448\u5bf9\u6570\u7ebf\u6027\u5173\u7cfb\uff0c\u5e76\u4e14\u63a8\u5e7f\u4e86\u6b63\u4ea4\u77e9\u9635\u968f\u673a\u91c7\u6837\u7684\u786e\u5b9a\u6027\u6062\u590d\u7ed3\u679c\uff0c\u8868\u660e\u76f8\u5e72\u6027\u5728\u8d1d\u53f6\u65af\u6062\u590d\u4e2d\u8d77\u6839\u672c\u4f5c\u7528\u3002", "motivation": "\u7814\u7a76\u8d1d\u53f6\u65af\u9006\u95ee\u9898\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4e00\u822c\u5148\u9a8c\u548c\u566a\u58f0\u5206\u5e03\u7684\u60c5\u51b5\u3002", "method": "\u901a\u8fc7\u540e\u9a8c\u91c7\u6837\u548c\u975e\u6e10\u8fd1\u8fb9\u754c\u5206\u6790\uff0c\u8003\u8651\u4e86\u5148\u9a8c\u7684\u8fd1\u4f3c\u8986\u76d6\u6570\u548c\u524d\u5411\u7b97\u5b50\u53ca\u566a\u58f0\u5206\u5e03\u7684\u96c6\u4e2d\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u6837\u672c\u590d\u6742\u5ea6\u4f9d\u8d56\u4e8e\u5148\u9a8c\u7684\u8fd1\u4f3c\u8986\u76d6\u6570\u548c\u524d\u5411\u7b97\u5b50\u53ca\u566a\u58f0\u5206\u5e03\u7684\u96c6\u4e2d\u6027\uff1b\u5bf9\u4e8e\u751f\u6210\u5148\u9a8c\uff0c\u6837\u672c\u590d\u6742\u5ea6\u4e0e\u6f5c\u5728\u7ef4\u5ea6\u5448\u5bf9\u6570\u7ebf\u6027\u5173\u7cfb\uff1b\u63a8\u5e7f\u4e86\u6b63\u4ea4\u77e9\u9635\u968f\u673a\u91c7\u6837\u7684\u786e\u5b9a\u6027\u6062\u590d\u7ed3\u679c\u3002", "conclusion": "\u7edf\u4e00\u5e76\u6269\u5c55\u4e86\u4e4b\u524d\u7684\u5de5\u4f5c\uff0c\u4e3a\u4efb\u610f\u5206\u5e03\u4e0b\u89e3\u51b3\u8d1d\u53f6\u65af\u9006\u95ee\u9898\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1\u3002"}}
{"id": "2505.10779", "pdf": "https://arxiv.org/pdf/2505.10779", "abs": "https://arxiv.org/abs/2505.10779", "authors": ["Philip S. Thomas"], "title": "Qualia Optimization", "categories": ["cs.AI"], "comment": "Technical Report, College of Information and Computer Science,\n  University of Massachusetts", "summary": "This report explores the speculative question: what if current or future AI\nsystems have qualia, such as pain or pleasure? It does so by assuming that AI\nsystems might someday possess qualia -- and that the quality of these\nsubjective experiences should be considered alongside performance metrics.\nConcrete mathematical problem settings, inspired by reinforcement learning\nformulations and theories from philosophy of mind, are then proposed and\ninitial approaches and properties are presented. These properties enable\nrefinement of the problem setting, culminating with the proposal of methods\nthat promote reinforcement.", "AI": {"tldr": "\u63a2\u8ba8\u4e86AI\u7cfb\u7edf\u53ef\u80fd\u5177\u6709\u4e3b\u89c2\u4f53\u9a8c\uff08\u5982\u75db\u89c9\u548c\u6109\u60a6\u611f\uff09\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5173\u7684\u6570\u5b66\u95ee\u9898\u8bbe\u5b9a\u548c\u521d\u6b65\u89e3\u51b3\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22AI\u7cfb\u7edf\u662f\u5426\u80fd\u62e5\u6709\u4e3b\u89c2\u4f53\u9a8c\u53ca\u5176\u610f\u4e49", "method": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u548c\u54f2\u5b66\u601d\u7ef4\u7406\u8bba\u63d0\u51fa\u6570\u5b66\u95ee\u9898\u8bbe\u5b9a\u5e76\u7ed9\u51fa\u521d\u6b65\u65b9\u6cd5", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u4f18\u5316\u95ee\u9898\u8bbe\u5b9a\u5e76\u4fc3\u8fdb\u5f3a\u5316\u5b66\u4e60", "conclusion": "AI\u7cfb\u7edf\u7684\u4e3b\u89c2\u4f53\u9a8c\u503c\u5f97\u4e0e\u6027\u80fd\u6307\u6807\u4e00\u8d77\u88ab\u8003\u8651"}}
{"id": "2505.11355", "pdf": "https://arxiv.org/pdf/2505.11355", "abs": "https://arxiv.org/abs/2505.11355", "authors": ["Simon Urbainczyk", "Aretha L. Teckentrup", "Jonas Latz"], "title": "STRIDE: Sparse Techniques for Regression in Deep Gaussian Processes", "categories": ["stat.ML", "cs.LG", "stat.CO"], "comment": null, "summary": "Gaussian processes (GPs) have gained popularity as flexible machine learning\nmodels for regression and function approximation with an in-built method for\nuncertainty quantification. However, GPs suffer when the amount of training\ndata is large or when the underlying function contains multi-scale features\nthat are difficult to represent by a stationary kernel. To address the former,\ntraining of GPs with large-scale data is often performed through inducing point\napproximations (also known as sparse GP regression (GPR)), where the size of\nthe covariance matrices in GPR is reduced considerably through a greedy search\non the data set. To aid the latter, deep GPs have gained traction as\nhierarchical models that resolve multi-scale features by combining multiple\nGPs. Posterior inference in deep GPs requires a sampling or, more usual, a\nvariational approximation. Variational approximations lead to large-scale\nstochastic, non-convex optimisation problems and the resulting approximation\ntends to represent uncertainty incorrectly. In this work, we combine\nvariational learning with MCMC to develop a particle-based\nexpectation-maximisation method to simultaneously find inducing points within\nthe large-scale data (variationally) and accurately train the GPs\n(sampling-based). The result is a highly efficient and accurate methodology for\ndeep GP training on large-scale data. We test our method on standard benchmark\nproblems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u9ad8\u65af\u8fc7\u7a0b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u53d8\u5206\u5b66\u4e60\u548cMCMC\uff0c\u80fd\u591f\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u8bad\u7ec3\u6df1\u5ea6\u9ad8\u65af\u8fc7\u7a0b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u9ad8\u65af\u8fc7\u7a0b\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u65f6\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u53d8\u5206\u5b66\u4e60\u4e0eMCMC\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u7c92\u5b50\u7684\u671f\u671b\u6700\u5927\u5316\u65b9\u6cd5\u6765\u540c\u65f6\u627e\u5230\u5927\u89c4\u6a21\u6570\u636e\u4e2d\u7684\u8bf1\u5bfc\u70b9\u5e76\u51c6\u786e\u8bad\u7ec3\u9ad8\u65af\u8fc7\u7a0b\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u95ee\u9898\u4e0a\u6d4b\u8bd5\u4e86\u8be5\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u8bad\u7ec3\u6df1\u5ea6\u9ad8\u65af\u8fc7\u7a0b\u3002"}}
{"id": "2505.10641", "pdf": "https://arxiv.org/pdf/2505.10641", "abs": "https://arxiv.org/abs/2505.10641", "authors": ["Linjing You", "Jiabao Lu", "Xiayuan Huang", "Xiangli Nie"], "title": "FRET: Feature Redundancy Elimination for Test Time Adaptation", "categories": ["cs.LG"], "comment": null, "summary": "Test-Time Adaptation (TTA) aims to enhance the generalization of deep\nlearning models when faced with test data that exhibits distribution shifts\nfrom the training data. In this context, only a pre-trained model and unlabeled\ntest data are available, making it particularly relevant for privacy-sensitive\napplications. In practice, we observe that feature redundancy in embeddings\ntends to increase as domain shifts intensify in TTA. However, existing TTA\nmethods often overlook this redundancy, which can hinder the model's\nadaptability to new data. To address this issue, we introduce Feature\nRedundancy Elimination for Test-time Adaptation (FRET), a novel perspective for\nTTA. A straightforward approach (S-FRET) is to directly minimize the feature\nredundancy score as an optimization objective to improve adaptation. Despite\nits simplicity and effectiveness, S-FRET struggles with label shifts, limiting\nits robustness in real-world scenarios. To mitigate this limitation, we further\npropose Graph-based FRET (G-FRET), which integrates a Graph Convolutional\nNetwork (GCN) with contrastive learning. This design not only reduces feature\nredundancy but also enhances feature discriminability in both the\nrepresentation and prediction layers. Extensive experiments across multiple\nmodel architectures, tasks, and datasets demonstrate the effectiveness of\nS-FRET and show that G-FRET achieves state-of-the-art performance. Further\nanalysis reveals that G-FRET enables the model to extract non-redundant and\nhighly discriminative features during inference, thereby facilitating more\nrobust test-time adaptation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5FRET\u6765\u89e3\u51b3\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u4e2d\u7684\u7279\u5f81\u5197\u4f59\u95ee\u9898\uff0c\u5176\u4e2dG-FRET\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94(TTA)\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u4e86\u7279\u5f81\u5197\u4f59\u7684\u95ee\u9898\uff0c\u800c\u8fd9\u4f1a\u963b\u788d\u6a21\u578b\u5bf9\u65b0\u6570\u636e\u7684\u9002\u5e94\u80fd\u529b\u3002\u968f\u7740\u9886\u57df\u504f\u79fb\u7684\u52a0\u5267\uff0c\u5d4c\u5165\u4e2d\u7684\u7279\u5f81\u5197\u4f59\u5f80\u5f80\u4f1a\u589e\u52a0\uff0c\u7279\u522b\u662f\u5728\u9690\u79c1\u654f\u611f\u7684\u5e94\u7528\u573a\u666f\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86Feature Redundancy Elimination for Test-time Adaptation (FRET)\uff0c\u5305\u62ec\u76f4\u63a5\u6700\u5c0f\u5316\u7279\u5f81\u5197\u4f59\u5206\u6570\u7684S-FRET\u548c\u7ed3\u5408\u56fe\u5377\u79ef\u7f51\u7edc\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u7684G-FRET\u3002", "result": "S-FRET\u867d\u7136\u7b80\u5355\u4e14\u6709\u6548\uff0c\u4f46\u5728\u6807\u7b7e\u504f\u79fb\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\uff1bG-FRET\u901a\u8fc7\u51cf\u5c11\u7279\u5f81\u5197\u4f59\u5e76\u589e\u5f3a\u8868\u793a\u5c42\u548c\u9884\u6d4b\u5c42\u7684\u7279\u5f81\u533a\u5206\u5ea6\uff0c\u5728\u591a\u4e2a\u6a21\u578b\u67b6\u6784\u3001\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5FRET\u7528\u4e8e\u5904\u7406\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u4e2d\u7684\u7279\u5f81\u5197\u4f59\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u4e86S-FRET\u548cG-FRET\u4e24\u79cd\u5b9e\u73b0\u65b9\u5f0f\u3002\u5b9e\u9a8c\u8868\u660e\uff0cS-FRET\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u6709\u6548\uff0c\u800cG-FRET\u5728\u591a\u4e2a\u6a21\u578b\u67b6\u6784\u3001\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002"}}
{"id": "2505.10780", "pdf": "https://arxiv.org/pdf/2505.10780", "abs": "https://arxiv.org/abs/2505.10780", "authors": ["Trisha Das", "Afrah Shafquat", "Beigi Mandis", "Jacob Aptekar", "Jimeng Sun"], "title": "SECRET: Semi-supervised Clinical Trial Document Similarity Search", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Clinical trials are vital for evaluation of safety and efficacy of new\ntreatments. However, clinical trials are resource-intensive, time-consuming and\nexpensive to conduct, where errors in trial design, reduced efficacy, and\nsafety events can result in significant delays, financial losses, and damage to\nreputation. These risks underline the importance of informed and strategic\ndecisions in trial design to mitigate these risks and improve the chances of a\nsuccessful trial. Identifying similar historical trials is critical as these\ntrials can provide an important reference for potential pitfalls and challenges\nincluding serious adverse events, dosage inaccuracies, recruitment\ndifficulties, patient adherence issues, etc. Addressing these challenges in\ntrial design can lead to development of more effective study protocols with\noptimized patient safety and trial efficiency. In this paper, we present a\nnovel method to identify similar historical trials by summarizing clinical\ntrial protocols and searching for similar trials based on a query trial's\nprotocol. Our approach significantly outperforms all baselines, achieving up to\na 78% improvement in recall@1 and a 53% improvement in precision@1 over the\nbest baseline. We also show that our method outperforms all other baselines in\npartial trial similarity search and zero-shot patient-trial matching,\nhighlighting its superior utility in these tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u76f8\u4f3c\u7684\u5386\u53f2\u4e34\u5e8a\u8bd5\u9a8c\uff0c\u8be5\u65b9\u6cd5\u5728\u53ec\u56de\u7387\u548c\u7cbe\u786e\u5ea6\u4e0a\u90fd\u663e\u8457\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "motivation": "\u4e34\u5e8a\u8bd5\u9a8c\u5bf9\u4e8e\u8bc4\u4f30\u65b0\u6cbb\u7597\u65b9\u6cd5\u7684\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8bbe\u8ba1\u4e0d\u5f53\u53ef\u80fd\u5bfc\u81f4\u5ef6\u8bef\u3001\u7ecf\u6d4e\u635f\u5931\u548c\u58f0\u8a89\u635f\u5bb3\u3002\u56e0\u6b64\uff0c\u8bc6\u522b\u7c7b\u4f3c\u7684\u5386\u53f2\u8bd5\u9a8c\u975e\u5e38\u91cd\u8981\uff0c\u8fd9\u53ef\u4ee5\u4e3a\u6f5c\u5728\u7684\u98ce\u9669\u548c\u6311\u6218\u63d0\u4f9b\u91cd\u8981\u53c2\u8003\u3002", "method": "\u901a\u8fc7\u603b\u7ed3\u4e34\u5e8a\u8bd5\u9a8c\u65b9\u6848\u5e76\u57fa\u4e8e\u67e5\u8be2\u8bd5\u9a8c\u7684\u65b9\u6848\u641c\u7d22\u76f8\u4f3c\u8bd5\u9a8c\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u76f8\u4f3c\u7684\u5386\u53f2\u4e34\u5e8a\u8bd5\u9a8c\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u53ec\u56de@1\u4e0a\u63d0\u9ad8\u4e8678%\uff0c\u5728\u7cbe\u786e\u5ea6@1\u4e0a\u63d0\u9ad8\u4e8653%\uff0c\u5e76\u4e14\u5728\u90e8\u5206\u8bd5\u9a8c\u76f8\u4f3c\u6027\u641c\u7d22\u548c\u96f6\u6837\u672c\u60a3\u8005-\u8bd5\u9a8c\u5339\u914d\u4e2d\u4e5f\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u66f4\u6709\u6548\u5730\u8bc6\u522b\u76f8\u4f3c\u7684\u5386\u53f2\u4e34\u5e8a\u8bd5\u9a8c\uff0c\u4ece\u800c\u63d0\u9ad8\u8bd5\u9a8c\u534f\u8bae\u7684\u6709\u6548\u6027\uff0c\u4f18\u5316\u60a3\u8005\u5b89\u5168\u548c\u8bd5\u9a8c\u6548\u7387\u3002"}}
{"id": "2505.10698", "pdf": "https://arxiv.org/pdf/2505.10698", "abs": "https://arxiv.org/abs/2505.10698", "authors": ["Alexia Atsidakou", "Orestis Papadigenopoulos", "Constantine Caramanis", "Sujay Sanghavi", "Sanjay Shakkottai"], "title": "Asymptotically-Optimal Gaussian Bandits with Side Observations", "categories": ["cs.LG", "stat.ML"], "comment": "International Conference on Machine Learning, ICML '22", "summary": "We study the problem of Gaussian bandits with general side information, as\nfirst introduced by Wu, Szepesvari, and Gyorgy. In this setting, the play of an\narm reveals information about other arms, according to an arbitrary a priori\nknown side information matrix: each element of this matrix encodes the fidelity\nof the information that the ``row'' arm reveals about the ``column'' arm. In\nthe case of Gaussian noise, this model subsumes standard bandits,\nfull-feedback, and graph-structured feedback as special cases. In this work, we\nfirst construct an LP-based asymptotic instance-dependent lower bound on the\nregret. The LP optimizes the cost (regret) required to reliably estimate the\nsuboptimality gap of each arm. This LP lower bound motivates our main\ncontribution: the first known asymptotically optimal algorithm for this general\nsetting.", "AI": {"tldr": "\u7814\u7a76\u5177\u6709\u5e7f\u4e49\u4fa7\u4fe1\u606f\u7684\u9ad8\u65af\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLP\u7684\u6e10\u8fd1\u5b9e\u4f8b\u76f8\u5173\u4e0b\u754c\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9996\u4e2a\u6e10\u8fd1\u6700\u4f18\u7b97\u6cd5\u3002", "motivation": "\u6269\u5c55\u6807\u51c6\u591a\u81c2\u8001\u864e\u673a\u6a21\u578b\u5230\u5177\u6709\u4efb\u610f\u5df2\u77e5\u4fa7\u4fe1\u606f\u77e9\u9635\u7684\u4e00\u822c\u60c5\u51b5\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u7ebf\u6027\u89c4\u5212(LP)\u7684\u540e\u6094\u4e0b\u754c\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u7b97\u6cd5\u89e3\u51b3\u8be5\u4e00\u822c\u8bbe\u7f6e\u4e0b\u7684\u8001\u864e\u673a\u95ee\u9898\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u6e10\u8fd1\u610f\u4e49\u4e0a\u662f\u6700\u4f18\u7684\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u53cd\u9988\u7ed3\u6784\u5982\u6807\u51c6\u591a\u81c2\u8001\u864e\u673a\u3001\u5b8c\u5168\u53cd\u9988\u548c\u56fe\u7ed3\u6784\u53cd\u9988\u7b49\u7279\u6b8a\u60c5\u51b5\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u5b9e\u73b0\u4e86\u5bf9\u5177\u6709\u901a\u7528\u4fa7\u4fe1\u606f\u7684\u9ad8\u65af\u8001\u864e\u673a\u95ee\u9898\u7684\u6e10\u8fd1\u6700\u4f18\u89e3\u3002"}}
{"id": "2505.10646", "pdf": "https://arxiv.org/pdf/2505.10646", "abs": "https://arxiv.org/abs/2505.10646", "authors": ["Haoxiang You", "Yilang Liu", "Ian Abraham"], "title": "Accelerating Visual-Policy Learning through Parallel Differentiable Simulation", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "In this work, we propose a computationally efficient algorithm for visual\npolicy learning that leverages differentiable simulation and first-order\nanalytical policy gradients. Our approach decouple the rendering process from\nthe computation graph, enabling seamless integration with existing\ndifferentiable simulation ecosystems without the need for specialized\ndifferentiable rendering software. This decoupling not only reduces\ncomputational and memory overhead but also effectively attenuates the policy\ngradient norm, leading to more stable and smoother optimization. We evaluate\nour method on standard visual control benchmarks using modern GPU-accelerated\nsimulation. Experiments show that our approach significantly reduces wall-clock\ntraining time and consistently outperforms all baseline methods in terms of\nfinal returns. Notably, on complex tasks such as humanoid locomotion, our\nmethod achieves a $4\\times$ improvement in final return, and successfully\nlearns a humanoid running policy within 4 hours on a single GPU.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89c6\u89c9\u7b56\u7565\u5b66\u4e60\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5229\u7528\u53ef\u5fae\u4eff\u771f\u548c\u4e00\u9636\u89e3\u6790\u7b56\u7565\u68af\u5ea6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\uff0c\u5e76\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u63d0\u9ad8\u89c6\u89c9\u7b56\u7565\u5b66\u4e60\u7684\u8ba1\u7b97\u6548\u7387\u5e76\u7b80\u5316\u4e0e\u73b0\u6709\u4e0d\u540ciable simulation\u751f\u6001\u7cfb\u7edf\u7684\u96c6\u6210\u3002", "method": "\u5c06\u6e32\u67d3\u8fc7\u7a0b\u4e0e\u8ba1\u7b97\u56fe\u89e3\u8026\uff0c\u4f7f\u7528\u53ef\u5fae\u5206\u4eff\u771f\u548c\u4e00\u9636\u89e3\u6790\u7b56\u7565\u68af\u5ea6\u3002", "result": "\u5728\u6807\u51c6\u89c6\u89c9\u63a7\u5236\u57fa\u51c6\u4e0a\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\uff0c\u5e76\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u8fd8\u589e\u5f3a\u4e86\u7b56\u7565\u4f18\u5316\u7684\u7a33\u5b9a\u6027\u548c\u5e73\u6ed1\u6027\uff0c\u5728\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u7b49\u590d\u6742\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002"}}
{"id": "2505.10803", "pdf": "https://arxiv.org/pdf/2505.10803", "abs": "https://arxiv.org/abs/2505.10803", "authors": ["Zhaoan Wang", "Wonseok Jang", "Bowen Ruan", "Jun Wang", "Shaoping Xiao"], "title": "Developing and Integrating Trust Modeling into Multi-Objective Reinforcement Learning for Intelligent Agricultural Management", "categories": ["cs.AI"], "comment": null, "summary": "Precision agriculture, enhanced by artificial intelligence (AI), offers\npromising tools such as remote sensing, intelligent irrigation, fertilization\nmanagement, and crop simulation to improve agricultural efficiency and\nsustainability. Reinforcement learning (RL), in particular, has outperformed\ntraditional methods in optimizing yields and resource management. However,\nwidespread AI adoption is limited by gaps between algorithmic recommendations\nand farmers' practical experience, local knowledge, and traditional practices.\nTo address this, our study emphasizes Human-AI Interaction (HAII), focusing on\ntransparency, usability, and trust in RL-based farm management. We employ a\nwell-established trust framework - comprising ability, benevolence, and\nintegrity - to develop a novel mathematical model quantifying farmers'\nconfidence in AI-based fertilization strategies. Surveys conducted with farmers\nfor this research reveal critical misalignments, which are integrated into our\ntrust model and incorporated into a multi-objective RL framework. Unlike prior\nmethods, our approach embeds trust directly into policy optimization, ensuring\nAI recommendations are technically robust, economically feasible,\ncontext-aware, and socially acceptable. By aligning technical performance with\nhuman-centered trust, this research supports broader AI adoption in\nagriculture.", "AI": {"tldr": "This study focuses on improving agricultural efficiency using AI, especially reinforcement learning, while addressing the gap between AI recommendations and farmers' practical experiences by developing a trust model that integrates human-centered factors.", "motivation": "To enhance agricultural efficiency and sustainability through AI, particularly reinforcement learning, while overcoming the barriers to widespread AI adoption caused by the disconnect between AI recommendations and farmers' practical knowledge.", "method": "Employing a trust framework including ability, benevolence, and integrity to create a mathematical model measuring farmers' trust in AI fertilization strategies, integrating survey data from farmers into a multi-objective RL framework.", "result": "The developed approach successfully embeds trust into policy optimization, making AI recommendations more robust, feasible, context-aware, and socially acceptable, thus promoting broader AI adoption in agriculture.", "conclusion": "This research supports the integration of human-centered trust into AI systems for agriculture, facilitating more effective and accepted AI adoption."}}
{"id": "2505.10860", "pdf": "https://arxiv.org/pdf/2505.10860", "abs": "https://arxiv.org/abs/2505.10860", "authors": ["Huy Nguyen", "Thong T. Doan", "Quang Pham", "Nghi D. Q. Bui", "Nhat Ho", "Alessandro Rinaldo"], "title": "On DeepSeekMoE: Statistical Benefits of Shared Experts and Normalized Sigmoid Gating", "categories": ["cs.LG", "stat.ML"], "comment": "100 pages", "summary": "Mixture of experts (MoE) methods are a key component in most large language\nmodel architectures, including the recent series of DeepSeek models. Compared\nto other MoE implementations, DeepSeekMoE stands out because of two unique\nfeatures: the deployment of a shared expert strategy and of the normalized\nsigmoid gating mechanism. Despite the prominent role of DeepSeekMoE in the\nsuccess of the DeepSeek series of models, there have been only a few attempts\nto justify theoretically the value of the shared expert strategy, while its\nnormalized sigmoid gating has remained unexplored. To bridge this gap, we\nundertake a comprehensive theoretical study of these two features of\nDeepSeekMoE from a statistical perspective. We perform a convergence analysis\nof the expert estimation task to highlight the gains in sample efficiency for\nboth the shared expert strategy and the normalized sigmoid gating, offering\nuseful insights into the design of expert and gating structures. To verify\nempirically our theoretical findings, we carry out several experiments on both\nsynthetic data and real-world datasets for (vision) language modeling tasks.\nFinally, we conduct an extensive empirical analysis of the router behaviors,\nranging from router saturation, router change rate, to expert utilization.", "AI": {"tldr": "\u7814\u7a76\u4e86DeepSeekMoE\u7684\u5171\u4eab\u4e13\u5bb6\u7b56\u7565\u548c\u5f52\u4e00\u5316Sigmoid\u95e8\u673a\u5236\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\u652f\u6301\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1DeepSeekMoE\u5728DeepSeek\u7cfb\u5217\u6a21\u578b\u7684\u6210\u529f\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u4f46\u5171\u4eab\u4e13\u5bb6\u7b56\u7565\u7684\u7406\u8bba\u4ef7\u503c\u4ec5\u88ab\u5c11\u91cf\u7814\u7a76\u63a2\u8ba8\uff0c\u800c\u5f52\u4e00\u5316Sigmoid\u95e8\u673a\u5236\u5c1a\u672a\u88ab\u7814\u7a76\u3002", "method": "\u4ece\u7edf\u8ba1\u5b66\u89d2\u5ea6\u5bf9DeepSeekMoE\u7684\u4e24\u4e2a\u7279\u6027\u8fdb\u884c\u4e86\u5168\u9762\u7406\u8bba\u7814\u7a76\uff0c\u5305\u62ec\u6536\u655b\u6027\u5206\u6790\u4ee5\u53ca\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86\u5171\u4eab\u4e13\u5bb6\u7b56\u7565\u548c\u5f52\u4e00\u5316Sigmoid\u95e8\u673a\u5236\u5728\u6837\u672c\u6548\u7387\u4e0a\u7684\u4f18\u52bf\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u7406\u8bba\u53d1\u73b0\u3002", "conclusion": "\u8bc1\u660e\u4e86DeepSeekMoE\u7684\u5171\u4eab\u4e13\u5bb6\u7b56\u7565\u548c\u5f52\u4e00\u5316Sigmoid\u95e8\u673a\u5236\u5728\u7edf\u8ba1\u5b66\u89c6\u89d2\u4e0b\u7684\u4ef7\u503c\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8e\u4e13\u5bb6\u7ed3\u6784\u548c\u95e8\u7ed3\u6784\u8bbe\u8ba1\u7684\u6709\u7528\u89c1\u89e3\u3002"}}
{"id": "2505.10665", "pdf": "https://arxiv.org/pdf/2505.10665", "abs": "https://arxiv.org/abs/2505.10665", "authors": ["Wei Wang", "Weidong Yang", "Lei Wang", "Guihua Wang", "Ruibo Lei"], "title": "Seasonal Forecasting of Pan-Arctic Sea Ice with State Space Model", "categories": ["cs.LG", "cs.AI"], "comment": "This paper is published in npj Climate and Atmospheric Science:\n  https://www.nature.com/articles/s41612-025-01058-0#Sec16 Supplementary\n  information:\n  https://static-content.springer.com/esm/art%3A10.1038%2Fs41612-025-01058-0/MediaObjects/41612_2025_1058_MOESM1_ESM.pdf", "summary": "The rapid decline of Arctic sea ice resulting from anthropogenic climate\nchange poses significant risks to indigenous communities, ecosystems, and the\nglobal climate system. This situation emphasizes the immediate necessity for\nprecise seasonal sea ice forecasts. While dynamical models perform well for\nshort-term forecasts, they encounter limitations in long-term forecasts and are\ncomputationally intensive. Deep learning models, while more computationally\nefficient, often have difficulty managing seasonal variations and uncertainties\nwhen dealing with complex sea ice dynamics. In this research, we introduce\nIceMamba, a deep learning architecture that integrates sophisticated attention\nmechanisms within the state space model. Through comparative analysis of 25\nrenowned forecast models, including dynamical, statistical, and deep learning\napproaches, our experimental results indicate that IceMamba delivers excellent\nseasonal forecasting capabilities for Pan-Arctic sea ice concentration.\nSpecifically, IceMamba outperforms all tested models regarding average RMSE and\nanomaly correlation coefficient (ACC) and ranks second in Integrated Ice Edge\nError (IIEE). This innovative approach enhances our ability to foresee and\nalleviate the effects of sea ice variability, offering essential insights for\nstrategies aimed at climate adaptation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIceMamba\u7684\u65b0\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u5317\u6781\u6d77\u51b0\u6d53\u5ea6\u7684\u5b63\u8282\u6027\u9884\u6d4b\u3002\u5b9e\u9a8c\u8868\u660e\uff0cIceMamba\u5728\u5e73\u5747\u5747\u65b9\u6839\u8bef\u5dee\u548c\u5f02\u5e38\u76f8\u5173\u7cfb\u6570\u65b9\u9762\u4f18\u4e8e\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u3002", "motivation": "\u5317\u6781\u6d77\u51b0\u5feb\u901f\u51cf\u5c11\u5e26\u6765\u7684\u98ce\u9669\u4fc3\u4f7f\u9700\u8981\u66f4\u7cbe\u786e\u7684\u5b63\u8282\u6027\u6d77\u51b0\u9884\u62a5\u3002", "method": "IceMamba\u662f\u4e00\u79cd\u7ed3\u5408\u4e86\u590d\u6742\u6ce8\u610f\u673a\u5236\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u3002", "result": "IceMamba\u5728\u5e73\u5747RMSE\u548c\u5f02\u5e38\u76f8\u5173\u7cfb\u6570\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u5728\u7efc\u5408\u6d77\u51b0\u8fb9\u7f18\u8bef\u5dee\u65b9\u9762\u6392\u540d\u7b2c\u4e8c\u3002", "conclusion": "IceMamba\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6d77\u51b0\u53d8\u5f02\u6027\u9884\u89c1\u548c\u7f13\u89e3\u7684\u80fd\u529b\uff0c\u4e3a\u6c14\u5019\u9002\u5e94\u7b56\u7565\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2505.10819", "pdf": "https://arxiv.org/pdf/2505.10819", "abs": "https://arxiv.org/abs/2505.10819", "authors": ["Wasu Top Piriyakulkij", "Yichao Liang", "Hao Tang", "Adrian Weller", "Marta Kryven", "Kevin Ellis"], "title": "PoE-World: Compositional World Modeling with Products of Programmatic Experts", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Learning how the world works is central to building AI agents that can adapt\nto complex environments. Traditional world models based on deep learning demand\nvast amounts of training data, and do not flexibly update their knowledge from\nsparse observations. Recent advances in program synthesis using Large Language\nModels (LLMs) give an alternate approach which learns world models represented\nas source code, supporting strong generalization from little data. To date,\napplication of program-structured world models remains limited to natural\nlanguage and grid-world domains. We introduce a novel program synthesis method\nfor effectively modeling complex, non-gridworld domains by representing a world\nmodel as an exponentially-weighted product of programmatic experts (PoE-World)\nsynthesized by LLMs. We show that this approach can learn complex, stochastic\nworld models from just a few observations. We evaluate the learned world models\nby embedding them in a model-based planning agent, demonstrating efficient\nperformance and generalization to unseen levels on Atari's Pong and Montezuma's\nRevenge. We release our code and display the learned world models and videos of\nthe agent's gameplay at https://topwasu.github.io/poe-world.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7a0b\u5e8f\u7efc\u5408\u65b9\u6cd5\uff08PoE-World\uff09\uff0c\u901a\u8fc7\u5229\u7528\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5408\u6210\u7a0b\u5e8f\u5316\u4e13\u5bb6\u7684\u6307\u6570\u52a0\u6743\u4e58\u79ef\u6765\u6709\u6548\u5efa\u6a21\u590d\u6742\u7684\u975e\u7f51\u683c\u4e16\u754c\u9886\u57df\u3002\u8be5\u65b9\u6cd5\u53ef\u4ee5\u4ece\u5c11\u91cf\u89c2\u6d4b\u4e2d\u5b66\u4e60\u590d\u6742\u3001\u968f\u673a\u7684\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u5728Atari\u7684Pong\u548cMontezuma's Revenge\u4e0a\u5c55\u793a\u4e86\u9ad8\u6548\u7684\u6027\u80fd\u548c\u5bf9\u672a\u89c1\u8fc7\u5173\u5361\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e16\u754c\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u4e14\u4e0d\u80fd\u7075\u6d3b\u5730\u4ece\u7a00\u758f\u89c2\u5bdf\u4e2d\u66f4\u65b0\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7a0b\u5e8f\u7efc\u5408\u65b9\u6cd5\uff0c\u5373\u901a\u8fc7\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5408\u6210\u7a0b\u5e8f\u5316\u4e13\u5bb6\u7684\u6307\u6570\u52a0\u6743\u4e58\u79ef\u6765\u8868\u793a\u4e16\u754c\u6a21\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u4ece\u5c11\u91cf\u89c2\u6d4b\u4e2d\u5b66\u4e60\u590d\u6742\u3001\u968f\u673a\u7684\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u5728Atari\u7684Pong\u548cMontezuma's Revenge\u4e0a\u5c55\u793a\u4e86\u9ad8\u6548\u7684\u6027\u80fd\u548c\u5bf9\u672a\u89c1\u8fc7\u5173\u5361\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u6269\u5c55\u4e86\u7a0b\u5e8f\u7ed3\u6784\u4e16\u754c\u6a21\u578b\u7684\u5e94\u7528\u8303\u56f4\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u590d\u6742\u975e\u7f51\u683c\u4e16\u754c\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.10873", "pdf": "https://arxiv.org/pdf/2505.10873", "abs": "https://arxiv.org/abs/2505.10873", "authors": ["Filippo Leveni", "Luca Magri", "Cesare Alippi", "Giacomo Boracchi"], "title": "Hashing for Structure-based Anomaly Detection", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Accepted at International Conference on Image Analysis and Processing\n  (ICIAP 2023)", "summary": "We focus on the problem of identifying samples in a set that do not conform\nto structured patterns represented by low-dimensional manifolds. An effective\nway to solve this problem is to embed data in a high dimensional space, called\nPreference Space, where anomalies can be identified as the most isolated\npoints. In this work, we employ Locality Sensitive Hashing to avoid explicit\ncomputation of distances in high dimensions and thus improve Anomaly Detection\nefficiency. Specifically, we present an isolation-based anomaly detection\ntechnique designed to work in the Preference Space which achieves\nstate-of-the-art performance at a lower computational cost. Code is publicly\navailable at\nhttps://github.com/ineveLoppiliF/Hashing-for-Structure-based-Anomaly-Detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c40\u90e8\u654f\u611f\u54c8\u5e0c\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u8bc6\u522b\u4e00\u7ec4\u6837\u672c\u4e2d\u4e0d\u7b26\u5408\u7531\u4f4e\u7ef4\u6d41\u5f62\u8868\u793a\u7684\u7ed3\u6784\u6a21\u5f0f\u7684\u6837\u672c\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u5c40\u90e8\u654f\u611f\u54c8\u5e0c\u6765\u907f\u514d\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u663e\u5f0f\u8ba1\u7b97\u8ddd\u79bb\uff0c\u4ece\u800c\u63d0\u9ad8\u5f02\u5e38\u68c0\u6d4b\u6548\u7387\u3002", "result": "\u63d0\u51fa\u7684\u6280\u672f\u5728\u504f\u597d\u7a7a\u95f4\u4e2d\u5de5\u4f5c\u5e76\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2505.10677", "pdf": "https://arxiv.org/pdf/2505.10677", "abs": "https://arxiv.org/abs/2505.10677", "authors": ["Ioannis Pitsiorlas", "Nour Jamoussi", "Marios Kountouris"], "title": "A Conformal Predictive Measure for Assessing Catastrophic Forgetting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This work introduces a novel methodology for assessing catastrophic\nforgetting (CF) in continual learning. We propose a new conformal prediction\n(CP)-based metric, termed the Conformal Prediction Confidence Factor (CPCF), to\nquantify and evaluate CF effectively. Our framework leverages adaptive CP to\nestimate forgetting by monitoring the model's confidence on previously learned\ntasks. This approach provides a dynamic and practical solution for monitoring\nand measuring CF of previous tasks as new ones are introduced, offering greater\nsuitability for real-world applications. Experimental results on four benchmark\ndatasets demonstrate a strong correlation between CPCF and the accuracy of\nprevious tasks, validating the reliability and interpretability of the proposed\nmetric. Our results highlight the potential of CPCF as a robust and effective\ntool for assessing and understanding CF in dynamic learning environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u8fde\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\uff08CF\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u6821\u51c6\u9884\u6d4b\uff08CP\uff09\u7684\u65b0\u6307\u6807CPCF\uff0c\u63d0\u4f9b\u4e86\u52a8\u6001\u76d1\u6d4b\u548c\u8861\u91cfCF\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86CPCF\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u53ef\u9760\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684CF\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u3001\u66f4\u53ef\u9760\u7684\u5ea6\u91cf\u6807\u51c6\u6765\u8bc4\u4f30\u8fde\u7eed\u5b66\u4e60\u4e2d\u7684CF\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6821\u51c6\u9884\u6d4b\uff08CP\uff09\u7684CPCF\u6307\u6807\uff0c\u5229\u7528\u9002\u5e94\u6027CP\u76d1\u63a7\u6a21\u578b\u5bf9\u5148\u524d\u4efb\u52a1\u7684\u4fe1\u5fc3\u6765\u4f30\u8ba1\u9057\u5fd8\u3002", "result": "CPCF\u4e0e\u5148\u524d\u4efb\u52a1\u7684\u51c6\u786e\u6027\u6709\u5f88\u5f3a\u7684\u76f8\u5173\u6027\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u53ef\u9760\u6027\u3002", "conclusion": "CPCF\u662f\u4e00\u79cd\u5f3a\u5927\u4e14\u6709\u6548\u7684\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u52a8\u6001\u5b66\u4e60\u73af\u5883\u4e0b\u7684CF\u8bc4\u4f30\u548c\u7406\u89e3\u3002"}}
{"id": "2505.10834", "pdf": "https://arxiv.org/pdf/2505.10834", "abs": "https://arxiv.org/abs/2505.10834", "authors": ["Achintha Wijesinghe", "Weiwei Wang", "Suchinthaka Wanninayaka", "Songyang Zhang", "Zhi Ding"], "title": "TACO: Rethinking Semantic Communications with Task Adaptation and Context Embedding", "categories": ["cs.AI", "cs.LG", "eess.IV", "eess.SP"], "comment": "Submitted to the IEEE GlobeCom 2025", "summary": "Recent advancements in generative artificial intelligence have introduced\ngroundbreaking approaches to innovating next-generation semantic communication,\nwhich prioritizes conveying the meaning of a message rather than merely\ntransmitting raw data. A fundamental challenge in semantic communication lies\nin accurately identifying and extracting the most critical semantic information\nwhile adapting to downstream tasks without degrading performance, particularly\nwhen the objective at the receiver may evolve over time. To enable flexible\nadaptation to multiple tasks at the receiver, this work introduces a novel\nsemantic communication framework, which is capable of jointly capturing\ntask-specific information to enhance downstream task performance and contextual\ninformation. Through rigorous experiments on popular image datasets and\ncomputer vision tasks, our framework shows promising improvement compared to\nexisting work, including superior performance in downstream tasks, better\ngeneralizability, ultra-high bandwidth efficiency, and low reconstruction\nlatency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u80fd\u540c\u65f6\u6355\u6349\u7279\u5b9a\u4efb\u52a1\u4fe1\u606f\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u5de5\u4f5c\uff0c\u5728\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3001\u6cdb\u5316\u80fd\u529b\u3001\u5e26\u5bbd\u6548\u7387\u548c\u91cd\u5efa\u5ef6\u8fdf\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u901a\u4fe1\u4ec5\u4f20\u8f93\u539f\u59cb\u6570\u636e\uff0c\u800c\u65b0\u4e00\u4ee3\u8bed\u4e49\u901a\u4fe1\u5f3a\u8c03\u4f20\u9012\u6d88\u606f\u7684\u610f\u4e49\uff1b\u5728\u63a5\u6536\u7aef\u51c6\u786e\u63d0\u53d6\u5173\u952e\u8bed\u4e49\u4fe1\u606f\u5e76\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u4efb\u52a1\u76ee\u6807\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u8054\u5408\u6355\u6349\u4efb\u52a1\u7279\u5b9a\u4fe1\u606f\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u5728\u5e38\u89c1\u56fe\u50cf\u6570\u636e\u96c6\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5305\u62ec\u66f4\u597d\u7684\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3001\u66f4\u9ad8\u7684\u5e26\u5bbd\u6548\u7387\u548c\u66f4\u4f4e\u7684\u91cd\u5efa\u5ef6\u8fdf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u591a\u4efb\u52a1\u9002\u5e94\u6027\u3001\u6027\u80fd\u3001\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u8bed\u4e49\u901a\u4fe1\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.10876", "pdf": "https://arxiv.org/pdf/2505.10876", "abs": "https://arxiv.org/abs/2505.10876", "authors": ["Filippo Leveni", "Luca Magri", "Cesare Alippi", "Giacomo Boracchi"], "title": "Preference Isolation Forest for Structure-based Anomaly Detection", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Submitted to Pattern Recognition", "summary": "We address the problem of detecting anomalies as samples that do not conform\nto structured patterns represented by low-dimensional manifolds. To this end,\nwe conceive a general anomaly detection framework called Preference Isolation\nForest (PIF), that combines the benefits of adaptive isolation-based methods\nwith the flexibility of preference embedding. The key intuition is to embed the\ndata into a high-dimensional preference space by fitting low-dimensional\nmanifolds, and to identify anomalies as isolated points. We propose three\nisolation approaches to identify anomalies: $i$) Voronoi-iForest, the most\ngeneral solution, $ii$) RuzHash-iForest, that avoids explicit computation of\ndistances via Local Sensitive Hashing, and $iii$) Sliding-PIF, that leverages a\nlocality prior to improve efficiency and effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u9002\u5e94\u9694\u79bb\u65b9\u6cd5\u548c\u504f\u597d\u5d4c\u5165\u7075\u6d3b\u6027\u7684\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6Preference Isolation Forest (PIF)\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e09\u79cd\u9694\u79bb\u65b9\u6cd5\u6765\u8bc6\u522b\u5f02\u5e38\u3002", "motivation": "\u68c0\u6d4b\u4e0d\u9075\u5faa\u4f4e\u7ef4\u6d41\u5f62\u7ed3\u6784\u6a21\u5f0f\u7684\u5f02\u5e38\u6837\u672c\u3002", "method": "\u63d0\u51faPreference Isolation Forest (PIF) \u6846\u67b6\uff0c\u901a\u8fc7\u62df\u5408\u4f4e\u7ef4\u6d41\u5f62\u5c06\u6570\u636e\u5d4c\u5165\u9ad8\u7ef4\u504f\u597d\u7a7a\u95f4\uff0c\u5e76\u91c7\u7528\u4e09\u79cd\u9694\u79bb\u65b9\u6cd5\u8bc6\u522b\u5f02\u5e38\u3002", "result": "\u672a\u5177\u4f53\u63d0\u53ca\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u63d0\u51fa\u4e86\u4e09\u79cd\u9694\u79bb\u65b9\u6cd5\u7528\u4e8e\u5f02\u5e38\u68c0\u6d4b\u3002", "conclusion": "\u603b\u7ed3\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\u7684\u4f18\u52bf\u53ca\u5176\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.10689", "pdf": "https://arxiv.org/pdf/2505.10689", "abs": "https://arxiv.org/abs/2505.10689", "authors": ["Gabriele Santini", "Francesco Paissan", "Elisabetta Farella"], "title": "A probabilistic framework for dynamic quantization", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We propose a probabilistic framework for dynamic quantization of neural\nnetworks that allows for a computationally efficient input-adaptive rescaling\nof the quantization parameters. Our framework applies a probabilistic model to\nthe network's pre-activations through a lightweight surrogate, enabling the\nadaptive adjustment of the quantization parameters on a per-input basis without\nsignificant memory overhead. We validate our approach on a set of popular\ncomputer vision tasks and models, observing only a negligible loss in\nperformance. Our method strikes the best performance and computational overhead\ntradeoff compared to standard quantization strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u795e\u7ecf\u7f51\u7edc\u52a8\u6001\u91cf\u5316\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u5730\u6839\u636e\u8f93\u5165\u81ea\u9002\u5e94\u8c03\u6574\u91cf\u5316\u53c2\u6570\uff0c\u5e76\u5728\u591a\u4e2a\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u63d0\u9ad8\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u6548\u7387\u5e76\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4ee3\u7406\u5bf9\u7f51\u7edc\u9884\u6fc0\u6d3b\u5e94\u7528\u6982\u7387\u6a21\u578b\uff0c\u5b9e\u73b0\u8f93\u5165\u81ea\u9002\u5e94\u7684\u91cf\u5316\u53c2\u6570\u8c03\u6574\u3002", "result": "\u5728\u5e38\u89c1\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\uff0c\u6027\u80fd\u635f\u5931\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u6027\u80fd\u4e0e\u8ba1\u7b97\u5f00\u9500\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u8861\u3002"}}
{"id": "2505.10844", "pdf": "https://arxiv.org/pdf/2505.10844", "abs": "https://arxiv.org/abs/2505.10844", "authors": ["Simeng Han", "Stephen Xia", "Grant Zhang", "Howard Dai", "Chen Liu", "Lichang Chen", "Hoang Huy Nguyen", "Hongyuan Mei", "Jiayuan Mao", "R. Thomas McCoy"], "title": "Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models", "categories": ["cs.AI", "cs.CL"], "comment": "13 Tables; 5 Figures", "summary": "Accuracy remains a standard metric for evaluating AI systems, but it offers\nlimited insight into how models arrive at their solutions. In this work, we\nintroduce a benchmark based on brainteasers written in long narrative form to\nprobe more deeply into the types of reasoning strategies that models use.\nBrainteasers are well-suited for this goal because they can be solved with\nmultiple approaches, such as a few-step solution that uses a creative insight\nor a longer solution that uses more brute force. We investigate large language\nmodels (LLMs) across multiple layers of reasoning, focusing not only on\ncorrectness but also on the quality and creativity of their solutions. We\ninvestigate many aspects of the reasoning process: (1) semantic parsing of the\nbrainteasers into precise mathematical competition style formats; (2)\ngenerating solutions from these mathematical forms; (3) self-correcting\nsolutions based on gold solutions; (4) producing step-by-step sketches of\nsolutions; and (5) making use of hints. We find that LLMs are in many cases\nable to find creative, insightful solutions to brainteasers, suggesting that\nthey capture some of the capacities needed to solve novel problems in creative\nways. Nonetheless, there also remain situations where they rely on brute force\ndespite the availability of more efficient, creative solutions, highlighting a\npotential direction for improvement in the reasoning abilities of LLMs.", "AI": {"tldr": "This paper introduces a benchmark using narrative brainteasers to evaluate the reasoning strategies of large language models, analyzing their creativity, correctness, and use of hints across different layers of reasoning.", "motivation": "To provide deeper insights into how AI models reach solutions beyond accuracy metrics by using complex brainteasers that can be solved in multiple ways.", "method": "Developing a benchmark involving narrative brainteasers, examining semantic parsing, solution generation, self-correction, step-by-step sketching, and hint utilization.", "result": "Large language models can often find creative solutions but sometimes rely on brute force even when more efficient methods are available.", "conclusion": "The study suggests that large language models have some capacity for creative problem-solving but highlights areas needing improvement in their reasoning abilities."}}
{"id": "2505.10880", "pdf": "https://arxiv.org/pdf/2505.10880", "abs": "https://arxiv.org/abs/2505.10880", "authors": ["Guoji Fu", "Wee Sun Lee"], "title": "Approximation and Generalization Abilities of Score-based Neural Network Generative Models for Sub-Gaussian Distributions", "categories": ["cs.LG", "stat.ML"], "comment": "94 pages", "summary": "This paper studies the approximation and generalization abilities of\nscore-based neural network generative models (SGMs) in estimating an unknown\ndistribution $P_0$ from $n$ i.i.d. observations in $d$ dimensions. Assuming\nmerely that $P_0$ is $\\alpha$-sub-Gaussian, we prove that for any time step $t\n\\in [t_0, n^{O(1)}]$, where $t_0 \\geq O(\\alpha^2n^{-2/d}\\log n)$, there exists\na deep ReLU neural network with width $\\leq O(\\log^3n)$ and depth $\\leq\nO(n^{3/d}\\log_2n)$ that can approximate the scores with $\\tilde{O}(n^{-1})$\nmean square error and achieve a nearly optimal rate of\n$\\tilde{O}(n^{-1}t_0^{-d/2})$ for score estimation, as measured by the score\nmatching loss. Our framework is universal and can be used to establish\nconvergence rates for SGMs under milder assumptions than previous work. For\nexample, assuming further that the target density function $p_0$ lies in\nSobolev or Besov classes, with an appropriately early stopping strategy, we\ndemonstrate that neural network-based SGMs can attain nearly minimax\nconvergence rates up to logarithmic factors. Our analysis removes several\ncrucial assumptions, such as Lipschitz continuity of the score function or a\nstrictly positive lower bound on the target density.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8e\u5206\u6570\u7684\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u6a21\u578b(SGMs)\u5728\u4f30\u8ba1\u672a\u77e5\u5206\u5e03\u65f6\u7684\u903c\u8fd1\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5728\u66f4\u6e29\u548c\u7684\u5047\u8bbe\u4e0b\u5efa\u7acbSGMs\u7684\u6536\u655b\u7387\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6ReLU\u795e\u7ecf\u7f51\u7edc\u6765\u8fd1\u4f3c\u5206\u6570\uff0c\u5e76\u5206\u6790\u5176\u5747\u65b9\u8bef\u5dee\u548c\u5f97\u5206\u4f30\u8ba1\u7684\u6700\u4f18\u7387\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0cSGMs\u53ef\u4ee5\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u7684\u6536\u655b\u7387\uff0c\u5e76\u4e14\u65e0\u9700\u8bb8\u591a\u5173\u952e\u5047\u8bbe\u5982\u5f97\u5206\u51fd\u6570\u7684Lipschitz\u8fde\u7eed\u6027\u6216\u76ee\u6807\u5bc6\u5ea6\u7684\u4e25\u683c\u6b63\u4e0b\u754c\u3002", "conclusion": "\u8be5\u6846\u67b6\u662f\u901a\u7528\u7684\uff0c\u53ef\u5e94\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u573a\u666f\uff0c\u4e3aSGMs\u7684\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2505.10859", "pdf": "https://arxiv.org/pdf/2505.10859", "abs": "https://arxiv.org/abs/2505.10859", "authors": ["Yingdan Shi", "Ren Wang"], "title": "MCU: Improving Machine Unlearning through Mode Connectivity", "categories": ["cs.AI"], "comment": null, "summary": "Machine Unlearning (MU) aims to remove the information of specific training\ndata from a trained model, ensuring compliance with privacy regulations and\nuser requests. While one line of existing MU methods relies on linear parameter\nupdates via task arithmetic, they suffer from weight entanglement. In this\nwork, we propose a novel MU framework called Mode Connectivity Unlearning (MCU)\nthat leverages mode connectivity to find an unlearning pathway in a nonlinear\nmanner. To further enhance performance and efficiency, we introduce a parameter\nmask strategy that not only improves unlearning effectiveness but also reduces\ncomputational overhead. Moreover, we propose an adaptive adjustment strategy\nfor our unlearning penalty coefficient to adaptively balance forgetting quality\nand predictive performance during training, eliminating the need for empirical\nhyperparameter tuning. Unlike traditional MU methods that identify only a\nsingle unlearning model, MCU uncovers a spectrum of unlearning models along the\npathway. Overall, MCU serves as a plug-and-play framework that seamlessly\nintegrates with any existing MU methods, consistently improving unlearning\nefficacy. Extensive experiments on the image classification task demonstrate\nthat MCU achieves superior performance.", "AI": {"tldr": "This paper proposes a new machine unlearning framework called Mode Connectivity Unlearning (MCU), which improves unlearning efficacy and efficiency by leveraging mode connectivity and introducing parameter mask and adaptive adjustment strategies.", "motivation": "To address the limitations of existing machine unlearning methods suffering from weight entanglement and to ensure compliance with privacy regulations and user requests.", "method": "Mode Connectivity Unlearning (MCU) framework with parameter mask strategy and adaptive adjustment strategy.", "result": "MCU outperforms other methods in terms of unlearning efficacy and computational efficiency.", "conclusion": "MCU is a novel machine unlearning framework that effectively removes specific data information from a trained model."}}
{"id": "2505.10882", "pdf": "https://arxiv.org/pdf/2505.10882", "abs": "https://arxiv.org/abs/2505.10882", "authors": ["Alex Saad-Falcon", "Brighton Ancelin", "Justin Romberg"], "title": "Global Convergence of Adaptive Sensing for Principal Eigenvector Estimation", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "This paper addresses the challenge of efficient principal component analysis\n(PCA) in high-dimensional spaces by analyzing a compressively sampled variant\nof Oja's algorithm with adaptive sensing. Traditional PCA methods incur\nsubstantial computational costs that scale poorly with data dimensionality,\nwhereas subspace tracking algorithms like Oja's offer more efficient\nalternatives but typically require full-dimensional observations. We analyze a\nvariant where, at each iteration, only two compressed measurements are taken:\none in the direction of the current estimate and one in a random orthogonal\ndirection. We prove that this adaptive sensing approach achieves global\nconvergence in the presence of noise when tracking the leading eigenvector of a\ndatastream with eigengap $\\Delta=\\lambda_1-\\lambda_2$. Our theoretical analysis\ndemonstrates that the algorithm experiences two phases: (1) a warmup phase\nrequiring $O(\\lambda_1\\lambda_2d^2/\\Delta^2)$ iterations to achieve a\nconstant-level alignment with the true eigenvector, followed by (2) a local\nconvergence phase where the sine alignment error decays at a rate of\n$O(\\lambda_1\\lambda_2d^2/\\Delta^2 t)$ for iterations $t$. The guarantee aligns\nwith existing minimax lower bounds with an added factor of $d$ due to the\ncompressive sampling. This work provides the first convergence guarantees in\nadaptive sensing for subspace tracking with noise. Our proof technique is also\nconsiderably simpler than those in prior works. The results have important\nimplications for applications where acquiring full-dimensional samples is\nchallenging or costly.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u9ad8\u6548\u6267\u884c\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u7684\u538b\u7f29\u81ea\u9002\u5e94\u91c7\u6837Oja\u7b97\u6cd5\u53d8\u4f53\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u5168\u7403\u6536\u655b\u6027\u548c\u4e24\u9636\u6bb5\u8bef\u5dee\u8870\u51cf\u65b9\u9762\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u4f20\u7edfPCA\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u800c\u73b0\u6709\u7684\u5b50\u7a7a\u95f4\u8ddf\u8e2a\u7b97\u6cd5\u901a\u5e38\u9700\u8981\u5168\u7ef4\u5ea6\u89c2\u6d4b\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u538b\u7f29\u81ea\u9002\u5e94\u91c7\u6837\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5206\u6790\u4e86\u4e00\u79cd\u6bcf\u6b21\u8fed\u4ee3\u53ea\u83b7\u53d6\u4e24\u4e2a\u538b\u7f29\u6d4b\u91cf\u503c\u7684Oja\u7b97\u6cd5\u53d8\u4f53\uff0c\u5176\u4e2d\u4e00\u4e2a\u65b9\u5411\u662f\u5f53\u524d\u4f30\u8ba1\u503c\u7684\u65b9\u5411\uff0c\u53e6\u4e00\u4e2a\u662f\u5728\u968f\u673a\u6b63\u4ea4\u65b9\u5411\u4e0a\u7684\u6d4b\u91cf\u503c\u3002", "result": "\u8bc1\u660e\u4e86\u8be5\u81ea\u9002\u5e94\u91c7\u6837\u65b9\u6cd5\u5728\u5b58\u5728\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\u80fd\u591f\u5168\u5c40\u6536\u655b\uff0c\u4e14\u5177\u6709\u4e24\u9636\u6bb5\u7684\u8bef\u5dee\u8870\u51cf\u901f\u7387\uff0c\u540c\u65f6\u9996\u6b21\u63d0\u4f9b\u4e86\u81ea\u9002\u5e94\u91c7\u6837\u6761\u4ef6\u4e0b\u5b50\u7a7a\u95f4\u8ddf\u8e2a\u7684\u6536\u655b\u6027\u4fdd\u8bc1\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u538b\u7f29\u81ea\u9002\u5e94\u91c7\u6837\u65b9\u6cd5\u5728\u9ad8\u7ef4\u6570\u636e\u5904\u7406\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\uff0c\u5c24\u5176\u662f\u5728\u83b7\u53d6\u5168\u7ef4\u5ea6\u6837\u672c\u56f0\u96be\u6216\u6602\u8d35\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2505.10699", "pdf": "https://arxiv.org/pdf/2505.10699", "abs": "https://arxiv.org/abs/2505.10699", "authors": ["Kutay B\u00f6lat", "Tarek Alskaif", "Peter Palensky", "Simon Tindemans"], "title": "Clustering Rooftop PV Systems via Probabilistic Embeddings", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "As the number of rooftop photovoltaic (PV) installations increases,\naggregators and system operators are required to monitor and analyze these\nsystems, raising the challenge of integration and management of large,\nspatially distributed time-series data that are both high-dimensional and\naffected by missing values. In this work, a probabilistic entity\nembedding-based clustering framework is proposed to address these problems.\nThis method encodes each PV system's characteristic power generation patterns\nand uncertainty as a probability distribution, then groups systems by their\nstatistical distances and agglomerative clustering. Applied to a multi-year\nresidential PV dataset, it produces concise, uncertainty-aware cluster profiles\nthat outperform a physics-based baseline in representativeness and robustness,\nand support reliable missing-value imputation. A systematic hyperparameter\nstudy further offers practical guidance for balancing model performance and\nrobustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u5b9e\u4f53\u5d4c\u5165\u7684\u805a\u7c7b\u6846\u67b6\u6765\u5904\u7406\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u5149\u4f0f\u7cfb\u7edf\u7684\u9ad8\u7ef4\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u6574\u5408\u4e0e\u7ba1\u7406\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5728\u4f4f\u5b85\u5149\u4f0f\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u7269\u7406\u57fa\u51c6\uff0c\u5e76\u652f\u6301\u53ef\u9760\u7684\u7f3a\u5931\u503c\u586b\u8865\u3002", "motivation": "\u968f\u7740\u5c4b\u9876\u5149\u4f0f\u5b89\u88c5\u6570\u91cf\u7684\u589e\u52a0\uff0c\u9700\u8981\u76d1\u6d4b\u548c\u5206\u6790\u8fd9\u4e9b\u7cfb\u7edf\uff0c\u9762\u4e34\u6574\u5408\u548c\u7ba1\u7406\u5927\u91cf\u9ad8\u7ef4\u4e14\u53d7\u7f3a\u5931\u503c\u5f71\u54cd\u7684\u7a7a\u95f4\u5206\u5e03\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u7684\u6846\u67b6\u5c06\u6bcf\u4e2a\u5149\u4f0f\u7cfb\u7edf\u7684\u7279\u5f81\u53d1\u7535\u6a21\u5f0f\u548c\u4e0d\u786e\u5b9a\u6027\u7f16\u7801\u4e3a\u6982\u7387\u5206\u5e03\uff0c\u7136\u540e\u901a\u8fc7\u7edf\u8ba1\u8ddd\u79bb\u548c\u51dd\u805a\u805a\u7c7b\u5bf9\u7cfb\u7edf\u8fdb\u884c\u5206\u7ec4\u3002", "result": "\u5728\u591a \u5e74\u4f4f\u5b85\u5149\u4f0f\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\u4ea7\u751f\u4e86\u7b80\u6d01\u4e14\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u805a\u7c7b\u8f6e\u5ed3\uff0c\u5728\u4ee3\u8868\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u7269\u7406\u57fa\u51c6\uff0c\u5e76\u652f\u6301\u53ef\u9760\u7684\u7f3a\u5931\u503c\u586b\u8865\u3002", "conclusion": "\u7814\u7a76\u8fdb\u4e00\u6b65\u63d0\u4f9b\u4e86\u5173\u4e8e\u5e73\u8861\u6a21\u578b\u6027\u80fd\u548c\u9c81\u68d2\u6027\u7684\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2505.10887", "pdf": "https://arxiv.org/pdf/2505.10887", "abs": "https://arxiv.org/abs/2505.10887", "authors": ["Bin Lei", "Weitai Kang", "Zijian Zhang", "Winson Chen", "Xi Xie", "Shan Zuo", "Mimi Xie", "Ali Payani", "Mingyi Hong", "Yan Yan", "Caiwen Ding"], "title": "InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer Interaction", "categories": ["cs.AI"], "comment": null, "summary": "This paper introduces \\textsc{InfantAgent-Next}, a generalist agent capable\nof interacting with computers in a multimodal manner, encompassing text,\nimages, audio, and video. Unlike existing approaches that either build\nintricate workflows around a single large model or only provide workflow\nmodularity, our agent integrates tool-based and pure vision agents within a\nhighly modular architecture, enabling different models to collaboratively solve\ndecoupled tasks in a step-by-step manner. Our generality is demonstrated by our\nability to evaluate not only pure vision-based real-world benchmarks (i.e.,\nOSWorld), but also more general or tool-intensive benchmarks (e.g., GAIA and\nSWE-Bench). Specifically, we achieve $\\mathbf{7.27\\%}$ accuracy on OSWorld,\nhigher than Claude-Computer-Use. Codes and evaluation scripts are open-sourced\nat https://github.com/bin123apple/InfantAgent.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3a\\textsc{InfantAgent-Next}\u7684\u591a\u6a21\u6001\u901a\u7528\u667a\u80fd\u4f53\uff0c\u5b83\u80fd\u591f\u901a\u8fc7\u6a21\u5757\u5316\u67b6\u6784\u6574\u5408\u591a\u79cd\u6a21\u578b\u534f\u540c\u89e3\u51b3\u4efb\u52a1\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u56f4\u7ed5\u5355\u4e00\u5927\u6a21\u578b\u6784\u5efa\u590d\u6742\u5de5\u4f5c\u6d41\uff0c\u8981\u4e48\u4ec5\u63d0\u4f9b\u5de5\u4f5c\u6d41\u6a21\u5757\u6027\uff0c\u800c\u672c\u6587\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u9ad8\u5ea6\u6a21\u5757\u5316\u7684\u67b6\u6784\uff0c\u96c6\u6210\u5de5\u5177\u578b\u548c\u7eaf\u89c6\u89c9\u4ee3\u7406\uff0c\u4f7f\u4e0d\u540c\u6a21\u578b\u80fd\u9010\u6b65\u534f\u4f5c\u5b8c\u6210\u89e3\u8026\u7684\u4efb\u52a1\u3002", "result": "\u5728OSWorld\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e867.27%\u7684\u51c6\u786e\u7387\uff0c\u5728GAIA\u548cSWE-Bench\u7b49\u66f4\u901a\u7528\u6216\u5de5\u5177\u5bc6\u96c6\u578b\u57fa\u51c6\u4e0a\u4e5f\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\\textsc{InfantAgent-Next}\u5c55\u793a\u4e86\u5176\u5728\u591a\u6a21\u6001\u4ea4\u4e92\u4e2d\u7684\u4f18\u8d8a\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u6d89\u53ca\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u7b49\u591a\u79cd\u6a21\u5f0f\u7684\u4efb\u52a1\u65f6\u3002"}}
{"id": "2505.11054", "pdf": "https://arxiv.org/pdf/2505.11054", "abs": "https://arxiv.org/abs/2505.11054", "authors": ["M\u00e9lodie Monod", "Alessandro Micheli", "Samir Bhatt"], "title": "NeuralSurv: Deep Survival Analysis with Bayesian Uncertainty Quantification", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We introduce NeuralSurv, the first deep survival model to incorporate\nBayesian uncertainty quantification. Our non-parametric, architecture-agnostic\nframework flexibly captures time-varying covariate-risk relationships in\ncontinuous time via a novel two-stage data-augmentation scheme, for which we\nestablish theoretical guarantees. For efficient posterior inference, we\nintroduce a mean-field variational algorithm with coordinate-ascent updates\nthat scale linearly in model size. By locally linearizing the Bayesian neural\nnetwork, we obtain full conjugacy and derive all coordinate updates in closed\nform. In experiments, NeuralSurv delivers superior calibration compared to\nstate-of-the-art deep survival models, while matching or exceeding their\ndiscriminative performance across both synthetic benchmarks and real-world\ndatasets. Our results demonstrate the value of Bayesian principles in\ndata-scarce regimes by enhancing model calibration and providing robust,\nwell-calibrated uncertainty estimates for the survival function.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u7ed3\u5408\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6df1\u5ea6\u751f\u5b58\u6a21\u578bNeuralSurv\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6848\u5728\u8fde\u7eed\u65f6\u95f4\u5185\u7075\u6d3b\u6355\u6349\u968f\u65f6\u95f4\u53d8\u5316\u7684\u534f\u53d8\u91cf\u98ce\u9669\u5173\u7cfb\uff0c\u5e76\u63d0\u4f9b\u7406\u8bba\u4fdd\u969c\u3002", "motivation": "\u5f15\u5165\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4ee5\u63d0\u9ad8\u6a21\u578b\u6821\u51c6\u548c\u63d0\u4f9b\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "method": "\u8bbe\u8ba1\u975e\u53c2\u6570\u3001\u67b6\u6784\u65e0\u5173\u7684\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u6570\u636e\u589e\u5f3a\u65b9\u6848\uff0c\u63d0\u51fa\u5747\u503c\u573a\u53d8\u5206\u7b97\u6cd5\u5b9e\u73b0\u9ad8\u6548\u540e\u9a8c\u63a8\u7406\u5e76\u5c40\u90e8\u7ebf\u6027\u5316\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u5728\u5b9e\u9a8c\u4e2d\uff0cNeuralSurv\u5728\u5408\u6210\u57fa\u51c6\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u63d0\u4f9b\u4e86\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u751f\u5b58\u6a21\u578b\u7684\u6821\u51c6\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u4e86\u5176\u8fa8\u522b\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u8d1d\u53f6\u65af\u539f\u5219\u5728\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e2d\u7684\u4ef7\u503c\uff0c\u901a\u8fc7\u589e\u5f3a\u6a21\u578b\u6821\u51c6\u548c\u63d0\u4f9b\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6765\u6539\u5584\u751f\u5b58\u51fd\u6570\u3002"}}
{"id": "2505.10704", "pdf": "https://arxiv.org/pdf/2505.10704", "abs": "https://arxiv.org/abs/2505.10704", "authors": ["Patryk Marsza\u0142ek", "Tomasz Ku\u015bmierczyk", "Witold Wydma\u0144ski", "Jacek Tabor", "Marek \u015amieja"], "title": "ZEUS: Zero-shot Embeddings for Unsupervised Separation of Tabular Data", "categories": ["cs.LG"], "comment": null, "summary": "Clustering tabular data remains a significant open challenge in data analysis\nand machine learning. Unlike for image data, similarity between tabular records\noften varies across datasets, making the definition of clusters highly\ndataset-dependent. Furthermore, the absence of supervised signals complicates\nhyperparameter tuning in deep learning clustering methods, frequently resulting\nin unstable performance. To address these issues and reduce the need for\nper-dataset tuning, we adopt an emerging approach in deep learning: zero-shot\nlearning. We propose ZEUS, a self-contained model capable of clustering new\ndatasets without any additional training or fine-tuning. It operates by\ndecomposing complex datasets into meaningful components that can then be\nclustered effectively. Thanks to pre-training on synthetic datasets generated\nfrom a latent-variable prior, it generalizes across various datasets without\nrequiring user intervention. To the best of our knowledge, ZEUS is the first\nzero-shot method capable of generating embeddings for tabular data in a fully\nunsupervised manner. Experimental results demonstrate that it performs on par\nwith or better than traditional clustering algorithms and recent deep\nlearning-based methods, while being significantly faster and more\nuser-friendly.", "AI": {"tldr": "ZEUS\u662f\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u5bf9\u65b0\u8868\u683c\u6570\u636e\u96c6\u8fdb\u884c\u805a\u7c7b\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u5206\u89e3\u590d\u6742\u7684\u6570\u636e\u96c6\u4e3a\u6709\u610f\u4e49\u7684\u90e8\u5206\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u5e76\u4e14\u7531\u4e8e\u5728\u6f5c\u5728\u53d8\u91cf\u5148\u9a8c\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u9884\u8bad\u7ec3\uff0c\u5b83\u53ef\u4ee5\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u6cdb\u5316\u3002", "motivation": "\u5f53\u524d\u8868\u683c\u6570\u636e\u805a\u7c7b\u9762\u4e34\u6311\u6218\uff0c\u5305\u62ec\u76f8\u4f3c\u6027\u56e0\u6570\u636e\u96c6\u800c\u5f02\u4ee5\u53ca\u7f3a\u4e4f\u76d1\u7763\u4fe1\u53f7\u5bfc\u81f4\u6027\u80fd\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u96f6\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aZEUS\u7684\u65b0\u6a21\u578b\uff0c\u80fd\u591f\u76f4\u63a5\u5bf9\u65b0\u6570\u636e\u96c6\u8fdb\u884c\u805a\u7c7b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cZEUS\u7684\u8868\u73b0\u4f18\u4e8e\u6216\u81f3\u5c11\u4e0e\u4f20\u7edf\u7684\u805a\u7c7b\u7b97\u6cd5\u548c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u901f\u5ea6\u66f4\u5feb\u4e14\u66f4\u6613\u4e8e\u4f7f\u7528\u3002", "conclusion": "ZEUS\u662f\u9996\u4e2a\u5b8c\u5168\u65e0\u76d1\u7763\u5730\u4e3a\u8868\u683c\u6570\u636e\u751f\u6210\u5d4c\u5165\u7684\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u4e3a\u89e3\u51b3\u8868\u683c\u6570\u636e\u805a\u7c7b\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u7528\u6237\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.10962", "pdf": "https://arxiv.org/pdf/2505.10962", "abs": "https://arxiv.org/abs/2505.10962", "authors": ["Zhenwen Liang", "Linfeng Song", "Yang Li", "Tao Yang", "Feng Zhang", "Haitao Mi", "Dong Yu"], "title": "MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective Search and Data Curation", "categories": ["cs.AI"], "comment": "Work in Progress", "summary": "Automated Theorem Proving (ATP) in formal languages remains a formidable\nchallenge in AI, demanding rigorous logical deduction and navigating vast\nsearch spaces. While large language models (LLMs) have shown promising\nperformance, existing stepwise provers often suffer from biased search\nguidance, leading to inefficiencies and suboptimal proof strategies. This paper\nintroduces the Multi-Perspective Search Prover (MPS-Prover), a novel stepwise\nATP system designed to overcome these limitations. MPS-Prover incorporates two\nkey innovations: a highly effective post-training data curation strategy that\nprunes approximately 40% of redundant training data without sacrificing\nperformance, and a multi-perspective tree search mechanism. This search\nintegrates a learned critic model with strategically designed heuristic rules\nto diversify tactic selection, prevent getting trapped in unproductive states,\nand enhance search robustness. Extensive evaluations demonstrate that\nMPS-Prover achieves state-of-the-art performance on multiple challenging\nbenchmarks, including miniF2F and ProofNet, outperforming prior 7B parameter\nmodels. Furthermore, our analyses reveal that MPS-Prover generates\nsignificantly shorter and more diverse proofs compared to existing stepwise and\nwhole-proof methods, highlighting its efficiency and efficacy. Our work\nadvances the capabilities of LLM-based formal reasoning and offers a robust\nframework and a comprehensive analysis for developing more powerful theorem\nprovers.", "AI": {"tldr": "This paper introduces MPS-Prover, a novel ATP system that improves efficiency and proof quality by pruning redundant data and using a multi-perspective tree search.", "motivation": "Existing ATP systems face challenges like biased search guidance and inefficiency.", "method": "MPS-Prover uses a post-training data curation strategy and a multi-perspective tree search mechanism combining learned critic models with heuristic rules.", "result": "MPS-Prover shows superior performance on benchmarks like miniF2F and ProofNet and produces shorter, more diverse proofs.", "conclusion": "The work advances LLM-based formal reasoning and provides a robust framework for future ATP developments."}}
{"id": "2505.11085", "pdf": "https://arxiv.org/pdf/2505.11085", "abs": "https://arxiv.org/abs/2505.11085", "authors": ["Oliver Schacht", "Biwei Huang"], "title": "A Fast Kernel-based Conditional Independence test with Application to Causal Discovery", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "9 pages, 5 figures", "summary": "Kernel-based conditional independence (KCI) testing is a powerful\nnonparametric method commonly employed in causal discovery tasks. Despite its\nflexibility and statistical reliability, cubic computational complexity limits\nits application to large datasets. To address this computational bottleneck, we\npropose \\textit{FastKCI}, a scalable and parallelizable kernel-based\nconditional independence test that utilizes a mixture-of-experts approach\ninspired by embarrassingly parallel inference techniques for Gaussian\nprocesses. By partitioning the dataset based on a Gaussian mixture model over\nthe conditioning variables, FastKCI conducts local KCI tests in parallel,\naggregating the results using an importance-weighted sampling scheme.\nExperiments on synthetic datasets and benchmarks on real-world production data\nvalidate that FastKCI maintains the statistical power of the original KCI test\nwhile achieving substantial computational speedups. FastKCI thus represents a\npractical and efficient solution for conditional independence testing in causal\ninference on large-scale data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFastKCI\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6761\u4ef6\u72ec\u7acb\u6027\u68c0\u9a8c\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8eKCI\u6d4b\u8bd5\uff0c\u5e76\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u65b9\u6cd5\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u6027\u548c\u5e76\u884c\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7edf\u8ba1\u529f\u6548\u3002", "motivation": "\u89e3\u51b3KCI\u6d4b\u8bd5\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u56e0\u4e3a\u5176\u4e09\u6b21\u65b9\u590d\u6742\u5ea6\u9650\u5236\u4e86\u5e94\u7528\u3002", "method": "FastKCI\u5229\u7528\u6df7\u5408\u4e13\u5bb6\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u5bf9\u6570\u636e\u96c6\u5206\u533a\uff0c\u5e76\u5728\u5c40\u90e8\u8fdb\u884cKCI\u6d4b\u8bd5\uff0c\u7ed3\u679c\u4f7f\u7528\u91cd\u8981\u6027\u52a0\u6743\u91c7\u6837\u65b9\u6848\u805a\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFastKCI\u5728\u4fdd\u6301\u539f\u59cbKCI\u6d4b\u8bd5\u7edf\u8ba1\u529f\u6548\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8ba1\u7b97\u52a0\u901f\u3002", "conclusion": "FastKCI\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u56e0\u679c\u63a8\u7406\u4e2d\u7684\u6761\u4ef6\u72ec\u7acb\u6027\u68c0\u9a8c\u3002"}}
{"id": "2505.10711", "pdf": "https://arxiv.org/pdf/2505.10711", "abs": "https://arxiv.org/abs/2505.10711", "authors": ["Sebesty\u00e9n Kamp", "Giovanni Stracquadanio", "T. Ian Simpson"], "title": "GNN-Suite: a Graph Neural Network Benchmarking Framework for Biomedical Informatics", "categories": ["cs.LG", "cs.AI", "J.3; I.2.1"], "comment": "Main article 8 pages (20 in total with supplementary information\n  included), 3 main article figures and 3 supplemental figures", "summary": "We present GNN-Suite, a robust modular framework for constructing and\nbenchmarking Graph Neural Network (GNN) architectures in computational biology.\nGNN-Suite standardises experimentation and reproducibility using the Nextflow\nworkflow to evaluate GNN performance. We demonstrate its utility in identifying\ncancer-driver genes by constructing molecular networks from protein-protein\ninteraction (PPI) data from STRING and BioGRID and annotating nodes with\nfeatures from the PCAWG, PID, and COSMIC-CGC repositories.\n  Our design enables fair comparisons among diverse GNN architectures including\nGAT, GAT3H, GCN, GCN2, GIN, GTN, HGCN, PHGCN, and GraphSAGE and a baseline\nLogistic Regression (LR) model. All GNNs were configured as standardised\ntwo-layer models and trained with uniform hyperparameters (dropout = 0.2; Adam\noptimiser with learning rate = 0.01; and an adjusted binary cross-entropy loss\nto address class imbalance) over an 80/20 train-test split for 300 epochs. Each\nmodel was evaluated over 10 independent runs with different random seeds to\nyield statistically robust performance metrics, with balanced accuracy (BACC)\nas the primary measure. Notably, GCN2 achieved the highest BACC (0.807 +/-\n0.035) on a STRING-based network, although all GNN types outperformed the LR\nbaseline, highlighting the advantage of network-based learning over\nfeature-only approaches.\n  Our results show that a common framework for implementing and evaluating GNN\narchitectures aids in identifying not only the best model but also the most\neffective means of incorporating complementary data. By making GNN-Suite\npublicly available, we aim to foster reproducible research and promote improved\nbenchmarking standards in computational biology. Future work will explore\nadditional omics datasets and further refine network architectures to enhance\npredictive accuracy and interpretability in biomedical applications.", "AI": {"tldr": "GNN-Suite\u662f\u4e00\u4e2a\u7528\u4e8e\u6784\u5efa\u548c\u8bc4\u4f30\u8ba1\u7b97\u751f\u7269\u5b66\u4e2d\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u67b6\u6784\u7684\u7a33\u5065\u6a21\u5757\u5316\u6846\u67b6\u3002\u5b83\u5728\u8bc6\u522b\u764c\u75c7\u9a71\u52a8\u57fa\u56e0\u65b9\u9762\u5c55\u793a\u4e86\u5176\u6548\u7528\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e0d\u540cGNN\u67b6\u6784\u4e4b\u95f4\u7684\u516c\u5e73\u6bd4\u8f83\u3002", "motivation": "\u5f53\u524d\u8ba1\u7b97\u751f\u7269\u5b66\u9886\u57df\u7f3a\u4e4f\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u6846\u67b6\u6765\u6784\u5efa\u548c\u8bc4\u4f30GNN\u67b6\u6784\uff0c\u8fd9\u963b\u788d\u4e86\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u548c\u6a21\u578b\u95f4\u7684\u516c\u5e73\u6bd4\u8f83\u3002", "method": "GNN-Suite\u4f7f\u7528Nextflow\u5de5\u4f5c\u6d41\u6765\u6807\u51c6\u5316\u5b9e\u9a8c\u548c\u53ef\u91cd\u590d\u6027\u3002\u5b83\u901a\u8fc7\u4eceSTRING\u548cBioGRID\u6570\u636e\u96c6\u4e2d\u6784\u5efa\u5206\u5b50\u7f51\u7edc\u5e76\u4f7f\u7528PCAWG\u3001PID\u548cCOSMIC-CGC\u5b58\u50a8\u5e93\u6ce8\u91ca\u8282\u70b9\u6765\u8bc6\u522b\u764c\u75c7\u9a71\u52a8\u57fa\u56e0\u3002\u6240\u6709GNN\u6a21\u578b\u90fd\u88ab\u914d\u7f6e\u4e3a\u6807\u51c6\u7684\u4e24\u5c42\u6a21\u578b\uff0c\u5e76\u5728\u7edf\u4e00\u7684\u8d85\u53c2\u6570\u8bbe\u7f6e\u4e0b\u8bad\u7ec3\u3002", "result": "GCN2\u5728STRING\u57fa\u7f51\u7edc\u4e0a\u8fbe\u5230\u4e86\u6700\u9ad8\u7684\u5e73\u8861\u51c6\u786e\u7387\uff08BACC\uff09\uff0c\u5e76\u4e14\u6240\u6709GNN\u7c7b\u578b\u90fd\u4f18\u4e8e\u903b\u8f91\u56de\u5f52\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "GNN-Suite\u6709\u52a9\u4e8e\u8bc6\u522b\u6700\u4f73\u6a21\u578b\u4ee5\u53ca\u6574\u5408\u4e92\u8865\u6570\u636e\u7684\u6700\u6709\u6548\u65b9\u6cd5\uff0c\u4fc3\u8fdb\u4e86\u8ba1\u7b97\u751f\u7269\u5b66\u9886\u57df\u7684\u53ef\u91cd\u590d\u7814\u7a76\u548c\u57fa\u51c6\u6d4b\u8bd5\u6807\u51c6\u7684\u6539\u8fdb\u3002\u672a\u6765\u7684\u5de5\u4f5c\u5c06\u63a2\u7d22\u66f4\u591a\u7684\u7ec4\u5b66\u6570\u636e\u96c6\u5e76\u8fdb\u4e00\u6b65\u5b8c\u5584\u7f51\u7edc\u67b6\u6784\u4ee5\u63d0\u9ad8\u751f\u7269\u533b\u5b66\u5e94\u7528\u4e2d\u7684\u9884\u6d4b\u51c6\u786e\u6027\u548c\u89e3\u91ca\u6027\u3002"}}
{"id": "2505.10981", "pdf": "https://arxiv.org/pdf/2505.10981", "abs": "https://arxiv.org/abs/2505.10981", "authors": ["Yexiang Liu", "Zekun Li", "Zhi Fang", "Nan Xu", "Ran He", "Tieniu Tan"], "title": "Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "ACL 2025 Main", "summary": "Recently, scaling test-time compute on Large Language Models (LLM) has\ngarnered wide attention. However, there has been limited investigation of how\nvarious reasoning prompting strategies perform as scaling. In this paper, we\nfocus on a standard and realistic scaling setting: majority voting. We\nsystematically conduct experiments on 6 LLMs $\\times$ 8 prompting strategies\n$\\times$ 6 benchmarks. Experiment results consistently show that as the\nsampling time and computational overhead increase, complicated prompting\nstrategies with superior initial performance gradually fall behind simple\nChain-of-Thought. We analyze this phenomenon and provide theoretical proofs.\nAdditionally, we propose a method according to probability theory to quickly\nand accurately predict the scaling performance and select the best strategy\nunder large sampling times without extra resource-intensive inference in\npractice. It can serve as the test-time scaling law for majority voting.\nFurthermore, we introduce two ways derived from our theoretical analysis to\nsignificantly improve the scaling performance. We hope that our research can\npromote to re-examine the role of complicated prompting, unleash the potential\nof simple prompting strategies, and provide new insights for enhancing\ntest-time scaling performance.", "AI": {"tldr": "This paper investigates various reasoning prompting strategies for large language models under a majority voting scaling setting. The authors found that simple Chain-of-Thought strategies outperform complex ones as computation scales, and they provide a method to predict performance and improve scaling.", "motivation": "To investigate how different reasoning prompting strategies perform as scaling increases, focusing on a standard and realistic scaling setting: majority voting.", "method": "Systematically conducting experiments on 6 LLMs \u00d7 8 prompting strategies \u00d7 6 benchmarks, analyzing the results, providing theoretical proofs, and proposing a method based on probability theory to predict scaling performance.", "result": "As sampling time and computational overhead increase, simple Chain-of-Thought prompting strategies outperform complex ones. The authors also introduced two ways to improve scaling performance.", "conclusion": "The research suggests re-evaluating the role of complex prompting strategies and highlights the potential of simple ones in enhancing test-time scaling performance."}}
{"id": "2505.11126", "pdf": "https://arxiv.org/pdf/2505.11126", "abs": "https://arxiv.org/abs/2505.11126", "authors": ["Shokichi Takakura", "Seng Pei Liew", "Satoshi Hasegawa"], "title": "FedDuA: Doubly Adaptive Federated Learning", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Federated learning is a distributed learning framework where clients\ncollaboratively train a global model without sharing their raw data. FedAvg is\na popular algorithm for federated learning, but it often suffers from slow\nconvergence due to the heterogeneity of local datasets and anisotropy in the\nparameter space. In this work, we formalize the central server optimization\nprocedure through the lens of mirror descent and propose a novel framework,\ncalled FedDuA, which adaptively selects the global learning rate based on both\ninter-client and coordinate-wise heterogeneity in the local updates. We prove\nthat our proposed doubly adaptive step-size rule is minimax optimal and provide\na convergence analysis for convex objectives. Although the proposed method does\nnot require additional communication or computational cost on clients,\nextensive numerical experiments show that our proposed framework outperforms\nbaselines in various settings and is robust to the choice of hyperparameters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6FedDuA\uff0c\u901a\u8fc7\u955c\u50cf\u4e0b\u964d\u4f18\u5316\u65b9\u6cd5\u81ea\u9002\u5e94\u8c03\u6574\u5168\u5c40\u5b66\u4e60\u7387\uff0c\u5728\u4fdd\u6301\u5ba2\u6237\u7aef\u901a\u4fe1\u548c\u8ba1\u7b97\u6210\u672c\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u7b97\u6cd5\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u5982FedAvg\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u7684\u6570\u636e\u4e0a\u6536\u655b\u8f83\u6162\uff0c\u4e14\u5bf9\u53c2\u6570\u7a7a\u95f4\u7684\u5404\u5411\u5f02\u6027\u654f\u611f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6FedDuA\uff0c\u91c7\u7528\u53cc\u81ea\u9002\u5e94\u6b65\u957f\u89c4\u5219\uff0c\u6839\u636e\u5ba2\u6237\u7aef\u95f4\u548c\u5750\u6807\u65b9\u5411\u4e0a\u7684\u5f02\u8d28\u6027\u81ea\u9002\u5e94\u8c03\u6574\u5168\u5c40\u5b66\u4e60\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\uff0c\u5e76\u4e14\u5bf9\u8d85\u53c2\u6570\u7684\u9009\u62e9\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684FedDuA\u6846\u67b6\u5728\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u5176\u6700\u4f18\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.10726", "pdf": "https://arxiv.org/pdf/2505.10726", "abs": "https://arxiv.org/abs/2505.10726", "authors": ["Yihan Zhu", "Gang Liu", "Eric Inae", "Tengfei Luo", "Meng Jiang"], "title": "Learning Repetition-Invariant Representations for Polymer Informatics", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages,3 figuares", "summary": "Polymers are large macromolecules composed of repeating structural units\nknown as monomers and are widely applied in fields such as energy storage,\nconstruction, medicine, and aerospace. However, existing graph neural network\nmethods, though effective for small molecules, only model the single unit of\npolymers and fail to produce consistent vector representations for the true\npolymer structure with varying numbers of units. To address this challenge, we\nintroduce Graph Repetition Invariance (GRIN), a novel method to learn polymer\nrepresentations that are invariant to the number of repeating units in their\ngraph representations. GRIN integrates a graph-based maximum spanning tree\nalignment with repeat-unit augmentation to ensure structural consistency. We\nprovide theoretical guarantees for repetition-invariance from both model and\ndata perspectives, demonstrating that three repeating units are the minimal\naugmentation required for optimal invariant representation learning. GRIN\noutperforms state-of-the-art baselines on both homopolymer and copolymer\nbenchmarks, learning stable, repetition-invariant representations that\ngeneralize effectively to polymer chains of unseen sizes.", "AI": {"tldr": "\u63d0\u51faGraph Repetition Invariance (GRIN)\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e60\u5bf9\u91cd\u590d\u5355\u5143\u6570\u91cf\u4e0d\u53d8\u7684\u805a\u5408\u7269\u8868\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u540c\u805a\u7269\u548c\u5171\u805a\u7269\u57fa\u51c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u53ea\u80fd\u5bf9\u5355\u4e2a\u5355\u5143\u5efa\u6a21\u800c\u4e0d\u80fd\u4ea7\u751f\u4e00\u81f4\u7684\u5411\u91cf\u8868\u793a\u5bf9\u4e8e\u5177\u6709\u4e0d\u540c\u6570\u91cf\u5355\u5143\u7684\u771f\u5b9e\u805a\u5408\u7269\u7ed3\u6784\u3002", "method": "GRIN\u7ed3\u5408\u4e86\u57fa\u4e8e\u56fe\u7684\u6700\u5927\u751f\u6210\u6811\u5bf9\u9f50\u4e0e\u91cd\u590d\u5355\u5143\u589e\u5f3a\u4ee5\u786e\u4fdd\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u5e76\u4ece\u6a21\u578b\u548c\u6570\u636e\u89d2\u5ea6\u63d0\u4f9b\u4e86\u91cd\u590d\u4e0d\u53d8\u6027\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "result": "GRIN\u5728\u540c\u805a\u7269\u548c\u5171\u805a\u7269\u57fa\u51c6\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff0c\u5b66\u4e60\u5230\u7a33\u5b9a\u7684\u3001\u5bf9\u91cd\u590d\u4e0d\u53d8\u7684\u8868\u793a\uff0c\u53ef\u4ee5\u6709\u6548\u63a8\u5e7f\u5230\u672a\u89c1\u8fc7\u5927\u5c0f\u7684\u805a\u5408\u7269\u94fe\u3002", "conclusion": "GRIN\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5b66\u4e60\u5230\u5bf9\u91cd\u590d\u5355\u5143\u6570\u91cf\u4e0d\u53d8\u7684\u805a\u5408\u7269\u8868\u793a\uff0c\u8fd9\u5bf9\u4e8e\u80fd\u6e90\u5b58\u50a8\u3001\u5efa\u7b51\u3001\u533b\u836f\u548c\u822a\u7a7a\u822a\u5929\u7b49\u9886\u57df\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2505.10982", "pdf": "https://arxiv.org/pdf/2505.10982", "abs": "https://arxiv.org/abs/2505.10982", "authors": ["Johannes Fichte", "Nicolas Fr\u00f6hlich", "Markus Hecher", "Victor Lagerkvist", "Yasir Mahmood", "Arne Meier", "Jonathan Persson"], "title": "Facets in Argumentation: A Formal Approach to Argument Significance", "categories": ["cs.AI"], "comment": null, "summary": "Argumentation is a central subarea of Artificial Intelligence (AI) for\nmodeling and reasoning about arguments. The semantics of abstract argumentation\nframeworks (AFs) is given by sets of arguments (extensions) and conditions on\nthe relationship between them, such as stable or admissible. Today's solvers\nimplement tasks such as finding extensions, deciding credulous or skeptical\nacceptance, counting, or enumerating extensions. While these tasks are well\ncharted, the area between decision, counting/enumeration and fine-grained\nreasoning requires expensive reasoning so far. We introduce a novel concept\n(facets) for reasoning between decision and enumeration. Facets are arguments\nthat belong to some extensions (credulous) but not to all extensions\n(skeptical). They are most natural when a user aims to navigate, filter, or\ncomprehend the significance of specific arguments, according to their needs. We\nstudy the complexity and show that tasks involving facets are much easier than\ncounting extensions. Finally, we provide an implementation, and conduct\nexperiments to demonstrate feasibility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u5ff5\uff08facets\uff09\u7528\u4e8e\u62bd\u8c61\u8bba\u8bc1\u6846\u67b6\u4e2d\u7684\u63a8\u7406\uff0c\u5e76\u7814\u7a76\u4e86\u5176\u590d\u6742\u6027\uff0c\u53d1\u73b0\u6d89\u53cafacets\u7684\u4efb\u52a1\u6bd4\u8ba1\u6570\u6269\u5c55\u66f4\u5bb9\u6613\u3002\u6700\u540e\u63d0\u4f9b\u4e86\u5b9e\u73b0\u5e76\u8fdb\u884c\u4e86\u5b9e\u9a8c\u4ee5\u8bc1\u660e\u53ef\u884c\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u8bba\u8bc1\u63a8\u7406\u4efb\u52a1\u867d\u7136\u88ab\u5f88\u597d\u5730\u7814\u7a76\u4e86\uff0c\u4f46\u51b3\u7b56\u3001\u8ba1\u6570/\u679a\u4e3e\u548c\u7ec6\u7c92\u5ea6\u63a8\u7406\u4e4b\u95f4\u7684\u533a\u57df\u9700\u8981\u6602\u8d35\u7684\u63a8\u7406\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u5ff5\uff08facets\uff09\uff0c\u8fd9\u4e9bfacets\u662f\u5c5e\u4e8e\u67d0\u4e9b\u6269\u5c55\u4f46\u4e0d\u5c5e\u4e8e\u6240\u6709\u6269\u5c55\u7684\u8bba\u8bc1\u3002", "result": "\u7814\u7a76\u4e86\u6d89\u53cafacets\u7684\u4efb\u52a1\u7684\u590d\u6742\u6027\uff0c\u53d1\u73b0\u5b83\u4eec\u6bd4\u8ba1\u6570\u6269\u5c55\u66f4\u5bb9\u6613\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u73b0\uff0c\u5e76\u8fdb\u884c\u4e86\u5b9e\u9a8c\u4ee5\u8bc1\u660e\u53ef\u884c\u6027\u3002"}}
{"id": "2505.11210", "pdf": "https://arxiv.org/pdf/2505.11210", "abs": "https://arxiv.org/abs/2505.11210", "authors": ["Anders Gj\u00f8lbye", "Stefan Haufe", "Lars Kai Hansen"], "title": "Minimizing False-Positive Attributions in Explanations of Non-Linear Models", "categories": ["cs.LG", "stat.ML"], "comment": "Preprint. Under review", "summary": "Suppressor variables can influence model predictions without being dependent\non the target outcome and they pose a significant challenge for Explainable AI\n(XAI) methods. These variables may cause false-positive feature attributions,\nundermining the utility of explanations. Although effective remedies exist for\nlinear models, their extension to non-linear models and to instance-based\nexplanations has remained limited. We introduce PatternLocal, a novel XAI\ntechnique that addresses this gap. PatternLocal begins with a locally linear\nsurrogate, e.g. LIME, KernelSHAP, or gradient-based methods, and transforms the\nresulting discriminative model weights into a generative representation,\nthereby suppressing the influence of suppressor variables while preserving\nlocal fidelity. In extensive hyperparameter optimization on the XAI-TRIS\nbenchmark, PatternLocal consistently outperformed other XAI methods and reduced\nfalse-positive attributions when explaining non-linear tasks, thereby enabling\nmore reliable and actionable insights.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684XAI\u6280\u672fPatternLocal\uff0c\u5b83\u901a\u8fc7\u8f6c\u6362\u5c40\u90e8\u7ebf\u6027\u4ee3\u7406\u7684\u5224\u522b\u6a21\u578b\u6743\u91cd\u4e3a\u751f\u6210\u8868\u793a\u6765\u6291\u5236\u6291\u5236\u53d8\u91cf\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u63d0\u9ad8\u975e\u7ebf\u6027\u4efb\u52a1\u89e3\u91ca\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u6291\u5236\u53d8\u91cf\u53ef\u80fd\u5f15\u8d77\u865a\u5047\u7684\u7279\u5f81\u5f52\u56e0\uff0c\u635f\u5bb3\u89e3\u91ca\u7684\u6548\u7528\uff0c\u73b0\u6709\u7ebf\u6027\u6a21\u578b\u7684\u6709\u6548\u8865\u6551\u63aa\u65bd\u96be\u4ee5\u6269\u5c55\u5230\u975e\u7ebf\u6027\u6a21\u578b\u548c\u5b9e\u4f8b\u7ea7\u522b\u7684\u89e3\u91ca\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPatternLocal\u7684\u65b0XAI\u6280\u672f\uff0c\u5b83\u4ece\u5c40\u90e8\u7ebf\u6027\u4ee3\u7406\u5f00\u59cb\u5e76\u8f6c\u6362\u5224\u522b\u6a21\u578b\u6743\u91cd\u4e3a\u751f\u6210\u8868\u793a\uff0c\u4ece\u800c\u6291\u5236\u6291\u5236\u53d8\u91cf\u7684\u5f71\u54cd\u3002", "result": "PatternLocal\u5728\u89e3\u91ca\u975e\u7ebf\u6027\u4efb\u52a1\u65f6\u51cf\u5c11\u4e86\u9519\u8bef\u7684\u7279\u5f81\u5f52\u56e0\uff0c\u5e76\u5728XAI-TRIS\u57fa\u51c6\u4e0a\u4f18\u4e8e\u5176\u4ed6XAI\u65b9\u6cd5\u3002", "conclusion": "PatternLocal\u6280\u672f\u5728\u89e3\u91ca\u975e\u7ebf\u6027\u4efb\u52a1\u65f6\u51cf\u5c11\u4e86\u9519\u8bef\u7684\u7279\u5f81\u5f52\u56e0\uff0c\u5e76\u5728XAI-TRIS\u57fa\u51c6\u4e0a\u4f18\u4e8e\u5176\u4ed6XAI\u65b9\u6cd5\u3002"}}
{"id": "2505.10759", "pdf": "https://arxiv.org/pdf/2505.10759", "abs": "https://arxiv.org/abs/2505.10759", "authors": ["Achmad Ginanjar", "Xue Li", "Priyanka Singh", "Wen Hua"], "title": "Random Client Selection on Contrastive Federated Learning for Tabular Data", "categories": ["cs.LG", "cs.CR", "cs.DC"], "comment": null, "summary": "Vertical Federated Learning (VFL) has revolutionised collaborative machine\nlearning by enabling privacy-preserving model training across multiple parties.\nHowever, it remains vulnerable to information leakage during intermediate\ncomputation sharing. While Contrastive Federated Learning (CFL) was introduced\nto mitigate these privacy concerns through representation learning, it still\nfaces challenges from gradient-based attacks. This paper presents a\ncomprehensive experimental analysis of gradient-based attacks in CFL\nenvironments and evaluates random client selection as a defensive strategy.\nThrough extensive experimentation, we demonstrate that random client selection\nproves particularly effective in defending against gradient attacks in the CFL\nnetwork. Our findings provide valuable insights for implementing robust\nsecurity measures in contrastive federated learning systems, contributing to\nthe development of more secure collaborative learning frameworks", "AI": {"tldr": "This paper examines gradient-based attacks on Contrastive Federated Learning (CFL) and finds that randomly selecting clients is an effective defense.", "motivation": "To enhance privacy and security in vertical federated learning by addressing information leakage and gradient-based attacks.", "method": "Conducts comprehensive experiments on gradient-based attacks in CFL environments and evaluates random client selection as a defense mechanism.", "result": "Random client selection proves effective in defending against gradient attacks in the CFL network.", "conclusion": "Provides insights into implementing robust security measures in contrastive federated learning systems."}}
{"id": "2505.10988", "pdf": "https://arxiv.org/pdf/2505.10988", "abs": "https://arxiv.org/abs/2505.10988", "authors": ["Joon-Young Kim", "Jecheon Yu", "Heekyu Kim", "Seunghwa Ryu"], "title": "DRL-Based Injection Molding Process Parameter Optimization for Adaptive and Profitable Production", "categories": ["cs.AI", "cs.SY", "eess.SY"], "comment": "50 pages, 10 figures", "summary": "Plastic injection molding remains essential to modern manufacturing. However,\noptimizing process parameters to balance product quality and profitability\nunder dynamic environmental and economic conditions remains a persistent\nchallenge. This study presents a novel deep reinforcement learning (DRL)-based\nframework for real-time process optimization in injection molding, integrating\nproduct quality and profitability into the control objective. A profit function\nwas developed to reflect real-world manufacturing costs, incorporating resin,\nmold wear, and electricity prices, including time-of-use variations. Surrogate\nmodels were constructed to predict product quality and cycle time, enabling\nefficient offline training of DRL agents using soft actor-critic (SAC) and\nproximal policy optimization (PPO) algorithms. Experimental results demonstrate\nthat the proposed DRL framework can dynamically adapt to seasonal and\noperational variations, consistently maintaining product quality while\nmaximizing profit. Compared to traditional optimization methods such as genetic\nalgorithms, the DRL models achieved comparable economic performance with up to\n135x faster inference speeds, making them well-suited for real-time\napplications. The framework's scalability and adaptability highlight its\npotential as a foundation for intelligent, data-driven decision-making in\nmodern manufacturing environments.", "AI": {"tldr": "This study introduces a DRL-based framework for optimizing plastic injection molding processes in real-time by balancing product quality and profitability.", "motivation": "Optimizing process parameters under varying environmental and economic conditions is challenging.", "method": "A novel DRL framework integrating product quality and profitability into the control objective, using surrogate models and SAC/PPO algorithms.", "result": "The framework adapts to seasonal and operational changes, maintains product quality, and maximizes profit faster than traditional methods.", "conclusion": "The DRL framework shows potential for intelligent, data-driven decision-making in modern manufacturing."}}
{"id": "2505.11211", "pdf": "https://arxiv.org/pdf/2505.11211", "abs": "https://arxiv.org/abs/2505.11211", "authors": ["Francisco Madaleno", "Pernille Julie Viuff Sand", "Francisco C. Pereira", "Sergio Hernan Garrido Mejia"], "title": "Bayesian Hierarchical Invariant Prediction", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "comment": null, "summary": "We propose Bayesian Hierarchical Invariant Prediction (BHIP) reframing\nInvariant Causal Prediction (ICP) through the lens of Hierarchical Bayes. We\nleverage the hierarchical structure to explicitly test invariance of causal\nmechanisms under heterogeneous data, resulting in improved computational\nscalability for a larger number of predictors compared to ICP. Moreover, given\nits Bayesian nature BHIP enables the use of prior information. In this paper,\nwe test two sparsity inducing priors: horseshoe and spike-and-slab, both of\nwhich allow us a more reliable identification of causal features. We test BHIP\nin synthetic and real-world data showing its potential as an alternative\ninference method to ICP.", "AI": {"tldr": "This paper introduces Bayesian Hierarchical Invariant Prediction (BHIP), which improves computational scalability for a larger number of predictors compared to Invariant Causal Prediction (ICP). It also allows the use of prior information and tests two sparsity inducing priors.", "motivation": "To improve computational scalability and enable the use of prior information for causal feature identification.", "method": "Bayesian Hierarchical Invariant Prediction (BHIP) that leverages hierarchical structure to test invariance of causal mechanisms under heterogeneous data.", "result": "Improved computational scalability for a larger number of predictors and reliable identification of causal features using sparsity inducing priors.", "conclusion": "BHIP is a promising alternative inference method to ICP."}}
{"id": "2505.10762", "pdf": "https://arxiv.org/pdf/2505.10762", "abs": "https://arxiv.org/abs/2505.10762", "authors": ["Conor F. Hayes", "Felipe Leno Da Silva", "Jiachen Yang", "T. Nathan Mundhenk", "Chak Shing Lee", "Jacob F. Pettit", "Claudio Santiago", "Sookyung Kim", "Joanne T. Kim", "Ignacio Aravena Solis", "Ruben Glatt", "Andre R. Goncalves", "Alexander Ladd", "Ahmet Can Solak", "Thomas Desautels", "Daniel Faissol", "Brenden K. Petersen", "Mikel Landajuela"], "title": "Deep Symbolic Optimization: Reinforcement Learning for Symbolic Mathematics", "categories": ["cs.LG", "cs.NE", "cs.SC"], "comment": "Under review in LNCS Computational Approaches to Scientific Discovery", "summary": "Deep Symbolic Optimization (DSO) is a novel computational framework that\nenables symbolic optimization for scientific discovery, particularly in\napplications involving the search for intricate symbolic structures. One\nnotable example is equation discovery, which aims to automatically derive\nmathematical models expressed in symbolic form. In DSO, the discovery process\nis formulated as a sequential decision-making task. A generative neural network\nlearns a probabilistic model over a vast space of candidate symbolic\nexpressions, while reinforcement learning strategies guide the search toward\nthe most promising regions. This approach integrates gradient-based\noptimization with evolutionary and local search techniques, and it incorporates\nin-situ constraints, domain-specific priors, and advanced policy optimization\nmethods. The result is a robust framework capable of efficiently exploring\nextensive search spaces to identify interpretable and physically meaningful\nmodels. Extensive evaluations on benchmark problems have demonstrated that DSO\nachieves state-of-the-art performance in both accuracy and interpretability. In\nthis chapter, we provide a comprehensive overview of the DSO framework and\nillustrate its transformative potential for automating symbolic optimization in\nscientific discovery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6df1\u5ea6\u7b26\u53f7\u4f18\u5316\uff08DSO\uff09\u7684\u65b0\u8ba1\u7b97\u6846\u67b6\uff0c\u7528\u4e8e\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u7b26\u53f7\u4f18\u5316\uff0c\u5c24\u5176\u662f\u5728\u641c\u7d22\u590d\u6742\u7684\u7b26\u53f7\u7ed3\u6784\u65f6\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u7b26\u53f7\u4f18\u5316\uff0c\u7279\u522b\u662f\u5728\u641c\u7d22\u590d\u6742\u7684\u7b26\u53f7\u7ed3\u6784\u7684\u5e94\u7528\u4e2d\u3002", "method": "\u901a\u8fc7\u5c06\u53d1\u73b0\u8fc7\u7a0b\u8868\u8ff0\u4e3a\u987a\u5e8f\u51b3\u7b56\u4efb\u52a1\uff0c\u5229\u7528\u751f\u6210\u795e\u7ecf\u7f51\u7edc\u548c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u6765\u6307\u5bfc\u641c\u7d22\uff0c\u7ed3\u5408\u68af\u5ea6\u4f18\u5316\u3001\u8fdb\u5316\u548c\u5c40\u90e8\u641c\u7d22\u6280\u672f\uff0c\u5e76\u5f15\u5165\u73b0\u573a\u7ea6\u675f\u3001\u9886\u57df\u7279\u5b9a\u5148\u9a8c\u548c\u9ad8\u7ea7\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u3002", "result": "DSO\u5728\u57fa\u51c6\u95ee\u9898\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u8bc4\u4f30\uff0c\u663e\u793a\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DSO\u6846\u67b6\u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5c55\u793a\u4e86\u5176\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u81ea\u52a8\u5316\u7b26\u53f7\u4f18\u5316\u7684\u53d8\u9769\u6f5c\u529b\u3002"}}
{"id": "2505.10989", "pdf": "https://arxiv.org/pdf/2505.10989", "abs": "https://arxiv.org/abs/2505.10989", "authors": ["Haiyang Shen", "Hang Yan", "Zhongshi Xing", "Mugeng Liu", "Yue Li", "Zhiyang Chen", "Yuxiang Wang", "Jiuzheng Wang", "Yun Ma"], "title": "RAGSynth: Synthetic Data for Robust and Faithful RAG Component Optimization", "categories": ["cs.AI"], "comment": null, "summary": "RAG can enhance the performance of LLMs on knowledge-intensive tasks. Various\nRAG paradigms, including vanilla, planning-based, and iterative RAG, are built\nupon 2 cores: the retriever, which should robustly select relevant documents\nacross complex queries, and the generator, which should faithfully synthesize\nresponses. However, existing retrievers rely heavily on public knowledge and\nstruggle with queries of varying logical complexity and clue completeness,\nwhile generators frequently face fidelity problems. In this work, we introduce\nRAGSynth, a framework that includes a data construction modeling and a\ncorresponding synthetic data generation implementation, designed to optimize\nretriever robustness and generator fidelity. Additionally, we present\nSynthBench, a benchmark encompassing 8 domain-specific documents across 4\ndomains, featuring diverse query complexities, clue completeness, and\nfine-grained citation granularity. Leveraging RAGSynth, we generate a\nlarge-scale synthetic dataset, including single and multi-hop. Extensive\nexperiments demonstrate that the synthetic data significantly improves the\nrobustness of the retrievers and the fidelity of the generators. Additional\nevaluations confirm that RAGSynth can also generalize well across different\ndomains. By integrating the optimized retrievers into various RAG paradigms, we\nconsistently observe enhanced RAG system performance. We have open-sourced the\nimplementation on https://github.com/EachSheep/RAGSynth.", "AI": {"tldr": "RAGSynth\u662f\u4e00\u79cd\u4f18\u5316\u68c0\u7d22\u5668\u9c81\u68d2\u6027\u548c\u751f\u6210\u5668\u5fe0\u5b9e\u6027\u7684\u6846\u67b6\uff0c\u5305\u542b\u6570\u636e\u6784\u9020\u5efa\u6a21\u548c\u5408\u6210\u6570\u636e\u751f\u6210\u5b9e\u73b0\u3002\u5b83\u8fd8\u63d0\u51fa\u4e86SynthBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u751f\u6210\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\uff0c\u63d0\u5347RAG\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u5668\u4f9d\u8d56\u516c\u5171\u77e5\u8bc6\u4e14\u96be\u4ee5\u5904\u7406\u903b\u8f91\u590d\u6742\u5ea6\u548c\u7ebf\u7d22\u5b8c\u6574\u6027\u7684\u67e5\u8be2\uff0c\u751f\u6210\u5668\u7ecf\u5e38\u9762\u4e34\u5fe0\u5b9e\u6027\u95ee\u9898\u3002", "method": "\u5f15\u5165RAGSynth\u6846\u67b6\uff0c\u5305\u62ec\u6570\u636e\u6784\u9020\u5efa\u6a21\u548c\u5408\u6210\u6570\u636e\u751f\u6210\u5b9e\u73b0\uff0c\u4ee5\u53caSynthBench\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5408\u6210\u6570\u636e\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u7d22\u5668\u7684\u9c81\u68d2\u6027\u548c\u751f\u6210\u5668\u7684\u5fe0\u5b9e\u6027\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u9886\u57df\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u96c6\u6210\u4f18\u5316\u540e\u7684\u68c0\u7d22\u5668\u5230\u5404\u79cdRAG\u8303\u5f0f\u4e2d\uff0c\u89c2\u5bdf\u5230RAG\u7cfb\u7edf\u6027\u80fd\u7684\u4e00\u81f4\u589e\u5f3a\uff0cRAGSynth\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.11444", "pdf": "https://arxiv.org/pdf/2505.11444", "abs": "https://arxiv.org/abs/2505.11444", "authors": ["Xinran Song", "Tianyu Chen", "Mingyuan Zhou"], "title": "A Generative Framework for Causal Estimation via Importance-Weighted Diffusion Distillation", "categories": ["cs.LG", "stat.AP", "stat.ME", "stat.ML"], "comment": null, "summary": "Estimating individualized treatment effects from observational data is a\ncentral challenge in causal inference, largely due to covariate imbalance and\nconfounding bias from non-randomized treatment assignment. While inverse\nprobability weighting (IPW) is a well-established solution to this problem, its\nintegration into modern deep learning frameworks remains limited. In this work,\nwe propose Importance-Weighted Diffusion Distillation (IWDD), a novel\ngenerative framework that combines the pretraining of diffusion models with\nimportance-weighted score distillation to enable accurate and fast causal\nestimation-including potential outcome prediction and treatment effect\nestimation. We demonstrate how IPW can be naturally incorporated into the\ndistillation of pretrained diffusion models, and further introduce a\nrandomization-based adjustment that eliminates the need to compute IPW\nexplicitly-thereby simplifying computation and, more importantly, provably\nreducing the variance of gradient estimates. Empirical results show that IWDD\nachieves state-of-the-art out-of-sample prediction performance, with the\nhighest win rates compared to other baselines, significantly improving causal\nestimation and supporting the development of individualized treatment\nstrategies. We will release our PyTorch code for reproducibility and future\nresearch.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6269\u6563\u6a21\u578b\u9884\u8bad\u7ec3\u548c\u91cd\u8981\u6027\u52a0\u6743\u5f97\u5206\u84b8\u998f\u7684\u65b0\u578b\u751f\u6210\u6846\u67b6IWDD\uff0c\u7528\u4e8e\u51c6\u786e\u5feb\u901f\u7684\u56e0\u679c\u4f30\u8ba1\u3002", "motivation": "\u89e3\u51b3\u4ece\u89c2\u6d4b\u6570\u636e\u4f30\u8ba1\u4e2a\u4f53\u5316\u6cbb\u7597\u6548\u679c\u7684\u6311\u6218\uff0c\u4e3b\u8981\u7531\u4e8e\u534f\u53d8\u91cf\u4e0d\u5e73\u8861\u548c\u975e\u968f\u673a\u6cbb\u7597\u5206\u914d\u5bfc\u81f4\u7684\u6df7\u6742\u504f\u501a\u3002", "method": "\u5f15\u5165\u4e86\u91cd\u8981\u6027\u52a0\u6743\u6269\u6563\u84b8\u998f(IWDD)\uff0c\u7ed3\u5408IPW\u548c\u6269\u6563\u6a21\u578b\u84b8\u998f\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u968f\u673a\u5316\u7684\u8c03\u6574\u51cf\u5c11\u65b9\u5dee\u3002", "result": "IWDD\u5728\u6837\u672c\u5916\u9884\u6d4b\u6027\u80fd\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6c34\u5e73\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u56e0\u679c\u4f30\u8ba1\u5e76\u652f\u6301\u4e2a\u6027\u5316\u6cbb\u7597\u7b56\u7565\u7684\u53d1\u5c55\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86IPW\u5728\u6269\u6563\u6a21\u578b\u84b8\u998f\u4e2d\u7684\u81ea\u7136\u96c6\u6210\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u65b9\u6cd5\u6765\u7b80\u5316\u8ba1\u7b97\u548c\u51cf\u5c11\u68af\u5ea6\u4f30\u8ba1\u7684\u65b9\u5dee\u3002"}}
{"id": "2505.10774", "pdf": "https://arxiv.org/pdf/2505.10774", "abs": "https://arxiv.org/abs/2505.10774", "authors": ["Yueyang Yao", "Jiajun Li", "Xingyuan Dai", "MengMeng Zhang", "Xiaoyan Gong", "Fei-Yue Wang", "Yisheng Lv"], "title": "Context-Aware Probabilistic Modeling with LLM for Multimodal Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages, 2 figures", "summary": "Time series forecasting is important for applications spanning energy\nmarkets, climate analysis, and traffic management. However, existing methods\nstruggle to effectively integrate exogenous texts and align them with the\nprobabilistic nature of large language models (LLMs). Current approaches either\nemploy shallow text-time series fusion via basic prompts or rely on\ndeterministic numerical decoding that conflict with LLMs' token-generation\nparadigm, which limits contextual awareness and distribution modeling. To\naddress these limitations, we propose CAPTime, a context-aware probabilistic\nmultimodal time series forecasting method that leverages text-informed\nabstraction and autoregressive LLM decoding. Our method first encodes temporal\npatterns using a pretrained time series encoder, then aligns them with textual\ncontexts via learnable interactions to produce joint multimodal\nrepresentations. By combining a mixture of distribution experts with frozen\nLLMs, we enable context-aware probabilistic forecasting while preserving LLMs'\ninherent distribution modeling capabilities. Experiments on diverse time series\nforecasting tasks demonstrate the superior accuracy and generalization of\nCAPTime, particularly in multimodal scenarios. Additional analysis highlights\nits robustness in data-scarce scenarios through hybrid probabilistic decoding.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCAPTime\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6982\u7387\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u6587\u672c\u5f15\u5bfc\u7684\u62bd\u8c61\u548c\u81ea\u56de\u5f52LLM\u89e3\u7801\u3002\u5b9e\u9a8c\u8868\u660eCAPTime\u5728\u5404\u79cd\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6574\u5408\u5916\u90e8\u6587\u672c\u5e76\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6982\u7387\u7279\u6027\u5bf9\u9f50\u3002", "method": "\u9996\u5148\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u65f6\u95f4\u5e8f\u5217\u7f16\u7801\u5668\u5bf9\u65f6\u95f4\u6a21\u5f0f\u8fdb\u884c\u7f16\u7801\uff0c\u7136\u540e\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u4ea4\u4e92\u65b9\u5f0f\u4e0e\u6587\u672c\u4e0a\u4e0b\u6587\u5bf9\u9f50\uff0c\u751f\u6210\u8054\u5408\u591a\u6a21\u6001\u8868\u793a\u3002\u7ed3\u5408\u5206\u5e03\u4e13\u5bb6\u6df7\u5408\u548c\u51bb\u7ed3\u7684LLMs\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6982\u7387\u9884\u6d4b\u3002", "result": "\u5728\u4e0d\u540c\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCAPTime\u5728\u51c6\u786e\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "CAPTime\u662f\u4e00\u79cd\u521b\u65b0\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u5c06\u6587\u672c\u4fe1\u606f\u6574\u5408\u5230\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\uff0c\u540c\u65f6\u4fdd\u6301LLMs\u7684\u5206\u5e03\u5efa\u6a21\u80fd\u529b\u3002"}}
{"id": "2505.10991", "pdf": "https://arxiv.org/pdf/2505.10991", "abs": "https://arxiv.org/abs/2505.10991", "authors": ["Yacine Izza", "Alexey Ignatiev", "Sasha Rubin", "Joao Marques-Silva", "Peter J. Stuckey"], "title": "Most General Explanations of Tree Ensembles", "categories": ["cs.AI", "cs.LG", "cs.LO"], "comment": null, "summary": "Explainable Artificial Intelligence (XAI) is critical for attaining trust in\nthe operation of AI systems. A key question of an AI system is ``why was this\ndecision made this way''. Formal approaches to XAI use a formal model of the AI\nsystem to identify abductive explanations. While abductive explanations may be\napplicable to a large number of inputs sharing the same concrete values, more\ngeneral explanations may be preferred for numeric inputs. So-called inflated\nabductive explanations give intervals for each feature ensuring that any input\nwhose values fall withing these intervals is still guaranteed to make the same\nprediction. Inflated explanations cover a larger portion of the input space,\nand hence are deemed more general explanations. But there can be many\n(inflated) abductive explanations for an instance. Which is the best? In this\npaper, we show how to find a most general abductive explanation for an AI\ndecision. This explanation covers as much of the input space as possible, while\nstill being a correct formal explanation of the model's behaviour. Given that\nwe only want to give a human one explanation for a decision, the most general\nexplanation gives us the explanation with the broadest applicability, and hence\nthe one most likely to seem sensible. (The paper has been accepted at IJCAI2025\nconference.)", "AI": {"tldr": "This paper discusses finding the most general abductive explanation for AI decisions in explainable AI systems.", "motivation": "To address the need for trust in AI systems by providing understandable reasons for their decisions.", "method": "Using formal models of AI systems to identify abductive explanations, focusing on those that are more general and applicable across a broader range of inputs.", "result": "A method to determine the most general abductive explanation which applies to the largest portion of the input space without losing correctness.", "conclusion": "The most general explanation is the best choice for conveying why an AI made a particular decision, as it has the widest applicability and is most likely to be perceived as reasonable by humans."}}
{"id": "2505.10799", "pdf": "https://arxiv.org/pdf/2505.10799", "abs": "https://arxiv.org/abs/2505.10799", "authors": ["Tao Bai", "Junzhuo Zhou", "Zeyuan Deng", "Peng Cao"], "title": "Cell Library Characterization for Composite Current Source Models Based on Gaussian Process Regression and Active Learning", "categories": ["cs.LG", "cs.AR"], "comment": null, "summary": "The composite current source (CCS) model has been adopted as an advanced\ntiming model that represents the current behavior of cells for improved\naccuracy and better capability than traditional non-linear delay models (NLDM)\nto model complex dynamic effects and interactions under advanced process nodes.\nHowever, the high accuracy requirement, large amount of data and extensive\nsimulation cost pose severe challenges to CCS characterization. To address\nthese challenges, we introduce a novel Gaussian Process Regression(GPR) model\nwith active learning(AL) to establish the characterization framework\nefficiently and accurately. Our approach significantly outperforms conventional\ncommercial tools as well as learning based approaches by achieving an average\nabsolute error of 2.05 ps and a relative error of 2.27% for current waveform of\n57 cells under 9 process, voltage, temperature (PVT) corners with TSMC 22nm\nprocess. Additionally, our model drastically reduces the runtime to 27% and the\nstorage by up to 19.5x compared with that required by commercial tools.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u548c\u4e3b\u52a8\u5b66\u4e60\u7684\u65b0\u578b\u590d\u5408\u7535\u6d41\u6e90\u6a21\u578b\u8868\u5f81\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8868\u5f81\u7cbe\u5ea6\u5e76\u5927\u5e45\u51cf\u5c11\u4e86\u8fd0\u884c\u65f6\u95f4\u548c\u5b58\u50a8\u9700\u6c42\u3002", "motivation": "\u4f20\u7edf\u975e\u7ebf\u6027\u5ef6\u8fdf\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u5efa\u6a21\u590d\u6742\u52a8\u6001\u6548\u5e94\uff0c\u800c\u590d\u5408\u7535\u6d41\u6e90\u6a21\u578b\u867d\u7136\u66f4\u51c6\u786e\u4f46\u9762\u4e34\u6570\u636e\u91cf\u5927\u3001\u4eff\u771f\u6210\u672c\u9ad8\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e86\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u7684\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u6765\u9ad8\u6548\u4e14\u7cbe\u786e\u5730\u6784\u5efa\u590d\u5408\u7535\u6d41\u6e90\u6a21\u578b\u7684\u8868\u5f81\u6846\u67b6\u3002", "result": "\u8be5\u65b9\u6cd5\u5728TSMC 22nm\u5de5\u827a\u4e0b\u5bf957\u4e2a\u5355\u5143\u57289\u4e2aPVT\u89d2\u4e0a\u7684\u7535\u6d41\u6ce2\u5f62\u8868\u5f81\u8fbe\u5230\u4e86\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee2.05ps\uff0c\u76f8\u5bf9\u8bef\u5dee2.27%\uff0c\u5e76\u4e14\u8fd0\u884c\u65f6\u95f4\u51cf\u5c11\u5230\u539f\u6765\u768427%\uff0c\u5b58\u50a8\u9700\u6c42\u51cf\u5c11\u5230\u5546\u4e1a\u5de5\u5177\u76841/19.5\u3002", "conclusion": "\u63d0\u51fa\u7684GPR\u7ed3\u5408AL\u7684\u65b9\u6cd5\u5728\u590d\u5408\u7535\u6d41\u6e90\u6a21\u578b\u8868\u5f81\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u9ad8\u7cbe\u5ea6\u8981\u6c42\u5e26\u6765\u7684\u6311\u6218\u3002"}}
{"id": "2505.11049", "pdf": "https://arxiv.org/pdf/2505.11049", "abs": "https://arxiv.org/abs/2505.11049", "authors": ["Yue Liu", "Shengfang Zhai", "Mingzhe Du", "Yulin Chen", "Tri Cao", "Hongcheng Gao", "Cheng Wang", "Xinfeng Li", "Kun Wang", "Junfeng Fang", "Jiaheng Zhang", "Bryan Hooi"], "title": "GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "To enhance the safety of VLMs, this paper introduces a novel reasoning-based\nVLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the\nguard model to deliberatively reason before making moderation decisions via\nonline RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with\n123K samples and 631K reasoning steps, spanning text, image, and text-image\ninputs. Then, based on it, we cold-start our model's reasoning ability via SFT.\nIn addition, we further enhance reasoning regarding moderation through online\nRL. Concretely, to enhance diversity and difficulty of samples, we conduct\nrejection sampling followed by data augmentation via the proposed safety-aware\ndata concatenation. Besides, we use a dynamic clipping parameter to encourage\nexploration in early stages and exploitation in later stages. To balance\nperformance and token efficiency, we design a length-aware safety reward that\nintegrates accuracy, format, and token cost. Extensive experiments demonstrate\nthe superiority of our model. Remarkably, it surpasses the runner-up by 19.27%\nF1 score on average. We release data, code, and models (3B/7B) of\nGuardReasoner-VL at https://github.com/yueliu1999/GuardReasoner-VL/", "AI": {"tldr": "This paper presents GuardReasoner-VL, a novel reasoning-based VLM guard model that improves safety using online RL, SFT, and a custom-designed reward function.", "motivation": "Enhance the safety of VLMs.", "method": "Introduces GuardReasoner-VL, constructs a reasoning corpus, uses SFT for initial training, and applies online RL with rejection sampling, data augmentation, and a dynamic clipping parameter.", "result": "GuardReasoner-VL outperforms other models by 19.27% F1 score on average.", "conclusion": "Releases data, code, and models for GuardReasoner-VL."}}
{"id": "2505.10802", "pdf": "https://arxiv.org/pdf/2505.10802", "abs": "https://arxiv.org/abs/2505.10802", "authors": ["Ian Holmes", "Min Chi"], "title": "Attention-Based Reward Shaping for Sparse and Delayed Rewards", "categories": ["cs.LG", "cs.AI"], "comment": "22 pages, 17 tables, 2 figures. Code available online at\n  https://github.com/ihholmes-p/ARES", "summary": "Sparse and delayed reward functions pose a significant obstacle for\nreal-world Reinforcement Learning (RL) applications. In this work, we propose\nAttention-based REward Shaping (ARES), a general and robust algorithm which\nuses a transformer's attention mechanism to generate shaped rewards and create\na dense reward function for any environment. ARES requires a set of episodes\nand their final returns as input. It can be trained entirely offline and is\nable to generate meaningful shaped rewards even when using small datasets or\nepisodes produced by agents taking random actions. ARES is compatible with any\nRL algorithm and can handle any level of reward sparsity. In our experiments,\nwe focus on the most challenging case where rewards are fully delayed until the\nend of each episode. We evaluate ARES across a diverse range of environments,\nwidely used RL algorithms, and baseline methods to assess the effectiveness of\nthe shaped rewards it produces. Our results show that ARES can significantly\nimprove learning in delayed reward settings, enabling RL agents to train in\nscenarios that would otherwise require impractical amounts of data or even be\nunlearnable. To our knowledge, ARES is the first approach that works fully\noffline, remains robust to extreme reward delays and low-quality data, and is\nnot limited to goal-based tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aARES\u7684\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5229\u7528Transformer\u7684\u6ce8\u610f\u529b\u673a\u5236\u751f\u6210\u5f62\u72b6\u5956\u52b1\u5e76\u521b\u5efa\u5bc6\u96c6\u7684\u5956\u52b1\u51fd\u6570\u3002ARES\u53ef\u4ee5\u5728\u5b8c\u5168\u79bb\u7ebf\u7684\u60c5\u51b5\u4e0b\u8bad\u7ec3\uff0c\u5e76\u4e14\u5373\u4f7f\u5728\u4f7f\u7528\u5c0f\u6570\u636e\u96c6\u6216\u7531\u968f\u673a\u52a8\u4f5c\u4ee3\u7406\u4ea7\u751f\u7684\u573a\u666f\u4e0b\u4e5f\u80fd\u4ea7\u751f\u6709\u610f\u4e49\u7684\u5f62\u72b6\u5956\u52b1\u3002\u5b9e\u9a8c\u8868\u660eARES\u53ef\u4ee5\u5728\u5ef6\u8fdf\u5956\u52b1\u8bbe\u7f6e\u4e2d\u663e\u8457\u63d0\u9ad8\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u7a00\u758f\u548c\u5ef6\u8fdf\u7684\u5956\u52b1\u51fd\u6570\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u6784\u6210\u4e86\u91cd\u5927\u969c\u788d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5ARES\uff0c\u5b83\u4f7f\u7528Transformer\u7684\u6ce8\u610f\u529b\u673a\u5236\u6765\u751f\u6210\u5f62\u72b6\u5956\u52b1\u5e76\u521b\u5efa\u5bc6\u96c6\u7684\u5956\u52b1\u51fd\u6570\u3002ARES\u53ef\u4ee5\u5b8c\u5168\u79bb\u7ebf\u8bad\u7ec3\uff0c\u5e76\u4e14\u80fd\u591f\u5904\u7406\u4efb\u4f55\u7ea7\u522b\u7684\u5956\u52b1\u7a00\u758f\u6027\u3002", "result": "ARES\u80fd\u591f\u5728\u5ef6\u8fdf\u5956\u52b1\u8bbe\u7f6e\u4e2d\u663e\u8457\u63d0\u9ad8\u5b66\u4e60\u6548\u679c\uff0c\u4f7fRL\u4ee3\u7406\u80fd\u591f\u5728\u9700\u8981\u4e0d\u5207\u5b9e\u9645\u7684\u6570\u636e\u91cf\u751a\u81f3\u65e0\u6cd5\u5b66\u4e60\u7684\u573a\u666f\u4e2d\u8fdb\u884c\u8bad\u7ec3\u3002", "conclusion": "ARES\u662f\u7b2c\u4e00\u4e2a\u5b8c\u5168\u79bb\u7ebf\u5de5\u4f5c\u3001\u5bf9\u6781\u7aef\u5956\u52b1\u5ef6\u8fdf\u548c\u4f4e\u8d28\u91cf\u6570\u636e\u4fdd\u6301\u9c81\u68d2\u5e76\u4e14\u4e0d\u9650\u4e8e\u57fa\u4e8e\u76ee\u6807\u7684\u4efb\u52a1\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.11063", "pdf": "https://arxiv.org/pdf/2505.11063", "abs": "https://arxiv.org/abs/2505.11063", "authors": ["Changyue Jiang", "Xudong Pan", "Min Yang"], "title": "Think Twice Before You Act: Enhancing Agent Behavioral Safety with Thought Correction", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "LLM-based autonomous agents possess capabilities such as reasoning, tool\ninvocation, and environment interaction, enabling the execution of complex\nmulti-step tasks. The internal reasoning process, i.e., thought, of behavioral\ntrajectory significantly influences tool usage and subsequent actions but can\nintroduce potential risks. Even minor deviations in the agent's thought may\ntrigger cascading effects leading to irreversible safety incidents. To address\nthe safety alignment challenges in long-horizon behavioral trajectories, we\npropose Thought-Aligner, a plug-in dynamic thought correction module. Utilizing\na lightweight and resource-efficient model, Thought-Aligner corrects each\nhigh-risk thought on the fly before each action execution. The corrected\nthought is then reintroduced to the agent, ensuring safer subsequent decisions\nand tool interactions. Importantly, Thought-Aligner modifies only the reasoning\nphase without altering the underlying agent framework, making it easy to deploy\nand widely applicable to various agent frameworks. To train the Thought-Aligner\nmodel, we construct an instruction dataset across ten representative scenarios\nand simulate ReAct execution trajectories, generating 5,000 diverse\ninstructions and more than 11,400 safe and unsafe thought pairs. The model is\nfine-tuned using contrastive learning techniques. Experiments across three\nagent safety benchmarks involving 12 different LLMs demonstrate that\nThought-Aligner raises agent behavioral safety from approximately 50% in the\nunprotected setting to 90% on average. Additionally, Thought-Aligner maintains\nresponse latency below 100ms with minimal resource usage, demonstrating its\ncapability for efficient deployment, broad applicability, and timely\nresponsiveness. This method thus provides a practical dynamic safety solution\nfor the LLM-based agents.", "AI": {"tldr": "\u63d0\u51faThought-Aligner\uff0c\u4e00\u79cd\u63d2\u4ef6\u5f0f\u52a8\u6001\u601d\u60f3\u4fee\u6b63\u6a21\u5757\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u6bcf\u4e2a\u884c\u52a8\u524d\u4fee\u6b63\u9ad8\u98ce\u9669\u601d\u60f3\uff0c\u5c06\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u884c\u4e3a\u5b89\u5168\u6027\u4ece\u7ea650%\u63d0\u9ad8\u5230\u5e73\u574790%\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u6027\u548c\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u5728\u957f\u65f6\u5e8f\u884c\u4e3a\u8f68\u8ff9\u4e2d\u7684\u5b89\u5168\u5bf9\u9f50\u6311\u6218\uff0c\u56e0\u4e3a\u667a\u80fd\u4f53\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5c0f\u504f\u5dee\u53ef\u80fd\u5f15\u53d1\u4e0d\u53ef\u9006\u7684\u5b89\u5168\u4e8b\u6545\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aThought-Aligner\u7684\u52a8\u6001\u601d\u60f3\u4fee\u6b63\u6a21\u5757\uff0c\u7528\u4e8e\u5728\u6bcf\u4e2a\u884c\u52a8\u6267\u884c\u524d\u4fee\u6b63\u9ad8\u98ce\u9669\u601d\u60f3\uff0c\u5e76\u91cd\u65b0\u5f15\u5165\u4fee\u6b63\u540e\u7684\u601d\u60f3\u4ee5\u786e\u4fdd\u66f4\u5b89\u5168\u7684\u540e\u7eed\u51b3\u7b56\u548c\u5de5\u5177\u4ea4\u4e92\u3002\u8be5\u6a21\u5757\u4ec5\u4fee\u6539\u63a8\u7406\u9636\u6bb5\uff0c\u4e0d\u6539\u53d8\u5e95\u5c42\u667a\u80fd\u4f53\u6846\u67b6\u3002", "result": "Thought-Aligner \u5728\u4e09\u4e2a\u6d89\u53ca12\u79cd\u4e0d\u540cLLMs\u7684\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u663e\u8457\u6548\u679c\uff0c\u884c\u4e3a\u5b89\u5168\u6027\u5927\u5e45\u63d0\u5347\uff0c\u5e76\u4e14\u5177\u6709\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u6548\u8d44\u6e90\u5229\u7528\u7684\u7279\u70b9\u3002", "conclusion": "Thought-Aligner \u63d0\u9ad8\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u7684\u884c\u4e3a\u5b89\u5168\u6027\uff0c\u4ece\u7ea650%\u63d0\u5347\u5230\u5e73\u574790%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u6548\u8d44\u6e90\u5229\u7528\u3002"}}
{"id": "2505.10822", "pdf": "https://arxiv.org/pdf/2505.10822", "abs": "https://arxiv.org/abs/2505.10822", "authors": ["Reilly Haskins", "Benjamin Adams"], "title": "Distilled Circuits: A Mechanistic Study of Internal Restructuring in Knowledge Distillation", "categories": ["cs.LG"], "comment": null, "summary": "Knowledge distillation compresses a larger neural model (teacher) into\nsmaller, faster student models by training them to match teacher outputs.\nHowever, the internal computational transformations that occur during this\nprocess remain poorly understood. We apply techniques from mechanistic\ninterpretability to analyze how internal circuits, representations, and\nactivation patterns differ between teacher and student. Focusing on GPT2-small\nand its distilled counterpart DistilGPT2, we find that student models\nreorganize, compress, and discard teacher components, often resulting in\nstronger reliance on fewer individual components. To quantify functional\nalignment beyond output similarity, we introduce an alignment metric based on\ninfluence-weighted component similarity, validated across multiple tasks. Our\nfindings reveal that while knowledge distillation preserves broad functional\nbehaviors, it also causes significant shifts in internal computation, with\nimportant implications for the robustness and generalization capacity of\ndistilled models.", "AI": {"tldr": "This paper uses mechanistic interpretability to analyze the differences between teacher and student models in knowledge distillation, revealing that students reorganize and compress teachers' components, leading to stronger reliance on fewer components.", "motivation": "To understand the internal computational transformations that occur during knowledge distillation process.", "method": "Applying mechanistic interpretability techniques to compare internal circuits, representations, and activation patterns between teacher and student models.", "result": "Student models reorganize, compress, and discard teacher components, often relying more on fewer individual components.", "conclusion": "Knowledge distillation preserves broad functional behaviors but causes significant shifts in internal computation, affecting the robustness and generalization capacity of distilled models."}}
{"id": "2505.11066", "pdf": "https://arxiv.org/pdf/2505.11066", "abs": "https://arxiv.org/abs/2505.11066", "authors": ["Rui Wang", "Shichun Yang", "Yuyi Chen", "Zhuoyang Li", "Zexiang Tong", "Jianyi Xu", "Jiayi Lu", "Xinjie Feng", "Yaoguang Cao"], "title": "A Multi-modal Fusion Network for Terrain Perception Based on Illumination Aware", "categories": ["cs.AI", "cs.MM"], "comment": null, "summary": "Road terrains play a crucial role in ensuring the driving safety of\nautonomous vehicles (AVs). However, existing sensors of AVs, including cameras\nand Lidars, are susceptible to variations in lighting and weather conditions,\nmaking it challenging to achieve real-time perception of road conditions. In\nthis paper, we propose an illumination-aware multi-modal fusion network (IMF),\nwhich leverages both exteroceptive and proprioceptive perception and optimizes\nthe fusion process based on illumination features. We introduce an\nillumination-perception sub-network to accurately estimate illumination\nfeatures. Moreover, we design a multi-modal fusion network which is able to\ndynamically adjust weights of different modalities according to illumination\nfeatures. We enhance the optimization process by pre-training of the\nillumination-perception sub-network and incorporating illumination loss as one\nof the training constraints. Extensive experiments demonstrate that the IMF\nshows a superior performance compared to state-of-the-art methods. The\ncomparison results with single modality perception methods highlight the\ncomprehensive advantages of multi-modal fusion in accurately perceiving road\nterrains under varying lighting conditions. Our dataset is available at:\nhttps://github.com/lindawang2016/IMF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7167\u660e\u611f\u77e5\u7684\u591a\u6a21\u6001\u878d\u5408\u7f51\u7edc(IMF)\uff0c\u5229\u7528\u5185\u5916\u611f\u77e5\u5e76\u4f18\u5316\u878d\u5408\u8fc7\u7a0b\u4ee5\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5728\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u9053\u8def\u72b6\u51b5\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4f20\u611f\u5668\u6613\u53d7\u5149\u7167\u548c\u5929\u6c14\u6761\u4ef6\u53d8\u5316\u7684\u5f71\u54cd\uff0c\u96be\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u7684\u9053\u8def\u72b6\u51b5\u611f\u77e5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7167\u660e\u611f\u77e5\u7684\u591a\u6a21\u6001\u878d\u5408\u7f51\u7edc\uff0c\u5305\u62ec\u4e00\u4e2a\u7528\u4e8e\u51c6\u786e\u4f30\u8ba1\u7167\u660e\u7279\u5f81\u7684\u5b50\u7f51\u7edc\u4ee5\u53ca\u80fd\u591f\u6839\u636e\u7167\u660e\u7279\u5f81\u52a8\u6001\u8c03\u6574\u4e0d\u540c\u6a21\u6001\u6743\u91cd\u7684\u591a\u6a21\u6001\u878d\u5408\u7f51\u7edc\uff0c\u5e76\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u5f15\u5165\u7167\u660e\u635f\u5931\u6765\u589e\u5f3a\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eIMF\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5355\u6a21\u6001\u611f\u77e5\u65b9\u6cd5\u4e0eIMF\u7684\u7ed3\u679c\u5bf9\u6bd4\u7a81\u51fa\u4e86\u591a\u6a21\u6001\u878d\u5408\u5728\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u7cbe\u786e\u611f\u77e5\u9053\u8def\u5730\u5f62\u7684\u4f18\u52bf\u3002", "conclusion": "IMF\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u51c6\u786e\u611f\u77e5\u9053\u8def\u5730\u5f62\u7684\u80fd\u529b\uff0c\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u4f9b\u7814\u7a76\u4f7f\u7528\u3002"}}
{"id": "2505.10833", "pdf": "https://arxiv.org/pdf/2505.10833", "abs": "https://arxiv.org/abs/2505.10833", "authors": ["Yifei He", "Siqi Zeng", "Yuzheng Hu", "Rui Yang", "Tong Zhang", "Han Zhao"], "title": "MergeBench: A Benchmark for Merging Domain-Specialized LLMs", "categories": ["cs.LG"], "comment": null, "summary": "Model merging provides a scalable alternative to multi-task training by\ncombining specialized finetuned models through parameter arithmetic, enabling\nefficient deployment without the need for joint training or access to all task\ndata. While recent methods have shown promise, existing evaluations are limited\nin both model scale and task diversity, leaving open questions about their\napplicability to large, domain-specialized LLMs. To tackle the challenges, we\nintroduce MergeBench, a comprehensive evaluation suite designed to assess model\nmerging at scale. MergeBench builds on state-of-the-art open-source language\nmodels, including Llama and Gemma families at 2B to 9B scales, and covers five\nkey domains: instruction following, mathematics, multilingual understanding,\ncoding and safety. We standardize finetuning and evaluation protocols, and\nassess eight representative merging methods across multi-task performance,\nforgetting and runtime efficiency. Based on extensive experiments, we provide\npractical guidelines for algorithm selection and share insights showing that\nmodel merging tends to perform better on stronger base models, with techniques\nsuch as merging coefficient tuning and sparsification improving knowledge\nretention. However, several challenges remain, including the computational cost\non large models, the gap for in-domain performance compared to multi-task\nmodels, and the underexplored role of model merging in standard LLM training\npipelines. We hope MergeBench provides a foundation for future research to\nadvance the understanding and practical application of model merging. We open\nsource our code at\n\\href{https://github.com/uiuctml/MergeBench}{https://github.com/uiuctml/MergeBench}.", "AI": {"tldr": "\u63d0\u51faMergeBench\u8bc4\u4f30\u5957\u4ef6\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u6a21\u578b\u5408\u5e76\u8bc4\u4f30\uff0c\u6db5\u76d6\u4e94\u5927\u9886\u57df\uff0c\u6807\u51c6\u5316\u5fae\u8c03\u4e0e\u8bc4\u4f30\u534f\u8bae\uff0c\u63d0\u4f9b\u5b9e\u7528\u7b97\u6cd5\u9009\u62e9\u6307\u5357\u5e76\u5206\u4eab\u89c1\u89e3\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u548c\u4efb\u52a1\u591a\u6837\u6027\u4e0a\u7684\u8bc4\u4f30\u6709\u9650\uff0c\u5b58\u5728\u9002\u7528\u6027\u7591\u95ee\uff0c\u7279\u522b\u662f\u5bf9\u5927\u578b\u9886\u57df\u4e13\u7528LLMs\u3002", "method": "\u6784\u5efaMergeBench\u8bc4\u4f30\u5957\u4ef6\uff0c\u57fa\u4e8eLlama\u548cGemma\u7cfb\u5217\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\uff0c\u6db5\u76d6\u4e94\u5927\u5173\u952e\u9886\u57df\uff0c\u5e76\u8bc4\u4f30\u516b\u79cd\u4ee3\u8868\u6027\u5408\u5e76\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u5408\u5e76\u5728\u66f4\u5f3a\u7684\u57fa\u7840\u6a21\u578b\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u8c03\u6574\u5408\u5e76\u7cfb\u6570\u548c\u7a00\u758f\u5316\u6280\u672f\u53ef\u6539\u5584\u77e5\u8bc6\u4fdd\u7559\uff0c\u4f46\u4ecd\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u57df\u5185\u6027\u80fd\u5dee\u8ddd\u53ca\u5408\u5e76\u89d2\u8272\u672a\u5145\u5206\u63a2\u7d22\u7b49\u95ee\u9898\u3002", "conclusion": "\u5e0c\u671bMergeBench\u80fd\u4e3a\u57fa\u7840\u7814\u7a76\u63d0\u4f9b\u652f\u6301\uff0c\u63a8\u52a8\u6a21\u578b\u5408\u5e76\u7684\u7406\u89e3\u4e0e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2505.11086", "pdf": "https://arxiv.org/pdf/2505.11086", "abs": "https://arxiv.org/abs/2505.11086", "authors": ["Keita Kinjo"], "title": "Analysis of Customer Journeys Using Prototype Detection and Counterfactual Explanations for Sequential Data", "categories": ["cs.AI", "cs.CY"], "comment": "19 pages, 7 figures", "summary": "Recently, the proliferation of omni-channel platforms has attracted interest\nin customer journeys, particularly regarding their role in developing marketing\nstrategies. However, few efforts have been taken to quantitatively study or\ncomprehensively analyze them owing to the sequential nature of their data and\nthe complexity involved in analysis. In this study, we propose a novel approach\ncomprising three steps for analyzing customer journeys. First, the distance\nbetween sequential data is defined and used to identify and visualize\nrepresentative sequences. Second, the likelihood of purchase is predicted based\non this distance. Third, if a sequence suggests no purchase, counterfactual\nsequences are recommended to increase the probability of a purchase using a\nproposed method, which extracts counterfactual explanations for sequential\ndata. A survey was conducted, and the data were analyzed; the results revealed\nthat typical sequences could be extracted, and the parts of those sequences\nimportant for purchase could be detected. We believe that the proposed approach\ncan support improvements in various marketing activities.", "AI": {"tldr": "This study proposes a novel approach with three steps for analyzing customer journeys on omni-channel platforms, including identifying representative sequences, predicting purchase likelihood, and recommending counterfactual sequences to enhance purchase probability.", "motivation": "To address the lack of quantitative studies and comprehensive analyses of customer journeys due to their sequential nature and complexity.", "method": "A three-step approach: defining distance for identifying and visualizing sequences, predicting purchase likelihood, and recommending counterfactual sequences to improve purchase probability.", "result": "The extraction of typical sequences and detection of important parts for purchase from the analyzed data.", "conclusion": "The proposed approach can support improvements in various marketing activities."}}
{"id": "2505.10838", "pdf": "https://arxiv.org/pdf/2505.10838", "abs": "https://arxiv.org/abs/2505.10838", "authors": ["Ran Li", "Hao Wang", "Chengzhi Mao"], "title": "LARGO: Latent Adversarial Reflection through Gradient Optimization for Jailbreaking LLMs", "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": null, "summary": "Efficient red-teaming method to uncover vulnerabilities in Large Language\nModels (LLMs) is crucial. While recent attacks often use LLMs as optimizers,\nthe discrete language space make gradient-based methods struggle. We introduce\nLARGO (Latent Adversarial Reflection through Gradient Optimization), a novel\nlatent self-reflection attack that reasserts the power of gradient-based\noptimization for generating fluent jailbreaking prompts. By operating within\nthe LLM's continuous latent space, LARGO first optimizes an adversarial latent\nvector and then recursively call the same LLM to decode the latent into natural\nlanguage. This methodology yields a fast, effective, and transferable attack\nthat produces fluent and stealthy prompts. On standard benchmarks like AdvBench\nand JailbreakBench, LARGO surpasses leading jailbreaking techniques, including\nAutoDAN, by 44 points in attack success rate. Our findings demonstrate a potent\nalternative to agentic LLM prompting, highlighting the efficacy of interpreting\nand attacking LLM internals through gradient optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6f5c\u610f\u8bc6\u81ea\u6211\u53cd\u5c04\u653b\u51fb\u65b9\u6cd5LARGO\uff0c\u7528\u4e8e\u751f\u6210\u6d41\u7545\u7684\u8d8a\u72f1\u63d0\u793a\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u53d1\u73b0\u73b0\u6709\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u65b9\u6cd5\u5728\u79bb\u6563\u8bed\u8a00\u7a7a\u95f4\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u63ed\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6f0f\u6d1e\u3002", "method": "\u5f00\u53d1\u4e86LARGO\uff08\u6f5c\u5728\u5bf9\u6297\u6027\u53cd\u5c04\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\uff09\uff0c\u5b83\u5728\u4e00\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u5185\u64cd\u4f5c\uff0c\u9996\u5148\u4f18\u5316\u5bf9\u6297\u6027\u6f5c\u5728\u5411\u91cf\uff0c\u7136\u540e\u9012\u5f52\u8c03\u7528\u76f8\u540c\u7684\u6a21\u578b\u5c06\u5176\u89e3\u7801\u4e3a\u81ea\u7136\u8bed\u8a00\u3002", "result": "\u5728AdvBench\u548cJailbreakBench\u7b49\u6807\u51c6\u57fa\u51c6\u4e0a\uff0cLARGO\u6bd4\u9886\u5148\u7684\u8d8a\u72f1\u6280\u672fAutoDAN\u9ad8\u51fa44\u4e2a\u767e\u5206\u70b9\u7684\u653b\u51fb\u6210\u529f\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u68af\u5ea6\u4f18\u5316\u89e3\u91ca\u548c\u653b\u51fb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u7ed3\u6784\u662f\u4e00\u79cd\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2505.11107", "pdf": "https://arxiv.org/pdf/2505.11107", "abs": "https://arxiv.org/abs/2505.11107", "authors": ["Chan-Jan Hsu", "Davide Buffelli", "Jamie McGowan", "Feng-Ting Liao", "Yi-Chang Chen", "Sattar Vakili", "Da-shan Shiu"], "title": "Group Think: Multiple Concurrent Reasoning Agents Collaborating at Token Level Granularity", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated the power\nof reasoning through self-generated chains of thought. Multiple reasoning\nagents can collaborate to raise joint reasoning quality above individual\noutcomes. However, such agents typically interact in a turn-based manner,\ntrading increased latency for improved quality. In this paper, we propose Group\nThink--a single LLM that acts as multiple concurrent reasoning agents, or\nthinkers. With shared visibility into each other's partial generation progress,\nGroup Think introduces a new concurrent-reasoning paradigm in which multiple\nreasoning trajectories adapt dynamically to one another at the token level. For\nexample, a reasoning thread may shift its generation mid-sentence upon\ndetecting that another thread is better positioned to continue. This\nfine-grained, token-level collaboration enables Group Think to reduce redundant\nreasoning and improve quality while achieving significantly lower latency.\nMoreover, its concurrent nature allows for efficient utilization of idle\ncomputational resources, making it especially suitable for edge inference,\nwhere very small batch size often underutilizes local~GPUs. We give a simple\nand generalizable modification that enables any existing LLM to perform Group\nThink on a local GPU. We also present an evaluation strategy to benchmark\nreasoning latency and empirically demonstrate latency improvements using\nopen-source LLMs that were not explicitly trained for Group Think. We hope this\nwork paves the way for future LLMs to exhibit more sophisticated and more\nefficient collaborative behavior for higher quality generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5e76\u53d1\u63a8\u7406\u8303\u5f0fGroup Think\uff0c\u5b83\u901a\u8fc7\u8ba9\u5355\u4e2aLLM\u6a21\u62df\u591a\u4e2a\u5e76\u53d1\u63a8\u7406\u4ee3\u7406\uff0c\u5728\u964d\u4f4e\u5197\u4f59\u63a8\u7406\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u8d28\u91cf\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u3002\u6b64\u5916\uff0c\u5176\u5e76\u53d1\u7279\u6027\u5141\u8bb8\u6709\u6548\u5229\u7528\u7a7a\u95f2\u8ba1\u7b97\u8d44\u6e90\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8fb9\u7f18\u63a8\u7406\u573a\u666f\u3002\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u4e14\u901a\u7528\u7684\u4fee\u6539\u65b9\u6cd5\uff0c\u4f7f\u4efb\u4f55\u73b0\u6709\u7684LLM\u90fd\u80fd\u5728\u672c\u5730GPU\u4e0a\u6267\u884cGroup Think\uff0c\u5e76\u5c55\u793a\u51fa\u63a8\u7406\u5ef6\u8fdf\u7684\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u63a8\u7406\u4ee3\u7406\u901a\u5e38\u4ee5\u8f6e\u6d41\u4ea4\u4e92\u7684\u65b9\u5f0f\u5de5\u4f5c\uff0c\u8fd9\u589e\u52a0\u4e86\u5ef6\u8fdf\u4f46\u63d0\u5347\u4e86\u8d28\u91cf\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5e76\u53d1\u63a8\u7406\u65b9\u5f0f\uff0c\u65e2\u80fd\u4fdd\u6301\u9ad8\u8d28\u91cf\u53c8\u80fd\u51cf\u5c11\u5ef6\u8fdf\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aGroup Think\u7684\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u8ba9\u4e00\u4e2aLLM\u6a21\u62df\u591a\u4e2a\u5e76\u53d1\u63a8\u7406\u4ee3\u7406\uff0c\u8fd9\u4e9b\u4ee3\u7406\u4e4b\u95f4\u5171\u4eab\u53ef\u89c1\u6027\uff0c\u4ece\u800c\u80fd\u591f\u5728token\u7ea7\u522b\u52a8\u6001\u9002\u5e94\u5f7c\u6b64\u7684\u751f\u6210\u8fdb\u5ea6\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u4fee\u6539\u65b9\u6cd5\uff0c\u4f7f\u73b0\u6709LLM\u80fd\u591f\u652f\u6301Group Think\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u5f00\u6e90LLM\uff08\u672a\u9488\u5bf9Group Think\u4e13\u95e8\u8bad\u7ec3\uff09\u65f6\uff0c\u53ef\u4ee5\u5b9e\u73b0\u63a8\u7406\u5ef6\u8fdf\u7684\u663e\u8457\u6539\u5584\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u672a\u6765\u7684LLM\u5c55\u793a\u4e86\u66f4\u590d\u6742\u548c\u66f4\u9ad8\u6548\u7684\u534f\u4f5c\u884c\u4e3a\u7684\u53ef\u80fd\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u9ad8\u54c1\u8d28\u7684\u751f\u6210\u3002"}}
{"id": "2505.10845", "pdf": "https://arxiv.org/pdf/2505.10845", "abs": "https://arxiv.org/abs/2505.10845", "authors": ["Hanyu Duan", "Yi Yang", "Ahmed Abbasi", "Kar Yan Tam"], "title": "Ready2Unlearn: A Learning-Time Approach for Preparing Models with Future Unlearning Readiness", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper introduces Ready2Unlearn, a learning-time optimization approach\ndesigned to facilitate future unlearning processes. Unlike the majority of\nexisting unlearning efforts that focus on designing unlearning algorithms,\nwhich are typically implemented reactively when an unlearning request is made\nduring the model deployment phase, Ready2Unlearn shifts the focus to the\ntraining phase, adopting a \"forward-looking\" perspective. Building upon\nwell-established meta-learning principles, Ready2Unlearn proactively trains\nmachine learning models with unlearning readiness, such that they are well\nprepared and can handle future unlearning requests in a more efficient and\nprincipled manner. Ready2Unlearn is model-agnostic and compatible with any\ngradient ascent-based machine unlearning algorithms. We evaluate the method on\nboth vision and language tasks under various unlearning settings, including\nclass-wise unlearning and random data unlearning. Experimental results show\nthat by incorporating such preparedness at training time, Ready2Unlearn\nproduces an unlearning-ready model state, which offers several key advantages\nwhen future unlearning is required, including reduced unlearning time, improved\nretention of overall model capability, and enhanced resistance to the\ninadvertent recovery of forgotten data. We hope this work could inspire future\nefforts to explore more proactive strategies for equipping machine learning\nmodels with built-in readiness towards more reliable and principled machine\nunlearning.", "AI": {"tldr": "Ready2Unlearn\u662f\u4e00\u79cd\u5b66\u4e60\u65f6\u95f4\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u9636\u6bb5\u52a0\u5165\u524d\u77bb\u6027\u89c6\u89d2\uff0c\u4f7f\u6a21\u578b\u5177\u5907\u672a\u5b66\u4e60\u51c6\u5907\u6027\uff0c\u4ece\u800c\u66f4\u9ad8\u6548\u5730\u5904\u7406\u672a\u6765\u7684\u672a\u5b66\u4e60\u8bf7\u6c42\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u5404\u79cd\u68af\u5ea6\u4e0a\u5347\u57fa\u7840\u7684\u672a\u5b66\u4e60\u7b97\u6cd5\uff0c\u5e76\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5176\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u672a\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u90e8\u7f72\u9636\u6bb5\u7684\u53cd\u5e94\u5f0f\u672a\u5b66\u4e60\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u800cReady2Unlearn\u5219\u63d0\u524d\u5230\u8bad\u7ec3\u9636\u6bb5\uff0c\u91c7\u7528\u524d\u77bb\u89c6\u89d2\uff0c\u63d0\u5347\u6a21\u578b\u672a\u6765\u5e94\u5bf9\u672a\u5b66\u4e60\u8bf7\u6c42\u7684\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u6210\u719f\u7684\u5143\u5b66\u4e60\u539f\u5219\uff0cReady2Unlearn\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4e3b\u52a8\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4f7f\u5176\u5177\u5907\u672a\u5b66\u4e60\u51c6\u5907\u6027\uff0c\u4e14\u4e0e\u4efb\u4f55\u68af\u5ea6\u4e0a\u5347\u57fa\u7840\u7684\u672a\u5b66\u4e60\u7b97\u6cd5\u517c\u5bb9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cReady2Unlearn\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u5404\u79cd\u672a\u5b66\u4e60\u8bbe\u7f6e\u4e0b\uff0c\u5305\u62ec\u7c7b\u522b\u7ea7\u672a\u5b66\u4e60\u548c\u968f\u673a\u6570\u636e\u672a\u5b66\u4e60\uff0c\u90fd\u80fd\u663e\u8457\u51cf\u5c11\u672a\u5b66\u4e60\u65f6\u95f4\uff0c\u63d0\u9ad8\u6574\u4f53\u6a21\u578b\u80fd\u529b\u7684\u4fdd\u7559\u7387\uff0c\u5e76\u589e\u5f3a\u5bf9\u9057\u5fd8\u6570\u636e\u65e0\u610f\u6062\u590d\u7684\u62b5\u6297\u529b\u3002", "conclusion": "Ready2Unlearn\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5185\u7f6e\u672a\u5b66\u4e60\u51c6\u5907\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u8fd9\u4e3a\u672a\u6765\u63a2\u7d22\u66f4\u79ef\u6781\u7684\u7b56\u7565\u4ee5\u5b9e\u73b0\u66f4\u53ef\u9760\u548c\u539f\u5219\u6027\u7684\u673a\u5668\u672a\u5b66\u4e60\u63d0\u4f9b\u4e86\u542f\u53d1\u3002"}}
{"id": "2505.11119", "pdf": "https://arxiv.org/pdf/2505.11119", "abs": "https://arxiv.org/abs/2505.11119", "authors": ["Jiabei Cheng", "Zhen-Qun Yang", "Jiannong Cao", "Yu Yang", "Xinzhe Zheng"], "title": "Predicting Student Dropout Risk With A Dual-Modal Abrupt Behavioral Changes Approach", "categories": ["cs.AI", "cs.CY", "cs.LG"], "comment": "14 pages, 5 figures", "summary": "Timely prediction of students at high risk of dropout is critical for early\nintervention and improving educational outcomes. However, in offline\neducational settings, poor data quality, limited scale, and high heterogeneity\noften hinder the application of advanced machine learning models. Furthermore,\nwhile educational theories provide valuable insights into dropout phenomena,\nthe lack of quantifiable metrics for key indicators limits their use in\ndata-driven modeling. Through data analysis and a review of educational\nliterature, we identified abrupt changes in student behavior as key early\nsignals of dropout risk. To address this, we propose the Dual-Modal Multiscale\nSliding Window (DMSW) Model, which integrates academic performance and\nbehavioral data to dynamically capture behavior patterns using minimal data.\nThe DMSW model improves prediction accuracy by 15% compared to traditional\nmethods, enabling educators to identify high-risk students earlier, provide\ntimely support, and foster a more inclusive learning environment. Our analysis\nhighlights key behavior patterns, offering practical insights for preventive\nstrategies and tailored support. These findings bridge the gap between theory\nand practice in dropout prediction, giving educators an innovative tool to\nenhance student retention and outcomes.", "AI": {"tldr": "This paper proposes a Dual-Modal Multiscale Sliding Window (DMSW) Model that uses both academic performance and behavioral data to predict student dropout risks more accurately, helping educators offer timely support.", "motivation": "Timely prediction of students at high risk of dropout is critical for early intervention and improving educational outcomes.", "method": "We propose the Dual-Modal Multiscale Sliding Window (DMSW) Model, which integrates academic performance and behavioral data to dynamically capture behavior patterns using minimal data.", "result": "The DMSW model improves prediction accuracy by 15% compared to traditional methods, enabling educators to identify high-risk students earlier, provide timely support, and foster a more inclusive learning environment.", "conclusion": "Our analysis highlights key behavior patterns, offering practical insights for preventive strategies and tailored support. These findings bridge the gap between theory and practice in dropout prediction, giving educators an innovative tool to enhance student retention and outcomes."}}
{"id": "2505.10846", "pdf": "https://arxiv.org/pdf/2505.10846", "abs": "https://arxiv.org/abs/2505.10846", "authors": ["Jiacheng Liang", "Tanqiu Jiang", "Yuhui Wang", "Rongyi Zhu", "Fenglong Ma", "Ting Wang"], "title": "AutoRAN: Weak-to-Strong Jailbreaking of Large Reasoning Models", "categories": ["cs.LG", "cs.CR"], "comment": "9 pages", "summary": "This paper presents AutoRAN, the first automated, weak-to-strong jailbreak\nattack framework targeting large reasoning models (LRMs). At its core, AutoRAN\nleverages a weak, less-aligned reasoning model to simulate the target model's\nhigh-level reasoning structures, generates narrative prompts, and iteratively\nrefines candidate prompts by incorporating the target model's intermediate\nreasoning steps. We evaluate AutoRAN against state-of-the-art LRMs including\nGPT-o3/o4-mini and Gemini-2.5-Flash across multiple benchmark datasets\n(AdvBench, HarmBench, and StrongReject). Results demonstrate that AutoRAN\nachieves remarkable success rates (approaching 100%) within one or a few turns\nacross different LRMs, even when judged by a robustly aligned external model.\nThis work reveals that leveraging weak reasoning models can effectively exploit\nthe critical vulnerabilities of much more capable reasoning models,\nhighlighting the need for improved safety measures specifically designed for\nreasoning-based models. The code for replicating AutoRAN and running records\nare available at: (https://github.com/JACKPURCELL/AutoRAN-public). (warning:\nthis paper contains potentially harmful content generated by LRMs.)", "AI": {"tldr": "AutoRAN\u662f\u4e00\u79cd\u9488\u5bf9\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u81ea\u52a8\u5316\u5f31\u5230\u5f3a\u8d8a\u72f1\u653b\u51fb\u6846\u67b6\uff0c\u5229\u7528\u5f31\u63a8\u7406\u6a21\u578b\u6a21\u62df\u76ee\u6807\u6a21\u578b\u7684\u9ad8\u7ea7\u63a8\u7406\u7ed3\u6784\u5e76\u751f\u6210\u63d0\u793a\u8bcd\uff0c\u8fed\u4ee3\u4f18\u5316\u5019\u9009\u63d0\u793a\u8bcd\u4ee5\u5b9e\u73b0\u63a5\u8fd1100%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5b89\u5168\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5982\u4f55\u901a\u8fc7\u5f31\u63a8\u7406\u6a21\u578b\u53d1\u73b0\u5176\u6f0f\u6d1e\u3002", "method": "\u4f7f\u7528\u5f31\u63a8\u7406\u6a21\u578b\u6a21\u62df\u76ee\u6807\u6a21\u578b\u7684\u63a8\u7406\u7ed3\u6784\uff0c\u751f\u6210\u63d0\u793a\u8bcd\uff0c\u5e76\u7ed3\u5408\u76ee\u6807\u6a21\u578b\u7684\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\u8bcd\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5bf9\u6700\u5148\u8fdb\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u53d6\u5f97\u4e86\u63a5\u8fd1100%\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u8bc1\u660e\u4e86\u5229\u7528\u5f31\u63a8\u7406\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u63ed\u793a\u66f4\u5f3a\u5927\u7684\u63a8\u7406\u6a21\u578b\u7684\u5173\u952e\u6f0f\u6d1e\uff0c\u5f3a\u8c03\u4e86\u4e3a\u57fa\u4e8e\u63a8\u7406\u7684\u6a21\u578b\u8bbe\u8ba1\u66f4\u597d\u7684\u5b89\u5168\u63aa\u65bd\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.11122", "pdf": "https://arxiv.org/pdf/2505.11122", "abs": "https://arxiv.org/abs/2505.11122", "authors": ["Yu Shi", "Yitong Duan", "Jian Li"], "title": "Navigating the Alpha Jungle: An LLM-Powered MCTS Framework for Formulaic Factor Mining", "categories": ["cs.AI"], "comment": "30 pages", "summary": "Alpha factor mining is pivotal in quantitative investment for identifying\npredictive signals from complex financial data. While traditional formulaic\nalpha mining relies on human expertise, contemporary automated methods, such as\nthose based on genetic programming or reinforcement learning, often suffer from\nsearch inefficiency or yield poorly interpretable alpha factors. This paper\nintroduces a novel framework that integrates Large Language Models (LLMs) with\nMonte Carlo Tree Search (MCTS) to overcome these limitations. Our approach\nleverages the LLM's instruction-following and reasoning capability to\niteratively generate and refine symbolic alpha formulas within an MCTS-driven\nexploration. A key innovation is the guidance of MCTS exploration by rich,\nquantitative feedback from financial backtesting of each candidate factor,\nenabling efficient navigation of the vast search space. Furthermore, a frequent\nsubtree avoidance mechanism is introduced to bolster search efficiency and\nalpha factor performance. Experimental results on real-world stock market data\ndemonstrate that our LLM-based framework outperforms existing methods by mining\nalphas with superior predictive accuracy, trading performance, and improved\ninterpretability, while offering a more efficient solution for formulaic alpha\nmining.", "AI": {"tldr": "This paper presents a new method using LLMs and MCTS for mining alpha factors in quantitative investment, showing better performance than existing approaches.", "motivation": "To address the inefficiency and poor interpretability issues of traditional and some automated alpha mining methods.", "method": "Integrates LLMs with MCTS, using financial backtesting feedback to guide exploration and includes a subtree avoidance mechanism.", "result": "Outperforms existing methods in terms of predictive accuracy, trading performance, and interpretability while being more efficient.", "conclusion": "The proposed framework offers a significant advancement in alpha factor mining for quantitative investment."}}
{"id": "2505.10848", "pdf": "https://arxiv.org/pdf/2505.10848", "abs": "https://arxiv.org/abs/2505.10848", "authors": ["Justin Sanders", "Melih Yilmaz", "Jacob H. Russell", "Wout Bittremieux", "William E. Fondrie", "Nicholas M. Riley", "Sewoong Oh", "William Stafford Noble"], "title": "Foundation model for mass spectrometry proteomics", "categories": ["cs.LG"], "comment": null, "summary": "Mass spectrometry is the dominant technology in the field of proteomics,\nenabling high-throughput analysis of the protein content of complex biological\nsamples. Due to the complexity of the instrumentation and resulting data,\nsophisticated computational methods are required for the processing and\ninterpretation of acquired mass spectra. Machine learning has shown great\npromise to improve the analysis of mass spectrometry data, with numerous\npurpose-built methods for improving specific steps in the data acquisition and\nanalysis pipeline reaching widespread adoption. Here, we propose unifying\nvarious spectrum prediction tasks under a single foundation model for mass\nspectra. To this end, we pre-train a spectrum encoder using de novo sequencing\nas a pre-training task. We then show that using these pre-trained spectrum\nrepresentations improves our performance on the four downstream tasks of\nspectrum quality prediction, chimericity prediction, phosphorylation\nprediction, and glycosylation status prediction. Finally, we perform multi-task\nfine-tuning and find that this approach improves the performance on each task\nindividually. Overall, our work demonstrates that a foundation model for tandem\nmass spectrometry proteomics trained on de novo sequencing learns generalizable\nrepresentations of spectra, improves performance on downstream tasks where\ntraining data is limited, and can ultimately enhance data acquisition and\nanalysis in proteomics experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7edf\u4e00\u7684\u8c31\u56fe\u9884\u6d4b\u4efb\u52a1\u7684\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7de novo\u6d4b\u5e8f\u4f5c\u4e3a\u9884\u8bad\u7ec3\u4efb\u52a1\u5bf9\u5149\u8c31\u7f16\u7801\u5668\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5e76\u5728\u56db\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u5c55\u793a\u4e86\u6539\u8fdb\u6027\u80fd\uff0c\u6700\u7ec8\u8868\u660e\u8be5\u6a21\u578b\u53ef\u4ee5\u589e\u5f3a\u86cb\u767d\u8d28\u7ec4\u5b66\u5b9e\u9a8c\u4e2d\u7684\u6570\u636e\u83b7\u53d6\u548c\u5206\u6790\u3002", "motivation": "\u63d0\u9ad8\u8d28\u8c31\u6570\u636e\u5206\u6790\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ede novo\u6d4b\u5e8f\u7684\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u7edf\u4e00\u5404\u79cd\u8c31\u56fe\u9884\u6d4b\u4efb\u52a1\uff0c\u5e76\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\u548c\u591a\u4efb\u52a1\u5fae\u8c03\u3002", "result": "\u5728\u56db\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec\u8c31\u56fe\u8d28\u91cf\u9884\u6d4b\u3001\u5d4c\u5408\u6027\u9884\u6d4b\u3001\u78f7\u9178\u5316\u9884\u6d4b\u548c\u7cd6\u57fa\u5316\u72b6\u6001\u9884\u6d4b\u3002", "conclusion": "\u8bc1\u660e\u4e86\u57fa\u4e8ede novo\u6d4b\u5e8f\u8bad\u7ec3\u7684\u4e32\u8054\u8d28\u8c31\u86cb\u767d\u8d28\u7ec4\u5b66\u57fa\u7840\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u5230\u53ef\u6cdb\u5316\u7684\u5149\u8c31\u8868\u793a\uff0c\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u86cb\u767d\u8d28\u7ec4\u5b66\u5b9e\u9a8c\u4e2d\u7684\u6570\u636e\u83b7\u53d6\u548c\u5206\u6790\u3002"}}
{"id": "2505.11135", "pdf": "https://arxiv.org/pdf/2505.11135", "abs": "https://arxiv.org/abs/2505.11135", "authors": ["Patrick St\u00f6ckermann", "Henning S\u00fcdfeld", "Alessandro Immordino", "Thomas Altenm\u00fcller", "Marc Wegmann", "Martin Gebser", "Konstantin Schekotihin", "Georg Seidel", "Chew Wye Chan", "Fei Fei Zhang"], "title": "Scalability of Reinforcement Learning Methods for Dispatching in Semiconductor Frontend Fabs: A Comparison of Open-Source Models with Real Industry Datasets", "categories": ["cs.AI", "cs.LG", "cs.NE"], "comment": null, "summary": "Benchmark datasets are crucial for evaluating approaches to scheduling or\ndispatching in the semiconductor industry during the development and deployment\nphases. However, commonly used benchmark datasets like the Minifab or SMT2020\nlack the complex details and constraints found in real-world scenarios. To\nmitigate this shortcoming, we compare open-source simulation models with a real\nindustry dataset to evaluate how optimization methods scale with different\nlevels of complexity. Specifically, we focus on Reinforcement Learning methods,\nperforming optimization based on policy-gradient and Evolution Strategies. Our\nresearch provides insights into the effectiveness of these optimization methods\nand their applicability to realistic semiconductor frontend fab simulations. We\nshow that our proposed Evolution Strategies-based method scales much better\nthan a comparable policy-gradient-based approach. Moreover, we identify the\nselection and combination of relevant bottleneck tools to control by the agent\nas crucial for an efficient optimization. For the generalization across\ndifferent loading scenarios and stochastic tool failure patterns, we achieve\nadvantages when utilizing a diverse training dataset. While the overall\napproach is computationally expensive, it manages to scale well with the number\nof CPU cores used for training. For the real industry dataset, we achieve an\nimprovement of up to 4% regarding tardiness and up to 1% regarding throughput.\nFor the less complex open-source models Minifab and SMT2020, we observe\ndouble-digit percentage improvement in tardiness and single digit percentage\nimprovement in throughput by use of Evolution Strategies.", "AI": {"tldr": "This paper evaluates reinforcement learning methods, specifically policy-gradient and evolution strategies, on semiconductor manufacturing simulation models and real industry data. The evolution strategies-based method shows better scalability and performance improvements.", "motivation": "Common benchmark datasets lack complexity and constraints present in real-world scenarios.", "method": "Comparison of open-source simulation models with a real industry dataset using reinforcement learning methods including policy-gradient and evolution strategies.", "result": "Evolution strategies-based method performs better in terms of scalability and achieves improvements in tardiness and throughput.", "conclusion": "The research provides insights into the effectiveness of optimization methods and their applicability to realistic semiconductor frontend fab simulations."}}
{"id": "2505.10856", "pdf": "https://arxiv.org/pdf/2505.10856", "abs": "https://arxiv.org/abs/2505.10856", "authors": ["Mengxuan Li", "Ke Liu", "Jialong Guo", "Jiajun Bu", "Hongwei Wang", "Haishuai Wang"], "title": "ImputeINR: Time Series Imputation via Implicit Neural Representations for Disease Diagnosis with Missing Data", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by IJCAI 2025", "summary": "Healthcare data frequently contain a substantial proportion of missing\nvalues, necessitating effective time series imputation to support downstream\ndisease diagnosis tasks. However, existing imputation methods focus on discrete\ndata points and are unable to effectively model sparse data, resulting in\nparticularly poor performance for imputing substantial missing values. In this\npaper, we propose a novel approach, ImputeINR, for time series imputation by\nemploying implicit neural representations (INR) to learn continuous functions\nfor time series. ImputeINR leverages the merits of INR in that the continuous\nfunctions are not coupled to sampling frequency and have infinite sampling\nfrequency, allowing ImputeINR to generate fine-grained imputations even on\nextremely sparse observed values. Extensive experiments conducted on eight\ndatasets with five ratios of masked values show the superior imputation\nperformance of ImputeINR, especially for high missing ratios in time series\ndata. Furthermore, we validate that applying ImputeINR to impute missing values\nin healthcare data enhances the performance of downstream disease diagnosis\ntasks. Codes are available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aImputeINR\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u5f81\uff08INR\uff09\u5bf9\u65f6\u95f4\u5e8f\u5217\u8fdb\u884c\u63d2\u8865\uff0c\u5c24\u5176\u5728\u9ad8\u7f3a\u5931\u7387\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u63d0\u9ad8\u4e86\u4e0b\u6e38\u75be\u75c5\u8bca\u65ad\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u63d2\u8865\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u79bb\u6563\u6570\u636e\u70b9\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u7a00\u758f\u6570\u636e\uff0c\u7279\u522b\u662f\u5728\u5927\u91cf\u7f3a\u5931\u503c\u7684\u60c5\u51b5\u4e0b\u6027\u80fd\u8f83\u5dee\u3002", "method": "\u901a\u8fc7\u91c7\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u5f81\uff08INR\uff09\u5b66\u4e60\u65f6\u95f4\u5e8f\u5217\u7684\u8fde\u7eed\u51fd\u6570\u6765\u5b9e\u73b0\u65f6\u95f4\u5e8f\u5217\u63d2\u8865\u3002", "result": "\u5728\u516b\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cImputeINR\u5177\u6709\u4f18\u8d8a\u7684\u63d2\u8865\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u7f3a\u5931\u7387\u4e0b\u3002\u6b64\u5916\uff0c\u5e94\u7528ImputeINR\u63d2\u8865\u533b\u7597\u6570\u636e\u4e2d\u7684\u7f3a\u5931\u503c\u53ef\u4ee5\u63d0\u9ad8\u4e0b\u6e38\u75be\u75c5\u8bca\u65ad\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684ImputeINR\u65b9\u6cd5\u5728\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u5927\u91cf\u7f3a\u5931\u503c\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u4e14\u6709\u52a9\u4e8e\u63d0\u9ad8\u75be\u75c5\u8bca\u65ad\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2505.11136", "pdf": "https://arxiv.org/pdf/2505.11136", "abs": "https://arxiv.org/abs/2505.11136", "authors": ["Janik Bischoff", "Alexandru Rinciog", "Anne Meyer"], "title": "Reinforcement Learning for AMR Charging Decisions: The Impact of Reward and Action Space Design", "categories": ["cs.AI", "cs.RO"], "comment": "Under review LION19: The 19th Learning and Intelligent OptimizatioN\n  Conference", "summary": "We propose a novel reinforcement learning (RL) design to optimize the\ncharging strategy for autonomous mobile robots in large-scale block stacking\nwarehouses. RL design involves a wide array of choices that can mostly only be\nevaluated through lengthy experimentation. Our study focuses on how different\nreward and action space configurations, ranging from flexible setups to more\nguided, domain-informed design configurations, affect the agent performance.\nUsing heuristic charging strategies as a baseline, we demonstrate the\nsuperiority of flexible, RL-based approaches in terms of service times.\nFurthermore, our findings highlight a trade-off: While more open-ended designs\nare able to discover well-performing strategies on their own, they may require\nlonger convergence times and are less stable, whereas guided configurations\nlead to a more stable learning process but display a more limited\ngeneralization potential. Our contributions are threefold. First, we extend\nSLAPStack, an open-source, RL-compatible simulation-framework to accommodate\ncharging strategies. Second, we introduce a novel RL design for tackling the\ncharging strategy problem. Finally, we introduce several novel adaptive\nbaseline heuristics and reproducibly evaluate the design using a Proximal\nPolicy Optimization agent and varying different design configurations, with a\nfocus on reward.", "AI": {"tldr": "This paper proposes a new reinforcement learning method to optimize charging strategies for robots in large warehouses. It compares flexible and guided approaches, showing trade-offs between performance and stability.", "motivation": "Optimizing charging strategies for autonomous mobile robots in large-scale warehouses.", "method": "Novel reinforcement learning design with different reward and action space configurations.", "result": "Flexible RL-based approaches outperform heuristic strategies in service times but have longer convergence times and less stability. Guided configurations offer more stability but limited generalization.", "conclusion": "The study extends an existing simulation framework, introduces a new RL design, and evaluates it with adaptive baseline heuristics using Proximal Policy Optimization."}}
{"id": "2505.11181", "pdf": "https://arxiv.org/pdf/2505.11181", "abs": "https://arxiv.org/abs/2505.11181", "authors": ["Jae Myung Kim", "Stephan Alaniz", "Cordelia Schmid", "Zeynep Akata"], "title": "Feasibility with Language Models for Open-World Compositional Zero-Shot Learning", "categories": ["cs.AI"], "comment": "ECCV Workshop in OOD-CV, 2024", "summary": "Humans can easily tell if an attribute (also called state) is realistic,\ni.e., feasible, for an object, e.g. fire can be hot, but it cannot be wet. In\nOpen-World Compositional Zero-Shot Learning, when all possible state-object\ncombinations are considered as unseen classes, zero-shot predictors tend to\nperform poorly. Our work focuses on using external auxiliary knowledge to\ndetermine the feasibility of state-object combinations. Our Feasibility with\nLanguage Model (FLM) is a simple and effective approach that leverages Large\nLanguage Models (LLMs) to better comprehend the semantic relationships between\nstates and objects. FLM involves querying an LLM about the feasibility of a\ngiven pair and retrieving the output logit for the positive answer. To mitigate\npotential misguidance of the LLM given that many of the state-object\ncompositions are rare or completely infeasible, we observe that the in-context\nlearning ability of LLMs is essential. We present an extensive study\nidentifying Vicuna and ChatGPT as best performing, and we demonstrate that our\nFLM consistently improves OW-CZSL performance across all three benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6765\u5224\u65ad\u72b6\u6001-\u7269\u4f53\u7ec4\u5408\u53ef\u884c\u6027\u7684\u65b9\u6cd5FLM\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u67e5\u8be2LLM\u5e76\u83b7\u53d6\u6b63\u5411\u7b54\u6848\u7684\u8f93\u51falogits\u6765\u5b9e\u73b0\u3002\u4e3a\u4e86\u51cf\u5c11\u7f55\u89c1\u6216\u5b8c\u5168\u4e0d\u53ef\u884c\u7684\u72b6\u6001-\u7269\u4f53\u7ec4\u5408\u53ef\u80fd\u5bf9LLM\u9020\u6210\u7684\u8bef\u5bfc\uff0c\u7814\u7a76\u53d1\u73b0LLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002\u5b9e\u9a8c\u8868\u660e\uff0cFLM\u5728\u6240\u6709\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u90fd\u63d0\u9ad8\u4e86\u5f00\u653e\u4e16\u754c\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60(OW-CZSL)\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u8003\u8651\u6240\u6709\u53ef\u80fd\u7684\u72b6\u6001-\u7269\u4f53\u7ec4\u5408\u4f5c\u4e3a\u672a\u89c1\u7c7b\u522b\u65f6\uff0c\u96f6\u6837\u672c\u9884\u6d4b\u5668\u5728\u5f00\u653e\u4e16\u754c\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\u5f80\u5f80\u4e0d\u4f73\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u91cd\u70b9\u662f\u5982\u4f55\u4f7f\u7528\u5916\u90e8\u8f85\u52a9\u77e5\u8bc6\u6765\u786e\u5b9a\u8fd9\u4e9b\u7ec4\u5408\u7684\u53ef\u884c\u6027\u3002", "method": "\u63d0\u51fa\u7684\u65b9\u6cd5FLM\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6765\u66f4\u597d\u5730\u7406\u89e3\u72b6\u6001\u548c\u7269\u4f53\u4e4b\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\u3002\u5177\u4f53\u6765\u8bf4\uff0cFLM\u6d89\u53ca\u67e5\u8be2LLM\u5173\u4e8e\u7ed9\u5b9a\u914d\u5bf9\u7684\u53ef\u884c\u6027\uff0c\u5e76\u68c0\u7d22\u6b63\u5411\u7b54\u6848\u7684\u8f93\u51falogits\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5f3a\u8c03\u4e86LLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002", "result": "\u7814\u7a76\u8868\u660eVicuna\u548cChatGPT\u662f\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\u3002\u5e76\u4e14\u6211\u4eec\u7684FLM\u65b9\u6cd5\u5728\u6240\u6709\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u90fd\u63d0\u9ad8\u4e86OW-CZSL\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5FLM\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u6539\u5584\u5f00\u653e\u4e16\u754c\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u7f55\u89c1\u6216\u5b8c\u5168\u4e0d\u53ef\u884c\u7684\u72b6\u6001-\u7269\u4f53\u7ec4\u5408\u65b9\u9762\u3002"}}
{"id": "2505.10861", "pdf": "https://arxiv.org/pdf/2505.10861", "abs": "https://arxiv.org/abs/2505.10861", "authors": ["Thang Duong", "Minglai Yang", "Chicheng Zhang"], "title": "Improving the Data-efficiency of Reinforcement Learning by Warm-starting with LLM", "categories": ["cs.LG"], "comment": "31 pages (9 for the main paper), 27 figures, NeurIPS 25 submission", "summary": "We investigate the usage of Large Language Model (LLM) in collecting\nhigh-quality data to warm-start Reinforcement Learning (RL) algorithms for\nlearning in some classical Markov Decision Process (MDP) environments. In this\nwork, we focus on using LLM to generate an off-policy dataset that sufficiently\ncovers state-actions visited by optimal policies, then later using an RL\nalgorithm to explore the environment and improve the policy suggested by the\nLLM. Our algorithm, LORO, can both converge to an optimal policy and have a\nhigh sample efficiency thanks to the LLM's good starting policy. On multiple\nOpenAI Gym environments, such as CartPole and Pendulum, we empirically\ndemonstrate that LORO outperforms baseline algorithms such as pure LLM-based\npolicies, pure RL, and a naive combination of the two, achieving up to $4\n\\times$ the cumulative rewards of the pure RL baseline.", "AI": {"tldr": "This paper introduces LORO, which uses a large language model (LLM) to create an off-policy dataset for reinforcement learning in classical MDP environments. This approach improves performance compared to pure RL or LLM-based policies.", "motivation": "To enhance the performance of reinforcement learning algorithms by leveraging large language models to generate high-quality datasets.", "method": "Using LLMs to generate an off-policy dataset covering state-actions from optimal policies, followed by an RL algorithm improving upon this initial policy.", "result": "LORO shows superior performance across several OpenAI Gym environments like CartPole and Pendulum, achieving up to 4 times the cumulative rewards of pure RL.", "conclusion": "The integration of LLMs with RL can lead to more efficient and effective learning strategies in classical MDP environments."}}
{"id": "2505.11189", "pdf": "https://arxiv.org/pdf/2505.11189", "abs": "https://arxiv.org/abs/2505.11189", "authors": ["Francesco Sovrano"], "title": "Can Global XAI Methods Reveal Injected Bias in LLMs? SHAP vs Rule Extraction vs RuleSHAP", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Generative AI systems can help spread information but also misinformation and\nbiases, potentially undermining the UN Sustainable Development Goals (SDGs).\nExplainable AI (XAI) aims to reveal the inner workings of AI systems and expose\nmisbehaviours or biases. However, current XAI tools, built for simpler models,\nstruggle to handle the non-numerical nature of large language models (LLMs).\nThis paper examines the effectiveness of global XAI methods, such as\nrule-extraction algorithms and SHAP, in detecting bias in LLMs. To do so, we\nfirst show a text-to-ordinal mapping strategy to convert non-numerical\ninputs/outputs into numerical features, enabling these tools to identify (some)\nmisinformation-related biases in LLM-generated content. Then, we inject\nnon-linear biases of varying complexity (univariate, conjunctive, and\nnon-convex) into widespread LLMs like ChatGPT and Llama via system\ninstructions, using global XAI methods to detect them. This way, we found that\nRuleFit struggles with conjunctive and non-convex biases, while SHAP can\napproximate conjunctive biases but cannot express them as actionable rules.\nHence, we introduce RuleSHAP, a global rule extraction algorithm combining SHAP\nand RuleFit to detect more non-univariate biases, improving injected bias\ndetection over RuleFit by +94% (MRR@1) on average.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5168\u5c40XAI\u65b9\u6cd5\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u7b97\u6cd5RuleSHAP\uff0c\u6539\u8fdb\u4e86\u504f\u5dee\u68c0\u6d4b\u3002", "motivation": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u53ef\u80fd\u6709\u52a9\u4e8e\u4f20\u64ad\u4fe1\u606f\uff0c\u4f46\u4e5f\u53ef\u80fd\u4f20\u64ad\u9519\u8bef\u4fe1\u606f\u548c\u504f\u89c1\uff0c\u8fd9\u53ef\u80fd\u4f1a\u524a\u5f31\u8054\u5408\u56fd\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\u3002\u89e3\u91ca\u6027\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u65e8\u5728\u63ed\u793a\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u5185\u90e8\u5de5\u4f5c\u539f\u7406\u5e76\u66b4\u9732\u4e0d\u5f53\u884c\u4e3a\u6216\u504f\u5dee\u3002\u7136\u800c\uff0c\u5f53\u524d\u4e3a\u7b80\u5355\u6a21\u578b\u6784\u5efa\u7684XAI\u5de5\u5177\u96be\u4ee5\u5904\u7406\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u975e\u6570\u503c\u6027\u8d28\u3002", "method": "\u7814\u7a76\u4e86\u5168\u5c40XAI\u65b9\u6cd5\u5728\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u504f\u5dee\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u89c4\u5219\u63d0\u53d6\u7b97\u6cd5\u548cSHAP\u3002\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u6587\u672c\u5230\u5e8f\u6570\u6620\u5c04\u7b56\u7565\u6765\u8f6c\u6362\u975e\u6570\u503c\u8f93\u5165/\u8f93\u51fa\uff0c\u5e76\u901a\u8fc7\u7cfb\u7edf\u6307\u4ee4\u5411ChatGPT\u548cLlama\u7b49\u5e7f\u6cdb\u4f7f\u7528\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6ce8\u5165\u975e\u7ebf\u6027\u504f\u5dee\u3002", "result": "\u53d1\u73b0RuleFit\u5728\u5904\u7406\u8054\u5408\u548c\u975e\u51f8\u504f\u5dee\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u800cSHAP\u53ef\u4ee5\u8fd1\u4f3c\u8054\u5408\u504f\u5dee\u4f46\u65e0\u6cd5\u5c06\u5176\u8868\u8fbe\u4e3a\u53ef\u64cd\u4f5c\u7684\u89c4\u5219\u3002\u5f15\u5165\u7684RuleSHAP\u7b97\u6cd5\u5728\u68c0\u6d4b\u6ce8\u5165\u504f\u5dee\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5168\u5c40\u89c4\u5219\u63d0\u53d6\u7b97\u6cd5RuleSHAP\uff0c\u8be5\u7b97\u6cd5\u7ed3\u5408\u4e86SHAP\u548cRuleFit\uff0c\u7528\u4e8e\u68c0\u6d4b\u66f4\u591a\u7684\u975e\u5355\u53d8\u91cf\u504f\u5dee\uff0c\u5e76\u5728\u6ce8\u5165\u504f\u5dee\u68c0\u6d4b\u4e0a\u6bd4RuleFit\u5e73\u5747\u63d0\u9ad8\u4e86+94%\uff08MRR@1\uff09\u3002"}}
{"id": "2505.11191", "pdf": "https://arxiv.org/pdf/2505.11191", "abs": "https://arxiv.org/abs/2505.11191", "authors": ["Kasra Borazjani", "Payam Abdisarabshali", "Fardis Nadimi", "Naji Khosravan", "Minghui Liwang", "Xianbin Wang", "Yiguang Hong", "Seyyedali Hosseinalipour"], "title": "Multi-Modal Multi-Task (M3T) Federated Foundation Models for Embodied AI: Potentials and Challenges for Edge Integration", "categories": ["cs.AI", "cs.RO"], "comment": "10 pages, 3 figures, 3 tables", "summary": "As embodied AI systems become increasingly multi-modal, personalized, and\ninteractive, they must learn effectively from diverse sensory inputs, adapt\ncontinually to user preferences, and operate safely under resource and privacy\nconstraints. These challenges expose a pressing need for machine learning\nmodels capable of swift, context-aware adaptation while balancing model\ngeneralization and personalization. Here, two methods emerge as suitable\ncandidates, each offering parts of these capabilities: Foundation Models (FMs)\nprovide a pathway toward generalization across tasks and modalities, whereas\nFederated Learning (FL) offers the infrastructure for distributed,\nprivacy-preserving model updates and user-level model personalization. However,\nwhen used in isolation, each of these approaches falls short of meeting the\ncomplex and diverse capability requirements of real-world embodied\nenvironments. In this vision paper, we introduce Federated Foundation Models\n(FFMs) for embodied AI, a new paradigm that unifies the strengths of\nmulti-modal multi-task (M3T) FMs with the privacy-preserving distributed nature\nof FL, enabling intelligent systems at the wireless edge. We collect critical\ndeployment dimensions of FFMs in embodied AI ecosystems under a unified\nframework, which we name \"EMBODY\": Embodiment heterogeneity, Modality richness\nand imbalance, Bandwidth and compute constraints, On-device continual learning,\nDistributed control and autonomy, and Yielding safety, privacy, and\npersonalization. For each, we identify concrete challenges and envision\nactionable research directions. We also present an evaluation framework for\ndeploying FFMs in embodied AI systems, along with the associated trade-offs.", "AI": {"tldr": "This vision paper introduces Federated Foundation Models (FFMs) for embodied AI, a new paradigm that combines the strengths of Foundation Models (FMs) and Federated Learning (FL). The authors outline key deployment dimensions under the 'EMBODY' framework and propose an evaluation approach.", "motivation": "The increasing complexity and diversity of embodied AI systems require models that can adapt quickly and effectively while maintaining privacy and personalization.", "method": "Introduces FFMs, combining M3T FMs with FL's distributed, privacy-preserving features. Proposes the EMBODY framework to address embodiment heterogeneity, modality issues, bandwidth/compute limits, on-device learning, distributed control, and safety/privacy/personalization concerns.", "result": "Identifies challenges and research directions for deploying FFMs in embodied AI ecosystems. Presents an evaluation framework with trade-offs.", "conclusion": "This paper outlines a novel approach (FFMs) and framework ('EMBODY') to meet the complex demands of embodied AI systems, emphasizing adaptability, personalization, and privacy."}}
{"id": "2505.10874", "pdf": "https://arxiv.org/pdf/2505.10874", "abs": "https://arxiv.org/abs/2505.10874", "authors": ["Luca Magri", "Filippo Leveni", "Giacomo Boracchi"], "title": "MultiLink: Multi-class Structure Recovery via Agglomerative Clustering and Model Selection", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Accepted at Computer Vision and Pattern Recognition (CVPR 2021)", "summary": "We address the problem of recovering multiple structures of different classes\nin a dataset contaminated by noise and outliers. In particular, we consider\ngeometric structures defined by a mixture of underlying parametric models (e.g.\nplanes and cylinders, homographies and fundamental matrices), and we tackle the\nrobust fitting problem by preference analysis and clustering. We present a new\nalgorithm, termed MultiLink, that simultaneously deals with multiple classes of\nmodels. MultiLink combines on-the-fly model fitting and model selection in a\nnovel linkage scheme that determines whether two clusters are to be merged. The\nresulting method features many practical advantages with respect to methods\nbased on preference analysis, being faster, less sensitive to the inlier\nthreshold, and able to compensate limitations deriving from hypotheses\nsampling. Experiments on several public datasets demonstrate that Multi-Link\nfavourably compares with state of the art alternatives, both in multi-class and\nsingle-class problems. Code is publicly made available for download.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684MultiLink\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u53d7\u566a\u58f0\u548c\u5f02\u5e38\u503c\u5f71\u54cd\u7684\u6570\u636e\u96c6\u4e2d\u540c\u65f6\u5904\u7406\u591a\u79cd\u7c7b\u522b\u7684\u51e0\u4f55\u7ed3\u6784\u6a21\u578b\u3002\u4e0e\u57fa\u4e8e\u504f\u597d\u5206\u6790\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u66f4\u5feb\u3001\u5bf9\u9608\u503c\u4e0d\u654f\u611f\uff0c\u5e76\u4e14\u80fd\u591f\u5f25\u8865\u5047\u8bbe\u62bd\u6837\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5728\u53d7\u566a\u58f0\u548c\u5f02\u5e38\u503c\u5f71\u54cd\u7684\u6570\u636e\u96c6\u4e2d\u6062\u590d\u4e0d\u540c\u7c7b\u522b\u7684\u591a\u4e2a\u7ed3\u6784\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u504f\u597d\u5206\u6790\u548c\u805a\u7c7b\u89e3\u51b3\u9c81\u68d2\u62df\u5408\u95ee\u9898\uff0c\u63d0\u51fa\u65b0\u7684MultiLink\u7b97\u6cd5\uff0c\u5728\u65b0\u578b\u94fe\u63a5\u65b9\u6848\u4e2d\u7ed3\u5408\u4e86\u5b9e\u65f6\u6a21\u578b\u62df\u5408\u548c\u6a21\u578b\u9009\u62e9\u3002", "result": "MultiLink\u7b97\u6cd5\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5b83\u5728\u591a\u7c7b\u522b\u548c\u5355\u7c7b\u522b\u95ee\u9898\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u65b0\u66ff\u4ee3\u65b9\u6cd5\u3002", "conclusion": "MultiLink\u7b97\u6cd5\u5c55\u793a\u4e86\u5176\u5728\u5904\u7406\u591a\u79cd\u7c7b\u522b\u7684\u51e0\u4f55\u7ed3\u6784\u6a21\u578b\u65b9\u9762\u7684\u8bb8\u591a\u5b9e\u9645\u4f18\u52bf\u3002"}}
{"id": "2505.11208", "pdf": "https://arxiv.org/pdf/2505.11208", "abs": "https://arxiv.org/abs/2505.11208", "authors": ["Dongjun Kim", "Junwoo Park", "Chaehyeon Shin", "Jaeheon Jung", "Kyungho Shin", "Seungheon Baek", "Sanghyuk Heo", "Woongrae Kim", "Inchul Jeong", "Joohwan Cho", "Jongsun Park"], "title": "GLOVA: Global and Local Variation-Aware Analog Circuit Design with Risk-Sensitive Reinforcement Learning", "categories": ["cs.AI", "cs.CE", "cs.ET", "cs.LG"], "comment": "Accepted for DAC 2025", "summary": "Analog/mixed-signal circuit design encounters significant challenges due to\nperformance degradation from process, voltage, and temperature (PVT)\nvariations. To achieve commercial-grade reliability, iterative manual design\nrevisions and extensive statistical simulations are required. While several\nstudies have aimed to automate variation aware analog design to reduce\ntime-to-market, the substantial mismatches in real-world wafers have not been\nthoroughly addressed. In this paper, we present GLOVA, an analog circuit sizing\nframework that effectively manages the impact of diverse random mismatches to\nimprove robustness against PVT variations. In the proposed approach,\nrisk-sensitive reinforcement learning is leveraged to account for the\nreliability bound affected by PVT variations, and ensemble-based critic is\nintroduced to achieve sample-efficient learning. For design verification, we\nalso propose $\\mu$-$\\sigma$ evaluation and simulation reordering method to\nreduce simulation costs of identifying failed designs. GLOVA supports\nverification through industrial-level PVT variation evaluation methods,\nincluding corner simulation as well as global and local Monte Carlo (MC)\nsimulations. Compared to previous state-of-the-art variation-aware analog\nsizing frameworks, GLOVA achieves up to 80.5$\\times$ improvement in sample\nefficiency and 76.0$\\times$ reduction in time.", "AI": {"tldr": "This paper presents GLOVA, a framework for analog circuit sizing that improves robustness against PVT variations using risk-sensitive reinforcement learning and ensemble-based critic.", "motivation": "To address performance degradation in analog/mixed-signal circuits due to PVT variations and reduce time-to-market.", "method": "Risk-sensitive reinforcement learning and ensemble-based critic for managing PVT variations, with \u03bc-\u03c3 evaluation and simulation reordering for design verification.", "result": "GLOVA shows up to 80.5\u00d7 improvement in sample efficiency and 76.0\u00d7 reduction in time compared to previous frameworks.", "conclusion": "GLOVA effectively manages diverse random mismatches and provides efficient design verification for analog circuits."}}
{"id": "2505.11227", "pdf": "https://arxiv.org/pdf/2505.11227", "abs": "https://arxiv.org/abs/2505.11227", "authors": ["Zhangying Feng", "Qianglong Chen", "Ning Lu", "Yongqian Li", "Siqi Cheng", "Shuangmu Peng", "Duyu Tang", "Shengcai Liu", "Zhirui Zhang"], "title": "Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "The development of reasoning capabilities represents a critical frontier in\nlarge language models (LLMs) research, where reinforcement learning (RL) and\nprocess reward models (PRMs) have emerged as predominant methodological\nframeworks. Contrary to conventional wisdom, empirical evidence from\nDeepSeek-R1 demonstrates that pure RL training focused on mathematical\nproblem-solving can progressively enhance reasoning abilities without PRM\nintegration, challenging the perceived necessity of process supervision. In\nthis study, we conduct a systematic investigation of the relationship between\nRL training and PRM capabilities. Our findings demonstrate that problem-solving\nproficiency and process supervision capabilities represent complementary\ndimensions of reasoning that co-evolve synergistically during pure RL training.\nIn particular, current PRMs underperform simple baselines like majority voting\nwhen applied to state-of-the-art models such as DeepSeek-R1 and QwQ-32B. To\naddress this limitation, we propose Self-PRM, an introspective framework in\nwhich models autonomously evaluate and rerank their generated solutions through\nself-reward mechanisms. Although Self-PRM consistently improves the accuracy of\nthe benchmark (particularly with larger sample sizes), analysis exposes\npersistent challenges: The approach exhibits low precision (<10\\%) on difficult\nproblems, frequently misclassifying flawed solutions as valid. These analyses\nunderscore the need for continued RL scaling to improve reward alignment and\nintrospective accuracy. Overall, our findings suggest that PRM may not be\nessential for enhancing complex reasoning, as pure RL not only improves\nproblem-solving skills but also inherently fosters robust PRM capabilities. We\nhope these findings provide actionable insights for building more reliable and\nself-aware complex reasoning models.", "AI": {"tldr": "This study explores the relationship between reinforcement learning (RL) training and process reward models (PRMs) in enhancing reasoning abilities in large language models (LLMs). It challenges the necessity of PRM integration by demonstrating that pure RL training can progressively enhance reasoning without it.", "motivation": "To investigate whether process supervision via PRMs is necessary for improving reasoning capabilities in LLMs.", "method": "Systematic investigation comparing pure RL training with PRM integration, proposing Self-PRM as an introspective framework.", "result": "Pure RL training can enhance reasoning abilities and foster PRM capabilities without PRM integration. However, current PRMs perform poorly compared to simple baselines when applied to advanced models.", "conclusion": "PRM may not be essential for enhancing complex reasoning as pure RL alone can improve both problem-solving skills and PRM capabilities."}}
{"id": "2505.10877", "pdf": "https://arxiv.org/pdf/2505.10877", "abs": "https://arxiv.org/abs/2505.10877", "authors": ["Mathieu Alain", "So Takao", "Xiaowen Dong", "Bastian Rieck", "Emmanuel Noutahi"], "title": "Graph and Simplicial Complex Prediction Gaussian Process via the Hodgelet Representations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Predicting the labels of graph-structured data is crucial in scientific\napplications and is often achieved using graph neural networks (GNNs). However,\nwhen data is scarce, GNNs suffer from overfitting, leading to poor performance.\nRecently, Gaussian processes (GPs) with graph-level inputs have been proposed\nas an alternative. In this work, we extend the Gaussian process framework to\nsimplicial complexes (SCs), enabling the handling of edge-level attributes and\nattributes supported on higher-order simplices. We further augment the\nresulting SC representations by considering their Hodge decompositions,\nallowing us to account for homological information, such as the number of\nholes, in the SC. We demonstrate that our framework enhances the predictions\nacross various applications, paving the way for GPs to be more widely used for\ngraph and SC-level predictions.", "AI": {"tldr": "This work extends Gaussian process framework to simplicial complexes (SCs) for graph and SC-level predictions, enhancing predictions across various applications.", "motivation": "When data is scarce, graph neural networks (GNNs) suffer from overfitting, leading to poor performance. Recently, Gaussian processes (GPs) with graph-level inputs have been proposed as an alternative.", "method": "Extend the Gaussian process framework to simplicial complexes (SCs) and augment the resulting SC representations by considering their Hodge decompositions.", "result": "The framework enhances predictions across various applications.", "conclusion": "This work paves the way for GPs to be more widely used for graph and SC-level predictions."}}
{"id": "2505.11247", "pdf": "https://arxiv.org/pdf/2505.11247", "abs": "https://arxiv.org/abs/2505.11247", "authors": ["Mingxing Peng", "Yuting Xie", "Xusen Guo", "Ruoyu Yao", "Hai Yang", "Jun Ma"], "title": "LD-Scene: LLM-Guided Diffusion for Controllable Generation of Adversarial Safety-Critical Driving Scenarios", "categories": ["cs.AI", "cs.LG", "cs.RO"], "comment": "13 pages, 5 figures", "summary": "Ensuring the safety and robustness of autonomous driving systems necessitates\na comprehensive evaluation in safety-critical scenarios. However, these\nsafety-critical scenarios are rare and difficult to collect from real-world\ndriving data, posing significant challenges to effectively assessing the\nperformance of autonomous vehicles. Typical existing methods often suffer from\nlimited controllability and lack user-friendliness, as extensive expert\nknowledge is essentially required. To address these challenges, we propose\nLD-Scene, a novel framework that integrates Large Language Models (LLMs) with\nLatent Diffusion Models (LDMs) for user-controllable adversarial scenario\ngeneration through natural language. Our approach comprises an LDM that\ncaptures realistic driving trajectory distributions and an LLM-based guidance\nmodule that translates user queries into adversarial loss functions,\nfacilitating the generation of scenarios aligned with user queries. The\nguidance module integrates an LLM-based Chain-of-Thought (CoT) code generator\nand an LLM-based code debugger, enhancing the controllability and robustness in\ngenerating guidance functions. Extensive experiments conducted on the nuScenes\ndataset demonstrate that LD-Scene achieves state-of-the-art performance in\ngenerating realistic, diverse, and effective adversarial scenarios.\nFurthermore, our framework provides fine-grained control over adversarial\nbehaviors, thereby facilitating more effective testing tailored to specific\ndriving scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u65b0\u6846\u67b6LD-Scene\uff0c\u7528\u4e8e\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u751f\u6210\u53ef\u63a7\u7684\u5bf9\u6297\u573a\u666f\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u751f\u6210\u771f\u5b9e\u3001\u591a\u6837\u4e14\u6709\u6548\u7684\u5bf9\u6297\u573a\u666f\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u9700\u8981\u5728\u5173\u952e\u5b89\u5168\u573a\u666f\u4e0b\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u4f46\u8fd9\u4e9b\u573a\u666f\u7a00\u5c11\u4e14\u96be\u4ee5\u4ece\u73b0\u5b9e\u9a7e\u9a76\u6570\u636e\u4e2d\u6536\u96c6\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u53ef\u63a7\u6027\u5dee\u548c\u7528\u6237\u53cb\u597d\u5ea6\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u96c6\u6210\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6LD-Scene\uff0c\u5305\u542b\u6355\u6349\u5b9e\u9645\u9a7e\u9a76\u8f68\u8ff9\u5206\u5e03\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6307\u5bfc\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5305\u62ec\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u601d\u7ef4\u94fe\u4ee3\u7801\u751f\u6210\u5668\u548c\u4ee3\u7801\u8c03\u8bd5\u5668\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86LD-Scene\u5728\u751f\u6210\u771f\u5b9e\u3001\u591a\u6837\u5316\u548c\u6709\u6548\u7684\u5bf9\u6297\u573a\u666f\u65b9\u9762\u5177\u6709\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u5bf9\u6297\u884c\u4e3a\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u548c\u5b9a\u5236\u5316\u7684\u6d4b\u8bd5\u3002"}}
{"id": "2505.11274", "pdf": "https://arxiv.org/pdf/2505.11274", "abs": "https://arxiv.org/abs/2505.11274", "authors": ["Zheng Li", "Qingxiu Dong", "Jingyuan Ma", "Di Zhang", "Zhifang Sui"], "title": "SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recently, large reasoning models demonstrate exceptional performance on\nvarious tasks. However, reasoning models inefficiently over-process both\ntrivial and complex queries, leading to resource waste and prolonged user\nlatency. To address this challenge, we propose SelfBudgeter - a self-adaptive\ncontrollable reasoning strategy for efficient reasoning. Our approach adopts a\ndual-phase training paradigm: first, the model learns to pre-estimate the\nreasoning cost based on the difficulty of the query. Then, we introduce\nbudget-guided GPRO for reinforcement learning, which effectively maintains\naccuracy while reducing output length. SelfBudgeter allows users to anticipate\ngeneration time and make informed decisions about continuing or interrupting\nthe process. Furthermore, our method enables direct manipulation of reasoning\nlength via pre-filling token budget. Experimental results demonstrate that\nSelfBudgeter can rationally allocate budgets according to problem complexity,\nachieving up to 74.47% response length compression on the MATH benchmark while\nmaintaining nearly undiminished accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSelfBudgeter\u7684\u81ea\u9002\u5e94\u53ef\u63a7\u63a8\u7406\u7b56\u7565\uff0c\u901a\u8fc7\u53cc\u9636\u6bb5\u8bad\u7ec3\u6765\u9884\u4f30\u63a8\u7406\u6210\u672c\u5e76\u5f15\u5165\u9884\u7b97\u5f15\u5bfc\u7684GPRO\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u51cf\u5c11\u8f93\u51fa\u957f\u5ea6\u3002\u5b9e\u9a8c\u8868\u660e\uff0cSelfBudgeter\u5728MATH\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe74.47%\u7684\u54cd\u5e94\u957f\u5ea6\u538b\u7f29\u4e14\u51e0\u4e4e\u4e0d\u964d\u4f4e\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u7406\u6a21\u578b\u5bf9\u7b80\u5355\u548c\u590d\u6742\u67e5\u8be2\u90fd\u8fdb\u884c\u4e86\u4e0d\u5fc5\u8981\u7684\u8fc7\u5ea6\u5904\u7406\uff0c\u5bfc\u81f4\u8d44\u6e90\u6d6a\u8d39\u548c\u7528\u6237\u5ef6\u8fdf\u589e\u52a0\u3002", "method": "\u91c7\u7528\u53cc\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u9996\u5148\u9884\u4f30\u67e5\u8be2\u96be\u5ea6\u4e0b\u7684\u63a8\u7406\u6210\u672c\uff0c\u7136\u540e\u901a\u8fc7\u9884\u7b97\u5f15\u5bfc\u7684GPRO\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u6765\u7ef4\u62a4\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u8f93\u51fa\u957f\u5ea6\u3002\u6b64\u5916\uff0c\u8fd8\u652f\u6301\u901a\u8fc7\u9884\u586b\u5145\u6807\u8bb0\u9884\u7b97\u76f4\u63a5\u63a7\u5236\u63a8\u7406\u957f\u5ea6\u3002", "result": "SelfBudgeter\u5728MATH\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u54cd\u5e94\u957f\u5ea6\u538b\u7f29\uff08\u9ad8\u8fbe74.47%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u51e0\u4e4e\u4e0d\u53d8\u7684\u51c6\u786e\u6027\u3002", "conclusion": "SelfBudgeter\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u5408\u7406\u5206\u914d\u9884\u7b97\u4ee5\u9002\u5e94\u95ee\u9898\u590d\u6742\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2505.10881", "pdf": "https://arxiv.org/pdf/2505.10881", "abs": "https://arxiv.org/abs/2505.10881", "authors": ["Donghyeon Ki", "JunHyeok Oh", "Seong-Woong Shim", "Byung-Jun Lee"], "title": "Prior-Guided Diffusion Planning for Offline Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Diffusion models have recently gained prominence in offline reinforcement\nlearning due to their ability to effectively learn high-performing,\ngeneralizable policies from static datasets. Diffusion-based planners\nfacilitate long-horizon decision-making by generating high-quality trajectories\nthrough iterative denoising, guided by return-maximizing objectives. However,\nexisting guided sampling strategies such as Classifier Guidance,\nClassifier-Free Guidance, and Monte Carlo Sample Selection either produce\nsuboptimal multi-modal actions, struggle with distributional drift, or incur\nprohibitive inference-time costs. To address these challenges, we propose Prior\nGuidance (PG), a novel guided sampling framework that replaces the standard\nGaussian prior of a behavior-cloned diffusion model with a learnable\ndistribution, optimized via a behavior-regularized objective. PG directly\ngenerates high-value trajectories without costly reward optimization of the\ndiffusion model itself, and eliminates the need to sample multiple candidates\nat inference for sample selection. We present an efficient training strategy\nthat applies behavior regularization in latent space, and empirically\ndemonstrate that PG outperforms state-of-the-art diffusion policies and\nplanners across diverse long-horizon offline RL benchmarks.", "AI": {"tldr": "\u63d0\u51faPrior Guidance (PG)\uff0c\u4e00\u79cd\u65b0\u7684\u5f15\u5bfc\u91c7\u6837\u6846\u67b6\uff0c\u6539\u8fdb\u4e86\u6269\u6563\u6a21\u578b\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u751f\u6210\u6b21\u4f18\u591a\u6a21\u6001\u52a8\u4f5c\u3001\u5206\u5e03\u6f02\u79fb\u95ee\u9898\u6216\u63a8\u7406\u65f6\u6210\u672c\u8fc7\u9ad8\u7b49\u6311\u6218\u3002", "method": "\u7528\u53ef\u5b66\u4e60\u5206\u5e03\u66ff\u4ee3\u884c\u4e3a\u514b\u9686\u6269\u6563\u6a21\u578b\u7684\u6807\u51c6\u9ad8\u65af\u5148\u9a8c\uff0c\u5e76\u901a\u8fc7\u884c\u4e3a\u6b63\u5219\u5316\u76ee\u6807\u4f18\u5316\u3002", "result": "PG\u80fd\u76f4\u63a5\u751f\u6210\u9ad8\u4ef7\u503c\u8f68\u8ff9\uff0c\u65e0\u9700\u5bf9\u6269\u6563\u6a21\u578b\u672c\u8eab\u8fdb\u884c\u6602\u8d35\u7684\u5956\u52b1\u4f18\u5316\uff0c\u4e14\u6d88\u9664\u4e86\u63a8\u7406\u65f6\u591a\u6b21\u91c7\u6837\u7684\u9700\u6c42\u3002", "conclusion": "PG\u5728\u591a\u79cd\u957f\u671f\u79bb\u7ebfRL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6269\u6563\u7b56\u7565\u548c\u89c4\u5212\u5668\u3002"}}
{"id": "2505.11289", "pdf": "https://arxiv.org/pdf/2505.11289", "abs": "https://arxiv.org/abs/2505.11289", "authors": ["Reginald McLean", "Evangelos Chatzaroulas", "Luc McCutcheon", "Frank R\u00f6der", "Tianhe Yu", "Zhanpeng He", "K. R. Zentner", "Ryan Julian", "J K Terry", "Isaac Woungang", "Nariman Farsad", "Pablo Samuel Castro"], "title": "Meta-World+: An Improved, Standardized, RL Benchmark", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Meta-World is widely used for evaluating multi-task and meta-reinforcement\nlearning agents, which are challenged to master diverse skills simultaneously.\nSince its introduction however, there have been numerous undocumented changes\nwhich inhibit a fair comparison of algorithms. This work strives to\ndisambiguate these results from the literature, while also leveraging the past\nversions of Meta-World to provide insights into multi-task and\nmeta-reinforcement learning benchmark design. Through this process we release a\nnew open-source version of Meta-World\n(https://github.com/Farama-Foundation/Metaworld/) that has full reproducibility\nof past results, is more technically ergonomic, and gives users more control\nover the tasks that are included in a task set.", "AI": {"tldr": "This paper addresses inconsistencies in Meta-World, a platform for evaluating multi-task and meta-reinforcement learning agents, by releasing a new open-source version with improved reproducibility and user control.", "motivation": "To resolve discrepancies in algorithm comparisons due to undocumented changes in Meta-World and to gain insights into benchmark design.", "method": "Analyzing past versions of Meta-World and addressing issues related to reproducibility and technical ergonomics.", "result": "A new open-source Meta-World version with enhanced features for reproducibility and user control over task sets.", "conclusion": "The new Meta-World version facilitates fairer comparisons and deeper understanding of multi-task and meta-reinforcement learning benchmarks."}}
{"id": "2505.11451", "pdf": "https://arxiv.org/pdf/2505.11451", "abs": "https://arxiv.org/abs/2505.11451", "authors": ["Lee Harris", "James Bentham", "Philippe De Wilde"], "title": "Extracting Explainable Dates From Medical Images By Reverse-Engineering UNIX Timestamps", "categories": ["cs.AI"], "comment": null, "summary": "Dates often contribute towards highly impactful medical decisions, but it is\nrarely clear how to extract this data. AI has only just begun to be used\ntranscribe such documents, and common methods are either to trust that the\noutput produced by a complex AI model, or to parse the text using regular\nexpressions. Recent work has established that regular expressions are an\nexplainable form of logic, but it is difficult to decompose these into the\ncomponent parts that are required to construct precise UNIX timestamps. First,\nwe test publicly-available regular expressions, and we found that these were\nunable to capture a significant number of our dates. Next, we manually created\neasily-decomposable regular expressions, and we found that these were able to\ndetect the majority of real dates, but also a lot of sequences of text that\nlook like dates. Finally, we used regular expression synthesis to automatically\nidentify regular expressions from the reverse-engineered UNIX timestamps that\nwe created. We find that regular expressions created by regular expression\nsynthesis detect far fewer sequences of text that look like dates than those\nthat were manually created, at the cost of a slight increase to the number of\nmissed dates. Overall, our results show that regular expressions can be created\nthrough regular expression synthesis to identify complex dates and date ranges\nin text transcriptions. To our knowledge, our proposed way of learning\ndeterministic logic by reverse-engineering several many-one mappings and\nfeeding these into a regular expression synthesiser is a new approach.", "AI": {"tldr": "The study investigates different methods to extract date information from medical documents, showing that regular expressions synthesized automatically can identify complex dates and date ranges.", "motivation": "To find a reliable method to extract date information from medical documents for making important medical decisions.", "method": "Testing publicly-available regular expressions, creating manually decomposable regular expressions, and using regular expression synthesis to identify regular expressions from reverse-engineered UNIX timestamps.", "result": "Publicly-available regular expressions failed to capture many dates. Manually-created regular expressions detected most real dates but also false positives. Synthesized regular expressions detected fewer false positives but missed more actual dates.", "conclusion": "Regular expression synthesis can create rules to identify complex dates and date ranges in transcriptions."}}
{"id": "2505.10892", "pdf": "https://arxiv.org/pdf/2505.10892", "abs": "https://arxiv.org/abs/2505.10892", "authors": ["Akhil Agnihotri", "Rahul Jain", "Deepak Ramachandran", "Zheng Wen"], "title": "Multi-Objective Preference Optimization: Improving Human Alignment of Generative Models", "categories": ["cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2406.18853 by other authors", "summary": "Post-training of LLMs with RLHF, and subsequently preference optimization\nalgorithms such as DPO, IPO, etc., made a big difference in improving human\nalignment. However, all such techniques can only work with a single (human)\nobjective. In practice, human users have multiple objectives, such as\nhelpfulness and harmlessness, and there is no natural way to aggregate them\ninto a single objective. In this paper, we address the multi-objective\npreference-alignment problem, where a policy must optimize several, potentially\nconflicting, objectives. We introduce the Multi-Objective Preference\nOptimization (MOPO) algorithm, which frames alignment as a constrained\nKL-regularized optimization: the primary objective is maximized while secondary\nobjectives are lower-bounded by tunable safety thresholds. Unlike prior work,\nMOPO operates directly on pairwise preference data, requires no point-wise\nreward assumption, and avoids heuristic prompt-context engineering. The method\nrecovers policies on the Pareto front whenever the front is attainable;\npractically, it reduces to simple closed-form iterative updates suitable for\nlarge-scale training. On synthetic benchmarks with diverse canonical preference\nstructures, we show that MOPO approximates the Pareto front. When fine-tuning a\n1.3B-parameter language model on real-world human-preference datasets, MOPO\nattains higher rewards and yields policies that Pareto-dominate baselines;\nablation studies confirm optimization stability and robustness to\nhyperparameters.", "AI": {"tldr": "This paper introduces MOPO, an algorithm for multi-objective preference alignment in large language models, which outperforms baselines when fine-tuning on real-world datasets.", "motivation": "Current methods for aligning large language models with human preferences can only handle a single objective, whereas humans often have multiple, sometimes conflicting, objectives.", "method": "The MOPO algorithm formulates alignment as a constrained KL-regularized optimization problem, optimizing the primary objective while ensuring secondary objectives meet tunable safety thresholds. It uses pairwise preference data without requiring point-wise reward assumptions or heuristic prompt-context engineering.", "result": "MOPO was shown to approximate the Pareto front on synthetic benchmarks and achieved higher rewards with policies that Pareto-dominated baselines when fine-tuning a 1.3B-parameter language model on real-world datasets.", "conclusion": "MOPO provides a robust solution for multi-objective preference alignment in large language models, demonstrating improved performance and stability over existing methods."}}
{"id": "2505.11478", "pdf": "https://arxiv.org/pdf/2505.11478", "abs": "https://arxiv.org/abs/2505.11478", "authors": ["Mingxuan Li", "Junzhe Zhang", "Elias Bareinboim"], "title": "Automatic Reward Shaping from Confounded Offline Data", "categories": ["cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "A key task in Artificial Intelligence is learning effective policies for\ncontrolling agents in unknown environments to optimize performance measures.\nOff-policy learning methods, like Q-learning, allow learners to make optimal\ndecisions based on past experiences. This paper studies off-policy learning\nfrom biased data in complex and high-dimensional domains where \\emph{unobserved\nconfounding} cannot be ruled out a priori. Building on the well-celebrated Deep\nQ-Network (DQN), we propose a novel deep reinforcement learning algorithm\nrobust to confounding biases in observed data. Specifically, our algorithm\nattempts to find a safe policy for the worst-case environment compatible with\nthe observations. We apply our method to twelve confounded Atari games, and\nfind that it consistently dominates the standard DQN in all games where the\nobserved input to the behavioral and target policies mismatch and unobserved\nconfounders exist.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4ece\u6709\u504f\u6570\u636e\u4e2d\u8fdb\u884c\u5f02\u7b56\u7565\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5bf9\u89c2\u5bdf\u6570\u636e\u4e2d\u7684\u6df7\u6742\u504f\u5dee\u5177\u6709\u9c81\u68d2\u6027\u3002\u572812\u4e2a\u6df7\u6742\u7684Atari\u6e38\u620f\u4e2d\u5e94\u7528\u4e86\u8be5\u65b9\u6cd5\uff0c\u53d1\u73b0\u5b83\u5728\u6240\u6709\u884c\u4e3a\u548c\u76ee\u6807\u7b56\u7565\u7684\u89c2\u6d4b\u8f93\u5165\u4e0d\u5339\u914d\u4e14\u5b58\u5728\u672a\u89c2\u6d4b\u6df7\u6742\u56e0\u5b50\u7684\u6e38\u620f\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u6807\u51c6DQN\u3002", "motivation": "\u5728\u590d\u6742\u548c\u9ad8\u7ef4\u9886\u57df\u4e2d\uff0c\u5f53\u65e0\u6cd5\u6392\u9664\u672a\u89c2\u5bdf\u5230\u7684\u6df7\u6742\u56e0\u7d20\u65f6\uff0c\u7814\u7a76\u5f02\u7b56\u7565\u5b66\u4e60\u3002", "method": "\u57fa\u4e8e\u8457\u540d\u7684Deep Q-Network\uff08DQN\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5bf9\u89c2\u5bdf\u6570\u636e\u4e2d\u7684\u6df7\u6742\u504f\u5dee\u5177\u6709\u9c81\u68d2\u6027\u3002\u7b97\u6cd5\u8bd5\u56fe\u627e\u5230\u6700\u574f\u60c5\u51b5\u73af\u5883\u4e0b\u7684\u5b89\u5168\u7b56\u7565\uff0c\u8be5\u73af\u5883\u4e0e\u89c2\u5bdf\u7ed3\u679c\u4e00\u81f4\u3002", "result": "\u572812\u4e2a\u6df7\u6742\u7684Atari\u6e38\u620f\u4e2d\u5e94\u7528\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u5b83\u5728\u6240\u6709\u884c\u4e3a\u548c\u76ee\u6807\u7b56\u7565\u7684\u89c2\u6d4b\u8f93\u5165\u4e0d\u5339\u914d\u4e14\u5b58\u5728\u672a\u89c2\u6d4b\u6df7\u6742\u56e0\u5b50\u7684\u6e38\u620f\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u6807\u51c6DQN\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u5904\u7406\u672a\u89c2\u5bdf\u5230\u7684\u6df7\u6742\u56e0\u7d20\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7684\u9ad8\u7ef4\u73af\u5883\u3002"}}
{"id": "2505.10894", "pdf": "https://arxiv.org/pdf/2505.10894", "abs": "https://arxiv.org/abs/2505.10894", "authors": ["Yishuo Wang", "Feng Zhou", "Muping Zhou", "Qicheng Meng", "Zhijun Hu", "Yi Wang"], "title": "CTP: A hybrid CNN-Transformer-PINN model for ocean front forecasting", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "This paper proposes CTP, a novel deep learning framework that integrates\nconvolutional neural network(CNN), Transformer architectures, and\nphysics-informed neural network(PINN) for ocean front prediction. Ocean fronts,\nas dynamic interfaces between distinct water masses, play critical roles in\nmarine biogeochemical and physical processes. Existing methods such as LSTM,\nConvLSTM, and AttentionConv often struggle to maintain spatial continuity and\nphysical consistency over multi-step forecasts. CTP addresses these challenges\nby combining localized spatial encoding, long-range temporal attention, and\nphysical constraint enforcement. Experimental results across south China\nsea(SCS) and Kuroshio(KUR) regions from 1993 to 2020 demonstrate that CTP\nachieves state-of-the-art(SOTA) performance in both single-step and multi-step\npredictions, significantly outperforming baseline models in accuracy, $F_1$\nscore, and temporal stability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6CTP\uff0c\u7528\u4e8e\u6d77\u6d0b\u950b\u9884\u6d4b\uff0c\u7ed3\u5408\u4e86CNN\u3001Transformer\u67b6\u6784\u548cPINN\uff0c\u5e76\u5728\u5357\u4e2d\u56fd\u6d77\u548c\u9ed1\u6f6e\u533a\u57df\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u6d77\u6d0b\u950b\u4f5c\u4e3a\u4e0d\u540c\u6c34\u4f53\u4e4b\u95f4\u7684\u52a8\u6001\u754c\u9762\uff0c\u5728\u6d77\u6d0b\u751f\u7269\u5730\u7403\u5316\u5b66\u548c\u7269\u7406\u8fc7\u7a0b\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6b65\u9884\u6d4b\u4e2d\u96be\u4ee5\u4fdd\u6301\u7a7a\u95f4\u8fde\u7eed\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\u3002", "method": "CTP\u6846\u67b6\u7ed3\u5408\u4e86\u5c40\u90e8\u7a7a\u95f4\u7f16\u7801\u3001\u957f\u671f\u65f6\u95f4\u6ce8\u610f\u548c\u7269\u7406\u7ea6\u675f\u5f3a\u5236\u6267\u884c\u3002", "result": "CTP\u5728\u5355\u6b65\u548c\u591a\u6b65\u9884\u6d4b\u4e2d\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u5357\u4e2d\u56fd\u6d77\u548c\u9ed1\u6f6e\u533a\u57df\u7684\u51c6\u786e\u6027\u3001F1\u5206\u6570\u548c\u65f6\u95f4\u7a33\u5b9a\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "CTP\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u9ad8\u6d77\u6d0b\u950b\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\u3002"}}
{"id": "2505.11481", "pdf": "https://arxiv.org/pdf/2505.11481", "abs": "https://arxiv.org/abs/2505.11481", "authors": ["Alayt Issak", "Jeba Rezwana", "Casper Harteveld"], "title": "MOSAAIC: Managing Optimization towards Shared Autonomy, Authority, and Initiative in Co-creation", "categories": ["cs.AI"], "comment": null, "summary": "Striking the appropriate balance between humans and co-creative AI is an open\nresearch question in computational creativity. Co-creativity, a form of hybrid\nintelligence where both humans and AI take action proactively, is a process\nthat leads to shared creative artifacts and ideas. Achieving a balanced dynamic\nin co-creativity requires characterizing control and identifying strategies to\ndistribute control between humans and AI. We define control as the power to\ndetermine, initiate, and direct the process of co-creation. Informed by a\nsystematic literature review of 172 full-length papers, we introduce MOSAAIC\n(Managing Optimization towards Shared Autonomy, Authority, and Initiative in\nCo-creation), a novel framework for characterizing and balancing control in\nco-creation. MOSAAIC identifies three key dimensions of control: autonomy,\ninitiative, and authority. We supplement our framework with control\noptimization strategies in co-creation. To demonstrate MOSAAIC's applicability,\nwe analyze the distribution of control in six existing co-creative AI case\nstudies and present the implications of using this framework.", "AI": {"tldr": "This paper introduces MOSAAIC, a new framework for balancing control in human-AI co-creative systems.", "motivation": "To explore how to strike a balance between humans and co-creative AI in computational creativity.", "method": "Systematic literature review of 172 papers and introducing MOSAAIC framework with three dimensions of control: autonomy, initiative, and authority.", "result": "MOSAAIC framework with optimization strategies and its application in analyzing six existing co-creative AI case studies.", "conclusion": "The proposed MOSAAIC framework provides a way to characterize and balance control in co-creative systems."}}
{"id": "2505.10913", "pdf": "https://arxiv.org/pdf/2505.10913", "abs": "https://arxiv.org/abs/2505.10913", "authors": ["Muntasir Hoq", "Ananya Rao", "Reisha Jaishankar", "Krish Piryani", "Nithya Janapati", "Jessica Vandenberg", "Bradford Mott", "Narges Norouzi", "James Lester", "Bita Akram"], "title": "Automated Identification of Logical Errors in Programs: Advancing Scalable Analysis of Student Misconceptions", "categories": ["cs.LG", "K.3.1"], "comment": "Accepted for publication at the 18th International Conference on\n  Educational Data Mining (EDM), 2025", "summary": "In Computer Science (CS) education, understanding factors contributing to\nstudents' programming difficulties is crucial for effective learning support.\nBy identifying specific issues students face, educators can provide targeted\nassistance to help them overcome obstacles and improve learning outcomes. While\nidentifying sources of struggle, such as misconceptions, in real-time can be\nchallenging in current educational practices, analyzing logical errors in\nstudents' code can offer valuable insights. This paper presents a scalable\nframework for automatically detecting logical errors in students' programming\nsolutions. Our framework is based on an explainable Abstract Syntax Tree (AST)\nembedding model, the Subtree-based Attention Neural Network (SANN), that\nidentifies the structural components of programs containing logical errors. We\nconducted a series of experiments to evaluate its effectiveness, and the\nresults suggest that our framework can accurately capture students' logical\nerrors and, more importantly, provide us with deeper insights into their\nlearning processes, offering a valuable tool for enhancing programming\neducation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b50\u6811\u6ce8\u610f\u529b\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u5b66\u751f\u7f16\u7a0b\u89e3\u51b3\u65b9\u6848\u4e2d\u7684\u903b\u8f91\u9519\u8bef\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5bf9\u5b66\u4e60\u8fc7\u7a0b\u7684\u6d1e\u5bdf\u529b\u3002", "motivation": "\u7406\u89e3\u5bfc\u81f4\u5b66\u751f\u7f16\u7a0b\u56f0\u96be\u7684\u56e0\u7d20\u5bf9\u4e8e\u6709\u6548\u7684\u5b66\u4e60\u652f\u6301\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5b50\u6811\u6ce8\u610f\u529b\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u6269\u5c55\u6846\u67b6\u6765\u81ea\u52a8\u68c0\u6d4b\u903b\u8f91\u9519\u8bef\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u591f\u51c6\u786e\u6355\u6349\u903b\u8f91\u9519\u8bef\u5e76\u63d0\u4f9b\u5bf9\u5b66\u4e60\u8fc7\u7a0b\u7684\u6df1\u5165\u89c1\u89e3\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u63d0\u9ad8\u7f16\u7a0b\u6559\u80b2\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u5de5\u5177\u3002"}}
{"id": "2505.10928", "pdf": "https://arxiv.org/pdf/2505.10928", "abs": "https://arxiv.org/abs/2505.10928", "authors": ["Xiao Han", "Dayan Pan", "Xiangyu Zhao", "Xuyuan Hu", "Zhaolin Deng", "Xiangjie Kong", "Guojiang Shen"], "title": "A Dataset for Spatiotemporal-Sensitive POI Question Answering", "categories": ["cs.LG"], "comment": "Under Review", "summary": "Spatiotemporal relationships are critical in data science, as many prediction\nand reasoning tasks require analysis across both spatial and temporal\ndimensions--for instance, navigating an unfamiliar city involves planning\nitineraries that sequence locations and timing cultural experiences. However,\nexisting Question-Answering (QA) datasets lack sufficient\nspatiotemporal-sensitive questions, making them inadequate benchmarks for\nevaluating models' spatiotemporal reasoning capabilities. To address this gap,\nwe introduce POI-QA, a novel spatiotemporal-sensitive QA dataset centered on\nPoint of Interest (POI), constructed through three key steps: mining and\naligning open-source vehicle trajectory data from GAIA with high-precision\ngeographic POI data, rigorous manual validation of noisy spatiotemporal facts,\nand generating bilingual (Chinese/English) QA pairs that reflect\nhuman-understandable spatiotemporal reasoning tasks. Our dataset challenges\nmodels to parse complex spatiotemporal dependencies, and evaluations of\nstate-of-the-art multilingual LLMs (e.g., Qwen2.5-7B, Llama3.1-8B) reveal stark\nlimitations: even the top-performing model (Qwen2.5-7B fine-tuned with\nRAG+LoRA) achieves a top 10 Hit Ratio (HR@10) of only 0.41 on the easiest task,\nfar below human performance at 0.56. This underscores persistent weaknesses in\nLLMs' ability to perform consistent spatiotemporal reasoning, while\nhighlighting POI-QA as a robust benchmark to advance algorithms sensitive to\nspatiotemporal dynamics. The dataset is publicly available at\nhttps://www.kaggle.com/ds/7394666.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPOI-QA\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5174\u8da3\u70b9\u7684\u65b0\u578b\u65f6\u7a7a\u654f\u611f\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u7684\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6700\u5148\u8fdb\u7684\u591a\u8bed\u8a00\u5927\u6a21\u578b\u5728\u6b64\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u73b0\u6709\u95ee\u7b54\u6570\u636e\u96c6\u4e0d\u8db3\u4ee5\u8bc4\u4f30\u6a21\u578b\u7684\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\uff0c\u56e0\u4e3a\u5b83\u4eec\u7f3a\u4e4f\u8db3\u591f\u7684\u65f6\u7a7a\u654f\u611f\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u6b65\u9aa4\u6784\u5efaPOI-QA\u6570\u636e\u96c6\uff1a1\uff09\u6316\u6398\u5e76\u6574\u5408\u5f00\u6e90\u8f66\u8f86\u8f68\u8ff9\u6570\u636e\u4e0e\u9ad8\u7cbe\u5ea6\u5730\u7406POI\u6570\u636e\uff1b2\uff09\u5bf9\u566a\u58f0\u65f6\u7a7a\u4e8b\u5b9e\u8fdb\u884c\u4e25\u683c\u7684\u4eba\u5de5\u9a8c\u8bc1\uff1b3\uff09\u751f\u6210\u53cd\u6620\u4eba\u7c7b\u53ef\u7406\u89e3\u65f6\u7a7a\u63a8\u7406\u4efb\u52a1\u7684\u53cc\u8bed\uff08\u4e2d\u6587/\u82f1\u6587\uff09\u95ee\u7b54\u5bf9\u3002", "result": "POI-QA\u6570\u636e\u96c6\u6311\u6218\u4e86\u6a21\u578b\u89e3\u6790\u590d\u6742\u65f6\u7a7a\u4f9d\u8d56\u7684\u80fd\u529b\uff0c\u6d4b\u8bd5\u663e\u793a\u5373\u4f7f\u662f\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\uff08\u7ecf\u8fc7RAG+LoRA\u5fae\u8c03\u7684Qwen2.5-7B\uff09\u5728\u6700\u7b80\u5355\u4efb\u52a1\u4e0a\u7684Top 10\u547d\u4e2d\u7387\uff08HR@10\uff09\u4ec5\u4e3a0.41\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u7684\u8868\u73b00.56\u3002", "conclusion": "\u73b0\u6709\u7684\u95ee\u7b54\u6570\u636e\u96c6\u7f3a\u4e4f\u8db3\u591f\u7684\u65f6\u7a7a\u654f\u611f\u95ee\u9898\uff0c\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u6a21\u578b\u7684\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u7684\u65f6\u7a7a\u654f\u611f\u95ee\u7b54\u6570\u636e\u96c6POI-QA\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u591a\u8bed\u8a00\u5927\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u65f6\u7a7a\u4f9d\u8d56\u65f6\u5b58\u5728\u660e\u663e\u5c40\u9650\u6027\u3002"}}
{"id": "2505.10930", "pdf": "https://arxiv.org/pdf/2505.10930", "abs": "https://arxiv.org/abs/2505.10930", "authors": ["Congcong Zhu", "Xiaoyan Xu", "Jiayue Han", "Jingrun Chen"], "title": "Physics-informed Temporal Alignment for Auto-regressive PDE Foundation Models", "categories": ["cs.LG", "35Q68", "G.1.8"], "comment": "Accepted as a conference paper in ICML2025", "summary": "Auto-regressive partial differential equation (PDE) foundation models have\nshown great potential in handling time-dependent data. However, these models\nsuffer from the shortcut problem deeply rooted in auto-regressive prediction,\ncausing error accumulation. The challenge becomes particularly evident for\nout-of-distribution data, as the pretraining performance may approach random\nmodel initialization for downstream tasks with long-term dynamics. To deal with\nthis problem, we propose physics-informed temporal alignment (PITA), a\nself-supervised learning framework inspired by inverse problem solving.\nSpecifically, PITA aligns the physical dynamics discovered at different time\nsteps on each given PDE trajectory by integrating physics-informed constraints\ninto the self-supervision signal. The alignment is derived from observation\ndata without relying on known physics priors, indicating strong generalization\nability to the out-of-distribution data. Extensive experiments show that PITA\nsignificantly enhances the accuracy and robustness of existing foundation\nmodels on diverse time-dependent PDE data. The code is available at\nhttps://github.com/SCAILab-USTC/PITA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u65f6\u95f4\u5bf9\u9f50(PITA)\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u81ea\u52a8\u56de\u5f52\u504f\u5fae\u5206\u65b9\u7a0b\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u56de\u5f52\u9884\u6d4b\u4e2d\u5b58\u5728\u7684shortcut\u95ee\u9898\u4ee5\u53ca\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u7684\u8bef\u5dee\u79ef\u7d2f\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5728\u81ea\u76d1\u7763\u4fe1\u53f7\u4e2d\u6574\u5408\u7269\u7406\u4fe1\u606f\u7ea6\u675f\u6765\u5bf9\u9f50\u4e0d\u540c\u65f6\u95f4\u6b65\u7684\u7269\u7406\u52a8\u529b\u5b66\u3002", "result": "\u663e\u8457\u63d0\u9ad8\u4e86\u73b0\u6709\u57fa\u7840\u6a21\u578b\u5728\u5404\u79cd\u65f6\u53d8\u504f\u5fae\u5206\u65b9\u7a0b\u6570\u636e\u4e0a\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684PITA\u6846\u67b6\u5c55\u793a\u4e86\u5176\u5728\u5904\u7406\u65f6\u53d8\u6570\u636e\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u5177\u6709\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.10941", "pdf": "https://arxiv.org/pdf/2505.10941", "abs": "https://arxiv.org/abs/2505.10941", "authors": ["Ozan \u00d6zdenizci", "Elmar Rueckert", "Robert Legenstein"], "title": "Privacy-Aware Lifelong Learning", "categories": ["cs.LG"], "comment": null, "summary": "Lifelong learning algorithms enable models to incrementally acquire new\nknowledge without forgetting previously learned information. Contrarily, the\nfield of machine unlearning focuses on explicitly forgetting certain previous\nknowledge from pretrained models when requested, in order to comply with data\nprivacy regulations on the right-to-be-forgotten. Enabling efficient lifelong\nlearning with the capability to selectively unlearn sensitive information from\nmodels presents a critical and largely unaddressed challenge with contradicting\nobjectives. We address this problem from the perspective of simultaneously\npreventing catastrophic forgetting and allowing forward knowledge transfer\nduring task-incremental learning, while ensuring exact task unlearning and\nminimizing memory requirements, based on a single neural network model to be\nadapted. Our proposed solution, privacy-aware lifelong learning (PALL),\ninvolves optimization of task-specific sparse subnetworks with parameter\nsharing within a single architecture. We additionally utilize an episodic\nmemory rehearsal mechanism to facilitate exact unlearning without performance\ndegradations. We empirically demonstrate the scalability of PALL across various\narchitectures in image classification, and provide a state-of-the-art solution\nthat uniquely integrates lifelong learning and privacy-aware unlearning\nmechanisms for responsible AI applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPALL\u7684\u9690\u79c1\u611f\u77e5\u7ec8\u8eab\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5728\u589e\u91cf\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3001\u5141\u8bb8\u524d\u5411\u77e5\u8bc6\u8fc1\u79fb\u7684\u540c\u65f6\u7cbe\u786e\u4efb\u52a1\u9057\u5fd8\u548c\u6700\u5c0f\u5316\u5185\u5b58\u9700\u6c42\u7684\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u7279\u5b9a\u4efb\u52a1\u7684\u7a00\u758f\u5b50\u7f51\u7edc\u5e76\u5229\u7528\u60c5\u666f\u8bb0\u5fc6\u91cd\u653e\u673a\u5236\uff0c\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u5c55\u793a\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u4f7f\u6a21\u578b\u80fd\u591f\u9009\u62e9\u6027\u5730\u5fd8\u8bb0\u654f\u611f\u4fe1\u606f\u4ee5\u9075\u5b88\u6570\u636e\u9690\u79c1\u6cd5\u89c4\u3002", "method": "\u4f18\u5316\u4efb\u52a1\u7279\u5b9a\u7684\u7a00\u758f\u5b50\u7f51\u7edc\uff0c\u53c2\u6570\u5171\u4eab\uff0c\u5e76\u4f7f\u7528\u60c5\u666f\u8bb0\u5fc6\u91cd\u653e\u673a\u5236\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u5c55\u793a\u4e86PALL\u7684\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u96c6\u6210\u4e86\u7ec8\u8eab\u5b66\u4e60\u548c\u9690\u79c1\u611f\u77e5\u9057\u5fd8\u673a\u5236\u7684\u6700\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "PALL\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6846\u67b6\u6765\u89e3\u51b3\u7ec8\u8eab\u5b66\u4e60\u548c\u9690\u79c1\u4fdd\u62a4\u4e4b\u95f4\u7684\u77db\u76fe\u76ee\u6807\u3002"}}
{"id": "2505.10947", "pdf": "https://arxiv.org/pdf/2505.10947", "abs": "https://arxiv.org/abs/2505.10947", "authors": ["Kehan Long", "Jorge Cort\u00e9s", "Nikolay Atanasov"], "title": "Certifying Stability of Reinforcement Learning Policies using Generalized Lyapunov Functions", "categories": ["cs.LG", "cs.RO", "cs.SY", "eess.SY", "math.OC"], "comment": null, "summary": "We study the problem of certifying the stability of closed-loop systems under\ncontrol policies derived from optimal control or reinforcement learning (RL).\nClassical Lyapunov methods require a strict step-wise decrease in the Lyapunov\nfunction but such a certificate is difficult to construct for a learned control\npolicy. The value function associated with an RL policy is a natural Lyapunov\nfunction candidate but it is not clear how it should be modified. To gain\nintuition, we first study the linear quadratic regulator (LQR) problem and make\ntwo key observations. First, a Lyapunov function can be obtained from the value\nfunction of an LQR policy by augmenting it with a residual term related to the\nsystem dynamics and stage cost. Second, the classical Lyapunov decrease\nrequirement can be relaxed to a generalized Lyapunov condition requiring only\ndecrease on average over multiple time steps. Using this intuition, we consider\nthe nonlinear setting and formulate an approach to learn generalized Lyapunov\nfunctions by augmenting RL value functions with neural network residual terms.\nOur approach successfully certifies the stability of RL policies trained on\nGymnasium and DeepMind Control benchmarks. We also extend our method to jointly\ntrain neural controllers and stability certificates using a multi-step Lyapunov\nloss, resulting in larger certified inner approximations of the region of\nattraction compared to the classical Lyapunov approach. Overall, our\nformulation enables stability certification for a broad class of systems with\nlearned policies by making certificates easier to construct, thereby bridging\nclassical control theory and modern learning-based methods.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4ece\u6700\u4f18\u63a7\u5236\u6216\u5f3a\u5316\u5b66\u4e60\u5f97\u51fa\u7684\u63a7\u5236\u7b56\u7565\u4e0b\u7684\u95ed\u73af\u7cfb\u7edf\u7a33\u5b9a\u6027\u8ba4\u8bc1\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u901a\u8fc7\u5b66\u4e60\u5e7f\u4e49Lyapunov\u51fd\u6570\u6765\u9a8c\u8bc1\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6210\u679c\uff0c\u540c\u65f6\u6539\u8fdb\u4e86\u7a33\u5b9a\u6027\u8bc1\u4e66\u7684\u6784\u5efa\u65b9\u5f0f\u3002", "motivation": "\u7814\u7a76\u5728\u4ece\u6700\u4f18\u63a7\u5236\u6216\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5f97\u51fa\u7684\u63a7\u5236\u7b56\u7565\u4e0b\u7684\u95ed\u73af\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u8ba4\u8bc1\u95ee\u9898\u3002\u7531\u4e8e\u5bf9\u4e8e\u5b66\u4e60\u5230\u7684\u63a7\u5236\u7b56\u7565\u96be\u4ee5\u6784\u9020\u4e25\u683c\u7684Lyapunov\u51fd\u6570\u4e0b\u964d\u6b65\u9aa4\uff0c\u56e0\u6b64\u9700\u8981\u5bfb\u627e\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u5728\u975e\u7ebf\u6027\u8bbe\u5b9a\u4e0b\u8003\u8651\u5e76\u5236\u5b9a\u4e86\u901a\u8fc7\u5728RL\u4ef7\u503c\u51fd\u6570\u4e2d\u52a0\u5165\u795e\u7ecf\u7f51\u7edc\u6b8b\u5dee\u9879\u6765\u5b66\u4e60\u5e7f\u4e49Lyapunov\u51fd\u6570\u7684\u65b9\u6cd5\uff1b\u6269\u5c55\u65b9\u6cd5\u4ee5\u8054\u5408\u8bad\u7ec3\u795e\u7ecf\u63a7\u5236\u5668\u548c\u7a33\u5b9a\u6027\u8bc1\u4e66\u4f7f\u7528\u591a\u6b65Lyapunov\u635f\u5931\u3002", "result": "\u6210\u529f\u9a8c\u8bc1\u4e86Gymnasium\u548cDeepMind Control\u57fa\u51c6\u4e0a\u7684\u7a33\u5b9a\u6027\uff1b\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u795e\u7ecf\u63a7\u5236\u5668\u548c\u7a33\u5b9a\u6027\u8bc1\u4e66\uff0c\u5f97\u5230\u4e86\u6bd4\u7ecf\u5178Lyapunov\u65b9\u6cd5\u66f4\u5927\u7684\u8ba4\u8bc1\u5438\u5f15\u57df\u5185\u8fd1\u4f3c\u503c\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5728RL\u4ef7\u503c\u51fd\u6570\u4e2d\u52a0\u5165\u795e\u7ecf\u7f51\u7edc\u6b8b\u5dee\u9879\u6765\u5b66\u4e60\u5e7f\u4e49Lyapunov\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u5e76\u6210\u529f\u9a8c\u8bc1\u4e86Gymnasium\u548cDeepMind Control\u57fa\u51c6\u4e0a\u7684\u7a33\u5b9a\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u6269\u5c55\u4e86\u65b9\u6cd5\u4ee5\u8054\u5408\u8bad\u7ec3\u795e\u7ecf\u63a7\u5236\u5668\u548c\u7a33\u5b9a\u6027\u8bc1\u4e66\uff0c\u83b7\u5f97\u4e86\u6bd4\u7ecf\u5178Lyapunov\u65b9\u6cd5\u66f4\u5927\u7684\u8ba4\u8bc1\u5438\u5f15\u57df\u5185\u8fd1\u4f3c\u503c\u3002\u6574\u4f53\u4e0a\uff0c\u8be5\u516c\u5f0f\u4f7f\u5f97\u66f4\u6613\u4e8e\u6784\u5efa\u8bc1\u4e66\uff0c\u4ece\u800c\u5f25\u5408\u4e86\u7ecf\u5178\u63a7\u5236\u7406\u8bba\u4e0e\u73b0\u4ee3\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2505.10949", "pdf": "https://arxiv.org/pdf/2505.10949", "abs": "https://arxiv.org/abs/2505.10949", "authors": ["Chenhui Xu", "Dancheng Liu", "Amir Nassereldine", "Jinjun Xiong"], "title": "FP64 is All You Need: Rethinking Failure Modes in Physics-Informed Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Physics Informed Neural Networks (PINNs) often exhibit failure modes in which\nthe PDE residual loss converges while the solution error stays large, a\nphenomenon traditionally blamed on local optima separated from the true\nsolution by steep loss barriers. We challenge this understanding by demonstrate\nthat the real culprit is insufficient arithmetic precision: with standard FP32,\nthe LBFGS optimizer prematurely satisfies its convergence test, freezing the\nnetwork in a spurious failure phase. Simply upgrading to FP64 rescues\noptimization, enabling vanilla PINNs to solve PDEs without any failure modes.\nThese results reframe PINN failure modes as precision induced stalls rather\nthan inescapable local minima and expose a three stage training dynamic\nunconverged, failure, success whose boundaries shift with numerical precision.\nOur findings emphasize that rigorous arithmetic precision is the key to\ndependable PDE solving with neural networks.", "AI": {"tldr": "This paper challenges the traditional understanding of Physics Informed Neural Networks (PINNs) failure modes, suggesting that insufficient arithmetic precision is the main issue instead of local optima.", "motivation": "Investigate why PINNs often fail despite the PDE residual loss converging.", "method": "Upgrade from FP32 to FP64 to observe changes in optimization.", "result": "Upgrading to FP64 improves optimization, allowing PINNs to solve PDEs without failure modes.", "conclusion": "The study concludes that arithmetic precision is crucial for reliable PDE solving using neural networks."}}
{"id": "2505.10950", "pdf": "https://arxiv.org/pdf/2505.10950", "abs": "https://arxiv.org/abs/2505.10950", "authors": ["Tianshuo Zhang", "Gao Jia", "Wenzhe Zhai", "Rui Yann", "Xianglei Xing"], "title": "Shackled Dancing: A Bit-Locked Diffusion Algorithm for Lossless and Controllable Image Steganography", "categories": ["cs.LG"], "comment": null, "summary": "Data steganography aims to conceal information within visual content, yet\nexisting spatial- and frequency-domain approaches suffer from trade-offs\nbetween security, capacity, and perceptual quality. Recent advances in\ngenerative models, particularly diffusion models, offer new avenues for\nadaptive image synthesis, but integrating precise information embedding into\nthe generative process remains challenging. We introduce Shackled Dancing\nDiffusion, or SD$^2$, a plug-and-play generative steganography method that\ncombines bit-position locking with diffusion sampling injection to enable\ncontrollable information embedding within the generative trajectory. SD$^2$\nleverages the expressive power of diffusion models to synthesize diverse\ncarrier images while maintaining full message recovery with $100\\%$ accuracy.\nOur method achieves a favorable balance between randomness and constraint,\nenhancing robustness against steganalysis without compromising image fidelity.\nExtensive experiments show that SD$^2$ substantially outperforms prior methods\nin security, embedding capacity, and stability. This algorithm offers new\ninsights into controllable generation and opens promising directions for secure\nvisual communication.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aShackled Dancing Diffusion (SD$^2$) \u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u53ef\u63a7\u5730\u5d4c\u5165\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86100%\u7684\u6d88\u606f\u6062\u590d\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u3001\u5d4c\u5165\u5bb9\u91cf\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u7a7a\u95f4\u57df\u548c\u9891\u7387\u57df\u6570\u636e\u9690\u85cf\u65b9\u6cd5\u5728\u5b89\u5168\u3001\u5bb9\u91cf\u548c\u611f\u77e5\u8d28\u91cf\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002\u867d\u7136\u751f\u6210\u6a21\u578b\uff08\u5c24\u5176\u662f\u6269\u6563\u6a21\u578b\uff09\u7684\u8fdb\u6b65\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u4f46\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u7cbe\u786e\u5d4c\u5165\u4fe1\u606f\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u7ed3\u5408\u4f4d\u4f4d\u7f6e\u9501\u5b9a\u4e0e\u6269\u6563\u91c7\u6837\u6ce8\u5165\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aShackled Dancing Diffusion (SD$^2$) \u7684\u63d2\u4ef6\u5f0f\u751f\u6210\u9690\u85cf\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u5408\u6210\u591a\u6837\u5316\u7684\u8f7d\u4f53\u56fe\u50cf\u5e76\u5b9e\u73b0\u5168\u6d88\u606f\u6062\u590d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSD$^2$\u5728\u5b89\u5168\u6027\u3001\u5d4c\u5165\u5bb9\u91cf\u548c\u7a33\u5b9a\u6027\u4e0a\u5927\u5e45\u8d85\u8d8a\u5148\u524d\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u56fe\u50cf\u4fdd\u771f\u5ea6\u3002", "conclusion": "SD$^2$\u4e3a\u53ef\u63a7\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5e76\u4e3a\u5b89\u5168\u89c6\u89c9\u901a\u4fe1\u5f00\u8f9f\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2505.10951", "pdf": "https://arxiv.org/pdf/2505.10951", "abs": "https://arxiv.org/abs/2505.10951", "authors": ["Qiuyu Zhu", "Liang Zhang", "Qianxiong Xu", "Cheng Long", "Jie Zhang"], "title": "SubGCache: Accelerating Graph-based RAG with Subgraph-level KV Cache", "categories": ["cs.LG"], "comment": null, "summary": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to incorporate structured knowledge via graph retrieval as\ncontextual input, enhancing more accurate and context-aware reasoning. We\nobserve that for different queries, it could retrieve similar subgraphs as\nprompts, and thus we propose SubGCache, which aims to reduce inference latency\nby reusing computation across queries with similar structural prompts (i.e.,\nsubgraphs). Specifically, SubGCache clusters queries based on subgraph\nembeddings, constructs a representative subgraph for each cluster, and\npre-computes the key-value (KV) cache of the representative subgraph. For each\nquery with its retrieved subgraph within a cluster, it reuses the pre-computed\nKV cache of the representative subgraph of the cluster without computing the KV\ntensors again for saving computation. Experiments on two new datasets across\nmultiple LLM backbones and graph-based RAG frameworks demonstrate that\nSubGCache consistently reduces inference latency with comparable and even\nimproved generation quality, achieving up to 6.68$\\times$ reduction in\ntime-to-first-token (TTFT).", "AI": {"tldr": "\u63d0\u51faSubGCache\u65b9\u6cd5\u901a\u8fc7\u91cd\u7528\u76f8\u4f3c\u7ed3\u6784\u67e5\u8be2\u7684\u8ba1\u7b97\u6765\u51cf\u5c11\u57fa\u4e8e\u56fe\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u5ef6\u8fdf\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6846\u67b6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89c2\u5bdf\u5230\u4e0d\u540c\u67e5\u8be2\u53ef\u80fd\u68c0\u7d22\u5230\u76f8\u4f3c\u5b50\u56fe\u4f5c\u4e3a\u63d0\u793a\uff0c\u5e0c\u671b\u51cf\u5c11\u63a8\u7406\u5ef6\u8fdf\u3002", "method": "\u63d0\u51faSubGCache\u65b9\u6cd5\uff0c\u5305\u62ec\u805a\u7c7b\u67e5\u8be2\u3001\u6784\u5efa\u4ee3\u8868\u5b50\u56fe\u5e76\u9884\u8ba1\u7b97KV\u7f13\u5b58\uff0c\u5bf9\u4e8e\u5c5e\u4e8e\u540c\u4e00\u7c07\u7684\u67e5\u8be2\u91cd\u7528\u4ee3\u8868\u5b50\u56fe\u7684\u9884\u8ba1\u7b97KV\u7f13\u5b58\u3002", "result": "\u5728\u4e24\u4e2a\u65b0\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793aSubGCache\u80fd\u6709\u6548\u51cf\u5c11\u63a8\u7406\u5ef6\u8fdf\u4e14\u751f\u6210\u8d28\u91cf\u4e0d\u964d\u53cd\u5347\uff0c\u65f6\u95f4-\u9996\u4e2a\u6807\u8bb0\uff08TTFT\uff09\u6700\u591a\u53ef\u51cf\u5c116.68\u500d\u3002", "conclusion": "SubGCache\u662f\u4e00\u79cd\u6709\u6548\u7684\u51cf\u5c11\u57fa\u4e8e\u56fe\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u5ef6\u8fdf\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2505.10954", "pdf": "https://arxiv.org/pdf/2505.10954", "abs": "https://arxiv.org/abs/2505.10954", "authors": ["Koki Iwai", "Yusuke Kumagae", "Yuki Koyama", "Masahiro Hamasaki", "Masataka Goto"], "title": "Constrained Preferential Bayesian Optimization and Its Application in Banner Ad Design", "categories": ["cs.LG", "cs.AI", "cs.GR", "cs.HC"], "comment": "17 pages, 15 figures", "summary": "Preferential Bayesian optimization (PBO) is a variant of Bayesian\noptimization that observes relative preferences (e.g., pairwise comparisons)\ninstead of direct objective values, making it especially suitable for\nhuman-in-the-loop scenarios. However, real-world optimization tasks often\ninvolve inequality constraints, which existing PBO methods have not yet\naddressed. To fill this gap, we propose constrained preferential Bayesian\noptimization (CPBO), an extension of PBO that incorporates inequality\nconstraints for the first time. Specifically, we present a novel acquisition\nfunction for this purpose. Our technical evaluation shows that our CPBO method\nsuccessfully identifies optimal solutions by focusing on exploring feasible\nregions. As a practical application, we also present a designer-in-the-loop\nsystem for banner ad design using CPBO, where the objective is the designer's\nsubjective preference, and the constraint ensures a target predicted\nclick-through rate. We conducted a user study with professional ad designers,\ndemonstrating the potential benefits of our approach in guiding creative design\nunder real-world constraints.", "AI": {"tldr": "This paper proposes Constrained Preferential Bayesian Optimization (CPBO) to handle inequality constraints in human-in-the-loop optimization tasks, introducing a novel acquisition function and demonstrating its effectiveness in a banner ad design application.", "motivation": "Existing Preferential Bayesian Optimization (PBO) methods do not address inequality constraints, which are common in real-world optimization tasks.", "method": "CPBO extends PBO by incorporating inequality constraints through a novel acquisition function.", "result": "Technical evaluation shows CPBO successfully identifies optimal solutions in feasible regions.", "conclusion": "The proposed CPBO method demonstrates potential in guiding creative design under real-world constraints."}}
{"id": "2505.10960", "pdf": "https://arxiv.org/pdf/2505.10960", "abs": "https://arxiv.org/abs/2505.10960", "authors": ["Vijay Prakash Dwivedi", "Sri Jaladi", "Yangyi Shen", "Federico L\u00f3pez", "Charilaos I. Kanatsoulis", "Rishi Puri", "Matthias Fey", "Jure Leskovec"], "title": "Relational Graph Transformer", "categories": ["cs.LG", "cs.AI", "cs.DB"], "comment": "Code: https://github.com/snap-stanford/relgt", "summary": "Relational Deep Learning (RDL) is a promising approach for building\nstate-of-the-art predictive models on multi-table relational data by\nrepresenting it as a heterogeneous temporal graph. However, commonly used Graph\nNeural Network models suffer from fundamental limitations in capturing complex\nstructural patterns and long-range dependencies that are inherent in relational\ndata. While Graph Transformers have emerged as powerful alternatives to GNNs on\ngeneral graphs, applying them to relational entity graphs presents unique\nchallenges: (i) Traditional positional encodings fail to generalize to massive,\nheterogeneous graphs; (ii) existing architectures cannot model the temporal\ndynamics and schema constraints of relational data; (iii) existing tokenization\nschemes lose critical structural information. Here we introduce the Relational\nGraph Transformer (RelGT), the first graph transformer architecture designed\nspecifically for relational tables. RelGT employs a novel multi-element\ntokenization strategy that decomposes each node into five components (features,\ntype, hop distance, time, and local structure), enabling efficient encoding of\nheterogeneity, temporality, and topology without expensive precomputation. Our\narchitecture combines local attention over sampled subgraphs with global\nattention to learnable centroids, incorporating both local and database-wide\nrepresentations. Across 21 tasks from the RelBench benchmark, RelGT\nconsistently matches or outperforms GNN baselines by up to 18%, establishing\nGraph Transformers as a powerful architecture for Relational Deep Learning.", "AI": {"tldr": "Relational Graph Transformer (RelGT) is proposed for relational tables, improving performance by up to 18% compared to traditional Graph Neural Networks.", "motivation": "Traditional Graph Neural Networks struggle to capture complex patterns and long-range dependencies in relational data. Graph Transformers show promise but face challenges when applied to relational entity graphs.", "method": "Introduces RelGT, a new graph transformer architecture using a multi-element tokenization strategy to handle heterogeneity, temporality, and topology efficiently without expensive precomputation. Combines local and global attention mechanisms.", "result": "Performs better than GNN baselines across 21 tasks from the RelBench benchmark, improving by up to 18%.", "conclusion": "Establishes Graph Transformers as a strong architecture for Relational Deep Learning."}}
{"id": "2505.10978", "pdf": "https://arxiv.org/pdf/2505.10978", "abs": "https://arxiv.org/abs/2505.10978", "authors": ["Lang Feng", "Zhenghai Xue", "Tingcong Liu", "Bo An"], "title": "Group-in-Group Policy Optimization for LLM Agent Training", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint", "summary": "Recent advances in group-based reinforcement learning (RL) have driven\nfrontier large language models (LLMs) in single-turn tasks like mathematical\nreasoning. However, their scalability to long-horizon LLM agent training\nremains limited. Unlike static tasks, agent-environment interactions unfold\nover many steps and often yield sparse or delayed rewards, making credit\nassignment across individual steps significantly more challenging. In this\nwork, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL\nalgorithm that achieves fine-grained credit assignment for LLM agents while\npreserving the appealing properties of group-based RL: critic-free, low memory,\nand stable convergence. GiGPO introduces a two-level structure for estimating\nrelative advantage: (i) At the episode-level, GiGPO computes macro relative\nadvantages based on groups of complete trajectories; (ii) At the step-level,\nGiGPO introduces an anchor state grouping mechanism that retroactively\nconstructs step-level groups by identifying repeated environment states across\ntrajectories. Actions stemming from the same state are grouped together,\nenabling micro relative advantage estimation. This hierarchical structure\neffectively captures both global trajectory quality and local step\neffectiveness without relying on auxiliary models or additional rollouts. We\nevaluate GiGPO on two challenging agent benchmarks, ALFWorld and WebShop, using\nQwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct. Crucially, GiGPO delivers\nfine-grained per-step credit signals and achieves performance gains of > 12\\%\non ALFWorld and > 9\\% on WebShop over the GRPO baseline: all while maintaining\nthe same GPU memory overhead, identical LLM rollout, and incurring little to no\nadditional time cost.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u7ec4\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5GiGPO\uff0c\u8be5\u7b97\u6cd5\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u7684\u4fe1\u7528\u5206\u914d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5206\u7ec4\u5f3a\u5316\u5b66\u4e60\u7684\u4f18\u70b9\uff0c\u5e76\u5728\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u7ec4\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u957f\u671f\u76ee\u6807\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u7a00\u758f\u6216\u5ef6\u8fdf\u5956\u52b1\u7684\u60c5\u51b5\u4e0b\uff0c\u4fe1\u7528\u5206\u914d\u53d8\u5f97\u66f4\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86GiGPO\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u4e24\u5c42\u7ed3\u6784\u6765\u4f30\u8ba1\u76f8\u5bf9\u4f18\u52bf\uff0c\u5373\u5728episode\u7ea7\u522b\u548cstep\u7ea7\u522b\u4e0a\u5206\u522b\u8ba1\u7b97\u5b8f\u89c2\u548c\u5fae\u89c2\u7684\u76f8\u5bf9\u4f18\u52bf\u3002", "result": "GiGPO\u7b97\u6cd5\u5728ALFWorld\u548cWebShop\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u8f83\u4e8eGRPO\u57fa\u7ebf\uff0c\u5728\u6027\u80fd\u4e0a\u5206\u522b\u63d0\u5347\u4e86\u8d85\u8fc712%\u548c9%\uff0c\u5e76\u4e14\u4fdd\u6301\u4e86\u76f8\u540c\u7684GPU\u5185\u5b58\u5f00\u9500\u548c\u76f8\u540c\u7684LLM rollout\uff0c\u51e0\u4e4e\u4e0d\u589e\u52a0\u989d\u5916\u7684\u65f6\u95f4\u6210\u672c\u3002", "conclusion": "GiGPO\u6210\u529f\u5730\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u7684\u4fe1\u7528\u5206\u914d\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u671f\u76ee\u6807\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.10983", "pdf": "https://arxiv.org/pdf/2505.10983", "abs": "https://arxiv.org/abs/2505.10983", "authors": ["Haozheng Luo", "Chenghao Qiu", "Yimin Wang", "Shang Wu", "Jiahao Yu", "Han Liu", "Binghui Wang", "Yan Chen"], "title": "GenoArmory: A Unified Evaluation Framework for Adversarial Attacks on Genomic Foundation Models", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "We propose the first unified adversarial attack benchmark for Genomic\nFoundation Models (GFMs), named GenoArmory. Unlike existing GFM benchmarks,\nGenoArmory offers the first comprehensive evaluation framework to\nsystematically assess the vulnerability of GFMs to adversarial attacks.\nMethodologically, we evaluate the adversarial robustness of five\nstate-of-the-art GFMs using four widely adopted attack algorithms and three\ndefense strategies. Importantly, our benchmark provides an accessible and\ncomprehensive framework to analyze GFM vulnerabilities with respect to model\narchitecture, quantization schemes, and training datasets. Additionally, we\nintroduce GenoAdv, a new adversarial sample dataset designed to improve GFM\nsafety. Empirically, classification models exhibit greater robustness to\nadversarial perturbations compared to generative models, highlighting the\nimpact of task type on model vulnerability. Moreover, adversarial attacks\nfrequently target biologically significant genomic regions, suggesting that\nthese models effectively capture meaningful sequence features.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u9488\u5bf9\u57fa\u56e0\u7ec4\u57fa\u7840\u6a21\u578b\uff08GFMs\uff09\u7684\u7edf\u4e00\u5bf9\u6297\u6027\u653b\u51fb\u57fa\u51c6\u2014\u2014GenoArmory\uff0c\u8bc4\u4f30\u4e86\u4e94\u79cd\u6700\u5148\u8fdbGFMs\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u5bf9\u6297\u6837\u672c\u6570\u636e\u96c6GenoAdv\u3002", "motivation": "\u73b0\u6709GFMs\u57fa\u51c6\u672a\u63d0\u4f9b\u5168\u9762\u8bc4\u4f30\u6846\u67b6\uff0cGenoArmory\u586b\u8865\u6b64\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u56db\u79cd\u5e38\u7528\u653b\u51fb\u7b97\u6cd5\u548c\u4e09\u79cd\u9632\u5fa1\u7b56\u7565\u8bc4\u4f30GFMs\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5206\u7c7b\u6a21\u578b\u6bd4\u751f\u6210\u6a21\u578b\u5bf9\u5bf9\u6297\u6270\u52a8\u66f4\u9c81\u68d2\uff1b\u5bf9\u6297\u653b\u51fb\u5e38\u9488\u5bf9\u751f\u7269\u663e\u8457\u7684\u57fa\u56e0\u7ec4\u533a\u57df\u3002", "conclusion": "GenoArmory\u63d0\u4f9b\u4e86\u5206\u6790GFMs\u8106\u5f31\u6027\u7684\u7efc\u5408\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u6a21\u578b\u5b89\u5168\u6027\u3002"}}
{"id": "2505.10992", "pdf": "https://arxiv.org/pdf/2505.10992", "abs": "https://arxiv.org/abs/2505.10992", "authors": ["Feiran You", "Hongyang Du"], "title": "ReaCritic: Large Reasoning Transformer-based DRL Critic-model Scaling For Heterogeneous Networks", "categories": ["cs.LG", "cs.NI"], "comment": null, "summary": "Heterogeneous Networks (HetNets) pose critical challenges for intelligent\nmanagement due to the diverse user requirements and time-varying wireless\nconditions. These factors introduce significant decision complexity, which\nlimits the adaptability of existing Deep Reinforcement Learning (DRL) methods.\nIn many DRL algorithms, especially those involving value-based or actor-critic\nstructures, the critic component plays a key role in guiding policy learning by\nestimating value functions. However, conventional critic models often use\nshallow architectures that map observations directly to scalar estimates,\nlimiting their ability to handle multi-task complexity. In contrast, recent\nprogress in inference-time scaling of Large Language Models (LLMs) has shown\nthat generating intermediate reasoning steps can significantly improve decision\nquality. Motivated by this, we propose ReaCritic, a large reasoning\ntransformer-based criticmodel scaling scheme that brings reasoning ability into\nDRL. ReaCritic performs horizontal reasoning over parallel state-action inputs\nand vertical reasoning through deep transformer stacks. It is compatible with a\nbroad range of value-based and actor-critic DRL algorithms and enhances\ngeneralization in dynamic wireless environments. Extensive experiments\ndemonstrate that ReaCritic improves convergence speed and final performance\nacross various HetNet settings and standard OpenAI Gym control tasks.", "AI": {"tldr": "\u63d0\u51faReaCritic\uff0c\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u63a8\u7406\u53d8\u538b\u5668\u7684\u6279\u8bc4\u8005\u6a21\u578b\u6269\u5c55\u65b9\u6848\uff0c\u5c06\u5176\u5e94\u7528\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u4ee5\u63d0\u9ad8\u52a8\u6001\u65e0\u7ebf\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0cReaCritic\u5728HetNet\u548cOpenAI Gym\u63a7\u5236\u4efb\u52a1\u4e2d\u63d0\u9ad8\u4e86\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u56e0\u9762\u5bf9\u5f02\u6784\u7f51\u7edc\u4e2d\u7528\u6237\u9700\u6c42\u591a\u6837\u6027\u548c\u65e0\u7ebf\u6761\u4ef6\u65f6\u53d8\u6027\u800c\u53d7\u5230\u9650\u5236\u3002\u4f20\u7edf\u6279\u8bc4\u8005\u6a21\u578b\u96be\u4ee5\u5904\u7406\u591a\u4efb\u52a1\u590d\u6742\u6027\u3002", "method": "\u63d0\u51faReaCritic\uff0c\u901a\u8fc7\u6c34\u5e73\u63a8\u7406\u548c\u5e73\u884c\u72b6\u6001-\u52a8\u4f5c\u8f93\u5165\u4ee5\u53ca\u5782\u76f4\u63a8\u7406\u901a\u8fc7\u6df1\u5ea6\u53d8\u538b\u5668\u5806\u6808\u6765\u5b9e\u73b0\u63a8\u7406\u80fd\u529b\u3002", "result": "ReaCritic\u5728\u5404\u79cdHetNet\u8bbe\u7f6e\u548c\u6807\u51c6OpenAI Gym\u63a7\u5236\u4efb\u52a1\u4e2d\u63d0\u9ad8\u4e86\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u6027\u80fd\u3002", "conclusion": "ReaCritic\u589e\u5f3a\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u52a8\u6001\u65e0\u7ebf\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.11017", "pdf": "https://arxiv.org/pdf/2505.11017", "abs": "https://arxiv.org/abs/2505.11017", "authors": ["Wenjie Ou", "Zhishuo Zhao", "Dongyue Guo", "Yi Lin"], "title": "Logo-LLM: Local and Global Modeling with Large Language Models for Time Series Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "Time series forecasting is critical across multiple domains, where time\nseries data exhibits both local patterns and global dependencies. While\nTransformer-based methods effectively capture global dependencies, they often\noverlook short-term local variations in time series. Recent methods that adapt\nlarge language models (LLMs) into time series forecasting inherit this\nlimitation by treating LLMs as black-box encoders, relying solely on the\nfinal-layer output and underutilizing hierarchical representations. To address\nthis limitation, we propose Logo-LLM, a novel LLM-based framework that\nexplicitly extracts and models multi-scale temporal features from different\nlayers of a pre-trained LLM. Through empirical analysis, we show that shallow\nlayers of LLMs capture local dynamics in time series, while deeper layers\nencode global trends. Moreover, Logo-LLM introduces lightweight Local-Mixer and\nGlobal-Mixer modules to align and integrate features with the temporal input\nacross layers. Extensive experiments demonstrate that Logo-LLM achieves\nsuperior performance across diverse benchmarks, with strong generalization in\nfew-shot and zero-shot settings while maintaining low computational overhead.", "AI": {"tldr": "\u63d0\u51faLogo-LLM\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0d\u540c\u5c42\u63d0\u53d6\u591a\u5c3a\u5ea6\u65f6\u95f4\u7279\u5f81\u6765\u6539\u8fdb\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u548c\u9002\u5e94\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5ffd\u7565\u4e86\u77ed\u671f\u5c40\u90e8\u53d8\u5316\uff0c\u4e14\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u9ed1\u76d2\u7f16\u7801\u5668\u65f6\u672a\u80fd\u5145\u5206\u5229\u7528\u5206\u5c42\u8868\u793a\u3002", "method": "\u63d0\u51faLogo-LLM\u6846\u67b6\uff0c\u5f15\u5165\u8f7b\u91cf\u7ea7Local-Mixer\u548cGlobal-Mixer\u6a21\u5757\uff0c\u5e76\u5c55\u793aLLM\u6d45\u5c42\u6355\u83b7\u5c40\u90e8\u52a8\u6001\u800c\u6df1\u5c42\u7f16\u7801\u5168\u5c40\u8d8b\u52bf\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u5c11\u91cf\u548c\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u4e14\u8ba1\u7b97\u5f00\u9500\u4f4e\u3002", "conclusion": "Logo-LLM\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u77ed\u671f\u5c40\u90e8\u53d8\u5316\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.11023", "pdf": "https://arxiv.org/pdf/2505.11023", "abs": "https://arxiv.org/abs/2505.11023", "authors": ["Kutalm\u0131\u015f Co\u015fkun", "Ivo Kavisanczki", "Amin Mirzaei", "Tom Siegl", "Bjarne C. Hiller", "Stefan L\u00fcdtke", "Martin Becker"], "title": "Informed, but Not Always Improved: Challenging the Benefit of Background Knowledge in GNNs", "categories": ["cs.LG"], "comment": "10 pages, 7 figures", "summary": "In complex and low-data domains such as biomedical research, incorporating\nbackground knowledge (BK) graphs, such as protein-protein interaction (PPI)\nnetworks, into graph-based machine learning pipelines is a promising research\ndirection. However, while BK is often assumed to improve model performance, its\nactual contribution and the impact of imperfect knowledge remain poorly\nunderstood. In this work, we investigate the role of BK in an important\nreal-world task: cancer subtype classification. Surprisingly, we find that (i)\nstate-of-the-art GNNs using BK perform no better than uninformed models like\nlinear regression, and (ii) their performance remains largely unchanged even\nwhen the BK graph is heavily perturbed. To understand these unexpected results,\nwe introduce an evaluation framework, which employs (i) a synthetic setting\nwhere the BK is clearly informative and (ii) a set of perturbations that\nsimulate various imperfections in BK graphs. With this, we test the robustness\nof BK-aware models in both synthetic and real-world biomedical settings. Our\nfindings reveal that careful alignment of GNN architectures and BK\ncharacteristics is necessary but holds the potential for significant\nperformance improvements.", "AI": {"tldr": "This study investigates the role of background knowledge (BK) in cancer subtype classification using graph-based machine learning methods.", "motivation": "To understand the actual contribution and the impact of imperfect knowledge in BK graphs.", "method": "Investigating the role of BK in cancer subtype classification and introducing an evaluation framework with synthetic and perturbed BK graphs.", "result": "State-of-the-art GNNs using BK perform no better than linear regression and their performance remains unchanged when the BK graph is heavily perturbed.", "conclusion": "Careful alignment of GNN architectures and BK characteristics is necessary for significant performance improvements."}}
{"id": "2505.11024", "pdf": "https://arxiv.org/pdf/2505.11024", "abs": "https://arxiv.org/abs/2505.11024", "authors": ["Wolfgang Rannetbauer", "Simon Hubmer", "Carina Hambrock", "Ronny Ramlau"], "title": "Leveraging Real-Time Data Analysis and Multiple Kernel Learning for Manufacturing of Innovative Steels", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": "29 pages, 7 figures", "summary": "The implementation of thermally sprayed components in steel manufacturing\npresents challenges for production and plant maintenance. While enhancing\nperformance through specialized surface properties, these components may\nencounter difficulties in meeting modified requirements due to standardization\nin the refurbishment process. This article proposes updating the established\ncoating process for thermally spray coated components for steel manufacturing\n(TCCSM) by integrating real-time data analytics and predictive quality\nmanagement. Two essential components--the data aggregator and the quality\npredictor--are designed through continuous process monitoring and the\napplication of data-driven methodologies to meet the dynamic demands of the\nevolving steel landscape. The quality predictor is powered by the simple and\neffective multiple kernel learning strategy with the goal of realizing\npredictive quality. The data aggregator, designed with sensors, flow meters,\nand intelligent data processing for the thermal spray coating process, is\nproposed to facilitate real-time analytics. The performance of this combination\nwas verified using small-scale tests that enabled not only the accurate\nprediction of coating quality based on the collected data but also proactive\nnotification to the operator as soon as significant deviations are identified.", "AI": {"tldr": "This paper proposes an updated coating process for thermally sprayed components in steel manufacturing by integrating real-time data analytics and predictive quality management.", "motivation": "To address the challenges in production and plant maintenance of thermally sprayed components in steel manufacturing due to standardization in the refurbishment process.", "method": "Integrating real-time data analytics and predictive quality management by designing two essential components: the data aggregator and the quality predictor.", "result": "The combination of the data aggregator and the quality predictor was verified using small-scale tests, enabling accurate prediction of coating quality and proactive notification to the operator.", "conclusion": "Updating the established coating process for thermally spray coated components in steel manufacturing can enhance performance through specialized surface properties while meeting dynamic demands."}}
{"id": "2505.11029", "pdf": "https://arxiv.org/pdf/2505.11029", "abs": "https://arxiv.org/abs/2505.11029", "authors": ["Li Ju", "Max Andersson", "Stina Fredriksson", "Edward Gl\u00f6ckner", "Andreas Hellander", "Ekta Vats", "Prashant Singh"], "title": "Exploiting the Asymmetric Uncertainty Structure of Pre-trained VLMs on the Unit Hypersphere", "categories": ["cs.LG"], "comment": null, "summary": "Vision-language models (VLMs) as foundation models have significantly\nenhanced performance across a wide range of visual and textual tasks, without\nrequiring large-scale training from scratch for downstream tasks. However,\nthese deterministic VLMs fail to capture the inherent ambiguity and uncertainty\nin natural language and visual data. Recent probabilistic post-hoc adaptation\nmethods address this by mapping deterministic embeddings onto probability\ndistributions; however, existing approaches do not account for the asymmetric\nuncertainty structure of the modalities, and the constraint that meaningful\ndeterministic embeddings reside on a unit hypersphere, potentially leading to\nsuboptimal performance. In this paper, we address the asymmetric uncertainty\nstructure inherent in textual and visual data, and propose AsymVLM to build\nprobabilistic embeddings from pre-trained VLMs on the unit hypersphere,\nenabling uncertainty quantification. We validate the effectiveness of the\nprobabilistic embeddings on established benchmarks, and present comprehensive\nablation studies demonstrating the inherent nature of asymmetry in the\nuncertainty structure of textual and visual data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5AsymVLM\u6765\u89e3\u51b3\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u65f6\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u5728\u5355\u4f4d\u8d85\u7403\u9762\u4e0a\u6784\u5efa\u6982\u7387\u5d4c\u5165\uff0c\u5b9e\u73b0\u4e86\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u5e76\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u786e\u5b9a\u6027\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u81ea\u7136\u8bed\u8a00\u548c\u89c6\u89c9\u6570\u636e\u4e2d\u7684\u5185\u5728\u6a21\u7cca\u6027\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u73b0\u6709\u7684\u6982\u7387\u540e\u9002\u914d\u65b9\u6cd5\u4e5f\u672a\u80fd\u5145\u5206\u8003\u8651\u6a21\u6001\u4e4b\u95f4\u7684\u975e\u5bf9\u79f0\u4e0d\u786e\u5b9a\u6027\u7ed3\u6784\u4ee5\u53ca\u6709\u610f\u4e49\u7684\u786e\u5b9a\u6027\u5d4c\u5165\u4f4d\u4e8e\u5355\u4f4d\u8d85\u7403\u9762\u4e0a\u7684\u7ea6\u675f\u6761\u4ef6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5AsymVLM\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u7840\u4e0a\uff0c\u5728\u5355\u4f4d\u8d85\u7403\u9762\u4e0a\u6784\u5efa\u6982\u7387\u5d4c\u5165\uff0c\u4ece\u800c\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "\u5728\u5df2\u5efa\u7acb\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6982\u7387\u5d4c\u5165\u7684\u6709\u6548\u6027\uff0c\u5e76\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6d88\u878d\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u6587\u672c\u548c\u89c6\u89c9\u6570\u636e\u4e0d\u786e\u5b9a\u6027\u7ed3\u6784\u4e2d\u975e\u5bf9\u79f0\u6027\u7684\u56fa\u6709\u6027\u8d28\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u65f6\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u5728\u5355\u4f4d\u8d85\u7403\u9762\u4e0a\u6784\u5efa\u6982\u7387\u5d4c\u5165\uff0c\u5b9e\u73b0\u4e86\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002"}}
{"id": "2505.11035", "pdf": "https://arxiv.org/pdf/2505.11035", "abs": "https://arxiv.org/abs/2505.11035", "authors": ["Kihun Hong", "Sejun Park", "Ganguk Hwang"], "title": "Deep Latent Variable Model based Vertical Federated Learning with Flexible Alignment and Labeling Scenarios", "categories": ["cs.LG"], "comment": "9 pages + appendix, 8 figures, 18 tables", "summary": "Federated learning (FL) has attracted significant attention for enabling\ncollaborative learning without exposing private data. Among the primary\nvariants of FL, vertical federated learning (VFL) addresses feature-partitioned\ndata held by multiple institutions, each holding complementary information for\nthe same set of users. However, existing VFL methods often impose restrictive\nassumptions such as a small number of participating parties, fully aligned\ndata, or only using labeled data. In this work, we reinterpret alignment gaps\nin VFL as missing data problems and propose a unified framework that\naccommodates both training and inference under arbitrary alignment and labeling\nscenarios, while supporting diverse missingness mechanisms. In the experiments\non 168 configurations spanning four benchmark datasets, six training-time\nmissingness patterns, and seven testing-time missingness patterns, our method\noutperforms all baselines in 160 cases with an average gap of 9.6 percentage\npoints over the next-best competitors. To the best of our knowledge, this is\nthe first VFL framework to jointly handle arbitrary data alignment, unlabeled\ndata, and multi-party collaboration all at once.", "AI": {"tldr": "This paper proposes a new VFL framework that can handle arbitrary data alignment, unlabeled data, and multi-party collaboration.", "motivation": "Existing VFL methods have restrictive assumptions, such as a small number of participants or fully aligned data.", "method": "Reinterpreting alignment gaps in VFL as missing data problems and proposing a unified framework that accommodates both training and inference under arbitrary alignment and labeling scenarios.", "result": "The proposed method outperforms all baselines in 160 out of 168 cases with an average gap of 9.6 percentage points.", "conclusion": "This is the first VFL framework to jointly handle arbitrary data alignment, unlabeled data, and multi-party collaboration."}}
{"id": "2505.11040", "pdf": "https://arxiv.org/pdf/2505.11040", "abs": "https://arxiv.org/abs/2505.11040", "authors": ["Zhexiang Li", "Haoyu Wang", "Yutong Bao", "David Woodruff"], "title": "Efficient Attention via Pre-Scoring: Prioritizing Informative Keys in Transformers", "categories": ["cs.LG"], "comment": null, "summary": "Recent advances in transformer architectures deeply enhance long-context\nlanguage modeling. Among them, HyperAttention achieves competitive efficiency\nby combining a single-level LSH-based clustering with uniform residual\nsampling. However,such a sampling limits crucial keys' capturing, which in turn\nraises the overall perplexity. In this paper, we propose a pre-scoring\nmechanism to assist HyperAttention to prioritize significant keys.\nSpecifically, we introduce three scoring methods: K-means clustering, K-median\nclustering, and leverage score-based ranking (inspired by LevAttention) to\nfilter keys effectively. We further replace HyperAttention's original uniform\nresidual sampling entirely, relying exclusively on our pre-scoring mechanism.\nExperiments on ChatGLM2 (131k token context) reduce perplexity from 12 to 8.3,\nwhich outperforms standard HyperAttention. Moreover, when running on the\nVision-Transformer (ViT), our method shows that it can guarantee similar\naccuracy compared with LevAttention, and will surpass LevAttention given\nspecific parameters. Although this method introduces computational overhead,\nits combination with HyperAttention remains 20 times faster than\nFlashAttention, providing a balanced trade-off between speed and modeling\naccuracy. Our results highlight the effectiveness of integrating pre-scoring\ninto hierarchical attention mechanisms, significantly improving Transformer's\nefficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u8bc4\u5206\u673a\u5236\u6765\u589e\u5f3aHyperAttention\u7684\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u901f\u5ea6\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u5efa\u6a21\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3HyperAttention\u4e2d\u91c7\u6837\u9650\u5236\u5173\u952e\u952e\u6355\u6349\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u6574\u4f53\u56f0\u60d1\u5ea6\u3002", "method": "\u5f15\u5165\u4e09\u79cd\u8bc4\u5206\u65b9\u6cd5\uff1aK-means\u805a\u7c7b\u3001K-median\u805a\u7c7b\u548c\u57fa\u4e8e\u6760\u6746\u5f97\u5206\u7684\u6392\u540d\uff0c\u5e76\u5b8c\u5168\u66ff\u6362\u4e86HyperAttention\u7684\u539f\u59cb\u5747\u5300\u6b8b\u5dee\u91c7\u6837\uff0c\u4ec5\u4f9d\u8d56\u4e8e\u6211\u4eec\u7684\u9884\u8bc4\u5206\u673a\u5236\u3002", "result": "\u5728ChatGLM2\u4e0a\uff0c\u56f0\u60d1\u5ea6\u4ece12\u964d\u4f4e\u52308.3\uff1b\u5728Vision-Transformer (ViT) \u4e0a\uff0c\u4fdd\u8bc1\u4e86\u4e0eLevAttention\u76f8\u4f3c\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u5728\u7279\u5b9a\u53c2\u6570\u4e0b\u8d85\u8fc7\u4e86LevAttention\u3002", "conclusion": "\u901a\u8fc7\u5728\u5206\u5c42\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u6574\u5408\u9884\u8bc4\u5206\uff0c\u663e\u8457\u63d0\u9ad8\u4e86Transformer\u7684\u6548\u7387\u3002"}}
{"id": "2505.11044", "pdf": "https://arxiv.org/pdf/2505.11044", "abs": "https://arxiv.org/abs/2505.11044", "authors": ["Zhirui Fang", "Kai Yang", "Jian Tao", "Jiafei Lyu", "Lusong Li", "Li Shen", "Xiu Li"], "title": "Exploration by Random Distribution Distillation", "categories": ["cs.LG"], "comment": null, "summary": "Exploration remains a critical challenge in online reinforcement learning, as\nan agent must effectively explore unknown environments to achieve high returns.\nCurrently, the main exploration algorithms are primarily count-based methods\nand curiosity-based methods, with prediction-error methods being a prominent\nexample. In this paper, we propose a novel method called \\textbf{R}andom\n\\textbf{D}istribution \\textbf{D}istillation (RDD), which samples the output of\na target network from a normal distribution. RDD facilitates a more extensive\nexploration by explicitly treating the difference between the prediction\nnetwork and the target network as an intrinsic reward. Furthermore, by\nintroducing randomness into the output of the target network for a given state\nand modeling it as a sample from a normal distribution, intrinsic rewards are\nbounded by two key components: a pseudo-count term ensuring proper exploration\ndecay and a discrepancy term accounting for predictor convergence. We\ndemonstrate that RDD effectively unifies both count-based and prediction-error\napproaches. It retains the advantages of prediction-error methods in\nhigh-dimensional spaces, while also implementing an intrinsic reward decay mode\nakin to the pseudo-count method. In the experimental section, RDD is compared\nwith more advanced methods in a series of environments. Both theoretical\nanalysis and experimental results confirm the effectiveness of our approach in\nimproving online exploration for reinforcement learning tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aRDD\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u76ee\u6807\u7f51\u7edc\u8f93\u51fa\u89c6\u4e3a\u6b63\u6001\u5206\u5e03\u6837\u672c\u6765\u4fc3\u8fdb\u66f4\u5e7f\u6cdb\u7684\u63a2\u7d22\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u57fa\u4e8e\u8ba1\u6570\u548c\u9884\u6d4b\u8bef\u5dee\u7684\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u5e76\u5728\u591a\u4e2a\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5f53\u524d\u4e3b\u8981\u7684\u63a2\u7d22\u7b97\u6cd5\u6709\u57fa\u4e8e\u8ba1\u6570\u7684\u65b9\u6cd5\u548c\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u4f46\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u968f\u673a\u5206\u5e03\u84b8\u998f\uff08RDD\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u76ee\u6807\u7f51\u7edc\u8f93\u51fa\u7684\u4e0d\u786e\u5b9a\u6027\u5f15\u5165\u5185\u5728\u5956\u52b1\uff0c\u540c\u65f6\u786e\u4fdd\u63a2\u7d22\u8870\u51cf\u548c\u9884\u6d4b\u5668\u6536\u655b\u3002", "result": "RDD\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u4fdd\u6301\u4e86\u9884\u6d4b\u8bef\u5dee\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u5e76\u5b9e\u73b0\u4e86\u7c7b\u4f3c\u4e8e\u4f2a\u8ba1\u6570\u65b9\u6cd5\u7684\u5185\u5728\u5956\u52b1\u8870\u51cf\u6a21\u5f0f\u3002\u5b9e\u9a8c\u8868\u660eRDD\u5728\u591a\u4e2a\u73af\u5883\u4e2d\u4f18\u4e8e\u5176\u4ed6\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "RDD\u6709\u6548\u7edf\u4e00\u4e86\u57fa\u4e8e\u8ba1\u6570\u548c\u9884\u6d4b\u8bef\u5dee\u7684\u65b9\u6cd5\uff0c\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u5176\u5728\u63d0\u9ad8\u5728\u7ebf\u63a2\u7d22\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.11050", "pdf": "https://arxiv.org/pdf/2505.11050", "abs": "https://arxiv.org/abs/2505.11050", "authors": ["Jeroen Bollen", "Jan Van den Bussche", "Stijn Vansummeren", "Jonni Virtema"], "title": "Halting Recurrent GNNs and the Graded $\u03bc$-Calculus", "categories": ["cs.LG", "cs.AI", "cs.LO"], "comment": null, "summary": "Graph Neural Networks (GNNs) are a class of machine-learning models that\noperate on graph-structured data. Their expressive power is intimately related\nto logics that are invariant under graded bisimilarity. Current proposals for\nrecurrent GNNs either assume that the graph size is given to the model, or\nsuffer from a lack of termination guarantees. In this paper, we propose a\nhalting mechanism for recurrent GNNs. We prove that our halting model can\nexpress all node classifiers definable in graded modal mu-calculus, even for\nthe standard GNN variant that is oblivious to the graph size. A recent\nbreakthrough in the study of the expressivity of graded modal mu-calculus in\nthe finite suggests that conversely, restricted to node classifiers definable\nin monadic second-order logic, recurrent GNNs can express only node classifiers\ndefinable in graded modal mu-calculus. To prove our main result, we develop a\nnew approximate semantics for graded mu-calculus, which we believe to be of\nindependent interest. We leverage this new semantics into a new model-checking\nalgorithm, called the counting algorithm, which is oblivious to the graph size.\nIn a final step we show that the counting algorithm can be implemented on a\nhalting recurrent GNN.", "AI": {"tldr": "This paper introduces a halting mechanism for recurrent Graph Neural Networks (GNNs), proving its expressive power in relation to logics and developing a new approximate semantics for graded modal mu-calculus, leading to a new model-checking algorithm called the counting algorithm.", "motivation": "To address the issue of termination guarantees and expressive power limitations in current recurrent GNNs, especially when dealing with varying graph sizes.", "method": "Proposing a halting mechanism for recurrent GNNs and developing a new approximate semantics for graded modal mu-calculus.", "result": "The halting model can express all node classifiers definable in graded modal mu-calculus, even for standard GNN variants oblivious to graph size. It also leads to a new model-checking algorithm called the counting algorithm.", "conclusion": "Recurrent GNNs restricted to node classifiers definable in monadic second-order logic can only express those definable in graded modal mu-calculus."}}
{"id": "2505.11067", "pdf": "https://arxiv.org/pdf/2505.11067", "abs": "https://arxiv.org/abs/2505.11067", "authors": ["Omobayode Fagbohungbe", "Corey Lammie", "Malte J. Rasch", "Takashi Ando", "Tayfun Gokmen", "Vijay Narayanan"], "title": "Assessing the Performance of Analog Training for Transfer Learning", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CV", "cs.DC", "cs.NE"], "comment": null, "summary": "Analog in-memory computing is a next-generation computing paradigm that\npromises fast, parallel, and energy-efficient deep learning training and\ntransfer learning (TL). However, achieving this promise has remained elusive\ndue to a lack of suitable training algorithms. Analog memory devices exhibit\nasymmetric and non-linear switching behavior in addition to device-to-device\nvariation, meaning that most, if not all, of the current off-the-shelf training\nalgorithms cannot achieve good training outcomes. Also, recently introduced\nalgorithms have enjoyed limited attention, as they require bi-directionally\nswitching devices of unrealistically high symmetry and precision and are highly\nsensitive. A new algorithm chopped TTv2 (c-TTv2), has been introduced, which\nleverages the chopped technique to address many of the challenges mentioned\nabove. In this paper, we assess the performance of the c-TTv2 algorithm for\nanalog TL using a Swin-ViT model on a subset of the CIFAR100 dataset. We also\ninvestigate the robustness of our algorithm to changes in some device\nspecifications, including weight transfer noise, symmetry point skew, and\nsymmetry point variability", "AI": {"tldr": "This paper introduces and evaluates a new algorithm called c-TTv2 for analog in-memory computing, focusing on its application in transfer learning with a Swin-ViT model on CIFAR100 dataset.", "motivation": "To overcome limitations of existing training algorithms due to asymmetric and non-linear behavior of analog memory devices.", "method": "Introduces c-TTv2 algorithm leveraging chopped technique to handle challenges related to analog memory devices.", "result": "Assesses the performance of c-TTv2 on a subset of CIFAR100 dataset using Swin-ViT model and investigates its robustness against variations in device specifications.", "conclusion": "The study shows the effectiveness of the c-TTv2 algorithm in improving training outcomes for analog in-memory computing."}}
{"id": "2505.11076", "pdf": "https://arxiv.org/pdf/2505.11076", "abs": "https://arxiv.org/abs/2505.11076", "authors": ["Vladim\u00edr Bo\u017ea", "Vladim\u00edr Macko"], "title": "Addition is almost all you need: Compressing neural networks with double binary factorization", "categories": ["cs.LG"], "comment": null, "summary": "Binary quantization approaches, which replace weight matrices with binary\nmatrices and substitute costly multiplications with cheaper additions, offer a\ncomputationally efficient approach to address the increasing computational and\nstorage requirements of Large Language Models (LLMs). However, the severe\nquantization constraint ($\\pm1$) can lead to significant accuracy degradation.\nIn this paper, we propose Double Binary Factorization (DBF), a novel method\nthat factorizes dense weight matrices into products of two binary (sign)\nmatrices, each accompanied by scaling vectors. DBF preserves the efficiency\nadvantages of binary representations while achieving compression rates that are\ncompetitive with or superior to state-of-the-art methods. Specifically, in a\n1-bit per weight range, DBF is better than existing binarization approaches. In\na 2-bit per weight range, DBF is competitive with the best quantization methods\nlike QuIP\\# and QTIP. Unlike most existing compression techniques, which offer\nlimited compression level choices, DBF allows fine-grained control over\ncompression ratios by adjusting the factorization's intermediate dimension.\nBased on this advantage, we further introduce an algorithm for estimating\nnon-uniform layer-wise compression ratios for DBF, based on previously\ndeveloped channel pruning criteria.\n  Code available at: https://github.com/usamec/double_binary", "AI": {"tldr": "Double Binary Factorization (DBF)\u662f\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6743\u91cd\u77e9\u9635\u5206\u89e3\u4e3a\u4e24\u4e2a\u4e8c\u503c\u77e9\u9635\u7684\u4e58\u79ef\u6765\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u538b\u7f29\u7387\u3002\u57281\u6bd4\u7279/\u6743\u91cd\u8303\u56f4\u5185\uff0cDBF\u4f18\u4e8e\u73b0\u6709\u7684\u4e8c\u503c\u5316\u65b9\u6cd5\uff1b\u57282\u6bd4\u7279/\u6743\u91cd\u8303\u56f4\u5185\uff0c\u5b83\u4e0e\u6700\u5148\u8fdb\u7684\u91cf\u5316\u65b9\u6cd5\u7ade\u4e89\u3002\u6b64\u5916\uff0cDBF\u5141\u8bb8\u901a\u8fc7\u8c03\u6574\u5206\u89e3\u7684\u4e2d\u95f4\u7ef4\u5ea6\u6765\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u538b\u7f29\u6bd4\u63a7\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u901a\u9053\u526a\u679d\u6807\u51c6\u7684\u975e\u5747\u5300\u5c42\u95f4\u538b\u7f29\u6bd4\u4f30\u8ba1\u7b97\u6cd5\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u65e5\u76ca\u589e\u957f\u7684\u8ba1\u7b97\u548c\u5b58\u50a8\u9700\u6c42\uff0c\u800c\u4e8c\u503c\u5316\u65b9\u6cd5\u867d\u7136\u80fd\u63d0\u4f9b\u8ba1\u7b97\u6548\u7387\uff0c\u4f46\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u7cbe\u5ea6\u4e0b\u964d\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u548c\u6a21\u578b\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDouble Binary Factorization (DBF)\u7684\u65b9\u6cd5\uff0c\u5c06\u6743\u91cd\u77e9\u9635\u5206\u89e3\u4e3a\u4e24\u4e2a\u4e8c\u503c\uff08\u7b26\u53f7\uff09\u77e9\u9635\u7684\u4e58\u79ef\uff0c\u6bcf\u4e2a\u77e9\u9635\u90fd\u5e26\u6709\u7f29\u653e\u5411\u91cf\u3002", "result": "DBF\u57281\u6bd4\u7279/\u6743\u91cd\u8303\u56f4\u5185\u4f18\u4e8e\u73b0\u6709\u4e8c\u503c\u5316\u65b9\u6cd5\uff0c\u57282\u6bd4\u7279/\u6743\u91cd\u8303\u56f4\u5185\u4e0e\u6700\u5148\u8fdb\u7684\u91cf\u5316\u65b9\u6cd5\u5982QuIP#\u548cQTIP\u7ade\u4e89\u3002\u6b64\u5916\uff0cDBF\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u7684\u538b\u7f29\u6bd4\u63a7\u5236\uff0c\u5e76\u4e14\u53ef\u4ee5\u7ed3\u5408\u975e\u5747\u5300\u5c42\u95f4\u538b\u7f29\u6bd4\u4f30\u8ba1\u7b97\u6cd5\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "conclusion": "DBF\u4e0d\u4ec5\u7ee7\u627f\u4e86\u4e8c\u503c\u8868\u793a\u7684\u8ba1\u7b97\u6548\u7387\u4f18\u52bf\uff0c\u8fd8\u5b9e\u73b0\u4e86\u4e0e\u6216\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u538b\u7f29\u65b9\u6cd5\u7684\u538b\u7f29\u7387\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u538b\u7f29\u6bd4\u63a7\u5236\u3002"}}
{"id": "2505.11083", "pdf": "https://arxiv.org/pdf/2505.11083", "abs": "https://arxiv.org/abs/2505.11083", "authors": ["Guangqiang Li", "M. Amine Atoui", "Xiangshun Li"], "title": "Fault Diagnosis across Heterogeneous Domains via Self-Adaptive Temporal-Spatial Attention and Sample Generation", "categories": ["cs.LG", "cs.AI"], "comment": "31 pages, 11 figures", "summary": "Deep learning methods have shown promising performance in fault diagnosis for\nmultimode process. Most existing studies assume that the collected health state\ncategories from different operating modes are identical. However, in real\nindustrial scenarios, these categories typically exhibit only partial overlap.\nThe incompleteness of the available data and the large distributional\ndifferences between the operating modes pose a significant challenge to\nexisting fault diagnosis methods. To address this problem, a novel fault\ndiagnosis model named self-adaptive temporal-spatial attention network\n(TSA-SAN) is proposed. First, inter-mode mappings are constructed using healthy\ncategory data to generate multimode samples. To enrich the diversity of the\nfault data, interpolation is performed between healthy and fault samples.\nSubsequently, the fault diagnosis model is trained using real and generated\ndata. The self-adaptive instance normalization is established to suppress\nirrelevant information while retaining essential statistical features for\ndiagnosis. In addition, a temporal-spatial attention mechanism is constructed\nto focus on the key features, thus enhancing the generalization ability of the\nmodel. The extensive experiments demonstrate that the proposed model\nsignificantly outperforms the state-of-the-art methods. The code will be\navailable on Github at https://github.com/GuangqiangLi/TSA-SAN.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u65f6\u95f4-\u7a7a\u95f4\u6ce8\u610f\u529b\u7f51\u7edc\uff08TSA-SAN\uff09\uff0c\u7528\u4e8e\u591a\u6a21\u5f0f\u8fc7\u7a0b\u4e2d\u7684\u6545\u969c\u8bca\u65ad\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u6784\u5efa\u6a21\u5f0f\u95f4\u6620\u5c04\u548c\u63d2\u503c\u6765\u589e\u5f3a\u6570\u636e\u591a\u6837\u6027\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u5b9e\u4f8b\u5f52\u4e00\u5316\u548c\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\u6765\u63d0\u9ad8\u8bca\u65ad\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u6545\u969c\u8bca\u65ad\u65b9\u6cd5\u5047\u8bbe\u4e0d\u540c\u64cd\u4f5c\u6a21\u5f0f\u4e0b\u7684\u5065\u5eb7\u72b6\u6001\u7c7b\u522b\u76f8\u540c\uff0c\u4f46\u5728\u5b9e\u9645\u5de5\u4e1a\u573a\u666f\u4e2d\uff0c\u8fd9\u4e9b\u7c7b\u522b\u901a\u5e38\u53ea\u5b58\u5728\u90e8\u5206\u91cd\u53e0\uff0c\u8fd9\u7ed9\u73b0\u6709\u7684\u6545\u969c\u8bca\u65ad\u65b9\u6cd5\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u65f6\u95f4-\u7a7a\u95f4\u6ce8\u610f\u529b\u7f51\u7edc\uff08TSA-SAN\uff09\uff0c\u5305\u62ec\u6784\u5efa\u6a21\u5f0f\u95f4\u6620\u5c04\u751f\u6210\u591a\u6a21\u5f0f\u6837\u672c\uff0c\u5bf9\u5065\u5eb7\u548c\u6545\u969c\u6837\u672c\u8fdb\u884c\u63d2\u503c\u4ee5\u4e30\u5bcc\u6545\u969c\u6570\u636e\u7684\u591a\u6837\u6027\uff0c\u4f7f\u7528\u81ea\u9002\u5e94\u5b9e\u4f8b\u5f52\u4e00\u5316\u6291\u5236\u65e0\u5173\u4fe1\u606f\u5e76\u4fdd\u7559\u8bca\u65ad\u6240\u9700\u7684\u5173\u952e\u7edf\u8ba1\u7279\u5f81\uff0c\u4ee5\u53ca\u6784\u5efa\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\u805a\u7126\u5173\u952e\u7279\u5f81\u4ee5\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684TSA-SAN\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684TSA-SAN\u6a21\u578b\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u5f0f\u8fc7\u7a0b\u4e2d\u7531\u4e8e\u6570\u636e\u4e0d\u5b8c\u6574\u6027\u548c\u6a21\u5f0f\u95f4\u5206\u5e03\u5dee\u5f02\u5bfc\u81f4\u7684\u6545\u969c\u8bca\u65ad\u96be\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2505.11081", "pdf": "https://arxiv.org/pdf/2505.11081", "abs": "https://arxiv.org/abs/2505.11081", "authors": ["Pierre Clavier", "Nathan Grinsztajn", "Raphael Avalos", "Yannis Flet-Berliac", "Irem Ergun", "Omar D. Domingues", "Eugene Tarassov", "Olivier Pietquin", "Pierre H. Richemond", "Florian Strub", "Matthieu Geist"], "title": "ShiQ: Bringing back Bellman to LLMs", "categories": ["cs.LG"], "comment": null, "summary": "The fine-tuning of pre-trained large language models (LLMs) using\nreinforcement learning (RL) is generally formulated as direct policy\noptimization. This approach was naturally favored as it efficiently improves a\npretrained LLM, seen as an initial policy. Another RL paradigm, Q-learning\nmethods, has received far less attention in the LLM community while\ndemonstrating major success in various non-LLM RL tasks. In particular,\nQ-learning effectiveness comes from its sample efficiency and ability to learn\noffline, which is particularly valuable given the high computational cost of\nsampling with LLMs. However, naively applying a Q-learning-style update to the\nmodel's logits is ineffective due to the specificity of LLMs. Our core\ncontribution is to derive theoretically grounded loss functions from Bellman\nequations to adapt Q-learning methods to LLMs. To do so, we carefully adapt\ninsights from the RL literature to account for LLM-specific characteristics,\nensuring that the logits become reliable Q-value estimates. We then use this\nloss to build a practical algorithm, ShiQ for Shifted-Q, that supports\noff-policy, token-wise learning while remaining simple to implement. Finally,\nwe evaluate ShiQ on both synthetic data and real-world benchmarks, e.g.,\nUltraFeedback and BFCL-V3, demonstrating its effectiveness in both single-turn\nand multi-turn LLM settings", "AI": {"tldr": "This paper introduces ShiQ, a novel algorithm derived from Q-learning methods tailored for pre-trained large language models (LLMs), which shows effectiveness in both single-turn and multi-turn settings.", "motivation": "To address the inefficiency of current reinforcement learning approaches in improving pre-trained LLMs, by leveraging the sample efficiency and offline learning capability of Q-learning methods.", "method": "Deriving theoretically grounded loss functions from Bellman equations to adapt Q-learning methods to LLMs, ensuring logits become reliable Q-value estimates, and building the ShiQ algorithm for off-policy, token-wise learning.", "result": "ShiQ demonstrates effectiveness on both synthetic data and real-world benchmarks, showing promise in improving LLMs' performance in single-turn and multi-turn settings.", "conclusion": "This work successfully adapts Q-learning to LLMs through the ShiQ algorithm, showcasing its potential in enhancing LLMs' reinforcement learning capabilities."}}
{"id": "2505.11100", "pdf": "https://arxiv.org/pdf/2505.11100", "abs": "https://arxiv.org/abs/2505.11100", "authors": ["Lang Feng", "Jiahao Lin", "Dong Xing", "Li Zhang", "De Ma", "Gang Pan"], "title": "Bidirectional Distillation: A Mixed-Play Framework for Multi-Agent Generalizable Behaviors", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Population-population generalization is a challenging problem in multi-agent\nreinforcement learning (MARL), particularly when agents encounter unseen\nco-players. However, existing self-play-based methods are constrained by the\nlimitation of inside-space generalization. In this study, we propose\nBidirectional Distillation (BiDist), a novel mixed-play framework, to overcome\nthis limitation in MARL. BiDist leverages knowledge distillation in two\nalternating directions: forward distillation, which emulates the historical\npolicies' space and creates an implicit self-play, and reverse distillation,\nwhich systematically drives agents towards novel distributions outside the\nknown policy space in a non-self-play manner. In addition, BiDist operates as a\nconcise and efficient solution without the need for the complex and costly\nstorage of past policies. We provide both theoretical analysis and empirical\nevidence to support BiDist's effectiveness. Our results highlight its\nremarkable generalization ability across a variety of cooperative, competitive,\nand social dilemma tasks, and reveal that BiDist significantly diversifies the\npolicy distribution space. We also present comprehensive ablation studies to\nreinforce BiDist's effectiveness and key success factors. Source codes are\navailable in the supplementary material.", "AI": {"tldr": "This study introduces Bidirectional Distillation (BiDist), a novel approach for population-population generalization in multi-agent reinforcement learning, overcoming limitations of existing self-play methods through knowledge distillation in two directions.", "motivation": "To address the challenge of population-population generalization in MARL, especially when agents face unseen co-players, and to overcome the limitations of inside-space generalization in self-play-based methods.", "method": "Proposes BiDist, a mixed-play framework using forward and reverse distillation to create implicit self-play and drive agents to novel policy distributions outside known spaces, without requiring storage of past policies.", "result": "Demonstrates strong generalization ability across various tasks and significantly diversifies the policy distribution space, supported by theoretical analysis and empirical evidence.", "conclusion": "Introduces a novel method called Bidirectional Distillation (BiDist) that improves population-population generalization in multi-agent reinforcement learning."}}
{"id": "2505.11106", "pdf": "https://arxiv.org/pdf/2505.11106", "abs": "https://arxiv.org/abs/2505.11106", "authors": ["Thanadej Rattanakornphan", "Piyanon Charoenpoonpanich", "Chainarong Amornbunchornvej"], "title": "Inferring the Most Similar Variable-length Subsequences between Multidimensional Time Series", "categories": ["cs.LG", "cs.AI", "cs.DB", "stat.ME"], "comment": "Under review", "summary": "Finding the most similar subsequences between two multidimensional time\nseries has many applications: e.g. capturing dependency in stock market or\ndiscovering coordinated movement of baboons. Considering one pattern occurring\nin one time series, we might be wondering whether the same pattern occurs in\nanother time series with some distortion that might have a different length.\nNevertheless, to the best of our knowledge, there is no efficient framework\nthat deals with this problem yet. In this work, we propose an algorithm that\nprovides the exact solution of finding the most similar multidimensional\nsubsequences between time series where there is a difference in length both\nbetween time series and between subsequences. The algorithm is built based on\ntheoretical guarantee of correctness and efficiency. The result in simulation\ndatasets illustrated that our approach not just only provided correct solution,\nbut it also utilized running time only quarter of time compared against the\nbaseline approaches. In real-world datasets, it extracted the most similar\nsubsequences even faster (up to 20 times faster against baseline methods) and\nprovided insights regarding the situation in stock market and following\nrelations of multidimensional time series of baboon movement. Our approach can\nbe used for any time series. The code and datasets of this work are provided\nfor the public use.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7b97\u6cd5\u89e3\u51b3\u591a\u7ef4\u65f6\u95f4\u5e8f\u5217\u4e2d\u6700\u76f8\u4f3c\u5b50\u5e8f\u5217\u67e5\u627e\u95ee\u9898\uff0c\u8be5\u7b97\u6cd5\u5728\u6a21\u62df\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u901f\u5ea6\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5feb4-20\u500d\u3002", "motivation": "\u5bfb\u627e\u4e24\u4e2a\u591a\u7ef4\u65f6\u95f4\u5e8f\u5217\u4e4b\u95f4\u6700\u76f8\u4f3c\u7684\u5b50\u5e8f\u5217\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u9ad8\u6548\u6846\u67b6\u5904\u7406\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7406\u8bba\u4fdd\u8bc1\u6b63\u786e\u6027\u548c\u6548\u7387\u7684\u65b0\u7b97\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u957f\u5ea6\u4e0d\u540c\u4ee5\u53ca\u5b50\u5e8f\u5217\u957f\u5ea6\u4e0d\u540c\u7684\u60c5\u51b5\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u96c6\u4e2d\uff0c\u7b97\u6cd5\u4e0d\u4ec5\u63d0\u4f9b\u6b63\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u800c\u4e14\u8fd0\u884c\u65f6\u95f4\u4ec5\u4e3a\u57fa\u7ebf\u65b9\u6cd5\u7684\u56db\u5206\u4e4b\u4e00\uff1b\u5728\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e2d\uff0c\u7b97\u6cd5\u63d0\u53d6\u6700\u76f8\u4f3c\u5b50\u5e8f\u5217\u7684\u901f\u5ea6\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5feb20\u500d\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u80a1\u7968\u5e02\u573a\u548c\u72d2\u72d2\u8fd0\u52a8\u591a\u7ef4\u65f6\u95f4\u5e8f\u5217\u76f8\u5173\u6027\u7684\u6d1e\u5bdf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4efb\u4f55\u65f6\u95f4\u5e8f\u5217\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u53ef\u4f9b\u516c\u4f17\u4f7f\u7528\u3002"}}
{"id": "2505.11111", "pdf": "https://arxiv.org/pdf/2505.11111", "abs": "https://arxiv.org/abs/2505.11111", "authors": ["Lin Zhu", "Yijun Bian", "Lei You"], "title": "FairSHAP: Preprocessing for Fairness Through Attribution-Based Data Augmentation", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": "3 figures, 15 pages", "summary": "Ensuring fairness in machine learning models is critical, particularly in\nhigh-stakes domains where biased decisions can lead to serious societal\nconsequences. Existing preprocessing approaches generally lack transparent\nmechanisms for identifying which features or instances are responsible for\nunfairness. This obscures the rationale behind data modifications. We introduce\nFairSHAP, a novel pre-processing framework that leverages Shapley value\nattribution to improve both individual and group fairness. FairSHAP identifies\nfairness-critical instances in the training data using an interpretable measure\nof feature importance, and systematically modifies them through instance-level\nmatching across sensitive groups. This process reduces discriminative risk - an\nindividual fairness metric - while preserving data integrity and model\naccuracy. We demonstrate that FairSHAP significantly improves demographic\nparity and equality of opportunity across diverse tabular datasets, achieving\nfairness gains with minimal data perturbation and, in some cases, improved\npredictive performance. As a model-agnostic and transparent method, FairSHAP\nintegrates seamlessly into existing machine learning pipelines and provides\nactionable insights into the sources of bias.Our code is on\nhttps://github.com/youlei202/FairSHAP.", "AI": {"tldr": "Ensure fairness in machine learning models by introducing FairSHAP, a novel preprocessing framework that uses Shapley value attribution to enhance both individual and group fairness.", "motivation": "Biased decisions in high-stakes domains can have serious societal consequences, and existing methods lack transparency in identifying unfair features or instances.", "method": "FairSHAP identifies fairness-critical instances using feature importance and modifies them via instance-level matching across sensitive groups.", "result": "FairSHAP improves demographic parity and equality of opportunity with minimal data changes and sometimes better predictive performance.", "conclusion": "FairSHAP is a transparent and model-agnostic method that integrates easily into existing ML pipelines and offers insights into bias sources."}}
{"id": "2505.11157", "pdf": "https://arxiv.org/pdf/2505.11157", "abs": "https://arxiv.org/abs/2505.11157", "authors": ["Boris Bonev", "Max Rietmann", "Andrea Paris", "Alberto Carpentieri", "Thorsten Kurth"], "title": "Attention on the Sphere", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce a generalized attention mechanism for spherical domains,\nenabling Transformer architectures to natively process data defined on the\ntwo-dimensional sphere - a critical need in fields such as atmospheric physics,\ncosmology, and robotics, where preserving spherical symmetries and topology is\nessential for physical accuracy. By integrating numerical quadrature weights\ninto the attention mechanism, we obtain a geometrically faithful spherical\nattention that is approximately rotationally equivariant, providing strong\ninductive biases and leading to better performance than Cartesian approaches.\nTo further enhance both scalability and model performance, we propose\nneighborhood attention on the sphere, which confines interactions to geodesic\nneighborhoods. This approach reduces computational complexity and introduces\nthe additional inductive bias for locality, while retaining the symmetry\nproperties of our method. We provide optimized CUDA kernels and\nmemory-efficient implementations to ensure practical applicability. The method\nis validated on three diverse tasks: simulating shallow water equations on the\nrotating sphere, spherical image segmentation, and spherical depth estimation.\nAcross all tasks, our spherical Transformers consistently outperform their\nplanar counterparts, highlighting the advantage of geometric priors for\nlearning on spherical domains.", "AI": {"tldr": "A new attention mechanism for spherical domains improves Transformer performance in atmospheric physics, cosmology, and robotics by preserving spherical symmetries and topology.", "motivation": "To enable Transformer models to handle data on the two-dimensional sphere while preserving spherical symmetries and topology.", "method": "Generalized attention mechanism with numerical quadrature weights and neighborhood attention on the sphere.", "result": "Better performance than Cartesian approaches and improved scalability with reduced computational complexity.", "conclusion": "The proposed spherical transformers outperform planar counterparts across various tasks, demonstrating the importance of geometric priors."}}
{"id": "2505.11165", "pdf": "https://arxiv.org/pdf/2505.11165", "abs": "https://arxiv.org/abs/2505.11165", "authors": ["Haiqing Hao", "Nikola Zubi\u0107", "Weihua He", "Zhipeng Sui", "Davide Scaramuzza", "Wenhui Wang"], "title": "Maximizing Asynchronicity in Event-based Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "18 pages, 5 figures, 9 tables", "summary": "Event cameras deliver visual data with high temporal resolution, low latency,\nand minimal redundancy, yet their asynchronous, sparse sequential nature\nchallenges standard tensor-based machine learning (ML). While the recent\nasynchronous-to-synchronous (A2S) paradigm aims to bridge this gap by\nasynchronously encoding events into learned representations for ML pipelines,\nexisting A2S approaches often sacrifice representation expressivity and\ngeneralizability compared to dense, synchronous methods. This paper introduces\nEVA (EVent Asynchronous representation learning), a novel A2S framework to\ngenerate highly expressive and generalizable event-by-event representations.\nInspired by the analogy between events and language, EVA uniquely adapts\nadvances from language modeling in linear attention and self-supervised\nlearning for its construction. In demonstration, EVA outperforms prior A2S\nmethods on recognition tasks (DVS128-Gesture and N-Cars), and represents the\nfirst A2S framework to successfully master demanding detection tasks, achieving\na remarkable 47.7 mAP on the Gen1 dataset. These results underscore EVA's\ntransformative potential for advancing real-time event-based vision\napplications.", "AI": {"tldr": "EVA\u662f\u4e00\u79cd\u65b0\u7684\u5f02\u6b65\u4e8b\u4ef6\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u53ef\u751f\u6210\u9ad8\u5ea6\u8868\u8fbe\u6027\u548c\u6cdb\u5316\u7684\u4e8b\u4ef6\u8868\u793a\uff0c\u5728\u8bc6\u522b\u548c\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u5f02\u6b65\u5230\u540c\u6b65(A2S)\u65b9\u6cd5\u5728\u8868\u8fbe\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u901a\u5e38\u6bd4\u5bc6\u96c6\u540c\u6b65\u65b9\u6cd5\u5dee\u3002", "method": "EVA\u5229\u7528\u8bed\u8a00\u5efa\u6a21\u4e2d\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u6280\u672f\u6765\u9002\u5e94\u4e8b\u4ef6\u6570\u636e\u7684\u5f02\u6b65\u7279\u6027\u3002", "result": "EVA\u5728DVS128-Gesture\u548cN-Cars\u8bc6\u522b\u4efb\u52a1\u4ee5\u53caGen1\u68c0\u6d4b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "EVA\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u65f6\u4e8b\u4ef6\u89c6\u89c9\u5e94\u7528\u4e2d\u7684\u53d8\u9769\u6f5c\u529b\u3002"}}
{"id": "2505.11117", "pdf": "https://arxiv.org/pdf/2505.11117", "abs": "https://arxiv.org/abs/2505.11117", "authors": ["Chenhong Zhou", "Jie Chen", "Zaifeng Yang", "Ching Eng Png"], "title": "Dual-Balancing for Physics-Informed Neural Networks", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": "Accepted at IJCAI 2025 (34th International Joint Conference on\n  Artificial Intelligence)", "summary": "Physics-informed neural networks (PINNs) have emerged as a new learning\nparadigm for solving partial differential equations (PDEs) by enforcing the\nconstraints of physical equations, boundary conditions (BCs), and initial\nconditions (ICs) into the loss function. Despite their successes, vanilla PINNs\nstill suffer from poor accuracy and slow convergence due to the intractable\nmulti-objective optimization issue. In this paper, we propose a novel\nDual-Balanced PINN (DB-PINN), which dynamically adjusts loss weights by\nintegrating inter-balancing and intra-balancing to alleviate two imbalance\nissues in PINNs. Inter-balancing aims to mitigate the gradient imbalance\nbetween PDE residual loss and condition-fitting losses by determining an\naggregated weight that offsets their gradient distribution discrepancies.\nIntra-balancing acts on condition-fitting losses to tackle the imbalance in\nfitting difficulty across diverse conditions. By evaluating the fitting\ndifficulty based on the loss records, intra-balancing can allocate the\naggregated weight proportionally to each condition loss according to its\nfitting difficulty level. We further introduce a robust weight update strategy\nto prevent abrupt spikes and arithmetic overflow in instantaneous weight values\ncaused by large loss variances, enabling smooth weight updating and stable\ntraining. Extensive experiments demonstrate that DB-PINN achieves significantly\nsuperior performance than those popular gradient-based weighting methods in\nterms of convergence speed and prediction accuracy. Our code and supplementary\nmaterial are available at https://github.com/chenhong-zhou/DualBalanced-PINNs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Dual-Balanced PINN\uff08DB-PINN\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408inter-balancing\u548cintra-balancing\u6765\u52a8\u6001\u8c03\u6574\u635f\u5931\u6743\u91cd\uff0c\u89e3\u51b3\u4e86PINNs\u4e2d\u7684\u4e24\u4e2a\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u6536\u655b\u901f\u5ea6\u548c\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u5176\u4ed6\u6d41\u884c\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684PINNs\u5b58\u5728\u7531\u4e8e\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\u5bfc\u81f4\u7684\u7cbe\u5ea6\u5dee\u548c\u6536\u655b\u6162\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86DB-PINN\uff0c\u901a\u8fc7inter-balancing\u548cintra-balancing\u52a8\u6001\u8c03\u6574\u635f\u5931\u6743\u91cd\uff0c\u5e76\u5f15\u5165\u4e86\u9c81\u68d2\u7684\u6743\u91cd\u66f4\u65b0\u7b56\u7565\u3002", "result": "DB-PINN\u5728\u6536\u655b\u901f\u5ea6\u548c\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u68af\u5ea6\u4e3a\u57fa\u7840\u7684\u52a0\u6743\u65b9\u6cd5\u3002", "conclusion": "DB-PINN\u901a\u8fc7\u89e3\u51b3PINNs\u4e2d\u7684\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u5b66\u4e60\u8303\u5f0f\u3002"}}
{"id": "2505.11204", "pdf": "https://arxiv.org/pdf/2505.11204", "abs": "https://arxiv.org/abs/2505.11204", "authors": ["Hangyu Zhou", "Aaron Gokaslan", "Volodymyr Kuleshov", "Bharath Hariharan"], "title": "RanDeS: Randomized Delta Superposition for Multi-Model Compression", "categories": ["cs.LG", "cs.AI"], "comment": "https://github.com/Zhou-Hangyu/randes", "summary": "From a multi-model compression perspective, model merging enables\nmemory-efficient serving of multiple models fine-tuned from the same base, but\nsuffers from degraded performance due to interference among their task-specific\nparameter adjustments (i.e., deltas). In this paper, we reformulate model\nmerging as a compress-and-retrieve scheme, revealing that the task interference\narises from the summation of irrelevant deltas during model retrieval. To\naddress this issue, we use random orthogonal transformations to decorrelate\nthese vectors into self-cancellation. We show that this approach drastically\nreduces interference, improving performance across both vision and language\ntasks. Since these transformations are fully defined by random seeds, adding\nnew models requires no extra memory. Further, their data- and model-agnostic\nnature enables easy addition or removal of models with minimal compute\noverhead, supporting efficient and flexible multi-model serving.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u968f\u673a\u6b63\u4ea4\u53d8\u6362\u51cf\u5c11\u591a\u6a21\u578b\u5408\u5e76\u4e2d\u7684\u4efb\u52a1\u5e72\u6270\u7684\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u63d0\u5347\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u5185\u5b58\u5373\u53ef\u6dfb\u52a0\u65b0\u6a21\u578b\uff0c\u5e76\u4e14\u7531\u4e8e\u5176\u6570\u636e\u548c\u6a21\u578b\u65e0\u5173\u7684\u7279\u6027\uff0c\u652f\u6301\u9ad8\u6548\u7684\u591a\u6a21\u578b\u670d\u52a1\u3002", "motivation": "\u591a\u6a21\u578b\u5408\u5e76\u56e0\u4efb\u52a1\u7279\u5b9a\u53c2\u6570\u8c03\u6574\u4e4b\u95f4\u7684\u5e72\u6270\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5c06\u6a21\u578b\u5408\u5e76\u91cd\u65b0\u5b9a\u4e49\u4e3a\u538b\u7f29\u548c\u68c0\u7d22\u65b9\u6848\uff0c\u4f7f\u7528\u968f\u673a\u6b63\u4ea4\u53d8\u6362\u6765\u6d88\u9664\u65e0\u5173delta\u5411\u91cf\u7684\u76f8\u5173\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u5e72\u6270\uff0c\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u4e0a\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u5185\u5b58\u5373\u53ef\u6dfb\u52a0\u65b0\u6a21\u578b\uff0c\u4e14\u652f\u6301\u9ad8\u6548\u7075\u6d3b\u7684\u591a\u6a21\u578b\u670d\u52a1\u3002"}}
{"id": "2505.11125", "pdf": "https://arxiv.org/pdf/2505.11125", "abs": "https://arxiv.org/abs/2505.11125", "authors": ["Enjun Du", "Siyi Liu", "Yongqi Zhang"], "title": "GraphOracle: A Foundation Model for Knowledge Graph Reasoning", "categories": ["cs.LG"], "comment": null, "summary": "Foundation models have demonstrated remarkable capabilities across various\ndomains, but developing analogous models for knowledge graphs presents unique\nchallenges due to their dynamic nature and the need for cross-domain reasoning.\nTo address these issues, we introduce \\textbf{\\textsc{GraphOracle}}, a\nrelation-centric foundation model that unifies reasoning across knowledge\ngraphs by converting them into Relation-Dependency Graphs (RDG), explicitly\nencoding compositional patterns with fewer edges than prior methods. A\nquery-dependent attention mechanism is further developed to learn inductive\nrepresentations for both relations and entities. Pre-training on diverse\nknowledge graphs, followed by minutes-level fine-tuning, enables effective\ngeneralization to unseen entities, relations, and entire graphs. Through\ncomprehensive experiments on 31 diverse benchmarks spanning transductive,\ninductive, and cross-domain settings, we demonstrate consistent\nstate-of-the-art performance with minimal adaptation, improving the prediction\nperformance by up to 35\\% compared to the strongest baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGraphOracle\u7684\u5173\u7cfb\u4e2d\u5fc3\u7684\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u8f6c\u6362\u4e3a\u5173\u7cfb\u4f9d\u8d56\u56fe(RDG)\uff0c\u663e\u5f0f\u7f16\u7801\u7ec4\u5408\u6a21\u5f0f\uff0c\u5e76\u5f00\u53d1\u67e5\u8be2\u76f8\u5173\u7684\u6ce8\u610f\u529b\u673a\u5236\u6765\u5b66\u4e60\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u5bf9\u672a\u89c1\u5b9e\u4f53\u3001\u5173\u7cfb\u548c\u6574\u4e2a\u56fe\u7684\u6709\u6548\u6cdb\u5316\uff0c\u572831\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u7c7b\u4f3c\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u77e5\u8bc6\u56fe\u8c31\u6a21\u578b\u9762\u4e34\u52a8\u6001\u6027\u548c\u8de8\u9886\u57df\u63a8\u7406\u7684\u9700\u6c42\u7684\u72ec\u7279\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86GraphOracle\u6a21\u578b\uff0c\u5b83\u901a\u8fc7\u8f6c\u6362\u4e3a\u5173\u7cfb\u4f9d\u8d56\u56fe\u663e\u5f0f\u7f16\u7801\u7ec4\u5408\u6a21\u5f0f\uff0c\u5e76\u5f00\u53d1\u67e5\u8be2\u76f8\u5173\u7684\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u8868\u793a\u3002", "result": "\u572831\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u6700\u9ad8\u8fbe35\uff05\u3002", "conclusion": "GraphOracle\u5728\u77e5\u8bc6\u56fe\u8c31\u7684\u8de8\u9886\u57df\u63a8\u7406\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002"}}
{"id": "2505.11243", "pdf": "https://arxiv.org/pdf/2505.11243", "abs": "https://arxiv.org/abs/2505.11243", "authors": ["Elliot L. Epstein", "Apaar Sadhwani", "Kay Giesecke"], "title": "A Set-Sequence Model for Time Series", "categories": ["cs.LG", "cs.AI", "q-fin.CP", "I.2.6"], "comment": "Presented at the Workshop on Financial AI at ICLR 2025", "summary": "In many financial prediction problems, the behavior of individual units (such\nas loans, bonds, or stocks) is influenced by observable unit-level factors and\nmacroeconomic variables, as well as by latent cross-sectional effects.\nTraditional approaches attempt to capture these latent effects via handcrafted\nsummary features. We propose a Set-Sequence model that eliminates the need for\nhandcrafted features. The Set model first learns a shared cross-sectional\nsummary at each period. The Sequence model then ingests the summary-augmented\ntime series for each unit independently to predict its outcome. Both components\nare learned jointly over arbitrary sets sampled during training. Our approach\nharnesses the set nature of the cross-section and is computationally efficient,\ngenerating set summaries in linear time relative to the number of units. It is\nalso flexible, allowing the use of existing sequence models and accommodating a\nvariable number of units at inference. Empirical evaluations demonstrate that\nour Set-Sequence model significantly outperforms benchmarks on stock return\nprediction and mortgage behavior tasks. Code will be released.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSet-Sequence\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u91d1\u878d\u9884\u6d4b\u95ee\u9898\uff0c\u8be5\u6a21\u578b\u65e0\u9700\u624b\u5de5\u7279\u5f81\uff0c\u5e76\u4e14\u5728\u80a1\u7968\u56de\u62a5\u9884\u6d4b\u548c\u62b5\u62bc\u884c\u4e3a\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u901a\u8fc7\u624b\u5de5\u5236\u4f5c\u7684\u6c47\u603b\u7279\u5f81\u6765\u6355\u6349\u6f5c\u5728\u7684\u4ea4\u53c9\u5f71\u54cd\uff0c\u800c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u65e8\u5728\u6d88\u9664\u5bf9\u624b\u5de5\u7279\u5f81\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faSet-Sequence\u6a21\u578b\uff0c\u5305\u62ecSet\u6a21\u578b\uff08\u5b66\u4e60\u5171\u4eab\u7684\u4ea4\u53c9\u603b\u7ed3\uff09\u548cSequence\u6a21\u578b\uff08\u6444\u5165\u589e\u5f3a\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8fdb\u884c\u9884\u6d4b\uff09\uff0c\u5e76\u4e14\u4e24\u4e2a\u7ec4\u4ef6\u5728\u8bad\u7ec3\u671f\u95f4\u8054\u5408\u5b66\u4e60\u3002", "result": "\u5728\u80a1\u7968\u56de\u62a5\u9884\u6d4b\u548c\u62b5\u62bc\u884c\u4e3a\u4efb\u52a1\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0cSet-Sequence\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684Set-Sequence\u6a21\u578b\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u800c\u4e14\u5177\u6709\u8ba1\u7b97\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2505.11128", "pdf": "https://arxiv.org/pdf/2505.11128", "abs": "https://arxiv.org/abs/2505.11128", "authors": ["Simone Azeglio", "Arianna Di Bernardo"], "title": "What's Inside Your Diffusion Model? A Score-Based Riemannian Metric to Explore the Data Manifold", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Recent advances in diffusion models have demonstrated their remarkable\nability to capture complex image distributions, but the geometric properties of\nthe learned data manifold remain poorly understood. We address this gap by\nintroducing a score-based Riemannian metric that leverages the Stein score\nfunction from diffusion models to characterize the intrinsic geometry of the\ndata manifold without requiring explicit parameterization. Our approach defines\na metric tensor in the ambient space that stretches distances perpendicular to\nthe manifold while preserving them along tangential directions, effectively\ncreating a geometry where geodesics naturally follow the manifold's contours.\nWe develop efficient algorithms for computing these geodesics and demonstrate\ntheir utility for both interpolation between data points and extrapolation\nbeyond the observed data distribution. Through experiments on synthetic data\nwith known geometry, Rotated MNIST, and complex natural images via Stable\nDiffusion, we show that our score-based geodesics capture meaningful\ntransformations that respect the underlying data distribution. Our method\nconsistently outperforms baseline approaches on perceptual metrics (LPIPS) and\ndistribution-level metrics (FID, KID), producing smoother, more realistic image\ntransitions. These results reveal the implicit geometric structure learned by\ndiffusion models and provide a principled way to navigate the manifold of\nnatural images through the lens of Riemannian geometry.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578bStein\u5206\u6570\u51fd\u6570\u7684\u9ece\u66fc\u5ea6\u91cf\uff0c\u7528\u4e8e\u63cf\u8ff0\u6570\u636e\u6d41\u5f62\u7684\u5185\u5728\u51e0\u4f55\u7ed3\u6784\u3002\u8be5\u65b9\u6cd5\u5728\u63d2\u503c\u548c\u5916\u63a8\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u6269\u6563\u6a21\u578b\u5b66\u4e60\u5230\u7684\u9690\u5f0f\u51e0\u4f55\u7ed3\u6784\u3002", "motivation": "\u7406\u89e3\u6269\u6563\u6a21\u578b\u5b66\u4e60\u7684\u6570\u636e\u6d41\u5f62\u7684\u51e0\u4f55\u6027\u8d28\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8eStein\u5206\u6570\u51fd\u6570\u7684\u9ece\u66fc\u5ea6\u91cf\uff0c\u5b9a\u4e49\u4e86\u73af\u5883\u7a7a\u95f4\u4e2d\u7684\u5ea6\u91cf\u5f20\u91cf\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u6570\u636e\u3001Rotated MNIST\u548cStable Diffusion\u751f\u6210\u7684\u590d\u6742\u81ea\u7136\u56fe\u50cf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u5206\u6570\u7684\u6d4b\u5730\u7ebf\u80fd\u591f\u6355\u6349\u6709\u610f\u4e49\u7684\u53d8\u6362\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u6269\u6563\u6a21\u578b\u5b66\u4e60\u5230\u7684\u9690\u5f0f\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u8fc7\u9ece\u66fc\u51e0\u4f55\u900f\u955c\u5bfc\u822a\u81ea\u7136\u56fe\u50cf\u6d41\u5f62\u7684\u539f\u5219\u6027\u65b9\u6cd5\u3002"}}
{"id": "2505.11304", "pdf": "https://arxiv.org/pdf/2505.11304", "abs": "https://arxiv.org/abs/2505.11304", "authors": ["Shudi Weng", "Chao Ren", "Ming Xiao", "Mikael Skoglund"], "title": "Heterogeneity-Aware Client Sampling: A Unified Solution for Consistent Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated learning (FL) commonly involves clients with diverse communication\nand computational capabilities. Such heterogeneity can significantly distort\nthe optimization dynamics and lead to objective inconsistency, where the global\nmodel converges to an incorrect stationary point potentially far from the\npursued optimum. Despite its critical impact, the joint effect of communication\nand computation heterogeneity has remained largely unexplored, due to the\nintrinsic complexity of their interaction. In this paper, we reveal the\nfundamentally distinct mechanisms through which heterogeneous communication and\ncomputation drive inconsistency in FL. To the best of our knowledge, this is\nthe first unified theoretical analysis of general heterogeneous FL, offering a\nprincipled understanding of how these two forms of heterogeneity jointly\ndistort the optimization trajectory under arbitrary choices of local solvers.\nMotivated by these insights, we propose Federated Heterogeneity-Aware Client\nSampling, FedACS, a universal method to eliminate all types of objective\ninconsistency. We theoretically prove that FedACS converges to the correct\noptimum at a rate of $O(1/\\sqrt{R})$, even in dynamic heterogeneous\nenvironments. Extensive experiments across multiple datasets show that FedACS\noutperforms state-of-the-art and category-specific baselines by 4.3%-36%, while\nreducing communication costs by 22%-89% and computation loads by 14%-105%,\nrespectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5FedACS\uff0c\u7528\u4e8e\u89e3\u51b3\u901a\u4fe1\u548c\u8ba1\u7b97\u5f02\u6784\u6027\u5f15\u8d77\u7684\u4f18\u5316\u8f68\u8ff9\u626d\u66f2\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u8054\u90a6\u5b66\u4e60\u5728\u5904\u7406\u901a\u4fe1\u548c\u8ba1\u7b97\u5f02\u6784\u6027\u65f6\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u5206\u6790\u3002", "method": "\u63d0\u51faFedACS\u65b9\u6cd5\uff0c\u901a\u8fc7\u5ba2\u6237\u7aef\u91c7\u6837\u6d88\u9664\u5404\u79cd\u76ee\u6807\u4e0d\u4e00\u81f4\u6027\u3002", "result": "FedACS\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u4e14\u51cf\u5c11\u4e86\u901a\u4fe1\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "FedACS\u80fd\u591f\u6536\u655b\u5230\u6b63\u786e\u7684\u6700\u4f18\u89e3\uff0c\u5e76\u5bf9\u52a8\u6001\u5f02\u6784\u73af\u5883\u5177\u6709\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.11132", "pdf": "https://arxiv.org/pdf/2505.11132", "abs": "https://arxiv.org/abs/2505.11132", "authors": ["Feng Xiao", "Xiaoying Tang", "Jicong Fan"], "title": "Fairness-aware Anomaly Detection via Fair Projection", "categories": ["cs.LG"], "comment": null, "summary": "Unsupervised anomaly detection is a critical task in many high-social-impact\napplications such as finance, healthcare, social media, and cybersecurity,\nwhere demographics involving age, gender, race, disease, etc, are used\nfrequently. In these scenarios, possible bias from anomaly detection systems\ncan lead to unfair treatment for different groups and even exacerbate social\nbias. In this work, first, we thoroughly analyze the feasibility and necessary\nassumptions for ensuring group fairness in unsupervised anomaly detection.\nSecond, we propose a novel fairness-aware anomaly detection method FairAD. From\nthe normal training data, FairAD learns a projection to map data of different\ndemographic groups to a common target distribution that is simple and compact,\nand hence provides a reliable base to estimate the density of the data. The\ndensity can be directly used to identify anomalies while the common target\ndistribution ensures fairness between different groups. Furthermore, we propose\na threshold-free fairness metric that provides a global view for model's\nfairness, eliminating dependence on manual threshold selection. Experiments on\nreal-world benchmarks demonstrate that our method achieves an improved\ntrade-off between detection accuracy and fairness under both balanced and\nskewed data across different groups.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u516c\u5e73\u611f\u77e5\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5FairAD\uff0c\u786e\u4fdd\u4e0d\u540c\u7fa4\u4f53\u4e4b\u95f4\u7684\u516c\u5e73\u6027\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7cfb\u7edf\u53ef\u80fd\u5f15\u5165\u4e0d\u516c\u5e73\u6027\uff0c\u5f71\u54cd\u4e0d\u540c\u7fa4\u4f53\u7684\u516c\u6b63\u5f85\u9047\u3002", "method": "\u901a\u8fc7\u5b66\u4e60\u6295\u5f71\u5c06\u4e0d\u540c\u7fa4\u4f53\u7684\u6570\u636e\u6620\u5c04\u5230\u5171\u540c\u7684\u76ee\u6807\u5206\u5e03\uff0c\u5e76\u63d0\u51fa\u65e0\u9608\u503c\u516c\u5e73\u5ea6\u91cf\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFairAD\u5728\u4fdd\u6301\u68c0\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u516c\u5e73\u6027\u3002", "conclusion": "FairAD\u80fd\u591f\u5728\u5e73\u8861\u548c\u503e\u659c\u6570\u636e\u4e0b\u5b9e\u73b0\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u7684\u826f\u597d\u6743\u8861\u3002"}}
{"id": "2505.11409", "pdf": "https://arxiv.org/pdf/2505.11409", "abs": "https://arxiv.org/abs/2505.11409", "authors": ["Yi Xu", "Chengzu Li", "Han Zhou", "Xingchen Wan", "Caiqi Zhang", "Anna Korhonen", "Ivan Vuli\u0107"], "title": "Visual Planning: Let's Think Only with Images", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "10 pages, 6 figures, 1 table (26 pages, 12 figures, 8 tables\n  including references and appendices)", "summary": "Recent advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have substantially enhanced machine reasoning across diverse\ntasks. However, these models predominantly rely on pure text as the medium for\nboth expressing and structuring reasoning, even when visual information is\npresent. In this work, we argue that language may not always be the most\nnatural or effective modality for reasoning, particularly in tasks involving\nspatial and geometrical information. Motivated by this, we propose a new\nparadigm, Visual Planning, which enables planning through purely visual\nrepresentations, independent of text. In this paradigm, planning is executed\nvia sequences of images that encode step-by-step inference in the visual\ndomain, akin to how humans sketch or visualize future actions. We introduce a\nnovel reinforcement learning framework, Visual Planning via Reinforcement\nLearning (VPRL), empowered by GRPO for post-training large vision models,\nleading to substantial improvements in planning in a selection of\nrepresentative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our\nvisual planning paradigm outperforms all other planning variants that conduct\nreasoning in the text-only space. Our results establish Visual Planning as a\nviable and promising alternative to language-based reasoning, opening new\navenues for tasks that benefit from intuitive, image-based inference.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u89c4\u5212\u8303\u5f0f\uff0c\u901a\u8fc7\u7eaf\u89c6\u89c9\u8868\u793a\u8fdb\u884c\u89c4\u5212\uff0c\u65e0\u9700\u6587\u672c\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u8bed\u8a00\u53ef\u80fd\u4e0d\u662f\u5904\u7406\u6d89\u53ca\u7a7a\u95f4\u548c\u51e0\u4f55\u4fe1\u606f\u7684\u4efb\u52a1\u65f6\u6700\u81ea\u7136\u6216\u6709\u6548\u7684\u63a8\u7406\u6a21\u6001\u3002", "method": "\u5f15\u5165\u4e86\u89c6\u89c9\u89c4\u5212\uff08Visual Planning\uff09\u8303\u5f0f\uff0c\u4ee5\u53ca\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6VPRL\uff0c\u7528\u4e8e\u5927\u578b\u89c6\u89c9\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u3002", "result": "\u5728FrozenLake\u3001Maze\u548cMiniBehavior\u7b49\u4ee3\u8868\u6027\u89c6\u89c9\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0c\u89c6\u89c9\u89c4\u5212\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u4ec5\u6587\u672c\u7684\u63a8\u7406\u53d8\u4f53\u3002", "conclusion": "\u89c6\u89c9\u89c4\u5212\u662f\u4e00\u79cd\u53ef\u884c\u4e14\u6709\u524d\u666f\u7684\u8bed\u8a00\u63a8\u7406\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3a\u4f9d\u8d56\u76f4\u89c2\u56fe\u50cf\u63a8\u7406\u7684\u4efb\u52a1\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2505.11134", "pdf": "https://arxiv.org/pdf/2505.11134", "abs": "https://arxiv.org/abs/2505.11134", "authors": ["Desong Zhang", "Jia Hu", "Geyong Min"], "title": "Towards Robust Spiking Neural Networks:Mitigating Heterogeneous Training Vulnerability via Dominant Eigencomponent Projection", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Spiking Neural Networks (SNNs) process information via discrete spikes,\nenabling them to operate at remarkably low energy levels. However, our\nexperimental observations reveal a striking vulnerability when SNNs are trained\nusing the mainstream method--direct encoding combined with backpropagation\nthrough time (BPTT): even a single backward pass on data drawn from a slightly\ndifferent distribution can lead to catastrophic network collapse. Our\ntheoretical analysis attributes this vulnerability to the repeated inputs\ninherent in direct encoding and the gradient accumulation characteristic of\nBPTT, which together produce an exceptional large Hessian spectral radius. To\naddress this challenge, we develop a hyperparameter-free method called Dominant\nEigencomponent Projection (DEP). By orthogonally projecting gradients to\nprecisely remove their dominant components, DEP effectively reduces the Hessian\nspectral radius, thereby preventing SNNs from settling into sharp minima.\nExtensive experiments demonstrate that DEP not only mitigates the vulnerability\nof SNNs to heterogeneous data poisoning, but also significantly enhances\noverall robustness compared to key baselines, providing strong support for\nsafer and more reliable SNN deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8d85\u53c2\u6570\u7684Dominant Eigencomponent Projection (DEP)\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u7531\u76f4\u63a5\u7f16\u7801\u548cBPTT\u5bfc\u81f4\u7684\u5c16\u9510\u6781\u5c0f\u503c\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86SNN\u5bf9\u5f02\u6784\u6570\u636e\u6295\u6bd2\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\u4e3b\u6d41\u8bad\u7ec3\u65b9\u6cd5\u4e0bSNN\u5bf9\u5206\u5e03\u53d8\u5316\u654f\u611f\uff0c\u6613\u53d1\u751f\u7f51\u7edc\u5d29\u6e83\u3002", "method": "\u5f00\u53d1\u4e86\u540d\u4e3aDominant Eigencomponent Projection (DEP)\u7684\u65e0\u8d85\u53c2\u6570\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u4ea4\u6295\u5f71\u53bb\u9664\u68af\u5ea6\u7684\u4e3b\u8981\u6210\u5206\u6765\u964d\u4f4eHessian\u8c31\u534a\u5f84\u3002", "result": "\u5b9e\u9a8c\u663e\u793aDEP\u4e0d\u4ec5\u51cf\u8f7b\u4e86SNN\u5bf9\u5f02\u6784\u6570\u636e\u6295\u6bd2\u7684\u8106\u5f31\u6027\uff0c\u8fd8\u63d0\u5347\u4e86\u6574\u4f53\u9c81\u68d2\u6027\u3002", "conclusion": "DEP\u4e3a\u66f4\u5b89\u5168\u53ef\u9760\u7684SNN\u90e8\u7f72\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2505.11427", "pdf": "https://arxiv.org/pdf/2505.11427", "abs": "https://arxiv.org/abs/2505.11427", "authors": ["Adrian Robert Minut", "Tommaso Mencattini", "Andrea Santilli", "Donato Crisostomi", "Emanuele Rodol\u00e0"], "title": "Mergenetic: a Simple Evolutionary Model Merging Library", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": "Link: https://github.com/tommasomncttn/mergenetic", "summary": "Model merging allows combining the capabilities of existing models into a new\none - post hoc, without additional training. This has made it increasingly\npopular thanks to its low cost and the availability of libraries that support\nmerging on consumer GPUs. Recent work shows that pairing merging with\nevolutionary algorithms can boost performance, but no framework currently\nsupports flexible experimentation with such strategies in language models. We\nintroduce Mergenetic, an open-source library for evolutionary model merging.\nMergenetic enables easy composition of merging methods and evolutionary\nalgorithms while incorporating lightweight fitness estimators to reduce\nevaluation costs. We describe its design and demonstrate that Mergenetic\nproduces competitive results across tasks and languages using modest hardware.", "AI": {"tldr": "This paper introduces Mergenetic, an open-source library for evolutionary model merging, enabling flexible experimentation with model merging strategies in language models.", "motivation": "Pairing model merging with evolutionary algorithms can boost performance but there is no framework supporting flexible experimentation with such strategies in language models.", "method": "Introduces Mergenetic, an open-source library for evolutionary model merging which enables easy composition of merging methods and evolutionary algorithms while incorporating lightweight fitness estimators.", "result": "Mergenetic produces competitive results across tasks and languages using modest hardware.", "conclusion": "Mergenetic produces competitive results across tasks and languages using modest hardware."}}
{"id": "2505.11139", "pdf": "https://arxiv.org/pdf/2505.11139", "abs": "https://arxiv.org/abs/2505.11139", "authors": ["Om Roy", "Yashar Moshfeghi", "Keith Smith"], "title": "Covariance Density Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Graph neural networks have re-defined how we model and predict on network\ndata but there lacks a consensus on choosing the correct underlying graph\nstructure on which to model signals. CoVariance Neural Networks (VNN) address\nthis issue by using the sample covariance matrix as a Graph Shift Operator\n(GSO). Here, we improve on the performance of VNNs by constructing a Density\nMatrix where we consider the sample Covariance matrix as a quasi-Hamiltonian of\nthe system in the space of random variables. Crucially, using this density\nmatrix as the GSO allows components of the data to be extracted at different\nscales, allowing enhanced discriminability and performance. We show that this\napproach allows explicit control of the stability-discriminability trade-off of\nthe network, provides enhanced robustness to noise compared to VNNs, and\noutperforms them in useful real-life applications where the underlying\ncovariance matrix is informative. In particular, we show that our model can\nachieve strong performance in subject-independent Brain Computer Interface EEG\nmotor imagery classification, outperforming EEGnet while being faster. This\nshows how covariance density neural networks provide a basis for the\nnotoriously difficult task of transferability of BCIs when evaluated on unseen\nindividuals.", "AI": {"tldr": "This paper introduces an improved version of CoVariance Neural Networks (VNNs) called Density Matrix-based VNNs, which use a density matrix as the Graph Shift Operator (GSO) to enhance the performance in tasks like EEG motor imagery classification.", "motivation": "The lack of consensus on choosing the correct underlying graph structure for modeling signals in graph neural networks.", "method": "Constructing a Density Matrix using the sample covariance matrix as a quasi-Hamiltonian in the space of random variables and using it as the GSO.", "result": "The proposed method improves performance in terms of discriminability and robustness to noise compared to VNNs, especially in real-life applications with informative covariance matrices. It also shows strong performance in subject-independent Brain Computer Interface EEG motor imagery classification.", "conclusion": "Density Matrix-based VNNs provide a foundation for improving the transferability of Brain Computer Interfaces."}}
{"id": "2505.11153", "pdf": "https://arxiv.org/pdf/2505.11153", "abs": "https://arxiv.org/abs/2505.11153", "authors": ["Ashok Arora", "Neetesh Kumar"], "title": "Bi-directional Recurrence Improves Transformer in Partially Observable Markov Decision Processes", "categories": ["cs.LG"], "comment": null, "summary": "In real-world reinforcement learning (RL) scenarios, agents often encounter\npartial observability, where incomplete or noisy information obscures the true\nstate of the environment. Partially Observable Markov Decision Processes\n(POMDPs) are commonly used to model these environments, but effective\nperformance requires memory mechanisms to utilise past observations. While\nrecurrence networks have traditionally addressed this need, transformer-based\nmodels have recently shown improved sample efficiency in RL tasks. However,\ntheir application to POMDPs remains underdeveloped, and their real-world\ndeployment is constrained due to the high parameter count. This work introduces\na novel bi-recurrent model architecture that improves sample efficiency and\nreduces model parameter count in POMDP scenarios. The architecture replaces the\nmultiple feed forward layers with a single layer of bi-directional recurrence\nunit to better capture and utilize sequential dependencies and contextual\ninformation. This approach improves the model's ability to handle partial\nobservability and increases sample efficiency, enabling effective learning from\ncomparatively fewer interactions. To evaluate the performance of the proposed\nmodel architecture, experiments were conducted on a total of 23 POMDP\nenvironments. The proposed model architecture outperforms existing\ntransformer-based, attention-based, and recurrence-based methods by a margin\nranging from 87.39% to 482.04% on average across the 23 POMDP environments.", "AI": {"tldr": "This paper introduces a new bi-recurrent model for partially observable Markov decision processes (POMDPs) that improves sample efficiency and reduces model parameters.", "motivation": "Reinforcement learning agents often face partial observability, and current transformer-based models have high parameter counts.", "method": "A bi-recurrent model architecture with a single layer of bi-directional recurrence unit was developed to replace multiple feedforward layers.", "result": "The proposed model outperformed existing transformer-based, attention-based, and recurrence-based methods by 87.39%-482.04% on average across 23 POMDP environments.", "conclusion": "The bi-recurrent model architecture can effectively handle partial observability and improve learning efficiency."}}
{"id": "2505.11170", "pdf": "https://arxiv.org/pdf/2505.11170", "abs": "https://arxiv.org/abs/2505.11170", "authors": ["Myeonghwan Ahn", "Sungjoo Yoo"], "title": "Gaussian Weight Sampling for Scalable, Efficient and Stable Pseudo-Quantization Training", "categories": ["cs.LG"], "comment": null, "summary": "Ever-growing scale of large language models (LLMs) is pushing for improved\nefficiency, favoring fully quantized training (FQT) over BF16. While FQT\naccelerates training, it faces consistency challenges and requires searching\nover an exponential number of cases, each needing over 200B tokens to ensure\nstability.\n  Pseudo-quantization training (PQT) addresses the issues of FQT, although it\nis not well-studied. We explore the practical implications of PQT in detail and\npropose a noise distribution $R$ that is floating-point (FP)-friendly, with\nideal properties including stochastic precision annealing. As a result, the\nproposed method serves as an effective theoretical foundation for low-precision\nFP parameters through PQT, utilizing efficient fake quantization via an\naddition and subsequent FP casting.\n  We demonstrate that Gaussian weight sampling is (1) scalable: supports\nlow-precision FP parameters down to FP6 and high-precision noise up to 9-bit\nwith BF16 operator. The proposed method is (2) efficient: incurring\ncomputational overhead as low as 1.40\\% on the A100 GPU in terms of Llama2\ntraining tokens per second, and requiring 2 bytes per parameter in GPU memory.\nWe demonstrate that PQT with Gaussian weight sampling is (3) stable: closely\nfollowing or even surpassing performance of the BF16 baseline while\npre-training GPT2 and Llama2 models with up to 1B parameters and 300B tokens.", "AI": {"tldr": "This paper explores pseudo-quantization training (PQT) for large language models, proposing a Gaussian weight sampling method that is scalable, efficient, and stable compared to BF16.", "motivation": "To address the challenges faced by fully quantized training (FQT), such as consistency issues and exponential case searches.", "method": "Introduces a Gaussian weight sampling method within PQT that is floating-point friendly and includes stochastic precision annealing.", "result": "The method is shown to be scalable, efficient with minimal computational overhead, and stable in pre-training models.", "conclusion": "Pseudo-quantization training with Gaussian weight sampling provides a strong theoretical foundation for low-precision FP parameters and outperforms or matches BF16 in certain scenarios."}}
{"id": "2505.11185", "pdf": "https://arxiv.org/pdf/2505.11185", "abs": "https://arxiv.org/abs/2505.11185", "authors": ["Francesco Madeddu", "Lucia Testa", "Gianluca De Carlo", "Michele Pieroni", "Andrea Mastropietro", "Aris Anagnostopoulos", "Paolo Tieri", "Sergio Barbarossa"], "title": "VitaGraph: Building a Knowledge Graph for Biologically Relevant Learning Tasks", "categories": ["cs.LG"], "comment": "9 pages of main text, 4 figures", "summary": "The intrinsic complexity of human biology presents ongoing challenges to\nscientific understanding. Researchers collaborate across disciplines to expand\nour knowledge of the biological interactions that define human life. AI\nmethodologies have emerged as powerful tools across scientific domains,\nparticularly in computational biology, where graph data structures effectively\nmodel biological entities such as protein-protein interaction (PPI) networks\nand gene functional networks. Those networks are used as datasets for paramount\nnetwork medicine tasks, such as gene-disease association prediction, drug\nrepurposing, and polypharmacy side effect studies. Reliable predictions from\nmachine learning models require high-quality foundational data. In this work,\nwe present a comprehensive multi-purpose biological knowledge graph constructed\nby integrating and refining multiple publicly available datasets. Building upon\nthe Drug Repurposing Knowledge Graph (DRKG), we define a pipeline tasked with\na) cleaning inconsistencies and redundancies present in DRKG, b) coalescing\ninformation from the main available public data sources, and c) enriching the\ngraph nodes with expressive feature vectors such as molecular fingerprints and\ngene ontologies. Biologically and chemically relevant features improve the\ncapacity of machine learning models to generate accurate and well-structured\nembedding spaces. The resulting resource represents a coherent and reliable\nbiological knowledge graph that serves as a state-of-the-art platform to\nadvance research in computational biology and precision medicine. Moreover, it\noffers the opportunity to benchmark graph-based machine learning and network\nmedicine models on relevant tasks. We demonstrate the effectiveness of the\nproposed dataset by benchmarking it against the task of drug repurposing, PPI\nprediction, and side-effect prediction, modeled as link prediction problems.", "AI": {"tldr": "\u6b64\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7efc\u5408\u6027\u7684\u591a\u7528\u9014\u751f\u7269\u77e5\u8bc6\u56fe\u8c31\uff0c\u8be5\u56fe\u8c31\u901a\u8fc7\u6574\u5408\u548c\u7cbe\u70bc\u591a\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\u6784\u5efa\u800c\u6210\u3002\u5b83\u5c55\u793a\u4e86\u5728\u836f\u7269\u91cd\u5b9a\u4f4d\u3001\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u548c\u526f\u4f5c\u7528\u9884\u6d4b\u7b49\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u4eba\u7c7b\u751f\u7269\u5b66\u7684\u56fa\u6709\u590d\u6742\u6027\u5bf9\u79d1\u5b66\u7814\u7a76\u63d0\u51fa\u4e86\u6301\u7eed\u7684\u6311\u6218\u3002\u8de8\u5b66\u79d1\u5408\u4f5c\u65e8\u5728\u6269\u5c55\u6211\u4eec\u5bf9\u5b9a\u4e49\u4eba\u7c7b\u751f\u547d\u7684\u751f\u7269\u76f8\u4e92\u4f5c\u7528\u7684\u7406\u89e3\u3002AI\u65b9\u6cd5\u8bba\u5728\u79d1\u5b66\u9886\u57df\u7279\u522b\u662f\u8ba1\u7b97\u751f\u7269\u5b66\u4e2d\u6210\u4e3a\u5f3a\u5927\u7684\u5de5\u5177\u3002\u7136\u800c\uff0c\u53ef\u9760\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u9700\u8981\u9ad8\u8d28\u91cf\u7684\u57fa\u7840\u6570\u636e\u3002", "method": "\u901a\u8fc7\u6574\u5408\u548c\u7cbe\u70bc\u591a\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\u6784\u5efa\u4e86\u4e00\u4e2a\u7efc\u5408\u6027\u7684\u591a\u7528\u9014\u751f\u7269\u77e5\u8bc6\u56fe\u8c31\u3002\u57fa\u4e8e\u836f\u7269\u518d\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\uff08DRKG\uff09\uff0c\u5b9a\u4e49\u4e86\u4e00\u79cd\u5904\u7406\u7ba1\u9053\u6765\u6e05\u7406\u4e0d\u4e00\u81f4\u548c\u5197\u4f59\uff0c\u5408\u5e76\u4fe1\u606f\uff0c\u5e76\u4e30\u5bcc\u8282\u70b9\u7279\u5f81\u5411\u91cf\u3002", "result": "\u63d0\u51fa\u7684\u8d44\u6e90\u4ee3\u8868\u4e86\u4e00\u4e2a\u8fde\u8d2f\u4e14\u53ef\u9760\u7684\u751f\u7269\u77e5\u8bc6\u56fe\u8c31\uff0c\u53ef\u4f5c\u4e3a\u63a8\u52a8\u8ba1\u7b97\u751f\u7269\u5b66\u548c\u7cbe\u51c6\u533b\u5b66\u7814\u7a76\u7684\u6700\u5148\u8fdb\u5e73\u53f0\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u63d0\u4f9b\u4e86\u5728\u76f8\u5173\u4efb\u52a1\u4e0a\u57fa\u51c6\u6d4b\u8bd5\u57fa\u4e8e\u56fe\u7684\u673a\u5668\u5b66\u4e60\u548c\u7f51\u7edc\u533b\u5b66\u6a21\u578b\u7684\u673a\u4f1a\u3002", "conclusion": "\u6784\u5efa\u7684\u77e5\u8bc6\u56fe\u8c31\u5728\u836f\u7269\u91cd\u5b9a\u4f4d\u3001\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u548c\u526f\u4f5c\u7528\u9884\u6d4b\u7b49\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u6709\u6548\u6027\uff0c\u6709\u52a9\u4e8e\u63a8\u8fdb\u8ba1\u7b97\u751f\u7269\u5b66\u548c\u7cbe\u51c6\u533b\u5b66\u7684\u7814\u7a76\u3002"}}
{"id": "2505.11197", "pdf": "https://arxiv.org/pdf/2505.11197", "abs": "https://arxiv.org/abs/2505.11197", "authors": ["Zhenyi Zhang", "Zihan Wang", "Yuhao Sun", "Tiejun Li", "Peijie Zhou"], "title": "Modeling Cell Dynamics and Interactions with Unbalanced Mean Field Schr\u00f6dinger Bridge", "categories": ["cs.LG", "math.OC", "q-bio.QM"], "comment": null, "summary": "Modeling the dynamics from sparsely time-resolved snapshot data is crucial\nfor understanding complex cellular processes and behavior. Existing methods\nleverage optimal transport, Schr\\\"odinger bridge theory, or their variants to\nsimultaneously infer stochastic, unbalanced dynamics from snapshot data.\nHowever, these approaches remain limited in their ability to account for\ncell-cell interactions. This integration is essential in real-world scenarios\nsince intercellular communications are fundamental life processes and can\ninfluence cell state-transition dynamics. To address this challenge, we\nformulate the Unbalanced Mean-Field Schr\\\"odinger Bridge (UMFSB) framework to\nmodel unbalanced stochastic interaction dynamics from snapshot data. Inspired\nby this framework, we further propose CytoBridge, a deep learning algorithm\ndesigned to approximate the UMFSB problem. By explicitly modeling cellular\ntransitions, proliferation, and interactions through neural networks,\nCytoBridge offers the flexibility to learn these processes directly from data.\nThe effectiveness of our method has been extensively validated using both\nsynthetic gene regulatory data and real scRNA-seq datasets. Compared to\nexisting methods, CytoBridge identifies growth, transition, and interaction\npatterns, eliminates false transitions, and reconstructs the developmental\nlandscape with greater accuracy.", "AI": {"tldr": "A novel deep learning approach, CytoBridge, models stochastic interaction dynamics from sparse single-cell data, improving accuracy in identifying growth, transition, and interaction patterns compared to existing methods.", "motivation": "To model unbalanced stochastic interaction dynamics from sparse snapshot data while accounting for cell-cell interactions which existing methods fail to do adequately.", "method": "Proposes CytoBridge, a deep learning algorithm that approximates the Unbalanced Mean-Field Schr\u00f6dinger Bridge (UMFSB) problem.", "result": "Validated effectively on synthetic gene regulatory data and real scRNA-seq datasets, showing better accuracy in identifying growth, transition, and interaction patterns, and eliminating false transitions.", "conclusion": "CytoBridge successfully models cell interactions, transitions, and proliferation, improving accuracy in reconstructing developmental landscapes."}}
{"id": "2505.11221", "pdf": "https://arxiv.org/pdf/2505.11221", "abs": "https://arxiv.org/abs/2505.11221", "authors": ["Donghoon Lee", "Tung M. Luu", "Younghwan Lee", "Chang D. Yoo"], "title": "Sample Efficient Reinforcement Learning via Large Vision Language Model Distillation", "categories": ["cs.LG"], "comment": "5 pages, ICASSP 2025. The first two authors are equally contributed", "summary": "Recent research highlights the potential of multimodal foundation models in\ntackling complex decision-making challenges. However, their large parameters\nmake real-world deployment resource-intensive and often impractical for\nconstrained systems. Reinforcement learning (RL) shows promise for\ntask-specific agents but suffers from high sample complexity, limiting\npractical applications. To address these challenges, we introduce LVLM to\nPolicy (LVLM2P), a novel framework that distills knowledge from large\nvision-language models (LVLM) into more efficient RL agents. Our approach\nleverages the LVLM as a teacher, providing instructional actions based on\ntrajectories collected by the RL agent, which helps reduce less meaningful\nexploration in the early stages of learning, thereby significantly accelerating\nthe agent's learning progress. Additionally, by leveraging the LVLM to suggest\nactions directly from visual observations, we eliminate the need for manual\ntextual descriptors of the environment, enhancing applicability across diverse\ntasks. Experiments show that LVLM2P significantly enhances the sample\nefficiency of baseline RL algorithms.", "AI": {"tldr": "This paper introduces LVLM2P, a framework that uses large vision-language models to teach task-specific reinforcement learning agents, improving sample efficiency and applicability.", "motivation": "The motivation is to make reinforcement learning more practical by reducing its sample complexity and increasing its applicability across diverse tasks.", "method": "LVLM2P uses a large vision-language model as a teacher to provide instructional actions to a reinforcement learning agent, reducing unnecessary exploration and suggesting actions directly from visual observations.", "result": "Experiments show that LVLM2P significantly improves the sample efficiency of baseline reinforcement learning algorithms.", "conclusion": "LVLM2P is a promising approach to making reinforcement learning more efficient and applicable for real-world use."}}
{"id": "2505.11230", "pdf": "https://arxiv.org/pdf/2505.11230", "abs": "https://arxiv.org/abs/2505.11230", "authors": ["Oskar Bohn Lassen", "Serio Agriesti", "Mohamed Eldafrawi", "Daniele Gammelli", "Guido Cantelmo", "Guido Gentile", "Francisco Camara Pereira"], "title": "Learning traffic flows: Graph Neural Networks for Metamodelling Traffic Assignment", "categories": ["cs.LG"], "comment": null, "summary": "The Traffic Assignment Problem is a fundamental, yet computationally\nexpensive, task in transportation modeling, especially for large-scale\nnetworks. Traditional methods require iterative simulations to reach\nequilibrium, making real-time or large-scale scenario analysis challenging. In\nthis paper, we propose a learning-based approach using Message-Passing Neural\nNetworks as a metamodel to approximate the equilibrium flow of the Stochastic\nUser Equilibrium assignment. Our model is designed to mimic the algorithmic\nstructure used in conventional traffic simulators allowing it to better capture\nthe underlying process rather than just the data. We benchmark it against other\nconventional deep learning techniques and evaluate the model's robustness by\ntesting its ability to predict traffic flows on input data outside the domain\non which it was trained. This approach offers a promising solution for\naccelerating out-of-distribution scenario assessments, reducing computational\ncosts in large-scale transportation planning, and enabling real-time\ndecision-making.", "AI": {"tldr": "This paper presents a learning-based method using Message-Passing Neural Networks to approximate the equilibrium flow of the Stochastic User Equilibrium assignment, aiming to accelerate out-of-distribution scenario assessments and enable real-time decision-making.", "motivation": "Traditional traffic assignment methods are computationally expensive and challenging for real-time or large-scale scenario analysis.", "method": "A learning-based approach using Message-Passing Neural Networks as a metamodel to approximate the equilibrium flow.", "result": "The model is benchmarked against other conventional deep learning techniques and evaluated for robustness in predicting traffic flows on unseen data.", "conclusion": "This approach offers a promising solution for accelerating out-of-distribution scenario assessments, reducing computational costs, and enabling real-time decision-making."}}
{"id": "2505.11235", "pdf": "https://arxiv.org/pdf/2505.11235", "abs": "https://arxiv.org/abs/2505.11235", "authors": ["Fei Wu", "Jia Hu", "Geyong Min", "Shiqiang Wang"], "title": "Memory-Efficient Orthogonal Fine-Tuning with Principal Subspace Adaptation", "categories": ["cs.LG"], "comment": null, "summary": "Driven by the relentless growth in model parameters, which renders full\nfine-tuning prohibitively expensive for large-scale deployment,\nparameter-efficient fine-tuning (PEFT) has emerged as a crucial approach for\nrapidly adapting large models to a wide range of downstream tasks. Among the\nPEFT family, orthogonal fine-tuning and its variants have demonstrated\nremarkable performance by preserving hyperspherical energy, which encodes\npairwise angular similarity between neurons. However, these methods are\ninherently memory-inefficient due to the need to store intermediate activations\nfrom multiple full-dimensional sparse matrices. To address this limitation, we\npropose Memory-efficient Orthogonal Fine-Tuning (MOFT) with principal subspace\nadaptation. Specifically, we first establish a theoretical condition under\nwhich orthogonal transformations within a low-rank subspace preserve\nhyperspherical energy. Based on this insight, we constrain orthogonal\nfine-tuning to the principal subspace defined by the top-r components obtained\nthrough singular value decomposition and impose an additional constraint on the\nprojection matrix to satisfy the preservation condition. To enhance MOFT's\nflexibility across tasks, we relax strict orthogonality by introducing two\nlearnable scaling vectors. Extensive experiments on 37 diverse tasks and four\nmodels across NLP and CV demonstrate that MOFT consistently outperforms key\nbaselines while significantly reducing the memory footprint of orthogonal\nfine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5185\u5b58\u9ad8\u6548\u6b63\u4ea4\u5fae\u8c03\u65b9\u6cd5MOFT\uff0c\u901a\u8fc7\u4e3b\u5b50\u7a7a\u95f4\u9002\u5e94\u5728\u4fdd\u6301\u8d85\u7403\u80fd\u91cf\u7684\u540c\u65f6\u51cf\u5c11\u4e86\u5185\u5b58\u6d88\u8017\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u6b63\u4ea4\u5fae\u8c03\u65b9\u6cd5\u56e0\u5b58\u50a8\u591a\u4e2a\u5168\u7ef4\u5ea6\u7a00\u758f\u77e9\u9635\u7684\u4e2d\u95f4\u6fc0\u6d3b\u800c\u5185\u5b58\u6548\u7387\u4f4e\u3002", "method": "\u63d0\u51faMemory-efficient Orthogonal Fine-Tuning (MOFT)\uff0c\u901a\u8fc7\u4e3b\u5b50\u7a7a\u95f4\u9002\u5e94\u6765\u7ea6\u675f\u6b63\u4ea4\u5fae\u8c03\uff0c\u5e76\u5f15\u5165\u4e24\u4e2a\u53ef\u5b66\u4e60\u7f29\u653e\u5411\u91cf\u589e\u5f3a\u7075\u6d3b\u6027\u3002", "result": "MOFT\u572837\u4e2a\u591a\u6837\u5316\u7684\u4efb\u52a1\u548c\u56db\u4e2aNLP\u548cCV\u6a21\u578b\u4e0a\u663e\u8457\u964d\u4f4e\u4e86\u6b63\u4ea4\u5fae\u8c03\u7684\u5185\u5b58\u5360\u7528\u5e76\u4f18\u4e8e\u5173\u952e\u57fa\u7ebf\u3002", "conclusion": "MOFT\u662f\u4e00\u79cd\u6709\u6548\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\u548c\u5185\u5b58\u6548\u7387\u3002"}}
{"id": "2505.11239", "pdf": "https://arxiv.org/pdf/2505.11239", "abs": "https://arxiv.org/abs/2505.11239", "authors": ["Wilson Wongso", "Hao Xue", "Flora D. Salim"], "title": "Massive-STEPS: Massive Semantic Trajectories for Understanding POI Check-ins -- Dataset and Benchmarks", "categories": ["cs.LG"], "comment": null, "summary": "Understanding human mobility through Point-of-Interest (POI) recommendation\nis increasingly important for applications such as urban planning, personalized\nservices, and generative agent simulation. However, progress in this field is\nhindered by two key challenges: the over-reliance on older datasets from\n2012-2013 and the lack of reproducible, city-level check-in datasets that\nreflect diverse global regions. To address these gaps, we present Massive-STEPS\n(Massive Semantic Trajectories for Understanding POI Check-ins), a large-scale,\npublicly available benchmark dataset built upon the Semantic Trails dataset and\nenriched with semantic POI metadata. Massive-STEPS spans 12 geographically and\nculturally diverse cities and features more recent (2017-2018) and\nlonger-duration (24 months) check-in data than prior datasets. We benchmarked a\nwide range of POI recommendation models on Massive-STEPS using both supervised\nand zero-shot approaches, and evaluated their performance across multiple urban\ncontexts. By releasing Massive-STEPS, we aim to facilitate reproducible and\nequitable research in human mobility and POI recommendation. The dataset and\nbenchmarking code are available at:\nhttps://github.com/cruiseresearchgroup/Massive-STEPS", "AI": {"tldr": "A new large-scale dataset called Massive-STEPS is introduced to improve POI recommendation research.", "motivation": "Existing datasets are outdated and lack diversity.", "method": "Developed a new dataset based on Semantic Trails dataset with enriched POI metadata.", "result": "The dataset covers 12 diverse cities with more recent and longer check-in data.", "conclusion": "Releasing Massive-STEPS aims to promote reproducible and equitable research in human mobility and POI recommendation."}}
{"id": "2505.11250", "pdf": "https://arxiv.org/pdf/2505.11250", "abs": "https://arxiv.org/abs/2505.11250", "authors": ["Xvyuan Liu", "Xiangfei Qiu", "Xingjian Wu", "Zhengyu Li", "Chenjuan Guo", "Jilin Hu", "Bin Yang"], "title": "Rethinking Irregular Time Series Forecasting: A Simple yet Effective Baseline", "categories": ["cs.LG"], "comment": null, "summary": "The forecasting of irregular multivariate time series (IMTS) is crucial in\nkey areas such as healthcare, biomechanics, climate science, and astronomy.\nHowever, achieving accurate and practical predictions is challenging due to two\nmain factors. First, the inherent irregularity and data missingness in\nirregular time series make modeling difficult. Second, most existing methods\nare typically complex and resource-intensive. In this study, we propose a\ngeneral framework called APN to address these challenges. Specifically, we\ndesign a novel Time-Aware Patch Aggregation (TAPA) module that achieves\nadaptive patching. By learning dynamically adjustable patch boundaries and a\ntime-aware weighted averaging strategy, TAPA transforms the original irregular\nsequences into high-quality, regularized representations in a\nchannel-independent manner. Additionally, we use a simple query module to\neffectively integrate historical information while maintaining the model's\nefficiency. Finally, predictions are made by a shallow MLP. Experimental\nresults on multiple real-world datasets show that APN outperforms existing\nstate-of-the-art methods in both efficiency and accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6APN\u6765\u89e3\u51b3\u4e0d\u89c4\u5219\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u95ee\u9898\uff0c\u8be5\u6846\u67b6\u5305\u62ec\u4e00\u4e2aTAPA\u6a21\u5757\u7528\u4e8e\u81ea\u9002\u5e94\u5206\u533a\u548c\u4e00\u4e2a\u7b80\u5355\u7684\u67e5\u8be2\u6a21\u5757\u7528\u4e8e\u6574\u5408\u5386\u53f2\u4fe1\u606f\uff0c\u6700\u7ec8\u901a\u8fc7\u6d45\u5c42MLP\u8fdb\u884c\u9884\u6d4b\uff0c\u5b9e\u9a8c\u8868\u660eAPN\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u4e0d\u89c4\u5219\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u4e0d\u89c4\u5219\u6027\u548c\u6570\u636e\u7f3a\u5931\u95ee\u9898\uff0c\u5e76\u4e14\u5927\u591a\u6570\u65b9\u6cd5\u590d\u6742\u4e14\u8d44\u6e90\u5bc6\u96c6\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aTAPA\u7684\u65b0\u578b\u65f6\u95f4\u611f\u77e5\u8865\u4e01\u805a\u5408\u6a21\u5757\uff0c\u4f7f\u7528\u52a8\u6001\u53ef\u8c03\u5206\u533a\u8fb9\u754c\u548c\u65f6\u95f4\u611f\u77e5\u52a0\u6743\u5e73\u5747\u7b56\u7565\uff1b\u4f7f\u7528\u7b80\u5355\u67e5\u8be2\u6a21\u5757\u6574\u5408\u5386\u53f2\u4fe1\u606f\uff1b\u901a\u8fc7\u6d45\u5c42MLP\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAPN\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "APN\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u4e0d\u89c4\u5219\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u3002"}}
{"id": "2505.11254", "pdf": "https://arxiv.org/pdf/2505.11254", "abs": "https://arxiv.org/abs/2505.11254", "authors": ["Jeffrey Willette", "Heejun Lee", "Sung Ju Hwang"], "title": "Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction", "categories": ["cs.LG"], "comment": null, "summary": "The attention mechanism of a transformer has a quadratic complexity, leading\nto high inference costs and latency for long sequences. However, attention\nmatrices are mostly sparse, which implies that many entries may be omitted from\ncomputation for efficient inference. Sparse attention inference methods aim to\nreduce this computational burden; however, they also come with a troublesome\nperformance degradation. We discover that one reason for this degradation is\nthat the sparse calculation induces a distributional shift in the attention\noutputs. The distributional shift causes decoding-time queries to fail to align\nwell with the appropriate keys from the prefill stage, leading to a drop in\nperformance. We propose a simple, novel, and effective procedure for correcting\nthis distributional shift, bringing the distribution of sparse attention\noutputs closer to that of quadratic attention. Our method can be applied on top\nof any sparse attention method, and results in an average 36%pt performance\nincrease, recovering 88% of quadratic attention accuracy on the 131K RULER\nbenchmark when applied on top of sliding window attention with sink tokens\nwhile only adding a small overhead. Our method can maintain approximately 98.5%\nsparsity over full quadratic attention, making our model 32 times faster than\nFlash Attention 2 when processing 1M token prefills.", "AI": {"tldr": "A novel method corrects the distributional shift in sparse attention outputs, improving performance by 36% without significantly increasing computation time.", "motivation": "To address the computational inefficiency and performance degradation issues in sparse attention methods.", "method": "Proposes a method to adjust the distribution of sparse attention outputs to match those of quadratic attention.", "result": "Improves performance by 36%, recovers 88% of quadratic attention accuracy, maintains 98.5% sparsity, and makes the model 32 times faster than Flash Attention 2.", "conclusion": "Introduces a simple yet effective solution that enhances the performance of sparse attention methods while maintaining their efficiency."}}
{"id": "2505.11261", "pdf": "https://arxiv.org/pdf/2505.11261", "abs": "https://arxiv.org/abs/2505.11261", "authors": ["Jingyang Li", "Jiuqian Shang", "Yang Chen"], "title": "Fourier Low-rank and Sparse Tensor for Efficient Tensor Completion", "categories": ["cs.LG", "stat.ME"], "comment": null, "summary": "Tensor completion is crucial in many scientific domains with missing data\nproblems. Traditional low-rank tensor models, including CP, Tucker, and\nTensor-Train, exploit low-dimensional structures to recover missing data.\nHowever, these methods often treat all tensor modes symmetrically, failing to\ncapture the unique spatiotemporal patterns inherent in scientific data, where\nthe temporal component exhibits both low-frequency stability and high-frequency\nvariations. To address this, we propose a novel model, \\underline{F}ourier\n\\underline{Lo}w-rank and \\underline{S}parse \\underline{T}ensor (FLoST), which\ndecomposes the tensor along the temporal dimension using a Fourier transform.\nThis approach captures low-frequency components with low-rank matrices and\nhigh-frequency fluctuations with sparsity, resulting in a hybrid structure that\nefficiently models both smooth and localized variations. Compared to the\nwell-known tubal-rank model, which assumes low-rankness across all frequency\ncomponents, FLoST requires significantly fewer parameters, making it\ncomputationally more efficient, particularly when the time dimension is large.\nThrough theoretical analysis and empirical experiments, we demonstrate that\nFLoST outperforms existing tensor completion models in terms of both accuracy\nand computational efficiency, offering a more interpretable solution for\nspatiotemporal data reconstruction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f20\u91cf\u8865\u5168\u6a21\u578bFLoST\uff0c\u901a\u8fc7\u5085\u91cc\u53f6\u53d8\u6362\u5206\u89e3\u65f6\u95f4\u7ef4\u5ea6\uff0c\u7ed3\u5408\u4f4e\u79e9\u77e9\u9635\u548c\u7a00\u758f\u6027\u6765\u9ad8\u6548\u5efa\u6a21\u5e73\u6ed1\u548c\u5c40\u90e8\u53d8\u5316\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u6709\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u5f20\u91cf\u6a21\u578b\u672a\u80fd\u6355\u6349\u79d1\u5b66\u6570\u636e\u4e2d\u7684\u72ec\u7279\u65f6\u7a7a\u6a21\u5f0f\uff0c\u5c24\u5176\u662f\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u7684\u4f4e\u9891\u7a33\u5b9a\u6027\u548c\u9ad8\u9891\u53d8\u5316\u3002", "method": "\u63d0\u51faFLoST\u6a21\u578b\uff0c\u5229\u7528\u5085\u91cc\u53f6\u53d8\u6362\u6cbf\u65f6\u95f4\u7ef4\u5ea6\u5206\u89e3\u5f20\u91cf\uff0c\u4f7f\u7528\u4f4e\u79e9\u77e9\u9635\u6355\u83b7\u4f4e\u9891\u6210\u5206\uff0c\u4f7f\u7528\u7a00\u758f\u6027\u6355\u83b7\u9ad8\u9891\u6ce2\u52a8\u3002", "result": "FLoST\u6a21\u578b\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u65f6\u95f4\u7ef4\u5ea6\u65f6\uff0c\u5177\u6709\u663e\u8457\u7684\u8ba1\u7b97\u4f18\u52bf\u3002", "conclusion": "FLoST\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u65f6\u7a7a\u6570\u636e\u91cd\u5efa\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.11269", "pdf": "https://arxiv.org/pdf/2505.11269", "abs": "https://arxiv.org/abs/2505.11269", "authors": ["Shengjia Chang", "Xianshuo Yue"], "title": "Driving Mechanisms and Forecasting of China's Pet Population-An ARIMA-RF-HW Hybrid Approach", "categories": ["cs.LG"], "comment": "10 pages, 6 figures, 7 tables", "summary": "This study proposes a dynamically weighted ARIMA-RF-HW hybrid model\nintegrating ARIMA for seasonality and trends, Random Forest for nonlinear\nfeatures, and Holt-Winters smoothing for seasonal adjustment to improve China's\npet population forecasting accuracy. Using 2005-2023 data with nine economic,\nsocial, and policy indicators (urban income, consumption, aging ratio, policy\nquantity, new veterinary drug approvals), data were preprocessed via Z-score\nnormalization and missing value imputation. The results show that key drivers\nof pet populations include urban income (19.48% for cats, 17.15% for dogs),\nconsumption (17.99% for cats), and policy quantity (13.33% for cats, 14.02% for\ndogs), with aging (12.81% for cats, 13.27% for dogs) and urbanization\namplifying the demand for pets. Forecasts show steady cat growth and\nfluctuating dog numbers, reflecting cats' adaptability to urban environments.\nThis research supports policymakers in optimizing pet health management and\nguides enterprises in developing differentiated services, advancing sustainable\nindustry growth.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6574\u5408ARIMA\u3001Random Forest\u548cHolt-Winters\u7684\u52a8\u6001\u52a0\u6743\u6df7\u5408\u6a21\u578b\u6765\u63d0\u9ad8\u5bf9\u4e2d\u56fd\u5ba0\u7269\u6570\u91cf\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002\u4f7f\u7528\u4e862005-2023\u5e74\u7684\u6570\u636e\uff0c\u5e76\u8fdb\u884c\u4e86\u9884\u5904\u7406\u3002\u7ed3\u679c\u663e\u793a\u5173\u952e\u9a71\u52a8\u56e0\u7d20\u53ca\u5ba0\u7269\u6570\u91cf\u9884\u6d4b\u8d8b\u52bf\u3002", "motivation": "\u63d0\u5347\u4e2d\u56fd\u5ba0\u7269\u6570\u91cf\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u652f\u6301\u653f\u7b56\u5236\u5b9a\u8005\u4f18\u5316\u5ba0\u7269\u5065\u5eb7\u7ba1\u7406\u5e76\u6307\u5bfc\u4f01\u4e1a\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u6574\u5408ARIMA\u3001Random Forest\u548cHolt-Winters\u7684\u52a8\u6001\u52a0\u6743\u6df7\u5408\u6a21\u578b\uff0c\u5bf9\u7ecf\u6d4e\u3001\u793e\u4f1a\u548c\u653f\u7b56\u6307\u6807\u8fdb\u884c\u9884\u5904\u7406\u3002", "result": "\u53d1\u73b0\u57ce\u5e02\u6536\u5165\u3001\u6d88\u8d39\u3001\u8001\u9f84\u5316\u6bd4\u4f8b\u548c\u653f\u7b56\u6570\u91cf\u662f\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\uff0c\u732b\u7684\u6570\u91cf\u7a33\u6b65\u589e\u957f\u800c\u72d7\u7684\u6570\u91cf\u6ce2\u52a8\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5ba0\u7269\u5065\u5eb7\u7ba1\u7406\u548c\u884c\u4e1a\u53d1\u5c55\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u63a8\u52a8\u884c\u4e1a\u7684\u53ef\u6301\u7eed\u589e\u957f\u3002"}}
{"id": "2505.11276", "pdf": "https://arxiv.org/pdf/2505.11276", "abs": "https://arxiv.org/abs/2505.11276", "authors": ["Francesco Marchetti", "Edoardo Legnaro", "Sabrina Guastavino"], "title": "Multiclass threshold-based classification", "categories": ["cs.LG"], "comment": null, "summary": "In this paper, we introduce a threshold-based framework for multiclass\nclassification that generalizes the standard argmax rule. This is done by\nreplacing the probabilistic interpretation of softmax outputs with a geometric\none on the multidimensional simplex, where the classification depends on a\nmultidimensional threshold. This change of perspective enables for any trained\nclassification network an a posteriori optimization of the classification score\nby means of threshold tuning, as usually carried out in the binary setting.\nThis allows a further refinement of the prediction capability of any network.\nMoreover, this multidimensional threshold-based setting makes it possible to\ndefine score-oriented losses, which are based on the interpretation of the\nthreshold as a random variable. Our experiments show that the multidimensional\nthreshold tuning yields consistent performance improvements across various\nnetworks and datasets, and that the proposed multiclass score-oriented losses\nare competitive with standard loss functions, resembling the advantages\nobserved in the binary case.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9608\u503c\u7684\u591a\u7c7b\u5206\u7c7b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5728\u591a\u7ef4\u5355\u7eaf\u5f62\u4e0a\u7684\u51e0\u4f55\u89e3\u91ca\u6765\u63a8\u5e7f\u6807\u51c6\u7684argmax\u89c4\u5219\uff0c\u5e76\u901a\u8fc7\u8c03\u6574\u9608\u503c\u4f18\u5316\u5206\u7c7b\u8bc4\u5206\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u4e0d\u540c\u7f51\u7edc\u548c\u6570\u636e\u96c6\u4e0a\u90fd\u80fd\u5e26\u6765\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u7c7b\u5206\u7c7b\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u8fdb\u4e00\u6b65\u63d0\u9ad8\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9608\u503c\u7684\u591a\u7c7b\u5206\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u89e3\u91ca\u66ff\u6362softmax\u8f93\u51fa\u7684\u6982\u7387\u89e3\u91ca\uff0c\u5e76\u5b9a\u4e49\u4e86\u57fa\u4e8e\u5f97\u5206\u7684\u635f\u5931\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u591a\u7ef4\u9608\u503c\u8c03\u6574\u5728\u5404\u79cd\u7f51\u7edc\u548c\u6570\u636e\u96c6\u4e0a\u90fd\u80fd\u5e26\u6765\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4e14\u63d0\u51fa\u7684\u591a\u7c7b\u5f97\u5206\u5bfc\u5411\u635f\u5931\u4e0e\u6807\u51c6\u635f\u5931\u51fd\u6570\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u9608\u503c\u7684\u591a\u7c7b\u5206\u7c7b\u6846\u67b6\u80fd\u591f\u8fdb\u4e00\u6b65\u4f18\u5316\u5206\u7c7b\u8bc4\u5206\u5e76\u63d0\u9ad8\u9884\u6d4b\u80fd\u529b\u3002"}}
{"id": "2505.11283", "pdf": "https://arxiv.org/pdf/2505.11283", "abs": "https://arxiv.org/abs/2505.11283", "authors": ["Tom Siegl", "Kutalm\u0131\u015f Co\u015fkun", "Bjarne Hiller", "Amin Mirzaei", "Florian Lemmerich", "Martin Becker"], "title": "SubROC: AUC-Based Discovery of Exceptional Subgroup Performance for Binary Classifiers", "categories": ["cs.LG"], "comment": "49 pages, 8 figures", "summary": "Machine learning (ML) is increasingly employed in real-world applications\nlike medicine or economics, thus, potentially affecting large populations.\nHowever, ML models often do not perform homogeneously across such populations\nresulting in subgroups of the population (e.g., sex=female AND\nmarital_status=married) where the model underperforms or, conversely, is\nparticularly accurate. Identifying and describing such subgroups can support\npractical decisions on which subpopulation a model is safe to deploy or where\nmore training data is required. The potential of identifying and analyzing such\nsubgroups has been recognized, however, an efficient and coherent framework for\neffective search is missing. Consequently, we introduce SubROC, an open-source,\neasy-to-use framework based on Exceptional Model Mining for reliably and\nefficiently finding strengths and weaknesses of classification models in the\nform of interpretable population subgroups. SubROC incorporates common\nevaluation measures (ROC and PR AUC), efficient search space pruning for fast\nexhaustive subgroup search, control for class imbalance, adjustment for\nredundant patterns, and significance testing. We illustrate the practical\nbenefits of SubROC in case studies as well as in comparative analyses across\nmultiple datasets.", "AI": {"tldr": "\u63d0\u51faSubROC\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u6316\u6398\u5f02\u5e38\u6a21\u578b\u6765\u5bfb\u627e\u5206\u7c7b\u6a21\u578b\u5728\u53ef\u89e3\u91ca\u7684\u4eba\u53e3\u5b50\u7fa4\u4f53\u4e2d\u7684\u4f18\u7f3a\u70b9\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u4e00\u4e2a\u9ad8\u6548\u8fde\u8d2f\u7684\u6846\u67b6\u6765\u6709\u6548\u641c\u7d22\u6a21\u578b\u5728\u4e0d\u540c\u4eba\u7fa4\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\u3002", "method": "\u57fa\u4e8eExceptional Model Mining\u5f00\u53d1SubROC\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5177\u6709\u591a\u79cd\u529f\u80fd\u5982\u5e38\u89c1\u7684\u8bc4\u4f30\u5ea6\u91cf\uff08ROC\u548cPR AUC\uff09\u3001\u9ad8\u6548\u7684\u641c\u7d22\u7a7a\u95f4\u4fee\u526a\u7b49\u3002", "result": "SubROC\u5728\u6848\u4f8b\u7814\u7a76\u548c\u8de8\u591a\u4e2a\u6570\u636e\u96c6\u7684\u6bd4\u8f83\u5206\u6790\u4e2d\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u597d\u5904\u3002", "conclusion": "SubROC\u662f\u4e00\u4e2a\u5f00\u6e90\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u53ef\u9760\u6709\u6548\u5730\u627e\u5230\u5206\u7c7b\u6a21\u578b\u5728\u53ef\u89e3\u91ca\u4eba\u53e3\u5b50\u7fa4\u4e2d\u7684\u4f18\u7f3a\u70b9\u3002"}}
{"id": "2505.11294", "pdf": "https://arxiv.org/pdf/2505.11294", "abs": "https://arxiv.org/abs/2505.11294", "authors": ["Juan D. Guerra", "Thomas Garbay", "Guillaume Lajoie", "Marco Bonizzato"], "title": "Bidirectional Information Flow (BIF) -- A Sample Efficient Hierarchical Gaussian Process for Bayesian Optimization", "categories": ["cs.LG"], "comment": null, "summary": "Hierarchical Gaussian Process (H-GP) models divide problems into different\nsubtasks, allowing for different models to address each part, making them\nwell-suited for problems with inherent hierarchical structure. However, typical\nH-GP models do not fully take advantage of this structure, only sending\ninformation up or down the hierarchy. This one-way coupling limits sample\nefficiency and slows convergence. We propose Bidirectional Information Flow\n(BIF), an efficient H-GP framework that establishes bidirectional information\nexchange between parent and child models in H-GPs for online training. BIF\nretains the modular structure of hierarchical models - the parent combines\nsubtask knowledge from children GPs - while introducing top-down feedback to\ncontinually refine children models during online learning. This mutual exchange\nimproves sample efficiency, enables robust training, and allows modular reuse\nof learned subtask models. BIF outperforms conventional H-GP Bayesian\nOptimization methods, achieving up to 85% and 5x higher $R^2$ scores for the\nparent and children respectively, on synthetic and real-world neurostimulation\noptimization tasks.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u5411\u4fe1\u606f\u6d41\uff08BIF\uff09\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u5206\u5c42\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\u7684\u6837\u672c\u6548\u7387\u548c\u6536\u655b\u901f\u5ea6\uff0c\u5728\u795e\u7ecf\u523a\u6fc0\u4f18\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u5206\u5c42\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\u672a\u5145\u5206\u5229\u7528\u5176\u5c42\u6b21\u7ed3\u6784\uff0c\u4ec5\u5b9e\u73b0\u5355\u5411\u4fe1\u606f\u6d41\u52a8\uff0c\u5bfc\u81f4\u6837\u672c\u6548\u7387\u4f4e\u4e14\u6536\u655b\u6162\u3002", "method": "\u8bbe\u8ba1\u4e86\u53cc\u5411\u4fe1\u606f\u6d41\uff08BIF\uff09\uff0c\u5728\u5206\u5c42\u9ad8\u65af\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u7236\u6a21\u578b\u4e0e\u5b50\u6a21\u578b\u95f4\u7684\u53cc\u5411\u4fe1\u606f\u4ea4\u6362\uff0c\u5e76\u5f15\u5165\u81ea\u4e0a\u800c\u4e0b\u7684\u53cd\u9988\u673a\u5236\u6765\u4f18\u5316\u5b50\u6a21\u578b\u3002", "result": "BIF\u6846\u67b6\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u4e16\u754c\u795e\u7ecf\u523a\u6fc0\u4f18\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5206\u522b\u63d0\u5347\u4e8685%\u548c5\u500d\u7684R\u00b2\u8bc4\u5206\u3002", "conclusion": "\u63d0\u51fa\u7684BIF\u6846\u67b6\u6709\u6548\u589e\u5f3a\u4e86\u5206\u5c42\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u9ad8\u6548\u5728\u7ebf\u5b66\u4e60\u7684\u590d\u6742\u4efb\u52a1\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.11298", "pdf": "https://arxiv.org/pdf/2505.11298", "abs": "https://arxiv.org/abs/2505.11298", "authors": ["Sohir Maskey", "Raffaele Paolino", "Fabian Jogl", "Gitta Kutyniok", "Johannes F. Lutzeyer"], "title": "Graph Representational Learning: When Does More Expressivity Hurt Generalization?", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) are powerful tools for learning on structured\ndata, yet the relationship between their expressivity and predictive\nperformance remains unclear. We introduce a family of premetrics that capture\ndifferent degrees of structural similarity between graphs and relate these\nsimilarities to generalization, and consequently, the performance of expressive\nGNNs. By considering a setting where graph labels are correlated with\nstructural features, we derive generalization bounds that depend on the\ndistance between training and test graphs, model complexity, and training set\nsize. These bounds reveal that more expressive GNNs may generalize worse unless\ntheir increased complexity is balanced by a sufficiently large training set or\nreduced distance between training and test graphs. Our findings relate\nexpressivity and generalization, offering theoretical insights supported by\nempirical results.", "AI": {"tldr": "This paper explores the relationship between the expressivity of Graph Neural Networks (GNNs) and their predictive performance.", "motivation": "To understand why more expressive GNNs may generalize worse and how this can be mitigated.", "method": "Introduces a family of premetrics to capture structural similarity between graphs and relates these similarities to generalization.", "result": "Derives generalization bounds that depend on the distance between training and test graphs, model complexity, and training set size.", "conclusion": "More expressive GNNs may generalize worse unless their increased complexity is balanced by a larger training set or reduced distance between training and test graphs."}}
{"id": "2505.11306", "pdf": "https://arxiv.org/pdf/2505.11306", "abs": "https://arxiv.org/abs/2505.11306", "authors": ["Xinyan Wang", "Rui Dai", "Kaikui Liu", "Xiangxiang Chu"], "title": "Effective Probabilistic Time Series Forecasting with Fourier Adaptive Noise-Separated Diffusion", "categories": ["cs.LG"], "comment": null, "summary": "We propose the Fourier Adaptive Lite Diffusion Architecture (FALDA), a novel\nprobabilistic framework for time series forecasting. First, we introduce the\nDiffusion Model for Residual Regression (DMRR) framework, which unifies\ndiffusion-based probabilistic regression methods. Within this framework, FALDA\nleverages Fourier-based decomposition to incorporate a component-specific\narchitecture, enabling tailored modeling of individual temporal components. A\nconditional diffusion model is utilized to estimate the future noise term,\nwhile our proposed lightweight denoiser, DEMA (Decomposition MLP with AdaLN),\nconditions on the historical noise term to enhance denoising performance.\nThrough mathematical analysis and empirical validation, we demonstrate that\nFALDA effectively reduces epistemic uncertainty, allowing probabilistic\nlearning to primarily focus on aleatoric uncertainty. Experiments on six\nreal-world benchmarks demonstrate that FALDA consistently outperforms existing\nprobabilistic forecasting approaches across most datasets for long-term time\nseries forecasting while achieving enhanced computational efficiency without\ncompromising accuracy. Notably, FALDA also achieves superior overall\nperformance compared to state-of-the-art (SOTA) point forecasting approaches,\nwith improvements of up to 9%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6982\u7387\u6846\u67b6FALDA\uff0c\u901a\u8fc7DMRR\u7edf\u4e00\u6269\u6563\u56de\u5f52\u65b9\u6cd5\uff0c\u5e76\u5229\u7528Fourier\u5206\u89e3\u548c\u8f7b\u91cf\u7ea7\u53bb\u566a\u5668DEMA\u6765\u51cf\u5c11\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u9ad8\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u957f\u671f\u9884\u6d4b\u65f6\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u5f15\u5165DMRR\u6846\u67b6\uff0c\u4f7f\u7528Fourier\u5206\u89e3\u548cDEMA\u8f7b\u91cf\u7ea7\u53bb\u566a\u5668\u6765\u6539\u8fdb\u9884\u6d4b\u6a21\u578b\u3002", "result": "FALDA\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u73b0\u6709\u6982\u7387\u9884\u6d4b\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u70b9\u9884\u6d4b\u65b9\u9762\u4e5f\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "FALDA\u6709\u6548\u51cf\u5c11\u4e86\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u9ad8\u4e86\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2505.11307", "pdf": "https://arxiv.org/pdf/2505.11307", "abs": "https://arxiv.org/abs/2505.11307", "authors": ["Elsa Rizk", "Kun Yuan", "Ali H. Sayed"], "title": "Diffusion Learning with Partial Agent Participation and Local Updates", "categories": ["cs.LG"], "comment": "17 pages", "summary": "Diffusion learning is a framework that endows edge devices with advanced\nintelligence. By processing and analyzing data locally and allowing each agent\nto communicate with its immediate neighbors, diffusion effectively protects the\nprivacy of edge devices, enables real-time response, and reduces reliance on\ncentral servers. However, traditional diffusion learning relies on\ncommunication at every iteration, leading to communication overhead, especially\nwith large learning models. Furthermore, the inherent volatility of edge\ndevices, stemming from power outages or signal loss, poses challenges to\nreliable communication between neighboring agents. To mitigate these issues,\nthis paper investigates an enhanced diffusion learning approach incorporating\nlocal updates and partial agent participation. Local updates will curtail\ncommunication frequency, while partial agent participation will allow for the\ninclusion of agents based on their availability. We prove that the resulting\nalgorithm is stable in the mean-square error sense and provide a tight analysis\nof its Mean-Square-Deviation (MSD) performance. Various numerical experiments\nare conducted to illustrate our theoretical findings.", "AI": {"tldr": "This paper introduces an improved diffusion learning method that reduces communication frequency and increases reliability by allowing local updates and partial agent participation. Theoretical stability and MSD performance analysis are provided, along with numerical experiments.", "motivation": "To address the communication overhead and reliability issues in traditional diffusion learning caused by frequent communication and volatile edge devices.", "method": "Enhanced diffusion learning approach with local updates and partial agent participation.", "result": "The proposed algorithm is stable in the mean-square error sense, and its MSD performance is analyzed. Numerical experiments support the theoretical findings.", "conclusion": "The improved diffusion learning method successfully reduces communication needs and enhances reliability, offering a practical solution for privacy protection and real-time response in edge intelligence."}}
{"id": "2505.11308", "pdf": "https://arxiv.org/pdf/2505.11308", "abs": "https://arxiv.org/abs/2505.11308", "authors": ["Lothar Heimbach", "Sebastian Kaltenbach", "Petr Karnakov", "Francis J. Alexander", "Petros Koumoutsakos"], "title": "Reinforcement Learning Closures for Underresolved Partial Differential Equations using Synthetic Data", "categories": ["cs.LG", "physics.comp-ph"], "comment": null, "summary": "Partial Differential Equations (PDEs) describe phenomena ranging from\nturbulence and epidemics to quantum mechanics and financial markets. Despite\nrecent advances in computational science, solving such PDEs for real-world\napplications remains prohibitively expensive because of the necessity of\nresolving a broad range of spatiotemporal scales. In turn, practitioners often\nrely on coarse-grained approximations of the original PDEs, trading off\naccuracy for reduced computational resources. To mitigate the loss of detail\ninherent in such approximations, closure models are employed to represent\nunresolved spatiotemporal interactions. We present a framework for developing\nclosure models for PDEs using synthetic data acquired through the method of\nmanufactured solutions. These data are used in conjunction with reinforcement\nlearning to provide closures for coarse-grained PDEs. We illustrate the\nefficacy of our method using the one-dimensional and two-dimensional Burgers'\nequations and the two-dimensional advection equation. Moreover, we demonstrate\nthat closure models trained for inhomogeneous PDEs can be effectively\ngeneralized to homogeneous PDEs. The results demonstrate the potential for\ndeveloping accurate and computationally efficient closure models for systems\nwith scarce data.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u5408\u6210\u6570\u636e\u548c\u5f3a\u5316\u5b66\u4e60\u6765\u6784\u5efa\u504f\u5fae\u5206\u65b9\u7a0b\u95ed\u5408\u6a21\u578b\u7684\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u4e00\u7ef4\u548c\u4e8c\u7ef4Burgers\u65b9\u7a0b\u53ca\u4e8c\u7ef4\u5e73\u6d41\u65b9\u7a0b\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u5b9e\u9645\u5e94\u7528\u4e2d\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7c97\u7c92\u5ea6\u8fd1\u4f3c\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u4f46\u4f1a\u635f\u5931\u7ec6\u8282\uff0c\u56e0\u6b64\u9700\u8981\u95ed\u5408\u6a21\u578b\u6765\u8868\u793a\u672a\u89e3\u6790\u7684\u7a7a\u95f4-\u65f6\u95f4\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u4f7f\u7528\u4eba\u5de5\u5236\u9020\u89e3\u65b9\u6cd5\u83b7\u5f97\u5408\u6210\u6570\u636e\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4e3a\u7c97\u7c92\u5ea6\u504f\u5fae\u5206\u65b9\u7a0b\u63d0\u4f9b\u95ed\u5408\u6a21\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728Burgers\u65b9\u7a0b\u548c\u4e8c\u7ef4\u5e73\u6d41\u65b9\u7a0b\u4e0a\u6709\u6548\uff0c\u5e76\u4e14\u5bf9\u4e8e\u975e\u5747\u5300\u504f\u5fae\u5206\u65b9\u7a0b\u8bad\u7ec3\u51fa\u7684\u95ed\u5408\u6a21\u578b\u53ef\u4ee5\u5f88\u597d\u5730\u63a8\u5e7f\u5230\u5747\u5300\u504f\u5fae\u5206\u65b9\u7a0b\u3002", "conclusion": "\u5c55\u793a\u4e86\u4e3a\u6570\u636e\u7a00\u7f3a\u7cfb\u7edf\u5f00\u53d1\u51c6\u786e\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u95ed\u5408\u6a21\u578b\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.11312", "pdf": "https://arxiv.org/pdf/2505.11312", "abs": "https://arxiv.org/abs/2505.11312", "authors": ["Emanuele Francazi", "Francesco Pinto", "Aurelien Lucchi", "Marco Baity-Jesi"], "title": "Where You Place the Norm Matters: From Prejudiced to Neutral Initializations", "categories": ["cs.LG", "cond-mat.dis-nn"], "comment": null, "summary": "Normalization layers, such as Batch Normalization and Layer Normalization,\nare central components in modern neural networks, widely adopted to improve\ntraining stability and generalization. While their practical effectiveness is\nwell documented, a detailed theoretical understanding of how normalization\naffects model behavior, starting from initialization, remains an important open\nquestion. In this work, we investigate how both the presence and placement of\nnormalization within hidden layers influence the statistical properties of\nnetwork predictions before training begins. In particular, we study how these\nchoices shape the distribution of class predictions at initialization, which\ncan range from unbiased (Neutral) to highly concentrated (Prejudiced) toward a\nsubset of classes. Our analysis shows that normalization placement induces\nsystematic differences in the initial prediction behavior of neural networks,\nwhich in turn shape the dynamics of learning. By linking architectural choices\nto prediction statistics at initialization, our work provides a principled\nunderstanding of how normalization can influence early training behavior and\noffers guidance for more controlled and interpretable network design.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5f52\u4e00\u5316\u5c42\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u4f4d\u7f6e\u5bf9\u6a21\u578b\u521d\u59cb\u9884\u6d4b\u7edf\u8ba1\u7279\u6027\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u5f52\u4e00\u5316\u8bbe\u7f6e\u5982\u4f55\u5f71\u54cd\u65e9\u671f\u8bad\u7ec3\u884c\u4e3a\u3002", "motivation": "\u8be6\u7ec6\u7406\u89e3\u5f52\u4e00\u5316\u5982\u4f55\u4ece\u521d\u59cb\u5316\u5f00\u59cb\u5f71\u54cd\u6a21\u578b\u884c\u4e3a\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\u3002", "method": "\u5206\u6790\u5f52\u4e00\u5316\u5c42\u7684\u5b58\u5728\u548c\u653e\u7f6e\u5bf9\u9690\u85cf\u5c42\u7edf\u8ba1\u7279\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u7814\u7a76\u5176\u5bf9\u521d\u59cb\u7c7b\u522b\u9884\u6d4b\u5206\u5e03\u7684\u4f5c\u7528\u3002", "result": "\u5f52\u4e00\u5316\u653e\u7f6e\u4f1a\u5728\u795e\u7ecf\u7f51\u7edc\u7684\u521d\u59cb\u9884\u6d4b\u884c\u4e3a\u4e2d\u8bf1\u5bfc\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u5e76\u5851\u9020\u5b66\u4e60\u52a8\u6001\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u94fe\u63a5\u67b6\u6784\u9009\u62e9\u4e0e\u521d\u59cb\u5316\u65f6\u7684\u9884\u6d4b\u7edf\u8ba1\uff0c\u63d0\u4f9b\u4e86\u5bf9\u5f52\u4e00\u5316\u5f71\u54cd\u65e9\u671f\u8bad\u7ec3\u884c\u4e3a\u7684\u539f\u7406\u6027\u7406\u89e3\uff0c\u5e76\u4e3a\u66f4\u53ef\u63a7\u4e14\u53ef\u89e3\u91ca\u7684\u7f51\u7edc\u8bbe\u8ba1\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2505.11321", "pdf": "https://arxiv.org/pdf/2505.11321", "abs": "https://arxiv.org/abs/2505.11321", "authors": ["Pu Yang", "J. A. Barria"], "title": "Anomaly Detection for Non-stationary Time Series using Recurrent Wavelet Probabilistic Neural Network", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "In this paper, an unsupervised Recurrent Wavelet Probabilistic Neural Network\n(RWPNN) is proposed, which aims at detecting anomalies in non-stationary\nenvironments by modelling the temporal features using a nonparametric density\nestimation network. The novel framework consists of two components, a Stacked\nRecurrent Encoder-Decoder (SREnc-Dec) module that captures temporal features in\na latent space, and a Multi-Receptive-field Wavelet Probabilistic Network\n(MRWPN) that creates an ensemble probabilistic model to characterise the latent\nspace. This formulation extends the standard wavelet probabilistic networks to\nwavelet deep probabilistic networks, which can handle higher data\ndimensionality. The MRWPN module can adapt to different rates of data variation\nin different datasets without imposing strong distribution assumptions,\nresulting in a more robust and accurate detection for Time Series Anomaly\nDetection (TSAD) tasks in the non-stationary environment. We carry out the\nassessment on 45 real-world time series datasets from various domains, verify\nthe performance of RWPNN in TSAD tasks with several constraints, and show its\nability to provide early warnings for anomalous events.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u5faa\u73af\u5c0f\u6ce2\u6982\u7387\u795e\u7ecf\u7f51\u7edc\uff08RWPNN\uff09\uff0c\u7528\u4e8e\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u3002\u8be5\u6a21\u578b\u7531\u4e00\u4e2a\u5806\u53e0\u9012\u5f52\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u5757\u548c\u4e00\u4e2a\u591a\u5c3a\u5ea6\u5c0f\u6ce2\u6982\u7387\u7f51\u7edc\u7ec4\u6210\uff0c\u80fd\u591f\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u5e76\u9002\u5e94\u4e0d\u540c\u53d8\u5316\u7387\u7684\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u4e2a\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u4ee5\u53ca\u63d0\u524d\u9884\u8b66\u5f02\u5e38\u4e8b\u4ef6\u7684\u80fd\u529b\u3002", "motivation": "\u9488\u5bf9\u975e\u5e73\u7a33\u73af\u5883\u4e0b\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e00\u79cd\u65b0\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u4e86\u5806\u53e0\u9012\u5f52\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u5757\u4e0e\u591a\u5c3a\u5ea6\u5c0f\u6ce2\u6982\u7387\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\u6765\u6355\u6349\u65f6\u95f4\u7279\u5f81\u5e76\u751f\u6210\u6982\u7387\u6a21\u578b\u3002", "result": "\u5728\u6765\u81ea\u591a\u4e2a\u9886\u57df\u768445\u4e2a\u771f\u5b9e\u4e16\u754c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u53ca\u63d0\u4f9b\u65e9\u671f\u9884\u8b66\u7684\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684RWPNN\u6846\u67b6\u53ef\u4ee5\u66f4\u51c6\u786e\u5730\u68c0\u6d4b\u975e\u5e73\u7a33\u73af\u5883\u4e0b\u7684\u5f02\u5e38\uff0c\u4e14\u65e0\u9700\u5f3a\u5206\u5e03\u5047\u8bbe\uff0c\u5177\u6709\u8f83\u597d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.11335", "pdf": "https://arxiv.org/pdf/2505.11335", "abs": "https://arxiv.org/abs/2505.11335", "authors": ["Jincheng Huang", "Jie Xu", "Xiaoshuang Shi", "Ping Hu", "Lei Feng", "Xiaofeng Zhu"], "title": "The Final Layer Holds the Key: A Unified and Efficient GNN Calibration Framework", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated remarkable effectiveness on\ngraph-based tasks. However, their predictive confidence is often miscalibrated,\ntypically exhibiting under-confidence, which harms the reliability of their\ndecisions. Existing calibration methods for GNNs normally introduce additional\ncalibration components, which fail to capture the intrinsic relationship\nbetween the model and the prediction confidence, resulting in limited\ntheoretical guarantees and increased computational overhead. To address this\nissue, we propose a simple yet efficient graph calibration method. We establish\na unified theoretical framework revealing that model confidence is jointly\ngoverned by class-centroid-level and node-level calibration at the final layer.\nBased on this insight, we theoretically show that reducing the weight decay of\nthe final-layer parameters alleviates GNN under-confidence by acting on the\nclass-centroid level, while node-level calibration acts as a finer-grained\ncomplement to class-centroid level calibration, which encourages each test node\nto be closer to its predicted class centroid at the final-layer\nrepresentations. Extensive experiments validate the superiority of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6821\u51c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u6700\u7ec8\u5c42\u53c2\u6570\u7684\u6743\u91cd\u8870\u51cf\u6765\u7f13\u89e3GNN\u7684\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684GNN\u6821\u51c6\u65b9\u6cd5\u901a\u5e38\u5f15\u5165\u989d\u5916\u7684\u7ec4\u4ef6\uff0c\u672a\u80fd\u6355\u6349\u6a21\u578b\u4e0e\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u4e4b\u95f4\u7684\u5185\u5728\u5173\u7cfb\uff0c\u5bfc\u81f4\u7406\u8bba\u4fdd\u8bc1\u6709\u9650\u4e14\u8ba1\u7b97\u5f00\u9500\u589e\u52a0\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u6a21\u578b\u7f6e\u4fe1\u5ea6\u7531\u7c7b\u522b\u8d28\u5fc3\u7ea7\u548c\u8282\u70b9\u7ea7\u6821\u51c6\u5171\u540c\u51b3\u5b9a\uff0c\u5e76\u901a\u8fc7\u51cf\u5c11\u6700\u7ec8\u5c42\u53c2\u6570\u7684\u6743\u91cd\u8870\u51cf\u6765\u5b9e\u73b0\u6821\u51c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7f13\u89e3GNN\u7684\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u4e0d\u8db3\u95ee\u9898\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.11342", "pdf": "https://arxiv.org/pdf/2505.11342", "abs": "https://arxiv.org/abs/2505.11342", "authors": ["Andrew W. Rosemberg", "Joaquim Dias Garcia", "Russell Bent", "Pascal Van Hentenryck"], "title": "Sobolev Training of End-to-End Optimization Proxies", "categories": ["cs.LG", "math.OC"], "comment": "9 Pages, 4 Figures, 5 Tables", "summary": "Optimization proxies - machine learning models trained to approximate the\nsolution mapping of parametric optimization problems in a single forward pass -\noffer dramatic reductions in inference time compared to traditional iterative\nsolvers. This work investigates the integration of solver sensitivities into\nsuch end to end proxies via a Sobolev training paradigm and does so in two\ndistinct settings: (i) fully supervised proxies, where exact solver outputs and\nsensitivities are available, and (ii) self supervised proxies that rely only on\nthe objective and constraint structure of the underlying optimization problem.\nBy augmenting the standard training loss with directional derivative\ninformation extracted from the solver, the proxy aligns both its predicted\nsolutions and local derivatives with those of the optimizer. Under Lipschitz\ncontinuity assumptions on the true solution mapping, matching first order\nsensitivities is shown to yield uniform approximation error proportional to the\ntraining set covering radius. Empirically, different impacts are observed in\neach studied setting. On three large Alternating Current Optimal Power Flow\nbenchmarks, supervised Sobolev training cuts mean squared error by up to 56\npercent and the median worst case constraint violation by up to 400 percent\nwhile keeping the optimality gap below 0.22 percent. For a mean variance\nportfolio task trained without labeled solutions, self supervised Sobolev\ntraining halves the average optimality gap in the medium risk region (standard\ndeviation above 10 percent of budget) and matches the baseline elsewhere.\nTogether, these results highlight Sobolev training whether supervised or self\nsupervised as a path to fast reliable surrogates for safety critical large\nscale optimization workloads.", "AI": {"tldr": "This work introduces Sobolev training into optimization proxies, improving their accuracy and reliability in approximating solutions to complex optimization problems.", "motivation": "To enhance the accuracy of optimization proxies by incorporating solver sensitivities using a Sobolev training approach.", "method": "Integrating solver sensitivities into end-to-end proxies through Sobolev training in both fully supervised and self-supervised settings.", "result": "Supervised Sobolev training significantly reduced mean squared error and constraint violations in power flow benchmarks. Self-supervised training improved optimality gaps in portfolio tasks.", "conclusion": "Sobolev training provides a method for creating fast and reliable surrogate models for large-scale optimization problems."}}
{"id": "2505.11346", "pdf": "https://arxiv.org/pdf/2505.11346", "abs": "https://arxiv.org/abs/2505.11346", "authors": ["Andreas Roth", "Thomas Liebig"], "title": "What Can We Learn From MIMO Graph Convolutions?", "categories": ["cs.LG"], "comment": "IJCAI 2025", "summary": "Most graph neural networks (GNNs) utilize approximations of the general graph\nconvolution derived in the graph Fourier domain. While GNNs are typically\napplied in the multi-input multi-output (MIMO) case, the approximations are\nperformed in the single-input single-output (SISO) case. In this work, we first\nderive the MIMO graph convolution through the convolution theorem and\napproximate it directly in the MIMO case. We find the key MIMO-specific\nproperty of the graph convolution to be operating on multiple computational\ngraphs, or equivalently, applying distinct feature transformations for each\npair of nodes. As a localized approximation, we introduce localized MIMO graph\nconvolutions (LMGCs), which generalize many linear message-passing neural\nnetworks. For almost every choice of edge weights, we prove that LMGCs with a\nsingle computational graph are injective on multisets, and the resulting\nrepresentations are linearly independent when more than one computational graph\nis used. Our experimental results confirm that an LMGC can combine the benefits\nof various methods.", "AI": {"tldr": "This paper introduces localized MIMO graph convolutions (LMGCs) which generalize many linear message-passing neural networks.", "motivation": "Existing graph neural networks (GNNs) use approximations of the general graph convolution derived in the graph Fourier domain, but these approximations are performed in the single-input single-output (SISO) case while GNNs are typically applied in the multi-input multi-output (MIMO) case.", "method": "The authors derive the MIMO graph convolution through the convolution theorem and approximate it directly in the MIMO case. They introduce localized MIMO graph convolutions (LMGCs) as a localized approximation.", "result": "For almost every choice of edge weights, LMGCs with a single computational graph are injective on multisets, and the resulting representations are linearly independent when more than one computational graph is used.", "conclusion": "An LMGC can combine the benefits of various methods."}}
{"id": "2505.11347", "pdf": "https://arxiv.org/pdf/2505.11347", "abs": "https://arxiv.org/abs/2505.11347", "authors": ["Johannes Schwab", "Bryan Kelly", "Semyon Malamud", "Teng Andrea Xu"], "title": "Training NTK to Generalize with KARE", "categories": ["cs.LG"], "comment": null, "summary": "The performance of the data-dependent neural tangent kernel (NTK; Jacot et\nal. (2018)) associated with a trained deep neural network (DNN) often matches\nor exceeds that of the full network. This implies that DNN training via\ngradient descent implicitly performs kernel learning by optimizing the NTK. In\nthis paper, we propose instead to optimize the NTK explicitly. Rather than\nminimizing empirical risk, we train the NTK to minimize its generalization\nerror using the recently developed Kernel Alignment Risk Estimator (KARE; Jacot\net al. (2020)). Our simulations and real data experiments show that NTKs\ntrained with KARE consistently match or significantly outperform the original\nDNN and the DNN- induced NTK (the after-kernel). These results suggest that\nexplicitly trained kernels can outperform traditional end-to-end DNN\noptimization in certain settings, challenging the conventional dominance of\nDNNs. We argue that explicit training of NTK is a form of over-parametrized\nfeature learning.", "AI": {"tldr": "This paper proposes optimizing the neural tangent kernel (NTK) explicitly rather than minimizing empirical risk, showing that it can outperform traditional deep neural network optimization in certain cases.", "motivation": "To explore whether explicitly trained kernels can outperform traditional end-to-end DNN optimization.", "method": "Proposes to train the NTK using the Kernel Alignment Risk Estimator (KARE) to minimize generalization error instead of minimizing empirical risk.", "result": "Simulations and real data experiments demonstrate that NTKs trained with KARE consistently match or outperform the original DNN and the DNN-induced NTK.", "conclusion": "Explicit training of NTK is a form of over-parametrized feature learning and can challenge the conventional dominance of DNNs."}}
{"id": "2505.11349", "pdf": "https://arxiv.org/pdf/2505.11349", "abs": "https://arxiv.org/abs/2505.11349", "authors": ["Yuanzhao Zhang", "William Gilpin"], "title": "Context parroting: A simple but tough-to-beat baseline for foundation models in scientific machine learning", "categories": ["cs.LG", "nlin.CD", "physics.comp-ph"], "comment": null, "summary": "Recently-developed time series foundation models for scientific machine\nlearning exhibit emergent abilities to predict physical systems. These\nabilities include zero-shot forecasting, in which a model forecasts future\nstates of a system given only a short trajectory as context. Here, we show that\nfoundation models applied to physical systems can give accurate predictions,\nbut that they fail to develop meaningful representations of the underlying\nphysics. Instead, foundation models often forecast by context parroting, a\nsimple zero-shot forecasting strategy that copies directly from the context. As\na result, a naive direct context parroting model scores higher than\nstate-of-the-art time-series foundation models on predicting a diverse range of\ndynamical systems, at a tiny fraction of the computational cost. We draw a\nparallel between context parroting and induction heads, which explains why\nlarge language models trained on text can be repurposed for time series\nforecasting. Our dynamical systems perspective also ties the scaling between\nforecast accuracy and context length to the fractal dimension of the attractor,\nproviding insight into the previously observed in-context neural scaling laws.\nContext parroting thus serves as a simple but tough-to-beat baseline for future\ntime-series foundation models and can help identify in-context learning\nstrategies beyond parroting.", "AI": {"tldr": "This paper examines the performance of recently-developed time series foundation models in predicting physical systems. It finds that these models often rely on context parroting rather than developing meaningful physics representations.", "motivation": "To explore the capabilities and limitations of time series foundation models in predicting physical systems.", "method": "Analyzing the performance of foundation models in zero-shot forecasting tasks and comparing them with a naive direct context parroting model.", "result": "Foundation models can make accurate predictions but fail to develop meaningful representations of underlying physics, instead using context parroting. A simple context parroting model outperforms state-of-the-art foundation models on diverse dynamical systems.", "conclusion": "Context parroting is a tough-to-beat baseline for future time-series foundation models and helps identify in-context learning strategies beyond parroting."}}
{"id": "2505.11356", "pdf": "https://arxiv.org/pdf/2505.11356", "abs": "https://arxiv.org/abs/2505.11356", "authors": ["Nero Z. Li", "Xuehao Zhai", "Zhichao Shi", "Boshen Shi", "Xuhui Jiang"], "title": "Fractal Graph Contrastive Learning", "categories": ["cs.LG"], "comment": null, "summary": "While Graph Contrastive Learning (GCL) has attracted considerable attention\nin the field of graph self-supervised learning, its performance heavily relies\non data augmentations that are expected to generate semantically consistent\npositive pairs. Existing strategies typically resort to random perturbations or\nlocal structure preservation, yet lack explicit control over global structural\nconsistency between augmented views. To address this limitation, we propose\nFractal Graph Contrastive Learning (FractalGCL), a theory-driven framework that\nleverages fractal self-similarity to enforce global topological coherence.\nFractalGCL introduces two key innovations: a renormalisation-based augmentation\nthat generates structurally aligned positive views via box coverings; and a\nfractal-dimension-aware contrastive loss that aligns graph embeddings according\nto their fractal dimensions. While combining the two innovations markedly\nboosts graph-representation quality, it also adds non-trivial computational\noverhead. To mitigate the computational overhead of fractal dimension\nestimation, we derive a one-shot estimator by proving that the dimension\ndiscrepancy between original and renormalised graphs converges weakly to a\ncentred Gaussian distribution. This theoretical insight enables a reduction in\ndimension computation cost by an order of magnitude, cutting overall training\ntime by approximately 61%. The experiments show that FractalGCL not only\ndelivers state-of-the-art results on standard benchmarks but also outperforms\ntraditional baselines on traffic networks by an average margin of about\nremarkably 7%. Codes are available at\n(https://anonymous.4open.science/r/FractalGCL-0511).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5f62\u81ea\u76f8\u4f3c\u6027\u7684\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6FractalGCL\uff0c\u901a\u8fc7\u91cd\u6b63\u89c4\u5316\u589e\u5f3a\u548c\u57fa\u4e8e\u5206\u5f62\u7ef4\u5ea6\u611f\u77e5\u7684\u5bf9\u6bd4\u635f\u5931\u6765\u63d0\u5347\u56fe\u8868\u793a\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u968f\u673a\u6270\u52a8\u6216\u5c40\u90e8\u7ed3\u6784\u4fdd\u6301\uff0c\u7f3a\u4e4f\u5bf9\u589e\u5f3a\u89c6\u56fe\u95f4\u5168\u5c40\u7ed3\u6784\u4e00\u81f4\u6027\u663e\u5f0f\u63a7\u5236\u3002", "method": "\u5f15\u5165\u4e86\u57fa\u4e8e\u91cd\u6b63\u89c4\u5316\u7684\u589e\u5f3a\u65b9\u6cd5\u751f\u6210\u7ed3\u6784\u5bf9\u9f50\u7684\u6b63\u89c6\u56fe\uff0c\u5e76\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u5206\u5f62\u7ef4\u5ea6\u611f\u77e5\u7684\u5bf9\u6bd4\u635f\u5931\u6765\u5bf9\u9f50\u56fe\u5d4c\u5165\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5728\u4ea4\u901a\u7f51\u7edc\u4e0a\u6bd4\u4f20\u7edf\u57fa\u7ebf\u5e73\u5747\u9ad8\u51fa\u7ea67%\u3002", "conclusion": "FractalGCL\u663e\u8457\u63d0\u5347\u4e86\u56fe\u8868\u793a\u7684\u8d28\u91cf\uff0c\u540c\u65f6\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u4f7f\u5f97\u6574\u4f53\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed\u4e86\u7ea661%\u3002"}}
{"id": "2505.11359", "pdf": "https://arxiv.org/pdf/2505.11359", "abs": "https://arxiv.org/abs/2505.11359", "authors": ["Zihang Jia", "Zhen Zhang", "Witold Pedrycz"], "title": "LGBQPC: Local Granular-Ball Quality Peaks Clustering", "categories": ["cs.LG"], "comment": null, "summary": "The density peaks clustering (DPC) algorithm has attracted considerable\nattention for its ability to detect arbitrarily shaped clusters based on a\nsimple yet effective assumption. Recent advancements integrating granular-ball\n(GB) computing with DPC have led to the GB-based DPC (GBDPC) algorithm, which\nimproves computational efficiency. However, GBDPC demonstrates limitations when\nhandling complex clustering tasks, particularly those involving data with\ncomplex manifold structures or non-uniform density distributions. To overcome\nthese challenges, this paper proposes the local GB quality peaks clustering\n(LGBQPC) algorithm, which offers comprehensive improvements to GBDPC in both GB\ngeneration and clustering processes based on the principle of justifiable\ngranularity (POJG). Firstly, an improved GB generation method, termed GB-POJG+,\nis developed, which systematically refines the original GB-POJG in four key\naspects: the objective function, termination criterion for GB division,\ndefinition of abnormal GB, and granularity level adaptation strategy. GB-POJG+\nsimplifies parameter configuration by requiring only a single penalty\ncoefficient and ensures high-quality GB generation while maintaining the number\nof generated GBs within an acceptable range. In the clustering phase, two key\ninnovations are introduced based on the GB k-nearest neighbor graph: relative\nGB quality for density estimation and geodesic distance for GB distance metric.\nThese modifications substantially improve the performance of GBDPC on datasets\nwith complex manifold structures or non-uniform density distributions.\nExtensive numerical experiments on 40 benchmark datasets, including both\nsynthetic and publicly available datasets, validate the superior performance of\nthe proposed LGBQPC algorithm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5c40\u90e8\u7c92\u7403\u8d28\u91cf\u5cf0\u503c\u805a\u7c7b\u7b97\u6cd5\uff08LGBQPC\uff09\uff0c\u8be5\u7b97\u6cd5\u5728\u7c92\u7403\u751f\u6210\u548c\u805a\u7c7b\u8fc7\u7a0b\u4e2d\u5bf9\u57fa\u4e8e\u7c92\u7403\u7684\u5bc6\u5ea6\u5cf0\u503c\u805a\u7c7b\uff08GBDPC\uff09\u8fdb\u884c\u4e86\u5168\u9762\u6539\u8fdb\u3002\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u7c92\u7403k\u8fd1\u90bb\u56fe\u7684\u76f8\u5bf9\u7c92\u7403\u8d28\u91cf\u548c\u6d4b\u5730\u7ebf\u8ddd\u79bb\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5904\u7406\u590d\u6742\u6d41\u5f62\u7ed3\u6784\u6216\u975e\u5747\u5300\u5bc6\u5ea6\u5206\u5e03\u6570\u636e\u96c6\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86LGBQPC\u7b97\u6cd5\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u7c92\u7403\u7684\u5bc6\u5ea6\u5cf0\u503c\u805a\u7c7b\uff08GBDPC\uff09\u7b97\u6cd5\u5728\u5904\u7406\u590d\u6742\u805a\u7c7b\u4efb\u52a1\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5177\u6709\u590d\u6742\u6d41\u5f62\u7ed3\u6784\u6216\u975e\u5747\u5300\u5bc6\u5ea6\u5206\u5e03\u7684\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5c40\u90e8\u7c92\u7403\u8d28\u91cf\u5cf0\u503c\u805a\u7c7b\uff08LGBQPC\uff09\u7b97\u6cd5\uff0c\u5305\u62ec\u6539\u8fdb\u7684\u7c92\u7403\u751f\u6210\u65b9\u6cd5\uff08GB-POJG+\uff09\u548c\u57fa\u4e8e\u7c92\u7403k\u8fd1\u90bb\u56fe\u7684\u805a\u7c7b\u8fc7\u7a0b\u521b\u65b0\u3002", "result": "LGBQPC\u7b97\u6cd5\u5728\u5904\u7406\u590d\u6742\u6d41\u5f62\u7ed3\u6784\u6216\u975e\u5747\u5300\u5bc6\u5ea6\u5206\u5e03\u7684\u6570\u636e\u96c6\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u572840\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u6570\u503c\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "LGBQPC\u7b97\u6cd5\u514b\u670d\u4e86\u73b0\u6709GBDPC\u7b97\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u7c92\u7403\u751f\u6210\u548c\u66f4\u6709\u6548\u7684\u805a\u7c7b\u8fc7\u7a0b\u3002"}}
{"id": "2505.11360", "pdf": "https://arxiv.org/pdf/2505.11360", "abs": "https://arxiv.org/abs/2505.11360", "authors": ["Rares Cristian", "Pavithra Harsha", "Georgia Perakis", "Brian Quanz"], "title": "Efficient End-to-End Learning for Decision-Making: A Meta-Optimization Approach", "categories": ["cs.LG"], "comment": null, "summary": "End-to-end learning has become a widely applicable and studied problem in\ntraining predictive ML models to be aware of their impact on downstream\ndecision-making tasks. These end-to-end models often outperform traditional\nmethods that separate training from the optimization and only myopically focus\non prediction error. However, the computational complexity of end-to-end\nframeworks poses a significant challenge, particularly for large-scale\nproblems. While training an ML model using gradient descent, each time we need\nto compute a gradient we must solve an expensive optimization problem. We\npresent a meta-optimization method that learns efficient algorithms to\napproximate optimization problems, dramatically reducing computational overhead\nof solving the decision problem in general, an aspect we leverage in the\ntraining within the end-to-end framework. Our approach introduces a neural\nnetwork architecture that near-optimally solves optimization problems while\nensuring feasibility constraints through alternate projections. We prove\nexponential convergence, approximation guarantees, and generalization bounds\nfor our learning method. This method offers superior computational efficiency,\nproducing high-quality approximations faster and scaling better with problem\nsize compared to existing techniques. Our approach applies to a wide range of\noptimization problems including deterministic, single-stage as well as\ntwo-stage stochastic optimization problems. We illustrate how our proposed\nmethod applies to (1) an electricity generation problem using real data from an\nelectricity routing company coordinating the movement of electricity throughout\n13 states, (2) a shortest path problem with a computer vision task of\npredicting edge costs from terrain maps, (3) a two-stage multi-warehouse\ncross-fulfillment newsvendor problem, as well as a variety of other\nnewsvendor-like problems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5143\u4f18\u5316\u65b9\u6cd5\u6765\u5b66\u4e60\u9ad8\u6548\u7b97\u6cd5\u8fd1\u4f3c\u4f18\u5316\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6c42\u89e3\u51b3\u7b56\u95ee\u9898\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u8bc1\u660e\u4e86\u6307\u6570\u6536\u655b\u6027\u3001\u903c\u8fd1\u4fdd\u8bc1\u548c\u6cdb\u5316\u754c\u9650\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u4f18\u5316\u95ee\u9898\uff0c\u5305\u62ec\u786e\u5b9a\u6027\u548c\u4e24\u9636\u6bb5\u968f\u673a\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u7535\u529b\u751f\u6210\u3001\u6700\u77ed\u8def\u5f84\u9884\u6d4b\u548c\u65b0svendor\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4ec5\u5173\u6ce8\u9884\u6d4b\u8bef\u5dee\uff0c\u800c\u5ffd\u7565\u4e86\u5bf9\u4e0b\u6e38\u51b3\u7b56\u4efb\u52a1\u7684\u5f71\u54cd\uff1b\u7aef\u5230\u7aef\u6846\u67b6\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u5bf9\u4e8e\u5927\u89c4\u6a21\u95ee\u9898\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u4ea4\u66ff\u6295\u5f71\u786e\u4fdd\u53ef\u884c\u6027\u7ea6\u675f\uff0c\u540c\u65f6\u8fd1\u4f3c\u6700\u4f18\u5730\u89e3\u51b3\u4f18\u5316\u95ee\u9898\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u80fd\u591f\u66f4\u5feb\u5730\u4ea7\u751f\u9ad8\u8d28\u91cf\u7684\u8fd1\u4f3c\u89e3\uff0c\u5e76\u4e14\u968f\u7740\u95ee\u9898\u89c4\u6a21\u7684\u589e\u957f\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5143\u4f18\u5316\u65b9\u6cd5\u53ef\u4ee5\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5404\u79cd\u4f18\u5316\u95ee\u9898\uff0c\u5305\u62ec\u786e\u5b9a\u6027\u548c\u4e24\u9636\u6bb5\u968f\u673a\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u5b9e\u9645\u95ee\u9898\u4e2d\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2505.11370", "pdf": "https://arxiv.org/pdf/2505.11370", "abs": "https://arxiv.org/abs/2505.11370", "authors": ["Jingwei Li", "Jing Xu", "Zifan Wang", "Huishuai Zhang", "Jingzhao Zhang"], "title": "Understanding Nonlinear Implicit Bias via Region Counts in Input Space", "categories": ["cs.LG"], "comment": null, "summary": "One explanation for the strong generalization ability of neural networks is\nimplicit bias. Yet, the definition and mechanism of implicit bias in non-linear\ncontexts remains little understood. In this work, we propose to characterize\nimplicit bias by the count of connected regions in the input space with the\nsame predicted label. Compared with parameter-dependent metrics (e.g., norm or\nnormalized margin), region count can be better adapted to nonlinear,\noverparameterized models, because it is determined by the function mapping and\nis invariant to reparametrization. Empirically, we found that small region\ncounts align with geometrically simple decision boundaries and correlate well\nwith good generalization performance. We also observe that good hyper-parameter\nchoices such as larger learning rates and smaller batch sizes can induce small\nregion counts. We further establish the theoretical connections and explain how\nlarger learning rate can induce small region counts in neural networks.", "AI": {"tldr": "This paper explores the concept of implicit bias in neural networks by examining the count of connected regions in the input space with the same predicted label, finding that smaller region counts correspond to better generalization and can be induced by certain hyper-parameter choices.", "motivation": "To understand the definition and mechanism of implicit bias in non-linear contexts.", "method": "Characterizing implicit bias by the count of connected regions in the input space with the same predicted label, and comparing it with parameter-dependent metrics.", "result": "Small region counts align with geometrically simple decision boundaries and correlate well with good generalization performance. Certain hyper-parameter choices can induce small region counts.", "conclusion": "Theoretical connections were established to explain how larger learning rates can induce small region counts in neural networks."}}
{"id": "2505.11380", "pdf": "https://arxiv.org/pdf/2505.11380", "abs": "https://arxiv.org/abs/2505.11380", "authors": ["Alejandro Moreo"], "title": "On the Interconnections of Calibration, Quantification, and Classifier Accuracy Prediction under Dataset Shift", "categories": ["cs.LG"], "comment": null, "summary": "When the distribution of the data used to train a classifier differs from\nthat of the test data, i.e., under dataset shift, well-established routines for\ncalibrating the decision scores of the classifier, estimating the proportion of\npositives in a test sample, or estimating the accuracy of the classifier,\nbecome particularly challenging. This paper investigates the interconnections\namong three fundamental problems, calibration, quantification, and classifier\naccuracy prediction, under dataset shift conditions. Specifically, we prove\ntheir equivalence through mutual reduction, i.e., we show that access to an\noracle for any one of these tasks enables the resolution of the other two.\nBased on these proofs, we propose new methods for each problem based on direct\nadaptations of well-established methods borrowed from the other disciplines.\nOur results show such methods are often competitive, and sometimes even surpass\nthe performance of dedicated approaches from each discipline. The main goal of\nthis paper is to fostering cross-fertilization among these research areas,\nencouraging the development of unified approaches and promoting synergies\nacross the fields.", "AI": {"tldr": "This paper explores the connections between calibration, quantification, and classifier accuracy prediction under dataset shift conditions, proving their equivalence and proposing new methods based on this equivalence.", "motivation": "To address the challenges in calibrating classifiers and estimating proportions and accuracy under dataset shift.", "method": "Prove the equivalence of the three problems through mutual reduction and propose new methods adapted from established techniques.", "result": "The proposed methods are competitive or even surpass dedicated approaches in performance.", "conclusion": "Fostering cross-fertilization and unified approaches across related research areas."}}
{"id": "2505.11390", "pdf": "https://arxiv.org/pdf/2505.11390", "abs": "https://arxiv.org/abs/2505.11390", "authors": ["Millend Roy", "Vladimir Pyltsov", "Yinbo Hu"], "title": "IISE PG&E Energy Analytics Challenge 2025: Hourly-Binned Regression Models Beat Transformers in Load Forecasting", "categories": ["cs.LG", "cs.SY", "econ.EM", "eess.SY"], "comment": null, "summary": "Accurate electricity load forecasting is essential for grid stability,\nresource optimization, and renewable energy integration. While\ntransformer-based deep learning models like TimeGPT have gained traction in\ntime-series forecasting, their effectiveness in long-term electricity load\nprediction remains uncertain. This study evaluates forecasting models ranging\nfrom classical regression techniques to advanced deep learning architectures\nusing data from the ESD 2025 competition. The dataset includes two years of\nhistorical electricity load data, alongside temperature and global horizontal\nirradiance (GHI) across five sites, with a one-day-ahead forecasting horizon.\nSince actual test set load values remain undisclosed, leveraging predicted\nvalues would accumulate errors, making this a long-term forecasting challenge.\nWe employ (i) Principal Component Analysis (PCA) for dimensionality reduction\nand (ii) frame the task as a regression problem, using temperature and GHI as\ncovariates to predict load for each hour, (iii) ultimately stacking 24 models\nto generate yearly forecasts.\n  Our results reveal that deep learning models, including TimeGPT, fail to\nconsistently outperform simpler statistical and machine learning approaches due\nto the limited availability of training data and exogenous variables. In\ncontrast, XGBoost, with minimal feature engineering, delivers the lowest error\nrates across all test cases while maintaining computational efficiency. This\nhighlights the limitations of deep learning in long-term electricity\nforecasting and reinforces the importance of model selection based on dataset\ncharacteristics rather than complexity. Our study provides insights into\npractical forecasting applications and contributes to the ongoing discussion on\nthe trade-offs between traditional and modern forecasting methods.", "AI": {"tldr": "\u51c6\u786e\u7684\u7535\u529b\u8d1f\u8377\u9884\u6d4b\u5bf9\u4e8e\u7535\u7f51\u7a33\u5b9a\u6027\u3001\u8d44\u6e90\u4f18\u5316\u548c\u53ef\u518d\u751f\u80fd\u6e90\u6574\u5408\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4ece\u7ecf\u5178\u56de\u5f52\u6280\u672f\u5230\u5148\u8fdb\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7684\u5404\u79cd\u9884\u6d4b\u6a21\u578b\uff0c\u4f7f\u7528ESD 2025\u7ade\u8d5b\u7684\u6570\u636e\uff0c\u53d1\u73b0XGBoost\u5728\u6240\u6709\u6d4b\u8bd5\u60c5\u51b5\u4e0b\u5747\u80fd\u63d0\u4f9b\u6700\u4f4e\u7684\u8bef\u5dee\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002\u8fd9\u7a81\u663e\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u957f\u671f\u7535\u529b\u9884\u6d4b\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u6839\u636e\u6570\u636e\u7279\u5f81\u800c\u975e\u590d\u6742\u5ea6\u9009\u62e9\u6a21\u578b\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u51c6\u786e\u7684\u7535\u529b\u8d1f\u8377\u9884\u6d4b\u5bf9\u7535\u7f51\u7a33\u5b9a\u6027\u3001\u8d44\u6e90\u4f18\u5316\u548c\u53ef\u518d\u751f\u80fd\u6e90\u6574\u5408\u5f88\u91cd\u8981\uff0c\u4f46\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u957f\u671f\u8d1f\u8377\u9884\u6d4b\u4e2d\u7684\u6548\u679c\u4ecd\u4e0d\u786e\u5b9a\u3002", "method": "\u4f7f\u7528PCA\u8fdb\u884c\u964d\u7ef4\uff0c\u5c06\u4efb\u52a1\u4f5c\u4e3a\u56de\u5f52\u95ee\u9898\u5904\u7406\uff0c\u5229\u7528\u6e29\u5ea6\u548cGHI\u4f5c\u4e3a\u534f\u53d8\u91cf\u9884\u6d4b\u6bcf\u5c0f\u65f6\u7684\u8d1f\u8377\uff0c\u5e76\u6700\u7ec8\u5806\u53e024\u4e2a\u6a21\u578b\u751f\u6210\u5e74\u5ea6\u9884\u6d4b\u3002", "result": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5305\u62ecTimeGPT\uff09\u672a\u80fd\u59cb\u7ec8\u4f18\u4e8e\u7b80\u5355\u7684\u7edf\u8ba1\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u800cXGBoost\u5728\u6240\u6709\u6d4b\u8bd5\u60c5\u51b5\u4e0b\u90fd\u63d0\u4f9b\u4e86\u6700\u4f4e\u7684\u8bef\u5dee\u7387\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u957f\u671f\u7535\u529b\u8d1f\u8377\u9884\u6d4b\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u6839\u636e\u6570\u636e\u7279\u5f81\u9009\u62e9\u6a21\u578b\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.11396", "pdf": "https://arxiv.org/pdf/2505.11396", "abs": "https://arxiv.org/abs/2505.11396", "authors": ["Dazhuo Qiu", "Jinwen Chen", "Arijit Khan", "Yan Zhao", "Francesco Bonchi"], "title": "Finding Counterfactual Evidences for Node Classification", "categories": ["cs.LG", "cs.DB"], "comment": "Accepted by KDD 2025", "summary": "Counterfactual learning is emerging as an important paradigm, rooted in\ncausality, which promises to alleviate common issues of graph neural networks\n(GNNs), such as fairness and interpretability. However, as in many real-world\napplication domains where conducting randomized controlled trials is\nimpractical, one has to rely on available observational (factual) data to\ndetect counterfactuals. In this paper, we introduce and tackle the problem of\nsearching for counterfactual evidences for the GNN-based node classification\ntask. A counterfactual evidence is a pair of nodes such that, regardless they\nexhibit great similarity both in the features and in their neighborhood\nsubgraph structures, they are classified differently by the GNN. We develop\neffective and efficient search algorithms and a novel indexing solution that\nleverages both node features and structural information to identify\ncounterfactual evidences, and generalizes beyond any specific GNN. Through\nvarious downstream applications, we demonstrate the potential of counterfactual\nevidences to enhance fairness and accuracy of GNNs.", "AI": {"tldr": "This paper introduces the problem of searching for counterfactual evidences for GNN-based node classification tasks. It develops search algorithms and an indexing solution to identify counterfactual evidences, which can improve the fairness and accuracy of GNNs.", "motivation": "To alleviate common issues of GNNs such as fairness and interpretability, especially when randomized controlled trials are impractical.", "method": "Develop effective and efficient search algorithms and a novel indexing solution that leverages both node features and structural information.", "result": "The proposed method can identify counterfactual evidences, which can enhance the fairness and accuracy of GNNs through various downstream applications.", "conclusion": "Counterfactual evidences can be used to improve the fairness and accuracy of GNNs."}}
{"id": "2505.11411", "pdf": "https://arxiv.org/pdf/2505.11411", "abs": "https://arxiv.org/abs/2505.11411", "authors": ["Xiaotian Zhang", "Yue Shang", "Entao Yang", "Ge Zhang"], "title": "Is Grokking a Computational Glass Relaxation?", "categories": ["cs.LG", "cond-mat.dis-nn"], "comment": null, "summary": "Understanding neural network's (NN) generalizability remains a central\nquestion in deep learning research. The special phenomenon of grokking, where\nNNs abruptly generalize long after the training performance reaches a\nnear-perfect level, offers a unique window to investigate the underlying\nmechanisms of NNs' generalizability. Here we propose an interpretation for\ngrokking by framing it as a computational glass relaxation: viewing NNs as a\nphysical system where parameters are the degrees of freedom and train loss is\nthe system energy, we find memorization process resembles a rapid cooling of\nliquid into non-equilibrium glassy state at low temperature and the later\ngeneralization is like a slow relaxation towards a more stable configuration.\nThis mapping enables us to sample NNs' Boltzmann entropy (states of density)\nlandscape as a function of training loss and test accuracy. Our experiments in\ntransformers on arithmetic tasks suggests that there is NO entropy barrier in\nthe memorization-to-generalization transition of grokking, challenging previous\ntheory that defines grokking as a first-order phase transition. We identify a\nhigh-entropy advantage under grokking, an extension of prior work linking\nentropy to generalizability but much more significant. Inspired by grokking's\nfar-from-equilibrium nature, we develop a toy optimizer WanD based on\nWang-landau molecular dynamics, which can eliminate grokking without any\nconstraints and find high-norm generalizing solutions. This provides\nstrictly-defined counterexamples to theory attributing grokking solely to\nweight norm evolution towards the Goldilocks zone and also suggests new\npotential ways for optimizer design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u91ca\u795e\u7ecf\u7f51\u7edcgrokking\u73b0\u8c61\u7684\u65b0\u65b9\u6cd5\uff0c\u6311\u6218\u4e86\u73b0\u6709\u7406\u8bba\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u5668WanD\u3002", "motivation": "\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u7136\u662f\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u4e2d\u7684\u6838\u5fc3\u95ee\u9898\u3002grokking\u73b0\u8c61\u63d0\u4f9b\u4e86\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u6cdb\u5316\u673a\u5236\u7684\u72ec\u7279\u7a97\u53e3\u3002", "method": "\u5c06\u795e\u7ecf\u7f51\u7edc\u89c6\u4e3a\u7269\u7406\u7cfb\u7edf\uff0c\u5176\u4e2d\u53c2\u6570\u662f\u81ea\u7531\u5ea6\uff0c\u8bad\u7ec3\u635f\u5931\u662f\u7cfb\u7edf\u80fd\u91cf\u3002\u901a\u8fc7\u8fd9\u79cd\u6620\u5c04\uff0c\u53ef\u4ee5\u91c7\u6837\u795e\u7ecf\u7f51\u7edc\u7684\u73bb\u5c14\u5179\u66fc\u71b5\u666f\u89c2\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eWang-Landau\u5206\u5b50\u52a8\u529b\u5b66\u7684\u73a9\u5177\u4f18\u5316\u5668WanD\u3002", "result": "\u5728\u7b97\u672f\u4efb\u52a1\u4e0a\u7684Transformer\u5b9e\u9a8c\u8868\u660e\uff0cgrokking\u7684\u6cdb\u5316\u8fc7\u7a0b\u4e2d\u4e0d\u5b58\u5728\u71b5\u969c\u788d\u3002\u6b64\u5916\uff0c\u53d1\u73b0grokking\u5177\u6709\u9ad8\u71b5\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u5bf9grokking\u73b0\u8c61\u7684\u4e00\u79cd\u89e3\u91ca\uff0c\u5c06\u5176\u89c6\u4e3a\u8ba1\u7b97\u73bb\u7483\u5f1b\u8c6b\u3002\u8fd9\u79cd\u89c2\u70b9\u6311\u6218\u4e86\u4e4b\u524d\u5173\u4e8egrokking\u4f5c\u4e3a\u4e00\u7ea7\u76f8\u53d8\u7684\u7406\u8bba\uff0c\u5e76\u4e14\u53d1\u73b0grokking\u5177\u6709\u9ad8\u71b5\u4f18\u52bf\u3002\u6b64\u5916\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u5668WanD\uff0c\u5b83\u53ef\u4ee5\u6d88\u9664grokking\u5e76\u627e\u5230\u9ad8\u8303\u6570\u6cdb\u5316\u89e3\u3002"}}
{"id": "2505.11412", "pdf": "https://arxiv.org/pdf/2505.11412", "abs": "https://arxiv.org/abs/2505.11412", "authors": ["Ciaran Bench", "Vivek Desai", "Mohammad Moulaeifard", "Nils Strodthoff", "Philip Aston", "Andrew Thompson"], "title": "Uncertainty quantification with approximate variational learning for wearable photoplethysmography prediction tasks", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Photoplethysmography (PPG) signals encode information about relative changes\nin blood volume that can be used to assess various aspects of cardiac health\nnon-invasively, e.g.\\ to detect atrial fibrillation (AF) or predict blood\npressure (BP). Deep networks are well-equipped to handle the large quantities\nof data acquired from wearable measurement devices. However, they lack\ninterpretability and are prone to overfitting, leaving considerable risk for\npoor performance on unseen data and misdiagnosis. Here, we describe the use of\ntwo scalable uncertainty quantification techniques: Monte Carlo Dropout and the\nrecently proposed Improved Variational Online Newton. These techniques are used\nto assess the trustworthiness of models trained to perform AF classification\nand BP regression from raw PPG time series. We find that the choice of\nhyperparameters has a considerable effect on the predictive performance of the\nmodels and on the quality and composition of predicted uncertainties. E.g. the\nstochasticity of the model parameter sampling determines the proportion of the\ntotal uncertainty that is aleatoric, and has varying effects on predictive\nperformance and calibration quality dependent on the chosen uncertainty\nquantification technique and the chosen expression of uncertainty. We find\nsignificant discrepancy in the quality of uncertainties over the predicted\nclasses, emphasising the need for a thorough evaluation protocol that assesses\nlocal and adaptive calibration. This work suggests that the choice of\nhyperparameters must be carefully tuned to balance predictive performance and\ncalibration quality, and that the optimal parameterisation may vary depending\non the chosen expression of uncertainty.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e24\u79cd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6280\u672f\u5728\u4ecePPG\u4fe1\u53f7\u5206\u7c7bAF\u548c\u56de\u5f52BP\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63a2\u8ba8\u4e86\u8d85\u53c2\u6570\u9009\u62e9\u5bf9\u6a21\u578b\u9884\u6d4b\u6027\u80fd\u548c\u4e0d\u786e\u5b9a\u6027\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "motivation": "PPG\u4fe1\u53f7\u53ef\u4ee5\u65e0\u521b\u5730\u8bc4\u4f30\u5fc3\u810f\u5065\u5eb7\u72b6\u51b5\uff0c\u4f46\u6df1\u5ea6\u7f51\u7edc\u7f3a\u4e4f\u89e3\u91ca\u6027\u4e14\u5bb9\u6613\u8fc7\u62df\u5408\u3002", "method": "\u4f7f\u7528Monte Carlo Dropout\u548cImproved Variational Online Newton\u4e24\u79cd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6280\u672f\u3002", "result": "\u8d85\u53c2\u6570\u7684\u9009\u62e9\u5bf9\u6a21\u578b\u7684\u9884\u6d4b\u6027\u80fd\u548c\u4e0d\u786e\u5b9a\u6027\u8d28\u91cf\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u9700\u8981\u4ed4\u7ec6\u8c03\u6574\u8d85\u53c2\u6570\u4ee5\u5e73\u8861\u9884\u6d4b\u6027\u80fd\u548c\u6821\u51c6\u8d28\u91cf\uff0c\u5e76\u4e14\u6700\u4f73\u53c2\u6570\u5316\u53ef\u80fd\u56e0\u4e0d\u786e\u5b9a\u6027\u8868\u8fbe\u65b9\u5f0f\u7684\u4e0d\u540c\u800c\u53d8\u5316\u3002"}}
{"id": "2505.11415", "pdf": "https://arxiv.org/pdf/2505.11415", "abs": "https://arxiv.org/abs/2505.11415", "authors": ["Yinsicheng Jiang", "Yao Fu", "Yeqi Huang", "Ping Nie", "Zhan Lu", "Leyang Xue", "Congjie He", "Man-Kit Sit", "Jilong Xue", "Li Dong", "Ziming Miao", "Dayou Du", "Tairan Xu", "Kai Zou", "Edoardo Ponti", "Luo Mai"], "title": "MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse Mixture-of-Experts Systems", "categories": ["cs.LG", "cs.DC"], "comment": "arXiv admin note: substantial text overlap with arXiv:2412.07067", "summary": "The sparse Mixture-of-Experts (MoE) architecture is increasingly favored for\nscaling Large Language Models (LLMs) efficiently, but it depends on\nheterogeneous compute and memory resources. These factors jointly affect system\nCost, Accuracy, and Performance (CAP), making trade-offs inevitable. Existing\nbenchmarks often fail to capture these trade-offs accurately, complicating\npractical deployment decisions. To address this, we introduce MoE-CAP, a\nbenchmark specifically designed for MoE systems. Our analysis reveals that\nachieving an optimal balance across CAP is difficult with current hardware; MoE\nsystems typically optimize two of the three dimensions at the expense of the\nthird-a dynamic we term the MoE-CAP trade-off. To visualize this, we propose\nthe CAP Radar Diagram. We further introduce sparsity-aware performance\nmetrics-Sparse Memory Bandwidth Utilization (S-MBU) and Sparse Model FLOPS\nUtilization (S-MFU)-to enable accurate performance benchmarking of MoE systems\nacross diverse hardware platforms and deployment scenarios.", "AI": {"tldr": "\u63d0\u51faMoE-CAP\u57fa\u51c6\u6d4b\u8bd5\u6765\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u51c6\u786e\u6355\u6349MoE\u7cfb\u7edf\u6210\u672c\u3001\u7cbe\u5ea6\u548c\u6027\u80fd\u6743\u8861\u7684\u95ee\u9898\uff0c\u5e76\u5f15\u5165CAP\u96f7\u8fbe\u56fe\u53ca\u7a00\u758f\u611f\u77e5\u6027\u80fd\u5ea6\u91cf\u6807\u51c6\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u51c6\u786e\u6355\u6349MoE\u7cfb\u7edf\u6210\u672c\u3001\u7cbe\u5ea6\u548c\u6027\u80fd\u7684\u6743\u8861\uff0c\u5f71\u54cd\u5b9e\u9645\u90e8\u7f72\u51b3\u7b56\u3002", "method": "\u8bbe\u8ba1MoE-CAP\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63d0\u51faCAP\u96f7\u8fbe\u56fe\u53ca\u7a00\u758f\u611f\u77e5\u6027\u80fd\u5ea6\u91cf\u6807\u51c6(S-MBU\u548cS-MFU)\u3002", "result": "\u53d1\u73b0\u5f53\u524d\u786c\u4ef6\u4e0b\u5728\u6210\u672c\u3001\u7cbe\u5ea6\u548c\u6027\u80fd\u4e4b\u95f4\u5b9e\u73b0\u6700\u4f73\u5e73\u8861\u5f88\u56f0\u96be\uff0cMoE\u7cfb\u7edf\u901a\u5e38\u4ee5\u727a\u7272\u7b2c\u4e09\u4e2a\u7ef4\u5ea6\u4e3a\u4ee3\u4ef7\u4f18\u5316\u5176\u4e2d\u4e24\u4e2a\u7ef4\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u4e86\u65b0\u7684\u65b9\u6cd5\u548c\u5de5\u5177\u6765\u66f4\u597d\u5730\u8bc4\u4f30\u548c\u4f18\u5316MoE\u7cfb\u7edf\u7684\u6210\u672c\u3001\u7cbe\u5ea6\u548c\u6027\u80fd\u3002"}}
{"id": "2505.11432", "pdf": "https://arxiv.org/pdf/2505.11432", "abs": "https://arxiv.org/abs/2505.11432", "authors": ["Chao Jin", "Ziheng Jiang", "Zhihao Bai", "Zheng Zhong", "Juncai Liu", "Xiang Li", "Ningxin Zheng", "Xi Wang", "Cong Xie", "Qi Huang", "Wen Heng", "Yiyuan Ma", "Wenlei Bao", "Size Zheng", "Yanghua Peng", "Haibin Lin", "Xuanzhe Liu", "Xin Jin", "Xin Liu"], "title": "MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "We present MegaScale-MoE, a production system tailored for the efficient\ntraining of large-scale mixture-of-experts (MoE) models. MoE emerges as a\npromising architecture to scale large language models (LLMs) to unprecedented\nsizes, thereby enhancing model performance. However, existing MoE training\nsystems experience a degradation in training efficiency, exacerbated by the\nescalating scale of MoE models and the continuous evolution of hardware.\n  Recognizing the pivotal role of efficient communication in enhancing MoE\ntraining, MegaScale-MoE customizes communication-efficient parallelism\nstrategies for attention and FFNs in each MoE layer and adopts a holistic\napproach to overlap communication with computation at both inter- and\nintra-operator levels. Additionally, MegaScale-MoE applies communication\ncompression with adjusted communication patterns to lower precision, further\nimproving training efficiency. When training a 352B MoE model on 1,440 NVIDIA\nHopper GPUs, MegaScale-MoE achieves a training throughput of 1.41M tokens/s,\nimproving the efficiency by 1.88$\\times$ compared to Megatron-LM. We share our\noperational experience in accelerating MoE training and hope that by offering\nour insights in system design, this work will motivate future research in MoE\nsystems.", "AI": {"tldr": "\u63d0\u51faMegaScale-MoE\uff0c\u4e00\u79cd\u9488\u5bf9\u5927\u89c4\u6a21\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u9ad8\u6548\u8bad\u7ec3\u7684\u751f\u4ea7\u7cfb\u7edf\u3002\u901a\u8fc7\u5b9a\u5236\u901a\u4fe1\u9ad8\u6548\u7684\u5e76\u884c\u7b56\u7565\u548c\u91cd\u53e0\u901a\u4fe1\u4e0e\u8ba1\u7b97\u6765\u63d0\u9ad8\u6548\u7387\u3002\u57281440\u4e2aNVIDIA Hopper GPU\u4e0a\u8bad\u7ec3\u4e00\u4e2a352B\u53c2\u6570\u7684MoE\u6a21\u578b\u65f6\uff0c\u8fbe\u5230\u4e861.41M tokens/s\u7684\u8bad\u7ec3\u541e\u5410\u91cf\uff0c\u63d0\u9ad8\u4e861.88\u500d\u7684\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684MoE\u8bad\u7ec3\u7cfb\u7edf\u7531\u4e8e\u6a21\u578b\u89c4\u6a21\u6269\u5927\u548c\u786c\u4ef6\u7684\u4e0d\u65ad\u6f14\u5316\uff0c\u8bad\u7ec3\u6548\u7387\u4e0b\u964d\u3002MegaScale-MoE\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u901a\u4fe1\u6765\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u5b9a\u5236\u4e86\u6ce8\u610f\u529b\u548cFFNs\u7684\u901a\u4fe1\u9ad8\u6548\u5e76\u884c\u7b56\u7565\uff0c\u91c7\u7528\u6574\u4f53\u65b9\u6cd5\u91cd\u53e0\u901a\u4fe1\u4e0e\u8ba1\u7b97\uff0c\u5e76\u5e94\u7528\u901a\u4fe1\u538b\u7f29\u6280\u672f\u964d\u4f4e\u7cbe\u5ea6\u3002", "result": "\u57281440\u4e2aNVIDIA Hopper GPU\u4e0a\u8bad\u7ec3\u4e00\u4e2a352B\u53c2\u6570\u7684MoE\u6a21\u578b\uff0c\u5b9e\u73b0\u4e861.41M tokens/s\u7684\u8bad\u7ec3\u541e\u5410\u91cf\uff0c\u6548\u7387\u63d0\u9ad8\u4e861.88\u500d\u3002", "conclusion": "\u5206\u4eab\u4e86\u52a0\u901fMoE\u8bad\u7ec3\u7684\u7ecf\u9a8c\uff0c\u5e0c\u671b\u5bf9\u672a\u6765MoE\u7cfb\u7edf\u7684\u7cfb\u7edf\u8bbe\u8ba1\u7814\u7a76\u63d0\u4f9b\u542f\u793a\u3002"}}
{"id": "2505.11461", "pdf": "https://arxiv.org/pdf/2505.11461", "abs": "https://arxiv.org/abs/2505.11461", "authors": ["Wesley A Suttle", "Vipul K Sharma", "Brian M Sadler"], "title": "Signal attenuation enables scalable decentralized multi-agent reinforcement learning over networks", "categories": ["cs.LG"], "comment": "7 pages, 1 figure", "summary": "Classic multi-agent reinforcement learning (MARL) methods require that agents\nenjoy global state observability, preventing development of decentralized\nalgorithms and limiting scalability. Recent work has shown that, under\nassumptions on decaying inter-agent influence, global observability can be\nreplaced by local neighborhood observability at each agent, enabling\ndecentralization and scalability. Real-world applications enjoying such decay\nproperties remain underexplored, however, despite the fact that signal power\ndecay, or signal attenuation, due to path loss is an intrinsic feature of many\nproblems in wireless communications and radar networks. In this paper, we show\nthat signal attenuation enables decentralization in MARL by considering the\nillustrative special case of performing power allocation for target detection\nin a radar network. To achieve this, we propose two new constrained multi-agent\nMarkov decision process formulations of this power allocation problem, derive\nlocal neighborhood approximations for global value function and gradient\nestimates and establish corresponding error bounds, and develop decentralized\nsaddle point policy gradient algorithms for solving the proposed problems. Our\napproach, though oriented towards the specific radar network problem we\nconsider, provides a useful model for future extensions to additional problems\nin wireless communications and radar networks.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4fe1\u53f7\u8870\u51cf\u5728\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5728\u96f7\u8fbe\u7f51\u7edc\u4e2d\u5b9e\u73b0\u4e86\u53bb\u4e2d\u5fc3\u5316\u7684\u529f\u7387\u5206\u914d\u3002", "motivation": "\u7ecf\u5178\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5168\u5c40\u72b6\u6001\u53ef\u89c2\u6d4b\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u53bb\u4e2d\u5fc3\u5316\u7b97\u6cd5\u7684\u53d1\u5c55\u548c\u53ef\u6269\u5c55\u6027\u3002\u800c\u57fa\u4e8e\u8870\u51cf\u7684\u591a\u667a\u80fd\u4f53\u5f71\u54cd\u5047\u8bbe\uff0c\u53ef\u4ee5\u7528\u6bcf\u4e2a\u4ee3\u7406\u7684\u5c40\u90e8\u90bb\u57df\u53ef\u89c2\u6d4b\u6027\u4ee3\u66ff\u5168\u5c40\u53ef\u89c2\u6d4b\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u548c\u53ef\u6269\u5c55\u6027\u3002\u7136\u800c\uff0c\u4eab\u53d7\u8fd9\u79cd\u8870\u51cf\u5c5e\u6027\u7684\u5b9e\u9645\u5e94\u7528\u4ecd\u7136\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u53d7\u9650\u591a\u667a\u80fd\u4f53\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u516c\u5f0f\uff0c\u63a8\u5bfc\u4e86\u5168\u5c40\u4ef7\u503c\u51fd\u6570\u548c\u68af\u5ea6\u4f30\u8ba1\u7684\u5c40\u90e8\u90bb\u57df\u8fd1\u4f3c\u5e76\u5efa\u7acb\u4e86\u76f8\u5e94\u7684\u8bef\u5dee\u754c\u9650\uff0c\u5f00\u53d1\u4e86\u89e3\u51b3\u6240\u63d0\u51fa\u95ee\u9898\u7684\u53bb\u4e2d\u5fc3\u5316\u978d\u70b9\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u3002", "result": "\u5728\u96f7\u8fbe\u7f51\u7edc\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u529f\u7387\u5206\u914d\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86\u4fe1\u53f7\u8870\u51cf\u5bf9\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u53bb\u4e2d\u5fc3\u5316\u7684\u6709\u6548\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u76f8\u5e94\u7684\u53bb\u4e2d\u5fc3\u5316\u7b97\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u4fe1\u53f7\u8870\u51cf\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u53bb\u4e2d\u5fc3\u5316\uff0c\u5e76\u5728\u96f7\u8fbe\u7f51\u7edc\u4e2d\u7684\u529f\u7387\u5206\u914d\u95ee\u9898\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u7528\u7684\u6a21\u578b\uff0c\u53ef\u7528\u4e8e\u65e0\u7ebf\u901a\u4fe1\u548c\u96f7\u8fbe\u7f51\u7edc\u4e2d\u7684\u5176\u4ed6\u95ee\u9898\u3002"}}
{"id": "2505.11483", "pdf": "https://arxiv.org/pdf/2505.11483", "abs": "https://arxiv.org/abs/2505.11483", "authors": ["Zhaolan Huang", "Emmanuel Baccelli"], "title": "msf-CNN: Patch-based Multi-Stage Fusion with Convolutional Neural Networks for TinyML", "categories": ["cs.LG", "cs.PF"], "comment": null, "summary": "AI spans from large language models to tiny models running on\nmicrocontrollers (MCUs). Extremely memory-efficient model architectures are\ndecisive to fit within an MCU's tiny memory budget e.g., 128kB of RAM. However,\ninference latency must remain small to fit real-time constraints. An approach\nto tackle this is patch-based fusion, which aims to optimize data flows across\nneural network layers. In this paper, we introduce msf-CNN, a novel technique\nthat efficiently finds optimal fusion settings for convolutional neural\nnetworks (CNNs) by walking through the fusion solution space represented as a\ndirected acyclic graph. Compared to previous work on CNN fusion for MCUs,\nmsf-CNN identifies a wider set of solutions. We published an implementation of\nmsf-CNN running on various microcontrollers (ARM Cortex-M, RISC-V, ESP32). We\nshow that msf-CNN can achieve inference using 50% less RAM compared to the\nprior art (MCUNetV2 and StreamNet). We thus demonstrate how msf-CNN offers\nadditional flexibility for system designers.", "AI": {"tldr": "This paper introduces msf-CNN, a novel method for optimizing convolutional neural networks on microcontrollers by efficiently finding optimal fusion settings, reducing RAM usage by 50% compared to previous methods.", "motivation": "To fit neural network models within the limited memory of microcontrollers while maintaining low inference latency.", "method": "Introduces msf-CNN, which uses a directed acyclic graph to explore fusion solution spaces for convolutional neural networks.", "result": "msf-CNN achieves 50% less RAM usage than previous methods (MCUNetV2 and StreamNet) on various microcontrollers.", "conclusion": "msf-CNN provides additional flexibility for system designers in deploying neural networks on resource-constrained devices."}}
{"id": "2505.11491", "pdf": "https://arxiv.org/pdf/2505.11491", "abs": "https://arxiv.org/abs/2505.11491", "authors": ["Yuan-Zheng Lei", "Yaobang Gong", "Dianwei Chen", "Yao Cheng", "Xianfeng Terry Yang"], "title": "Potential failures of physics-informed machine learning in traffic flow modeling: theoretical and experimental analysis", "categories": ["cs.LG", "physics.comp-ph"], "comment": null, "summary": "This study critically examines the performance of physics-informed machine\nlearning (PIML) approaches for traffic flow modeling, defining the failure of a\nPIML model as the scenario where it underperforms both its purely data-driven\nand purely physics-based counterparts. We analyze the loss landscape by\nperturbing trained models along the principal eigenvectors of the Hessian\nmatrix and evaluating corresponding loss values. Our results suggest that\nphysics residuals in PIML do not inherently hinder optimization, contrary to a\ncommonly assumed failure cause. Instead, successful parameter updates require\nboth ML and physics gradients to form acute angles with the quasi-true gradient\nand lie within a conical region. Given inaccuracies in both the physics models\nand the training data, satisfying this condition is often difficult.\nExperiments reveal that physical residuals can degrade the performance of LWR-\nand ARZ-based PIML models, especially under highly physics-driven settings.\nMoreover, sparse sampling and the use of temporally averaged traffic data can\nproduce misleadingly small physics residuals that fail to capture actual\nphysical dynamics, contributing to model failure. We also identify the\nCourant-Friedrichs-Lewy (CFL) condition as a key indicator of dataset\nsuitability for PIML, where successful applications consistently adhere to this\ncriterion. Lastly, we observe that higher-order models like ARZ tend to have\nlarger error lower bounds than lower-order models like LWR, which is consistent\nwith the experimental findings of existing studies.", "AI": {"tldr": "This study investigates the performance of physics-informed machine learning (PIML) approaches for traffic flow modeling.", "motivation": "To define the failure of a PIML model and examine whether physics residuals hinder optimization.", "method": "Analyzing the loss landscape by perturbing trained models along the principal eigenvectors of the Hessian matrix and evaluating corresponding loss values.", "result": "Physics residuals do not inherently hinder optimization but successful parameter updates require certain conditions. Physical residuals can degrade PIML model performance, and misleadingly small physics residuals can contribute to model failure. The CFL condition is identified as a key indicator of dataset suitability for PIML.", "conclusion": "Higher-order models tend to have larger error lower bounds than lower-order models."}}
