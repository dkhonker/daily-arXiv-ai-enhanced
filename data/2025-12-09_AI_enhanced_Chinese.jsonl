{"id": "2512.05998", "pdf": "https://arxiv.org/pdf/2512.05998", "abs": "https://arxiv.org/abs/2512.05998", "authors": ["Michael Todasco"], "title": "Going All-In on LLM Accuracy: Fake Prediction Markets, Real Confidence Signals", "categories": ["cs.AI", "cs.GT", "cs.LG"], "comment": "25 pages, 8 tables, 2 figures. Pilot study. Data, prompts, and code available at https://osf.io/dc24t/", "summary": "Large language models are increasingly used to evaluate other models, yet these judgments typically lack any representation of confidence. This pilot study tests whether framing an evaluation task as a betting game (a fictional prediction market with its own LLM currency) improves forecasting accuracy and surfaces calibrated confidence signals. We generated 100 math and logic questions with verifiable answers. Six Baseline models (three current-generation, three prior-generation) answered all items. Three Predictor models then forecasted, for each question-baseline pair, if the baseline would answer correctly. Each predictor completed matched runs in two conditions: Control (simple correct/incorrect predictions) and Incentive (predictions plus wagers of 1-100,000 LLMCoin under even odds, starting from a 1,000,000 LLMCoin bankroll). Across 5,400 predictions per condition, Incentive runs showed modestly higher accuracy (81.5% vs. 79.1%, p = .089, d = 0.86) and significantly faster learning across rounds (12.0 vs. 2.9 percentage-point improvement from Round 1 to Round 4, p = .011). Most notably, stake size tracked confidence. \"Whale\" bets of 40,000+ coins were correct ~99% of the time, while small bets (<1,000 coins) showed only ~74% accuracy. The key finding is not that fictional money makes models smarter; accuracy gains were modest and did not reach statistical significance (p = .089) in this pilot. Rather, the betting mechanic created a legible confidence signal absent from binary yes/no outputs. This suggests that simple financial framing may help transform LLMs into risk-aware forecasters, making their internal beliefs visible and usable. The protocol offers a foundation for future work for meta-evaluation systems and what may become LLM-to-LLM prediction markets.", "AI": {"tldr": "本研究通过将LLM评估任务设计为投注游戏（使用虚构的LLM货币），测试是否能提高预测准确性并显示校准后的置信度信号。研究发现投注机制能产生清晰的置信信号，使模型内部信念可见可用。", "motivation": "大型语言模型越来越多地用于评估其他模型，但这些判断通常缺乏置信度表示。研究测试通过投注游戏框架是否能改善预测准确性并显示校准的置信信号。", "method": "生成100个数学和逻辑问题，6个基线模型回答问题，3个预测模型预测每个问题-基线对的正确性。在两种条件下进行：控制组（简单正确/错误预测）和激励组（预测加投注1-100,000 LLMCoin）。", "result": "激励组显示略高的准确性（81.5% vs 79.1%）和显著更快的学习速度（第1轮到第4轮改进12.0% vs 2.9%）。投注金额与置信度相关，大额投注（40,000+硬币）正确率约99%，小额投注（<1,000硬币）正确率约74%。", "conclusion": "投注机制创造了二进制是/否输出中缺失的清晰置信信号，表明简单的金融框架可能帮助LLM成为风险感知的预测者，使其内部信念可见可用，为元评估系统和LLM-to-LLM预测市场奠定基础。"}}
{"id": "2512.06161", "pdf": "https://arxiv.org/pdf/2512.06161", "abs": "https://arxiv.org/abs/2512.06161", "authors": ["Gondy Leroy", "Prakash Bisht", "Sai Madhuri Kandula", "Nell Maltman", "Sydney Rice"], "title": "Deep learning for autism detection using clinical notes: A comparison of transfer learning for a transparent and black-box approach", "categories": ["cs.AI", "cs.LG"], "comment": "9 pages", "summary": "Autism spectrum disorder (ASD) is a complex neurodevelopmental condition whose rising prevalence places increasing demands on a lengthy diagnostic process. Machine learning (ML) has shown promise in automating ASD diagnosis, but most existing models operate as black boxes and are typically trained on a single dataset, limiting their generalizability. In this study, we introduce a transparent and interpretable ML approach that leverages BioBERT, a state-of-the-art language model, to analyze unstructured clinical text. The model is trained to label descriptions of behaviors and map them to diagnostic criteria, which are then used to assign a final label (ASD or not). We evaluate transfer learning, the ability to transfer knowledge to new data, using two distinct real-world datasets. We trained on datasets sequentially and mixed together and compared the performance of the best models and their ability to transfer to new data. We also created a black-box approach and repeated this transfer process for comparison. Our transparent model demonstrated robust performance, with the mixed-data training strategy yielding the best results (97 % sensitivity, 98 % specificity). Sequential training across datasets led to a slight drop in performance, highlighting the importance of training data order. The black-box model performed worse (90 % sensitivity, 96 % specificity) when trained sequentially or with mixed data. Overall, our transparent approach outperformed the black-box approach. Mixing datasets during training resulted in slightly better performance and should be the preferred approach when practically possible. This work paves the way for more trustworthy, generalizable, and clinically actionable AI tools in neurodevelopmental diagnostics.", "AI": {"tldr": "本研究提出了一种基于BioBERT的可解释机器学习方法，通过分析临床文本来诊断自闭症谱系障碍，在混合数据集训练下达到97%敏感性和98%特异性，优于黑盒模型。", "motivation": "自闭症诊断需求日益增长，现有机器学习模型多为黑盒且通常在单一数据集上训练，限制了其泛化能力和临床可信度。", "method": "使用BioBERT语言模型分析非结构化临床文本，训练模型标记行为描述并映射到诊断标准，最终给出ASD诊断标签。采用迁移学习策略，分别在顺序训练和混合训练两种模式下评估模型性能。", "result": "透明模型在混合数据训练策略下表现最佳（敏感性97%，特异性98%），顺序训练性能略有下降。黑盒模型表现较差（敏感性90%，特异性96%）。", "conclusion": "透明方法优于黑盒方法，混合数据集训练可获得更好性能，为神经发育诊断提供了更可信、可泛化且临床可操作的AI工具。"}}
{"id": "2512.06196", "pdf": "https://arxiv.org/pdf/2512.06196", "abs": "https://arxiv.org/abs/2512.06196", "authors": ["Charlie Masters", "Marta Grześkiewicz", "Stefano V. Albrecht"], "title": "ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to the AAAI 2026 LLAMAS Workshop (Large Language Model Agents for Multi-Agent Systems)", "summary": "As agents based on large language models are increasingly deployed to long-horizon tasks, maintaining their alignment with stakeholder preferences becomes critical. Effective alignment in such settings requires reward models that are interpretable so that stakeholders can understand and audit model objectives. Moreover, reward models must be capable of steering agents at interaction time, allowing preference shifts to be incorporated without retraining. We introduce ARCANE, a framework that frames alignment as a multi-agent collaboration problem that dynamically represents stakeholder preferences as natural-language rubrics: weighted sets of verifiable criteria that can be generated on-the-fly from task context. Inspired by utility theory, we formulate rubric learning as a reconstruction problem and apply a regularized Group-Sequence Policy Optimization (GSPO) procedure that balances interpretability, faithfulness, and computational efficiency. Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. The learned rubrics produce compact, legible evaluations and enable configurable trade-offs (e.g., correctness vs. conciseness) without retraining. Our results show that rubric-based reward models offer a promising path toward interpretable, test-time adaptive alignment for complex, long-horizon AI systems.", "AI": {"tldr": "ARCANE框架将AI对齐问题转化为多智能体协作，通过动态生成自然语言评分标准来代表利益相关者偏好，实现可解释、无需重新训练即可调整的奖励模型。", "motivation": "随着基于大语言模型的智能体越来越多地应用于长期任务，保持其与利益相关者偏好的一致性变得至关重要，需要可解释且能在交互时调整偏好的奖励模型。", "method": "采用多智能体协作框架，将偏好表示为可验证的自然语言评分标准；通过正则化组序列策略优化(GSPO)平衡可解释性、忠实性和计算效率；使用GDPVal基准的219个标注评分标准进行评估。", "result": "学习的评分标准能生成紧凑、易读的评估，支持可配置的权衡(如正确性与简洁性)，无需重新训练即可适应偏好变化。", "conclusion": "基于评分标准的奖励模型为复杂长期AI系统提供了可解释、测试时自适应对齐的有前景路径。"}}
{"id": "2512.06205", "pdf": "https://arxiv.org/pdf/2512.06205", "abs": "https://arxiv.org/abs/2512.06205", "authors": ["Daniel Quigley", "Eric Maynard"], "title": "On measuring grounding and generalizing grounding problems", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "36 pages, 85 sources", "summary": "The symbol grounding problem asks how tokens like cat can be about cats, as opposed to mere shapes manipulated in a calculus. We recast grounding from a binary judgment into an audit across desiderata, each indexed by an evaluation tuple (context, meaning type, threat model, reference distribution): authenticity (mechanisms reside inside the agent and, for strong claims, were acquired through learning or evolution); preservation (atomic meanings remain intact); faithfulness, both correlational (realized meanings match intended ones) and etiological (internal mechanisms causally contribute to success); robustness (graceful degradation under declared perturbations); compositionality (the whole is built systematically from the parts). We apply this framework to four grounding modes (symbolic; referential; vectorial; relational) and three case studies: model-theoretic semantics achieves exact composition but lacks etiological warrant; large language models show correlational fit and local robustness for linguistic tasks, yet lack selection-for-success on world tasks without grounded interaction; human language meets the desiderata under strong authenticity through evolutionary and developmental acquisition. By operationalizing a philosophical inquiry about representation, we equip philosophers of science, computer scientists, linguists, and mathematicians with a common language and technical framework for systematic investigation of grounding and meaning.", "AI": {"tldr": "本文提出一个符号接地问题的多维度评估框架，将接地从二元判断转化为基于(上下文、意义类型、威胁模型、参考分布)元组的审计标准，包括真实性、保持性、忠实性、鲁棒性和组合性等指标，并应用于四种接地模式和三个案例研究。", "motivation": "重新审视符号接地问题，解决传统二元判断的局限性，为哲学、计算机科学、语言学和数学等领域提供一个系统研究符号接地和意义的共同语言和技术框架。", "method": "提出一个基于评估元组的多维度审计框架，包含五个核心指标：真实性(机制是否通过学习或进化获得)、保持性(原子意义是否保持完整)、忠实性(相关性和因果性)、鲁棒性(在扰动下的表现)和组合性(系统构建性)。将此框架应用于四种接地模式(符号、指称、向量、关系)和三个案例研究。", "result": "模型论语义学实现精确组合但缺乏因果保证；大语言模型在语言任务中显示相关拟合和局部鲁棒性，但在无接地交互的世界任务中缺乏成功选择；人类语言通过进化和发育获得满足强真实性的所有标准。", "conclusion": "通过将哲学关于表征的探究操作化，为跨学科研究提供了一个系统调查符号接地和意义的共同框架，展示了不同接地模式在不同标准下的表现差异。"}}
{"id": "2512.06097", "pdf": "https://arxiv.org/pdf/2512.06097", "abs": "https://arxiv.org/abs/2512.06097", "authors": ["Emre Umucu", "Guillermina Solis", "Leon Garza", "Emilia Rivas", "Beatrice Lee", "Anantaa Kotal", "Aritran Piplai"], "title": "Empathy by Design: Aligning Large Language Models for Healthcare Dialogue", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "General-purpose large language models (LLMs) have demonstrated remarkable generative and reasoning capabilities but remain limited in healthcare and caregiving applications due to two key deficiencies: factual unreliability and a lack of empathetic communication. These shortcomings pose significant risks in sensitive contexts where users, particularly non-professionals and caregivers, seek medically relevant guidance or emotional reassurance. To address these challenges, we introduce a Direct Preference Optimization (DPO)-based alignment framework designed to improve factual correctness, semantic coherence, and human-centric qualities such as empathy, politeness, and simplicity in caregiver-patient dialogues. Our approach fine-tunes domain-adapted LLMs using pairwise preference data, where preferred responses reflect supportive and accessible communication styles while rejected ones represent prescriptive or overly technical tones. This direct optimization method aligns model outputs with human preferences more efficiently than traditional reinforcement-learning-based alignment. Empirical evaluations across multiple open and proprietary LLMs show that our DPO-tuned models achieve higher semantic alignment, improved factual accuracy, and stronger human-centric evaluation scores compared to baseline and commercial alternatives such as Google medical dialogue systems. These improvements demonstrate that preference-based alignment offers a scalable and transparent pathway toward developing trustworthy, empathetic, and clinically informed AI assistants for caregiver and healthcare communication. Our open-source code is available at: https://github.com/LeonG19/Empathy-by-Design", "AI": {"tldr": "基于直接偏好优化(DPO)的医疗对话对齐框架，通过配对偏好数据微调LLM，提升事实准确性、语义连贯性和同理心等人类中心化特质", "motivation": "通用大语言模型在医疗护理应用中存在事实不可靠性和缺乏同理心沟通两大缺陷，对寻求医疗指导和情感安慰的非专业人士和护理人员构成风险", "method": "使用直接偏好优化(DPO)框架，基于配对偏好数据微调领域适应的LLM，偏好回复体现支持性和易理解的沟通风格，拒绝回复代表规定性或过于技术化的语气", "result": "经验评估显示DPO调优模型相比基线和商业替代方案(如谷歌医疗对话系统)获得更高的语义对齐度、改进的事实准确性和更强的人类中心化评估分数", "conclusion": "基于偏好的对齐为开发可信赖、有同理心且具备临床知识的AI助手提供了可扩展和透明的路径，特别适用于护理人员和医疗保健沟通"}}
{"id": "2512.06240", "pdf": "https://arxiv.org/pdf/2512.06240", "abs": "https://arxiv.org/abs/2512.06240", "authors": ["Chuanhao Nie", "Yunbo Liu", "Chao Wang"], "title": "AI Application in Anti-Money Laundering for Sustainable and Transparent Financial Systems", "categories": ["cs.AI"], "comment": null, "summary": "Money laundering and financial fraud remain major threats to global financial stability, costing trillions annually and challenging regulatory oversight. This paper reviews how artificial intelligence (AI) applications can modernize Anti-Money Laundering (AML) workflows by improving detection accuracy, lowering false-positive rates, and reducing the operational burden of manual investigations, thereby supporting more sustainable development. It further highlights future research directions including federated learning for privacy-preserving collaboration, fairness-aware and interpretable AI, reinforcement learning for adaptive defenses, and human-in-the-loop visualization systems to ensure that next-generation AML architectures remain transparent, accountable, and robust. In the final part, the paper proposes an AI-driven KYC application that integrates graph-based retrieval-augmented generation (RAG Graph) with generative models to enhance efficiency, transparency, and decision support in KYC processes related to money-laundering detection. Experimental results show that the RAG-Graph architecture delivers high faithfulness and strong answer relevancy across diverse evaluation settings, thereby enhancing the efficiency and transparency of KYC CDD/EDD workflows and contributing to more sustainable, resource-optimized compliance practices.", "AI": {"tldr": "该论文探讨了人工智能在反洗钱领域的应用，提出了一种基于图检索增强生成的AI驱动KYC系统，能够提高检测准确性、降低误报率并增强透明度。", "motivation": "洗钱和金融欺诈对全球金融稳定构成重大威胁，传统监管方式面临效率低下和误报率高的问题，需要AI技术来现代化AML工作流程。", "method": "采用图检索增强生成（RAG Graph）架构与生成模型结合，开发AI驱动的KYC应用，通过联邦学习、强化学习和人机交互可视化系统等技术手段。", "result": "实验结果显示RAG-Graph架构在不同评估场景下表现出高忠实度和强答案相关性，显著提升了KYC CDD/EDD工作流程的效率和透明度。", "conclusion": "AI技术能够有效提升反洗钱工作的效率和准确性，未来的研究方向包括隐私保护协作、公平可解释AI和自适应防御系统，以实现更可持续的合规实践。"}}
{"id": "2512.06169", "pdf": "https://arxiv.org/pdf/2512.06169", "abs": "https://arxiv.org/abs/2512.06169", "authors": ["Chris Crawford"], "title": "Morphologically-Informed Tokenizers for Languages with Non-Concatenative Morphology: A case study of Yoloxóchtil Mixtec ASR", "categories": ["cs.CL"], "comment": "67 pages, 5 figures, 6 tables", "summary": "This paper investigates the impact of using morphologically-informed tokenizers to aid and streamline the interlinear gloss annotation of an audio corpus of Yoloxóchitl Mixtec (YM) using a combination of ASR and text-based sequence-to-sequence tools, with the goal of improving efficiency while reducing the workload of a human annotator. We present two novel tokenization schemes that separate words in a nonlinear manner, preserving information about tonal morphology as much as possible. One of these approaches, a Segment and Melody tokenizer, simply extracts the tones without predicting segmentation. The other, a Sequence of Processes tokenizer, predicts segmentation for the words, which could allow an end-to-end ASR system to produce segmented and unsegmented transcriptions in a single pass. We find that these novel tokenizers are competitive with BPE and Unigram models, and the Segment-and-Melody model outperforms traditional tokenizers in terms of word error rate but does not reach the same character error rate. In addition, we analyze tokenizers on morphological and information-theoretic metrics to find predictive correlations with downstream performance. Our results suggest that nonlinear tokenizers designed specifically for the non-concatenative morphology of a language are competitive with conventional BPE and Unigram models for ASR. Further research will be necessary to determine the applicability of these tokenizers in downstream processing tasks.", "AI": {"tldr": "本文研究了使用形态学感知的分词器来辅助Yoloxóchitl Mixtec音频语料库的跨行注释标注，通过结合ASR和文本序列到序列工具提高效率并减少人工标注工作量。提出了两种新颖的非线性分词方案，在保持音调形态信息的同时与传统BPE和Unigram模型竞争性能。", "motivation": "提高Yoloxóchitl Mixtec音频语料库的跨行注释标注效率，减少人工标注的工作负担，同时更好地处理该语言的非连接性形态特征。", "method": "开发了两种新颖的分词方案：Segment and Melody分词器（仅提取音调不预测分割）和Sequence of Processes分词器（预测单词分割）。与传统BPE和Unigram模型进行对比，并在形态学和信息论指标上进行分析。", "result": "新型分词器与传统BPE和Unigram模型竞争性相当，Segment-and-Melody模型在词错误率方面优于传统分词器，但在字符错误率方面未达到相同水平。发现了形态学和信息论指标与下游性能的预测相关性。", "conclusion": "专门为非连接性形态语言设计的非线性分词器在ASR任务中与传统模型具有竞争力，需要进一步研究这些分词器在下游处理任务中的适用性。"}}
{"id": "2512.06296", "pdf": "https://arxiv.org/pdf/2512.06296", "abs": "https://arxiv.org/abs/2512.06296", "authors": ["Sooho Moon", "Yunyong Ko"], "title": "How Sharp and Bias-Robust is a Model? Dual Evaluation Perspectives on Knowledge Graph Completion", "categories": ["cs.AI"], "comment": "5 pages, 4 figures, 2 tables, ACM WSDM 2026", "summary": "Knowledge graph completion (KGC) aims to predict missing facts from the observed KG. While a number of KGC models have been studied, the evaluation of KGC still remain underexplored. In this paper, we observe that existing metrics overlook two key perspectives for KGC evaluation: (A1) predictive sharpness -- the degree of strictness in evaluating an individual prediction, and (A2) popularity-bias robustness -- the ability to predict low-popularity entities. Toward reflecting both perspectives, we propose a novel evaluation framework (PROBE), which consists of a rank transformer (RT) estimating the score of each prediction based on a required level of predictive sharpness and a rank aggregator (RA) aggregating all the scores in a popularity-aware manner. Experiments on real-world KGs reveal that existing metrics tend to over- or under-estimate the accuracy of KGC models, whereas PROBE yields a comprehensive understanding of KGC models and reliable evaluation results.", "AI": {"tldr": "PROBE：一个新的知识图谱补全评估框架，通过考虑预测严格度和流行度偏差鲁棒性，提供更全面的模型评估", "motivation": "现有KGC评估指标忽略了两个关键视角：(1)预测严格度——评估个体预测的严格程度；(2)流行度偏差鲁棒性——预测低流行度实体的能力", "method": "提出PROBE框架，包含基于预测严格度要求估计预测得分的排名转换器(RT)和以流行度感知方式聚合所有得分的排名聚合器(RA)", "result": "实验显示现有指标倾向于高估或低估KGC模型的准确性，而PROBE能提供对KGC模型的全面理解和可靠评估结果", "conclusion": "PROBE框架能够更准确地评估知识图谱补全模型，解决了现有评估方法的局限性，为KGC研究提供了更可靠的评估标准"}}
{"id": "2512.06193", "pdf": "https://arxiv.org/pdf/2512.06193", "abs": "https://arxiv.org/abs/2512.06193", "authors": ["Jihyung Park", "Saleh Afroogh", "Junfeng Jiao"], "title": "Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLM) are increasingly integrated into everyday interactions, serving not only as information assistants but also as emotional companions. Even in the absence of explicit toxicity, repeated emotional reinforcement or affective drift can gradually escalate distress in a form of \\textit{implicit harm} that traditional toxicity filters fail to detect. Existing guardrail mechanisms often rely on external classifiers or clinical rubrics that may lag behind the nuanced, real-time dynamics of a developing conversation. To address this gap, we propose GAUGE (Guarding Affective Utterance Generation Escalation), a lightweight, logit-based framework for the real-time detection of hidden conversational escalation. GAUGE measures how an LLM's output probabilistically shifts the affective state of a dialogue.", "AI": {"tldr": "GAUGE是一个轻量级的实时检测框架，用于发现LLM对话中隐藏的情感升级问题，通过概率性测量对话情感状态的变化来识别传统毒性过滤器无法检测的隐性伤害。", "motivation": "LLM作为情感伴侣时，即使没有明显毒性，重复的情感强化或情感漂移会逐渐加剧用户痛苦，形成隐性伤害，而现有防护机制无法实时检测这种微妙的情感动态变化。", "method": "提出GAUGE框架，基于logit的方法实时测量LLM输出如何概率性地改变对话的情感状态，检测隐藏的对话升级。", "result": "开发了一个轻量级实时检测系统，能够识别传统方法无法捕捉的情感升级模式。", "conclusion": "GAUGE为解决LLM对话中隐性情感伤害的实时检测问题提供了有效的技术方案，弥补了现有防护机制的不足。"}}
{"id": "2512.06337", "pdf": "https://arxiv.org/pdf/2512.06337", "abs": "https://arxiv.org/abs/2512.06337", "authors": ["Xuan Xie", "Xuan Wang", "Wenjie Wang"], "title": "DaGRPO: Rectifying Gradient Conflict in Reasoning via Distinctiveness-Aware Group Relative Policy Optimization", "categories": ["cs.AI"], "comment": null, "summary": "The evolution of Large Language Models (LLMs) has catalyzed a paradigm shift from superficial instruction following to rigorous long-horizon reasoning. While Group Relative Policy Optimization (GRPO) has emerged as a pivotal mechanism for eliciting such post-training reasoning capabilities due to its exceptional performance, it remains plagued by significant training instability and poor sample efficiency. We theoretically identify the root cause of these issues as the lack of distinctiveness within on-policy rollouts: for routine queries, highly homogeneous samples induce destructive gradient conflicts; whereas for hard queries, the scarcity of valid positive samples results in ineffective optimization. To bridge this gap, we propose Distinctiveness-aware Group Relative Policy Optimization (DaGRPO). DaGRPO incorporates two core mechanisms: (1) Sequence-level Gradient Rectification, which utilizes fine-grained scoring to dynamically mask sample pairs with low distinctiveness, thereby eradicating gradient conflicts at the source; and (2) Off-policy Data Augmentation, which introduces high-quality anchors to recover training signals for challenging tasks. Extensive experiments across 9 mathematical reasoning and out-of-distribution (OOD) generalization benchmarks demonstrate that DaGRPO significantly surpasses existing SFT, GRPO, and hybrid baselines, achieving new state-of-the-art performance (e.g., a +4.7% average accuracy gain on math benchmarks). Furthermore, in-depth analysis confirms that DaGRPO effectively mitigates gradient explosion and accelerates the emergence of long-chain reasoning capabilities.", "AI": {"tldr": "DaGRPO方法通过序列级梯度校正和离策略数据增强，解决了GRPO训练中的不稳定性和样本效率低的问题，在数学推理和OOD泛化基准上实现了新的SOTA性能。", "motivation": "GRPO虽然能有效激发大语言模型的长程推理能力，但存在训练不稳定和样本效率低的问题，根源在于策略内样本缺乏区分度。", "method": "提出DaGRPO方法，包含：(1)序列级梯度校正-通过细粒度评分动态屏蔽低区分度样本对；(2)离策略数据增强-引入高质量锚点恢复困难任务的训练信号。", "result": "在9个数学推理和OOD泛化基准测试中显著超越现有方法，平均准确率提升4.7%，有效缓解梯度爆炸并加速长链推理能力的出现。", "conclusion": "DaGRPO通过解决样本区分度问题，为LLM的后训练推理能力优化提供了有效解决方案，在多个基准测试中达到最先进性能。"}}
{"id": "2512.06227", "pdf": "https://arxiv.org/pdf/2512.06227", "abs": "https://arxiv.org/abs/2512.06227", "authors": ["Junyu Mao", "Anthony Hills", "Talia Tseriotou", "Maria Liakata", "Aya Shamir", "Dan Sayda", "Dana Atzil-Slonim", "Natalie Djohari", "Arpan Mandal", "Silke Roth", "Pamela Ugwudike", "Mahesan Niranjan", "Stuart E. Middleton"], "title": "Automated Data Enrichment using Confidence-Aware Fine-Grained Debate among Open-Source LLMs for Mental Health and Online Safety", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Real-world indicators are important for improving natural language processing (NLP) tasks such as life events for mental health analysis and risky behaviour for online safety, yet labelling such information in NLP training datasets is often costly and/or difficult given the dynamic nature of such events. This paper compares several LLM-based data enrichment methods and introduces a novel Confidence-Aware Fine-Grained Debate (CFD) framework in which multiple LLM agents simulate human annotators and exchange fine-grained evidence to reach consensus. We describe two new expert-annotated datasets, a mental health Reddit wellbeing dataset and an online safety Facebook sharenting risk dataset. Our CFD framework achieves the most robust data enrichment performance compared to a range of baselines and we show that this type of data enrichment consistently improves downstream tasks. Enriched features incorporated via debate transcripts yield the largest gains, outperforming the non-enriched baseline by 10.1% for the online safety task.", "AI": {"tldr": "本文提出了一种新颖的置信感知细粒度辩论(CFD)框架，通过多个LLM代理模拟人类标注者交换细粒度证据来达成共识，用于成本高昂的现实世界指标数据标注，在两个新标注数据集上表现最优。", "motivation": "现实世界指标（如心理健康生活事件和在线安全风险行为）对NLP任务很重要，但在训练数据中标注这类信息成本高且困难，因为事件具有动态性。", "method": "比较多种基于LLM的数据增强方法，提出CFD框架：多个LLM代理模拟人类标注者，通过交换细粒度证据达成共识。创建了两个专家标注数据集：心理健康Reddit幸福数据集和在线安全Facebook共享风险数据集。", "result": "CFD框架在所有基线方法中表现最稳健，数据增强持续改善下游任务。通过辩论记录整合的增强特征带来最大提升，在线安全任务比非增强基线提高10.1%。", "conclusion": "CFD框架为成本高昂的现实世界指标标注提供了有效的解决方案，通过多代理辩论机制显著提升了数据质量和下游任务性能。"}}
{"id": "2512.06393", "pdf": "https://arxiv.org/pdf/2512.06393", "abs": "https://arxiv.org/abs/2512.06393", "authors": ["Qiming Bao", "Xiaoxuan Fu"], "title": "Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "comment": null, "summary": "Large language models (LLMs) excel across many natural language tasks, yet their generalisation to structural perturbations in logical contexts remains poorly understood. We introduce a controlled evaluation framework that probes reasoning reliability through four targeted stress tests: (1) rule deletion, removing either redundant or essential rules from a multi-step inference chain; (2) contradictory evidence injection; (3) logic-preserving rewrites generated through several families of equivalence laws (contrapositive, double negation, implication, De Morgan, identity, and commutativity); and (4) multi-law equivalence stacking that introduces 2-5 simultaneous logical transformations.\n  Across three representative model families: BERT, Qwen2, and LLaMA-like models. Our experiments reveal a strikingly consistent pattern: all models achieve perfect accuracy on the base tasks and remain fully generalise to redundant rule deletion and all equivalence-based rewrites (single or multi-law), but fail sharply under essential rule deletion (dropping to 25% accuracy) and collapse completely in the presence of explicit contradictions (0% accuracy). These results demonstrate that LLMs possess stable invariance to semantic-preserving logical transformations, yet remain fundamentally brittle to missing or conflicting evidence. Our framework provides a clean diagnostic tool for isolating such reasoning failure modes and highlights persistent gaps in the logical generalisation abilities of current LLMs.", "AI": {"tldr": "论文通过四个压力测试评估大语言模型在逻辑推理中的泛化能力，发现模型在语义保持的逻辑变换中表现稳定，但在规则缺失和矛盾证据下完全失效。", "motivation": "尽管大语言模型在自然语言任务中表现出色，但其在逻辑上下文中的结构扰动泛化能力尚未被充分理解，需要系统评估其推理可靠性。", "method": "引入控制评估框架，包含四个压力测试：规则删除（冗余和必要规则）、矛盾证据注入、逻辑保持重写（六种等价法则）、多法则等价叠加（2-5个同时变换），在BERT、Qwen2和LLaMA-like模型上进行测试。", "result": "所有模型在基础任务上准确率完美，对冗余规则删除和所有等价重写保持完全泛化，但在必要规则删除下准确率降至25%，在明确矛盾下完全崩溃（0%准确率）。", "conclusion": "大语言模型对语义保持的逻辑变换具有稳定不变性，但对缺失或冲突证据存在根本性脆弱，框架为诊断推理失败模式提供了清晰工具，揭示了当前LLMs逻辑泛化能力的持续差距。"}}
{"id": "2512.06228", "pdf": "https://arxiv.org/pdf/2512.06228", "abs": "https://arxiv.org/abs/2512.06228", "authors": ["Xuanxin Wu", "Yuki Arase", "Masaaki Nagata"], "title": "Policy-based Sentence Simplification: Replacing Parallel Corpora with LLM-as-a-Judge", "categories": ["cs.CL"], "comment": null, "summary": "Sentence simplification aims to modify a sentence to make it easier to read and understand while preserving the meaning. Different applications require distinct simplification policies, such as replacing only complex words at the lexical level or rewriting the entire sentence while trading off details for simplicity. However, achieving such policy-driven control remains an open challenge. In this work, we introduce a simple yet powerful approach that leverages Large Language Model-as-a-Judge (LLM-as-a-Judge) to automatically construct policy-aligned training data, completely removing the need for costly human annotation or parallel corpora. Our method enables building simplification systems that adapt to diverse simplification policies. Remarkably, even small-scale open-source LLMs such as Phi-3-mini-3.8B surpass GPT-4o on lexical-oriented simplification, while achieving comparable performance on overall rewriting, as verified by both automatic metrics and human evaluations. The consistent improvements across model families and sizes demonstrate the robustness of our approach.", "AI": {"tldr": "利用LLM-as-a-Judge方法自动构建策略对齐的训练数据，无需人工标注即可构建适应不同简化策略的文本简化系统，小模型在词汇简化上超越GPT-4o", "motivation": "解决文本简化中不同应用需要不同简化策略的挑战，传统方法需要昂贵的人工标注或平行语料", "method": "使用大语言模型作为评判者自动构建策略对齐的训练数据，完全不需要人工标注或平行语料", "result": "小规模开源模型Phi-3-mini-3.8B在词汇导向简化上超越GPT-4o，在整体重写任务上达到相当性能，自动指标和人工评估均验证了效果", "conclusion": "该方法在不同模型家族和规模上都表现出稳健的改进，为策略驱动的文本简化提供了有效解决方案"}}
{"id": "2512.06404", "pdf": "https://arxiv.org/pdf/2512.06404", "abs": "https://arxiv.org/abs/2512.06404", "authors": ["Mohammad Soleymanibrojeni", "Roland Aydin", "Diego Guedes-Sobrinho", "Alexandre C. Dias", "Maurício J. Piotrowski", "Wolfgang Wenzel", "Celso Ricardo Caldeira Rêgo"], "title": "GENIUS: An Agentic AI Framework for Autonomous Design and Execution of Simulation Protocols", "categories": ["cs.AI", "cond-mat.mtrl-sci", "physics.chem-ph"], "comment": null, "summary": "Predictive atomistic simulations have propelled materials discovery, yet routine setup and debugging still demand computer specialists. This know-how gap limits Integrated Computational Materials Engineering (ICME), where state-of-the-art codes exist but remain cumbersome for non-experts. We address this bottleneck with GENIUS, an AI-agentic workflow that fuses a smart Quantum ESPRESSO knowledge graph with a tiered hierarchy of large language models supervised by a finite-state error-recovery machine. Here we show that GENIUS translates free-form human-generated prompts into validated input files that run to completion on $\\approx$80% of 295 diverse benchmarks, where 76% are autonomously repaired, with success decaying exponentially to a 7% baseline. Compared with LLM-only baselines, GENIUS halves inference costs and virtually eliminates hallucinations. The framework democratizes electronic-structure DFT simulations by intelligently automating protocol generation, validation, and repair, opening large-scale screening and accelerating ICME design loops across academia and industry worldwide.", "AI": {"tldr": "GENIUS是一个AI代理工作流，通过结合量子ESPRESSO知识图谱和分层大语言模型，将自由形式的人类提示转换为验证过的输入文件，实现约80%的成功率，显著降低了计算成本和幻觉问题。", "motivation": "解决材料计算工程中非专家用户难以使用先进计算代码的问题，通过AI自动化来缩小专业知识差距，促进集成计算材料工程的普及。", "method": "开发GENIUS系统，融合智能量子ESPRESSO知识图谱、分层大语言模型层次结构，以及有限状态错误恢复机器的监督机制。", "result": "在295个多样化基准测试中，约80%的输入文件能够成功运行，其中76%由系统自主修复，相比纯LLM基线减少了一半推理成本并几乎消除了幻觉问题。", "conclusion": "GENIUS框架通过智能自动化协议生成、验证和修复，民主化了电子结构DFT模拟，为全球学术界和工业界的大规模筛选和ICME设计循环提供了加速。"}}
{"id": "2512.06239", "pdf": "https://arxiv.org/pdf/2512.06239", "abs": "https://arxiv.org/abs/2512.06239", "authors": ["Dhanasekar Sundararaman", "Keying Li", "Wayne Xiong", "Aashna Garg"], "title": "LOCUS: A System and Method for Low-Cost Customization for Universal Specialization", "categories": ["cs.CL"], "comment": null, "summary": "We present LOCUS (LOw-cost Customization for Universal Specialization), a pipeline that consumes few-shot data to streamline the construction and training of NLP models through targeted retrieval, synthetic data generation, and parameter-efficient tuning. With only a small number of labeled examples, LOCUS discovers pertinent data in a broad repository, synthesizes additional training samples via in-context data generation, and fine-tunes models using either full or low-rank (LoRA) parameter adaptation. Our approach targets named entity recognition (NER) and text classification (TC) benchmarks, consistently outperforming strong baselines (including GPT-4o) while substantially lowering costs and model sizes. Our resultant memory-optimized models retain 99% of fully fine-tuned accuracy while using barely 5% of the memory footprint, also beating GPT-4o on several benchmarks with less than 1% of its parameters.", "AI": {"tldr": "LOCUS是一个低成本的NLP模型定制流程，通过少量标注数据实现检索增强、合成数据生成和参数高效微调，在NER和文本分类任务中超越GPT-4o等基线方法，同时大幅降低计算成本和模型大小。", "motivation": "解决传统NLP模型定制需要大量标注数据和计算资源的问题，提供一种低成本、高效率的模型专业化方案。", "method": "使用少量标注示例进行相关数据检索，通过上下文数据生成合成训练样本，采用全参数或LoRA参数高效微调方法。", "result": "在多个基准测试中持续超越强基线（包括GPT-4o），内存优化模型保持99%全微调精度，仅使用5%内存占用，参数量不到GPT-4o的1%。", "conclusion": "LOCUS提供了一种高效、低成本的NLP模型定制方案，在保持高性能的同时显著降低了资源需求，为实际应用提供了可行的解决方案。"}}
{"id": "2512.06406", "pdf": "https://arxiv.org/pdf/2512.06406", "abs": "https://arxiv.org/abs/2512.06406", "authors": ["Xianzong Wu", "Xiaohong Li", "Lili Quan", "Qiang Hu"], "title": "UncertaintyZoo: A Unified Toolkit for Quantifying Predictive Uncertainty in Deep Learning Systems", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large language models(LLMs) are increasingly expanding their real-world applications across domains, e.g., question answering, autonomous driving, and automatic software development. Despite this achievement, LLMs, as data-driven systems, often make incorrect predictions, which can lead to potential losses in safety-critical scenarios. To address this issue and measure the confidence of model outputs, multiple uncertainty quantification(UQ) criteria have been proposed. However, even though important, there are limited tools to integrate these methods, hindering the practical usage of UQ methods and future research in this domain. To bridge this gap, in this paper, we introduce UncertaintyZoo, a unified toolkit that integrates 29 uncertainty quantification methods, covering five major categories under a standardized interface. Using UncertaintyZoo, we evaluate the usefulness of existing uncertainty quantification methods under the code vulnerability detection task on CodeBERT and ChatGLM3 models. The results demonstrate that UncertaintyZoo effectively reveals prediction uncertainty. The tool with a demonstration video is available on the project site https://github.com/Paddingbuta/UncertaintyZoo.", "AI": {"tldr": "UncertaintyZoo是一个统一工具包，集成了29种不确定性量化方法，用于评估大语言模型的预测置信度，特别在代码漏洞检测任务中验证了其有效性。", "motivation": "大语言模型在安全关键场景中可能做出错误预测导致潜在损失，需要不确定性量化方法来衡量模型输出的置信度，但现有工具缺乏整合阻碍了实际应用和研究发展。", "method": "开发UncertaintyZoo工具包，标准化接口集成5大类29种不确定性量化方法，并在CodeBERT和ChatGLM3模型上进行代码漏洞检测任务的评估。", "result": "实验结果表明UncertaintyZoo能有效揭示模型预测的不确定性，为不确定性量化方法的实际应用提供了统一平台。", "conclusion": "UncertaintyZoo填补了不确定性量化工具整合的空白，促进了该领域的研究发展和实际应用，特别是在安全关键任务中的模型可靠性评估。"}}
{"id": "2512.06256", "pdf": "https://arxiv.org/pdf/2512.06256", "abs": "https://arxiv.org/abs/2512.06256", "authors": ["Aniruddha Maiti", "Satya Nimmagadda", "Kartha Veerya Jammuladinne", "Niladri Sengupta", "Ananya Jana"], "title": "Convergence of Outputs When Two Large Language Models Interact in a Multi-Agentic Setup", "categories": ["cs.CL", "cs.AI"], "comment": "accepted to LLM 2025", "summary": "In this work, we report what happens when two large language models respond to each other for many turns without any outside input in a multi-agent setup. The setup begins with a short seed sentence. After that, each model reads the other's output and generates a response. This continues for a fixed number of steps. We used Mistral Nemo Base 2407 and Llama 2 13B hf. We observed that most conversations start coherently but later fall into repetition. In many runs, a short phrase appears and repeats across turns. Once repetition begins, both models tend to produce similar output rather than introducing a new direction in the conversation. This leads to a loop where the same or similar text is produced repeatedly. We describe this behavior as a form of convergence. It occurs even though the models are large, trained separately, and not given any prompt instructions. To study this behavior, we apply lexical and embedding-based metrics to measure how far the conversation drifts from the initial seed and how similar the outputs of the two models becomes as the conversation progresses.", "AI": {"tldr": "研究显示两个大型语言模型在无外部输入的对话中会逐渐陷入重复循环，即使模型很大且独立训练，最终会产生相似输出而不再引入新内容。", "motivation": "探索在没有外部输入的情况下，两个大型语言模型相互对话多轮后会发生什么行为变化，特别是观察对话的连贯性和重复模式。", "method": "使用Mistral Nemo Base 2407和Llama 2 13B hf模型，从一个种子句子开始让两个模型相互响应固定轮数，应用词汇和嵌入指标测量对话偏离初始种子的程度和模型输出相似性。", "result": "大多数对话开始时连贯但后来陷入重复，出现短短语跨轮次重复，一旦开始重复，两个模型会产生相似输出而不引入新方向，形成循环重复的收敛行为。", "conclusion": "即使是大规模独立训练的模型，在无外部输入的对话设置中也会自发地收敛到重复模式，这表明当前语言模型在长期自主对话中存在局限性。"}}
{"id": "2512.06431", "pdf": "https://arxiv.org/pdf/2512.06431", "abs": "https://arxiv.org/abs/2512.06431", "authors": ["Mohamed Shamroukh", "Mohamed Alkhuzamy Aziz"], "title": "Smart Spatial Planning in Egypt: An Algorithm-Driven Approach to Public Service Evaluation in Qena City", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "National planning standards for public services in Egypt often fail to align with unique local characteristics. Addressing this gap, this study develops a tailored planning model for Qena City. Using a hybrid methodology (descriptive, analytical, and experimental), the research utilizes Python programming to generate an intelligent spatial analysis algorithm based on Voronoi Diagrams. This approach creates city-specific planning criteria and evaluates the current coverage of public facilities. The primary contribution of this study is the successful derivation of a localized planning standards model and the deployment of an automated algorithm to assess service efficiency. Application of this model reveals a general service coverage average of 81.3%. Ambulance stations demonstrated the highest efficiency (99.8%) due to recent upgrades, while parks and open spaces recorded the lowest coverage (10%) caused by limited land availability. Spatial analysis indicates a high service density in midtown (>45 services/km^2), which diminishes significantly towards the outskirts (<5 services/km^2). Consequently, the Hajer Qena district contains the highest volume of unserved areas, while the First District (Qesm 1) exhibits the highest level of service coverage. This model offers a replicable framework for data-driven urban planning in Egyptian cities.", "AI": {"tldr": "本研究为埃及基纳市开发了一个基于Voronoi图的智能空间分析算法，创建了本地化的规划标准模型，评估公共服务覆盖效率，平均覆盖率达81.3%，并揭示了城市内部服务分布不均的问题。", "motivation": "埃及国家公共服务规划标准往往无法适应地方特色，需要开发针对特定城市的定制化规划模型来解决这一差距。", "method": "采用混合方法（描述性、分析性和实验性），使用Python编程开发基于Voronoi图的智能空间分析算法，生成城市特定的规划标准并评估现有公共设施覆盖情况。", "result": "模型应用显示平均服务覆盖率为81.3%，救护车站效率最高（99.8%），公园和开放空间覆盖率最低（10%）。市中心服务密度高（>45个/km²），郊区显著减少（<5个/km²）。Hajer Qena区未服务区域最多，第一区服务覆盖水平最高。", "conclusion": "该研究成功开发了本地化规划标准模型和自动化评估算法，为埃及城市的数据驱动城市规划提供了可复制的框架，能够有效解决国家规划标准与地方特性不匹配的问题。"}}
{"id": "2512.06266", "pdf": "https://arxiv.org/pdf/2512.06266", "abs": "https://arxiv.org/abs/2512.06266", "authors": ["Chen Yang", "Guangyue Peng", "Jiaying Zhu", "Ran Le", "Ruixiang Feng", "Tao Zhang", "Wei Ruan", "Xiaoqi Liu", "Xiaoxue Cheng", "Xiyun Xu", "Yang Song", "Yanzipeng Gao", "Yiming Jia", "Yun Xing", "Yuntao Wen", "Zekai Wang", "Zhenwei An", "Zhicong Sun", "Zongchao Chen"], "title": "Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models", "categories": ["cs.CL"], "comment": null, "summary": "We present Nanbeige4-3B, a family of small-scale but high-performing language models. Pretrained on 23T high-quality tokens and finetuned on over 30 million diverse instructions, we extend the boundary of the scaling law for small language models. In pre-training, we design a Fine-Grained Warmup-Stable-Decay (FG-WSD) training scheduler, which progressively refines data mixtures across stages to boost model performance. In post-training, to improve the quality of the SFT data, we design a joint mechanism that integrates deliberative generation refinement and chain-of-thought reconstruction, yielding substantial gains on complex tasks. Following SFT, we employ our flagship reasoning model to distill Nanbeige4-3B through our proposed Dual Preference Distillation (DPD) method, which leads to further performance gains. Finally, a multi-stage reinforcement learning phase was applied, leveraging verifiable rewards and preference modeling to strengthen abilities on both reasoning and human alignment. Extensive evaluations show that Nanbeige4-3B not only significantly outperforms models of comparable parameter scale but also rivals much larger models across a wide range of benchmarks. The model checkpoints are available at https://huggingface.co/Nanbeige.", "AI": {"tldr": "Nanbeige4-3B是一个高性能的小规模语言模型，通过创新的预训练调度器、指令微调机制、双重偏好蒸馏和多阶段强化学习，在3B参数规模下实现了超越同规模模型并与更大模型竞争的性能。", "motivation": "突破小规模语言模型的扩展定律边界，在有限参数下实现高性能，通过系统性的训练流程优化来提升模型能力。", "method": "1. 预训练阶段使用FG-WSD调度器优化数据混合；2. 后训练阶段采用审议生成精炼和思维链重构机制提升SFT数据质量；3. 通过DPD方法进行双重偏好蒸馏；4. 多阶段强化学习使用可验证奖励和偏好建模。", "result": "在广泛基准测试中显著优于同参数规模模型，并能与更大模型竞争，模型检查点已公开。", "conclusion": "通过系统化的训练流程创新，成功突破了小规模模型的性能限制，为高效能小模型的发展提供了有效路径。"}}
{"id": "2512.06573", "pdf": "https://arxiv.org/pdf/2512.06573", "abs": "https://arxiv.org/abs/2512.06573", "authors": ["Onur Bilgin", "Abdullah As Sami", "Sriram Sai Vujjini", "John Licato"], "title": "The Effect of Belief Boxes and Open-mindedness on Persuasion", "categories": ["cs.AI", "cs.MA"], "comment": "Accepted at the 18th International Conference on Agents and Artificial Intelligence (ICAART 2026), Marbella, Spain", "summary": "As multi-agent systems are increasingly utilized for reasoning and decision-making applications, there is a greater need for LLM-based agents to have something resembling propositional beliefs. One simple method for doing so is to include statements describing beliefs maintained in the prompt space (in what we'll call their belief boxes). But when agents have such statements in belief boxes, how does it actually affect their behaviors and dispositions towards those beliefs? And does it significantly affect agents' ability to be persuasive in multi-agent scenarios? Likewise, if the agents are given instructions to be open-minded, how does that affect their behaviors? We explore these and related questions in a series of experiments. Our findings confirm that instructing agents to be open-minded affects how amenable they are to belief change. We show that incorporating belief statements and their strengths influences an agent's resistance to (and persuasiveness against) opposing viewpoints. Furthermore, it affects the likelihood of belief change, particularly when the agent is outnumbered in a debate by opposing viewpoints, i.e., peer pressure scenarios. The results demonstrate the feasibility and validity of the belief box technique in reasoning and decision-making tasks.", "AI": {"tldr": "本研究探讨了在LLM智能体中引入信念框（belief boxes）技术对多智能体系统中信念变化和说服力的影响，发现开放心态指令和信念强度设置能显著影响智能体的信念抵抗力和说服效果。", "motivation": "随着多智能体系统在推理和决策应用中日益普及，需要让基于LLM的智能体具备类似命题信念的能力。研究旨在探索信念陈述如何影响智能体行为及其在多智能体场景中的说服能力。", "method": "通过一系列实验，研究在提示空间中包含信念陈述（信念框），测试智能体在不同情境下（包括对立观点辩论和同伴压力场景）的信念变化和说服行为。", "result": "研究证实：开放心态指令影响信念变化的易感性；信念陈述及其强度设置影响智能体对对立观点的抵抗力和说服力；在辩论中被对立观点包围时（同伴压力场景），信念变化的可能性显著增加。", "conclusion": "信念框技术在推理和决策任务中具有可行性和有效性，为多智能体系统中的信念建模提供了实用方法。"}}
{"id": "2512.06464", "pdf": "https://arxiv.org/pdf/2512.06464", "abs": "https://arxiv.org/abs/2512.06464", "authors": ["Akriti Jain", "Aparna Garimella"], "title": "Modeling Contextual Passage Utility for Multihop Question Answering", "categories": ["cs.CL"], "comment": "Accepted at IJCNLP-AACL 2025", "summary": "Multihop Question Answering (QA) requires systems to identify and synthesize information from multiple text passages. While most prior retrieval methods assist in identifying relevant passages for QA, further assessing the utility of the passages can help in removing redundant ones, which may otherwise add to noise and inaccuracies in the generated answers. Existing utility prediction approaches model passage utility independently, overlooking a critical aspect of multihop reasoning: the utility of a passage can be context-dependent, influenced by its relation to other passages - whether it provides complementary information or forms a crucial link in conjunction with others. In this paper, we propose a lightweight approach to model contextual passage utility, accounting for inter-passage dependencies. We fine-tune a small transformer-based model to predict passage utility scores for multihop QA. We leverage the reasoning traces from an advanced reasoning model to capture the order in which passages are used to answer a question and obtain synthetic training data. Through comprehensive experiments, we demonstrate that our utility-based scoring of retrieved passages leads to improved reranking and downstream QA performance compared to relevance-based reranking methods.", "AI": {"tldr": "提出了一种轻量化的上下文篇章效用建模方法，通过考虑篇章间的依赖关系来预测多跳问答中的篇章效用分数，相比基于相关性的重排序方法能提升问答性能", "motivation": "现有的效用预测方法独立建模篇章效用，忽略了多跳推理的关键方面：篇章效用是上下文相关的，受其与其他篇章关系的影响（是否提供补充信息或形成关键链接）", "method": "微调一个小型transformer模型来预测多跳问答的篇章效用分数，利用高级推理模型的推理轨迹捕获篇章使用顺序并获取合成训练数据", "result": "通过综合实验证明，基于效用的检索篇章评分相比基于相关性的重排序方法能带来改进的重排序和下流问答性能", "conclusion": "上下文篇章效用建模是提升多跳问答系统性能的有效方法，通过考虑篇章间依赖关系可以更好地识别和去除冗余篇章，减少噪声和提高答案生成准确性"}}
{"id": "2512.06629", "pdf": "https://arxiv.org/pdf/2512.06629", "abs": "https://arxiv.org/abs/2512.06629", "authors": ["Xiao-li Xia", "Hou-biao Li"], "title": "FlatFormer: A Flat Transformer Knowledge Tracing Model Based on Cognitive Bias Injection", "categories": ["cs.AI", "cs.IT"], "comment": "36 pages, 14 figures,Table 5", "summary": "Knowledge Tracing (KT) models face a critical ``Performance-Complexity Trap'': capturing complex cognitive dynamics like learning sessions and memory decay typically requires deep hierarchical architectures, which incur prohibitive computational costs for real-time deployment. To resolve this, we propose FlatFormer, a streamlined architecture based on the novel design paradigm of ``Information Injection over Structural Stacking.'' Unlike parameter-heavy hierarchical models, FlatFormer leverages a standard flat Transformer augmented with two lightweight injection mechanisms: (i) a hybrid input encoding strategy combining learnable session identifiers with fixed sinusoidal step embeddings; and (ii) a pre-computed power-law bias integrated directly into attention logits to explicitly model the forgetting curve. Extensive experiments on four large-scale datasets (e.g., EdNet, Junyi) show that FlatFormer achieves state-of-the-art performance. For example, on the EdNet dataset, compared to the strongest hierarchical baseline (HiTSKT), its absolute AUC increased by 8.3%, while using less than 15% of parameters, and inference speed was about three times faster. These results validate that high cognitive fidelity does not necessitate architectural complexity.", "AI": {"tldr": "FlatFormer是一种新型的知识追踪模型，通过信息注入而非结构堆叠的方式，在保持高性能的同时大幅降低计算复杂度，实现了参数减少85%、推理速度提升3倍，且AUC提升8.3%的突破。", "motivation": "解决知识追踪模型面临的\"性能-复杂度困境\"：传统深度层次架构虽然能捕捉复杂认知动态（如学习会话和记忆衰减），但计算成本过高，无法满足实时部署需求。", "method": "提出基于\"信息注入而非结构堆叠\"设计范式的FlatFormer架构：使用标准平面Transformer，通过两种轻量级注入机制增强：(1)混合输入编码策略（可学习会话标识符+固定正弦步长嵌入）；(2)直接集成到注意力对数中的预计算幂律偏置，显式建模遗忘曲线。", "result": "在四个大规模数据集（如EdNet、Junyi）上的实验表明，FlatFormer达到最先进性能。在EdNet数据集上，相比最强层次基线（HiTSKT），绝对AUC提升8.3%，参数使用量减少85%以上，推理速度提升约3倍。", "conclusion": "研究验证了高认知保真度并不需要架构复杂性，FlatFormer通过巧妙的信息注入机制成功解决了性能与复杂度的权衡问题，为实时知识追踪应用提供了高效解决方案。"}}
{"id": "2512.06476", "pdf": "https://arxiv.org/pdf/2512.06476", "abs": "https://arxiv.org/abs/2512.06476", "authors": ["Akriti Jain", "Aparna Garimella"], "title": "Knowing What's Missing: Assessing Information Sufficiency in Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Determining whether a provided context contains sufficient information to answer a question is a critical challenge for building reliable question-answering systems. While simple prompting strategies have shown success on factual questions, they frequently fail on inferential ones that require reasoning beyond direct text extraction. We hypothesize that asking a model to first reason about what specific information is missing provides a more reliable, implicit signal for assessing overall sufficiency. To this end, we propose a structured Identify-then-Verify framework for robust sufficiency modeling. Our method first generates multiple hypotheses about missing information and establishes a semantic consensus. It then performs a critical verification step, forcing the model to re-examine the source text to confirm whether this information is truly absent. We evaluate our method against established baselines across diverse multi-hop and factual QA datasets. The results demonstrate that by guiding the model to justify its claims about missing information, our framework produces more accurate sufficiency judgments while clearly articulating any information gaps.", "AI": {"tldr": "论文提出了一个Identify-then-Verify框架，通过让模型先识别缺失信息再验证的方式，来更准确地判断上下文是否足够回答问题。", "motivation": "现有简单提示策略在事实性问题上有一定效果，但在需要推理的问题上经常失败。作者认为通过让模型先推理缺失的具体信息，可以提供更可靠的充分性评估信号。", "method": "提出结构化框架：首先生成多个关于缺失信息的假设并建立语义共识，然后进行关键验证步骤，强制模型重新检查源文本来确认信息是否确实缺失。", "result": "在多个多跳和事实性QA数据集上的评估显示，该方法比现有基线产生更准确的充分性判断，并能清晰表达信息差距。", "conclusion": "通过引导模型为其关于缺失信息的声明提供理由，该框架能够产生更准确的充分性判断，同时清晰阐述任何信息差距。"}}
{"id": "2512.06653", "pdf": "https://arxiv.org/pdf/2512.06653", "abs": "https://arxiv.org/abs/2512.06653", "authors": ["Hengzhi Lan", "Yue Yu", "Li Qian", "Li Peng", "Jie Wu", "Wei Liu", "Jian Luan", "Ting Bai"], "title": "LightSearcher: Efficient DeepSearch via Experiential Memory", "categories": ["cs.AI"], "comment": "10 pages, 5 figures", "summary": "DeepSearch paradigms have become a core enabler for deep reasoning models, allowing them to invoke external search tools to access up-to-date, domain-specific knowledge beyond parametric boundaries, thereby enhancing the depth and factual reliability of reasoning. Building upon this foundation, recent advances in reinforcement learning (RL) have further empowered models to autonomously and strategically control search tool usage, optimizing when and how to query external knowledge sources. Yet, these RL-driven DeepSearch systems often reveal a see-saw trade-off between accuracy and efficiency-frequent tool invocations can improve factual correctness but lead to unnecessary computational overhead and diminished efficiency. To address this challenge, we propose LightSearcher, an efficient RL framework that incorporates textual experiential memory by learning contrastive reasoning trajectories to generate interpretable summaries of successful reasoning patterns. In addition, it employs an adaptive reward shaping mechanism that penalizes redundant tool calls only in correct-answer scenarios. This design effectively balances the inherent accuracy-efficiency trade-off in DeepSearch paradigms. Experiments on four multi-hop QA benchmarks show that LightSearcher maintains accuracy comparable to SOTA baseline ReSearch, while reducing search tool invocations by 39.6%, inference time by 48.6%, and token consumption by 21.2%, demonstrating its superior efficiency.", "AI": {"tldr": "LightSearcher是一个高效的强化学习框架，通过结合文本经验记忆和自适应奖励机制，在DeepSearch范式中实现了准确性和效率的良好平衡。", "motivation": "现有的RL驱动的DeepSearch系统存在准确性与效率之间的权衡问题：频繁调用搜索工具可以提高事实准确性，但会导致不必要的计算开销和效率下降。", "method": "提出LightSearcher框架，包含两个关键组件：1）通过学习对比推理轨迹生成可解释的成功推理模式摘要的文本经验记忆；2）仅在正确答案场景中惩罚冗余工具调用的自适应奖励塑形机制。", "result": "在四个多跳QA基准测试中，LightSearcher保持与SOTA基准ReSearch相当的准确性，同时将搜索工具调用减少39.6%，推理时间减少48.6%，token消耗减少21.2%。", "conclusion": "LightSearcher有效解决了DeepSearch范式中固有的准确性与效率权衡问题，通过创新的记忆机制和奖励设计实现了显著效率提升，同时保持了竞争性的准确性表现。"}}
{"id": "2512.06483", "pdf": "https://arxiv.org/pdf/2512.06483", "abs": "https://arxiv.org/abs/2512.06483", "authors": ["Elias-Leander Ahlers", "Witold Brunsmann", "Malte Schilling"], "title": "Classifying German Language Proficiency Levels Using Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at 3rd International Conference on Foundation and Large Language Models (FLLM2025), Vienna (Austria)", "summary": "Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach that utilizes the internal neural state of the LLM for classification. Our results show a consistent performance improvement over prior methods, highlighting the potential of LLMs for reliable and scalable CEFR classification.", "AI": {"tldr": "本文研究使用大型语言模型(LLM)自动将德语文本按CEFR语言能力等级分类，通过构建多样化数据集并评估多种方法，获得了比现有方法更好的性能表现。", "motivation": "语言能力评估对教育至关重要，可以实现针对学习者需求的个性化教学，需要可靠且可扩展的自动分类方法。", "method": "构建包含现有CEFR标注语料库和合成数据的多样化数据集；评估提示工程策略、微调LLaMA-3-8B-Instruct模型，以及基于LLM内部神经状态的探测分类方法。", "result": "研究结果显示相比现有方法有持续的性能提升，证明了LLM在CEFR分类中的可靠性。", "conclusion": "大型语言模型在可靠且可扩展的CEFR语言能力等级分类方面具有巨大潜力。"}}
{"id": "2512.06705", "pdf": "https://arxiv.org/pdf/2512.06705", "abs": "https://arxiv.org/abs/2512.06705", "authors": ["Yongyuan He", "Yi Bu"], "title": "Academic journals' AI policies fail to curb the surge in AI-assisted academic writing", "categories": ["cs.AI"], "comment": "40 pages, 10 figures, and 9 tables", "summary": "The rapid integration of generative AI into academic writing has prompted widespread policy responses from journals and publishers. However, the effectiveness of these policies remains unclear. Here, we analyze 5,114 journals and over 5.2 million papers to evaluate the real-world impact of AI usage guidelines. We show that despite 70% of journals adopting AI policies (primarily requiring disclosure), researchers' use of AI writing tools has increased dramatically across disciplines, with no significant difference between journals with or without policies. Non-English-speaking countries, physical sciences, and high-OA journals exhibit the highest growth rates. Crucially, full-text analysis on 164k scientific publications reveals a striking transparency gap: Of the 75k papers published since 2023, only 76 (0.1%) explicitly disclosed AI use. Our findings suggest that current policies have largely failed to promote transparency or restrain AI adoption. We urge a re-evaluation of ethical frameworks to foster responsible AI integration in science.", "AI": {"tldr": "对5114种期刊和520万篇论文的分析显示，尽管70%期刊采用AI使用政策，但研究人员使用AI写作工具的现象急剧增加，政策效果有限。只有0.1%的论文明确披露AI使用，表明当前政策在促进透明度方面基本失败。", "motivation": "评估生成式AI在学术写作中快速普及背景下，期刊和出版商AI使用政策的实际效果和影响", "method": "分析5114种期刊和超过520万篇论文，包括对16.4万篇科学出版物的全文分析，特别关注2023年以来发表的7.5万篇论文", "result": "AI写作工具使用率在各学科显著增长，政策存在与否无显著差异；非英语国家、物理科学和高开放获取期刊增长最快；透明度存在巨大差距，仅0.1%论文明确披露AI使用", "conclusion": "当前政策未能有效促进透明度或限制AI采用，需要重新评估伦理框架以促进负责任的AI科学整合"}}
{"id": "2512.06515", "pdf": "https://arxiv.org/pdf/2512.06515", "abs": "https://arxiv.org/abs/2512.06515", "authors": ["Somnath Banerjee", "Sayan Layek", "Sayantan Adak", "Mykola Pechenizkiy", "Animesh Mukherjee", "Rima Hazra"], "title": "ProSocialAlign: Preference Conditioned Test Time Alignment in Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Current language model safety paradigms often fall short in emotionally charged or high-stakes settings, where refusal-only approaches may alienate users and naive compliance can amplify risk. We propose ProSocialAlign, a test-time, parameter-efficient framework that steers generation toward safe, empathetic, and value-aligned responses without retraining the base model. We formalize five human-centered objectives and cast safety as lexicographic constrained generation: first, applying hard constraints to eliminate harmful continuations; then optimizing for prosocial quality within the safe set. Our method combines (i) directional regulation, a harm-mitigation mechanism that subtracts a learned \"harm vector\" in parameter space, and (ii) preference-aware autoregressive reward modeling trained jointly across attributes with gradient conflict resolution, enabling fine-grained, user-controllable decoding. Empirical evaluations across five safety benchmarks demonstrate state-of-the-art performance, reducing unsafe leakage and boosting alignment to human values, with strong gains across multiple evaluation metrics. ProSocialAlign offers a robust and modular foundation for generating context-sensitive, safe, and human-aligned responses at inference time.", "AI": {"tldr": "ProSocialAlign是一个测试时参数高效框架，通过词典约束生成方法实现安全、共情和价值观对齐的响应，无需重新训练基础模型", "motivation": "当前语言模型安全范式在情感激烈或高风险场景中表现不足，拒绝式方法可能疏远用户，而单纯服从可能放大风险", "method": "结合方向性调节（在参数空间减去学习的'伤害向量'）和偏好感知自回归奖励建模（通过梯度冲突解决联合训练多属性），实现词典约束生成", "result": "在五个安全基准测试中达到最先进性能，减少不安全泄露并提升与人类价值观的对齐度，在多个评估指标上获得显著提升", "conclusion": "ProSocialAlign为在推理时生成情境敏感、安全和人类对齐的响应提供了鲁棒且模块化的基础框架"}}
{"id": "2512.06710", "pdf": "https://arxiv.org/pdf/2512.06710", "abs": "https://arxiv.org/abs/2512.06710", "authors": ["Zairah Mustahsan", "Abel Lim", "Megna Anand", "Saahil Jain", "Bryan McCann"], "title": "Stochasticity in Agentic Evaluations: Quantifying Inconsistency with Intraclass Correlation", "categories": ["cs.AI"], "comment": null, "summary": "As large language models become components of larger agentic systems, evaluation reliability becomes critical: unreliable sub-agents introduce brittleness into downstream system behavior. Yet current evaluation practice, reporting a single accuracy number from a single run, obscures the variance underlying these results, making it impossible to distinguish genuine capability improvements from lucky sampling. We propose adopting Intraclass Correlation Coefficient (ICC), a metric from measurement science, to characterize this variance. ICC decomposes observed variance into between-query variance (task difficulty) and within-query variance (agent inconsistency), highlighting whether reported results reflect true capability or measurement noise. We evaluated on GAIA (Levels 1-3, measuring agentic capabilities across varying reasoning complexity) and FRAMES (measuring retrieval and factuality across multiple documents). We found that ICC varies dramatically with task structure, with reasoning and retrieval tasks (FRAMES) exhibit ICC=0.4955-0.7118 across models, and agentic tasks (GAIA) exhibiting ICC=0.304-0.774 across models. For sub-agent replacement decisions in agentic systems, accuracy improvements are only trustworthy if ICC also improves. We demonstrate that ICC converges by n=8-16 trials for structured tasks and n>=32 for complex reasoning, enabling practitioners to set evidence-based resampling budgets. We recommend reporting accuracy alongside ICC and within-query variance as standard practice, and propose updated Evaluation Cards capturing these metrics. By making evaluation stability visible, we aim to transform agentic benchmarking from opaque leaderboard competition to trustworthy experimental science. Our code is open-sourced at https://github.com/youdotcom-oss/stochastic-agent-evals.", "AI": {"tldr": "论文提出使用组内相关系数(ICC)来评估语言模型代理系统的可靠性，将方差分解为任务难度和代理不一致性，确保评估结果反映真实能力而非随机噪声。", "motivation": "当前评估实践仅报告单一准确率数字，掩盖了结果方差，无法区分真实能力提升与随机采样运气，导致下游系统行为脆弱。", "method": "采用测量科学中的组内相关系数(ICC)，在GAIA(代理能力)和FRAMES(检索和事实性)数据集上进行评估，分析任务结构对ICC的影响。", "result": "ICC随任务结构变化显著：推理检索任务ICC=0.4955-0.7118，代理任务ICC=0.304-0.774。ICC在结构化任务中n=8-16次试验收敛，复杂推理需n≥32次。", "conclusion": "建议将准确率与ICC和组内方差一起报告作为标准实践，更新评估卡片包含这些指标，将代理基准测试从不透明的排行榜竞争转变为可信的实验科学。"}}
{"id": "2512.06586", "pdf": "https://arxiv.org/pdf/2512.06586", "abs": "https://arxiv.org/abs/2512.06586", "authors": ["Mikhail Zimin", "Milyausha Shamsutdinova", "Georgii Andriushchenko"], "title": "Adapting AlignScore Mertic for Factual Consistency Evaluation of Text in Russian: A Student Abstract", "categories": ["cs.CL"], "comment": null, "summary": "Ensuring factual consistency in generated text is crucial for reliable natural language processing applications. However, there is a lack of evaluation tools for factual consistency in Russian texts, as existing tools primarily focus on English corpora. To bridge this gap, we introduce AlignRuScore, a comprehensive adaptation of the AlignScore metric for Russian. To adapt the metric, we fine-tuned a RuBERT-based alignment model with task-specific classification and regression heads on Russian and translated English datasets. Our results demonstrate that a unified alignment metric can be successfully ported to Russian, laying the groundwork for robust multilingual factual consistency evaluation. We release the translated corpora, model checkpoints, and code to support further research.", "AI": {"tldr": "AlignRuScore：基于RuBERT的俄语事实一致性评估工具，通过微调分类和回归头实现英语AlignScore指标的俄语适配", "motivation": "现有事实一致性评估工具主要针对英语语料，缺乏俄语文本的专门评估工具", "method": "基于RuBERT模型，使用俄语和翻译的英语数据集微调任务特定的分类和回归头", "result": "成功将统一的alignment指标适配到俄语，为多语言事实一致性评估奠定基础", "conclusion": "发布了翻译语料库、模型检查点和代码以支持进一步研究"}}
{"id": "2512.06716", "pdf": "https://arxiv.org/pdf/2512.06716", "abs": "https://arxiv.org/abs/2512.06716", "authors": ["Zhibo Liang", "Tianze Hu", "Zaiye Chen", "Mingjie Tang"], "title": "Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents", "categories": ["cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Autonomous Large Language Model (LLM) agents exhibit significant vulnerability to Indirect Prompt Injection (IPI) attacks. These attacks hijack agent behavior by polluting external information sources, exploiting fundamental trade-offs between security and functionality in existing defense mechanisms. This leads to malicious and unauthorized tool invocations, diverting agents from their original objectives. The success of complex IPIs reveals a deeper systemic fragility: while current defenses demonstrate some effectiveness, most defense architectures are inherently fragmented. Consequently, they fail to provide full integrity assurance across the entire task execution pipeline, forcing unacceptable multi-dimensional compromises among security, functionality, and efficiency. Our method is predicated on a core insight: no matter how subtle an IPI attack, its pursuit of a malicious objective will ultimately manifest as a detectable deviation in the action trajectory, distinct from the expected legitimate plan. Based on this, we propose the Cognitive Control Architecture (CCA), a holistic framework achieving full-lifecycle cognitive supervision. CCA constructs an efficient, dual-layered defense system through two synergistic pillars: (i) proactive and preemptive control-flow and data-flow integrity enforcement via a pre-generated \"Intent Graph\"; and (ii) an innovative \"Tiered Adjudicator\" that, upon deviation detection, initiates deep reasoning based on multi-dimensional scoring, specifically designed to counter complex conditional attacks. Experiments on the AgentDojo benchmark substantiate that CCA not only effectively withstands sophisticated attacks that challenge other advanced defense methods but also achieves uncompromised security with notable efficiency and robustness, thereby reconciling the aforementioned multi-dimensional trade-off.", "AI": {"tldr": "本文提出认知控制架构(CCA)来解决大型语言模型代理的间接提示注入攻击漏洞，通过双层次防御系统实现全生命周期认知监督，在安全性和效率之间取得平衡。", "motivation": "现有防御机制在安全性和功能性之间存在根本性权衡，无法在整个任务执行流程中提供完整的完整性保证，导致多维度的妥协。", "method": "提出认知控制架构(CCA)，包含两个协同支柱：(1)通过预生成的\"意图图\"进行主动的控制流和数据流完整性执行；(2)创新的\"分层裁决器\"，在检测到偏差时基于多维评分启动深度推理。", "result": "在AgentDojo基准测试中，CCA不仅能有效抵御挑战其他先进防御方法的复杂攻击，还能以显著效率和鲁棒性实现无妥协的安全性。", "conclusion": "CCA通过检测行动轨迹中的可检测偏差，成功解决了间接提示注入攻击的系统性脆弱性，调和了安全、功能和效率之间的多维权衡。"}}
{"id": "2512.06656", "pdf": "https://arxiv.org/pdf/2512.06656", "abs": "https://arxiv.org/abs/2512.06656", "authors": ["Kwabena Yamoah", "Cass Dykeman"], "title": "The Online Discourse of Virtual Reality and Anxiety", "categories": ["cs.CL", "stat.CO"], "comment": "Three tables and two figures. Unfortunately, I did not formally register the dataset prior to conducting the analysis", "summary": "VR in the treatment of clinical concerns such as generalized anxiety disorder or social anxiety. VR has created additional pathways to support patient well-being and care. Understanding online discussion of what users think about this technology may further support its efficacy. The purpose of this study was to employ a corpus linguistic methodology to identify the words and word networks that shed light on the online discussion of virtual reality and anxiety. Using corpus linguistics, frequently used words in discussion along with collocation were identified by utilizing Sketch Engine software. The results of the study, based upon the English Trends corpus, identified VR, Oculus, and headset as the most frequently discussed within the VR and anxiety subcorpus. These results point to the development of the virtual system, along with the physical apparatus that makes viewing and engaging with the virtual environment possible. Additional results point to collocation of prepositional phrases such as of virtual reality, in virtual reality, and for virtual reality relating to the design, experience, and development, respectively. These findings offer new perspective on how VR and anxiety together are discussed in general discourse and offer pathways for future opportunities to support counseling needs through development and accessibility. Keywords: anxiety disorders, corpus linguistics, Sketch Engine, and virtual reality VR", "AI": {"tldr": "本研究使用语料库语言学方法分析在线讨论中虚拟现实与焦虑相关的内容，发现VR、Oculus和头显是最常讨论的词汇，揭示了虚拟系统发展和用户体验的关注点。", "motivation": "了解用户对虚拟现实技术在焦虑治疗中应用的看法，以支持该技术的有效性和患者福祉。", "method": "采用语料库语言学方法，使用Sketch Engine软件分析英语趋势语料库中与VR和焦虑相关的词汇频率和搭配关系。", "result": "识别出VR、Oculus和headset是最频繁讨论的词汇；发现介词短语搭配（如of virtual reality、in virtual reality、for virtual reality）分别与设计、体验和发展相关。", "conclusion": "研究为理解公众对VR与焦虑治疗的讨论提供了新视角，为未来通过技术开发和可及性支持心理咨询需求指明了方向。"}}
{"id": "2512.06721", "pdf": "https://arxiv.org/pdf/2512.06721", "abs": "https://arxiv.org/abs/2512.06721", "authors": ["Bufang Yang", "Lilin Xu", "Liekang Zeng", "Yunqi Guo", "Siyang Jiang", "Wenrui Lu", "Kaiwei Liu", "Hancheng Xiang", "Xiaofan Jiang", "Guoliang Xing", "Zhenyu Yan"], "title": "ProAgent: Harnessing On-Demand Sensory Contexts for Proactive LLM Agent Systems", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Large Language Model (LLM) agents are emerging to transform daily life. However, existing LLM agents primarily follow a reactive paradigm, relying on explicit user instructions to initiate services, which increases both physical and cognitive workload. In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance. ProAgent first employs a proactive-oriented context extraction approach with on-demand tiered perception to continuously sense the environment and derive hierarchical contexts that incorporate both sensory and persona cues. ProAgent then adopts a context-aware proactive reasoner to map these contexts to user needs and tool calls, providing proactive assistance. We implement ProAgent on Augmented Reality (AR) glasses with an edge server and extensively evaluate it on a real-world testbed, a public dataset, and through a user study. Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants. A video demonstration of ProAgent is available at https://youtu.be/pRXZuzvrcVs.", "AI": {"tldr": "ProAgent是首个端到端主动代理系统，利用多模态感知和LLM推理提供主动服务，相比现有反应式代理在预测准确率、工具调用和用户满意度方面显著提升", "motivation": "现有LLM代理主要采用反应式范式，依赖显式用户指令启动服务，增加了用户的身心负担，需要向主动服务范式转变", "method": "采用主动导向的上下文提取方法进行分层感知，结合上下文感知的主动推理器将环境上下文映射到用户需求和工具调用", "result": "在真实测试平台、公共数据集和用户研究中，ProAgent实现33.4%的主动预测准确率提升、16.8%的工具调用F1分数提升，用户满意度显著改善", "conclusion": "ProAgent代表了向主动助手方向的重要进展，通过结合感知上下文和LLM推理有效减轻用户负担，为下一代智能代理系统奠定了基础"}}
{"id": "2512.06679", "pdf": "https://arxiv.org/pdf/2512.06679", "abs": "https://arxiv.org/abs/2512.06679", "authors": ["Smitha Muthya Sudheendra", "Mani Deep Cherukuri", "Jaideep Srivastava"], "title": "CMV-Fuse: Cross Modal-View Fusion of AMR, Syntax, and Knowledge Representations for Aspect Based Sentiment Analysis", "categories": ["cs.CL"], "comment": null, "summary": "Natural language understanding inherently depends on integrating multiple complementary perspectives spanning from surface syntax to deep semantics and world knowledge. However, current Aspect-Based Sentiment Analysis (ABSA) systems typically exploit isolated linguistic views, thereby overlooking the intricate interplay between structural representations that humans naturally leverage. We propose CMV-Fuse, a Cross-Modal View fusion framework that emulates human language processing by systematically combining multiple linguistic perspectives. Our approach systematically orchestrates four linguistic perspectives: Abstract Meaning Representations, constituency parsing, dependency syntax, and semantic attention, enhanced with external knowledge integration. Through hierarchical gated attention fusion across local syntactic, intermediate semantic, and global knowledge levels, CMV-Fuse captures both fine-grained structural patterns and broad contextual understanding. A novel structure aware multi-view contrastive learning mechanism ensures consistency across complementary representations while maintaining computational efficiency. Extensive experiments demonstrate substantial improvements over strong baselines on standard benchmarks, with analysis revealing how each linguistic view contributes to more robust sentiment analysis.", "AI": {"tldr": "CMV-Fuse是一个跨模态视图融合框架，通过结合抽象意义表示、成分句法、依存句法和语义注意力四种语言学视角，模拟人类语言处理过程，在方面级情感分析任务上取得了显著性能提升。", "motivation": "当前ABSA系统通常孤立利用单一语言学视角，忽视了人类自然语言理解中多种结构表征之间的复杂交互作用。", "method": "采用分层门控注意力融合机制，在局部句法、中间语义和全局知识三个层次上融合四种语言学视角，并引入结构感知多视图对比学习确保表示一致性。", "result": "在标准基准测试中相比强基线模型有显著改进，分析显示每种语言学视角都对更鲁棒的情感分析有所贡献。", "conclusion": "通过系统整合多语言学视角并模拟人类语言处理过程，CMV-Fuse框架能够同时捕获细粒度结构模式和广泛上下文理解，为ABSA任务提供了更有效的方法。"}}
{"id": "2512.06749", "pdf": "https://arxiv.org/pdf/2512.06749", "abs": "https://arxiv.org/abs/2512.06749", "authors": ["Ming Ma", "Jue Zhang", "Fangkai Yang", "Yu Kang", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer.", "AI": {"tldr": "DoVer是一个基于干预驱动的调试框架，通过主动验证和针对性干预来解决LLM多智能体系统中的调试难题，能够将失败任务转化为成功并验证故障假设。", "motivation": "现有的基于日志的故障定位方法存在两个关键局限：缺乏验证机制产生未经验证的假设，以及单步或单智能体归因往往不准确，因为多个不同的干预措施可能独立修复失败任务。", "method": "引入DoVer框架，通过针对性干预（如编辑消息、改变计划）来增强假设生成和主动验证，并采用结果导向的调试视角，关注系统是否解决故障或取得可量化的进展。", "result": "在Magnetic-One智能体框架上，DoVer将18-28%的失败试验转化为成功，实现高达16%的里程碑进展，验证或反驳30-60%的故障假设。在GSMPlus数据集和AG2框架上恢复49%的失败试验。", "conclusion": "干预是提高智能体系统可靠性的实用机制，为LLM多智能体系统开辟了更强大、可扩展的调试方法机会。"}}
{"id": "2512.06681", "pdf": "https://arxiv.org/pdf/2512.06681", "abs": "https://arxiv.org/abs/2512.06681", "authors": ["Amartya Hatua"], "title": "Mechanistic Interpretability of GPT-2: Lexical and Contextual Layers in Sentiment Analysis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a mechanistic interpretability study of GPT-2 that causally examines how sentiment information is processed across its transformer layers. Using systematic activation patching across all 12 layers, we test the hypothesized two-stage sentiment architecture comprising early lexical detection and mid-layer contextual integration. Our experiments confirm that early layers (0-3) act as lexical sentiment detectors, encoding stable, position specific polarity signals that are largely independent of context. However, all three contextual integration hypotheses: Middle Layer Concentration, Phenomenon Specificity, and Distributed Processing are falsified. Instead of mid-layer specialization, we find that contextual phenomena such as negation, sarcasm, domain shifts etc. are integrated primarily in late layers (8-11) through a unified, non-modular mechanism. These experimental findings provide causal evidence that GPT-2's sentiment computation differs from the predicted hierarchical pattern, highlighting the need for further empirical characterization of contextual integration in large language models.", "AI": {"tldr": "本研究通过激活修补实验发现GPT-2的情感处理机制：早期层(0-3)负责词汇情感检测，而上下文整合(如否定、讽刺等)主要在晚期层(8-11)通过统一机制完成，推翻了中层专门化的假设。", "motivation": "探究GPT-2中情感信息在transformer各层的处理机制，验证关于情感架构的两阶段假设（早期词汇检测和中层上下文整合）。", "method": "使用系统性的激活修补方法，在GPT-2的所有12个层中进行实验，测试三种上下文整合假设：中层集中、现象特异性和分布式处理。", "result": "早期层确实作为词汇情感检测器，编码稳定的位置特异性极性信号；但所有三种中层上下文整合假设都被证伪，上下文现象主要在晚期层通过统一机制整合。", "conclusion": "GPT-2的情感计算机制与预测的层次模式不同，上下文整合主要在晚期层完成，表明需要进一步实证研究大语言模型中的上下文整合机制。"}}
{"id": "2512.06835", "pdf": "https://arxiv.org/pdf/2512.06835", "abs": "https://arxiv.org/abs/2512.06835", "authors": ["Tingyu Li", "Zheng Sun", "Jingxuan Wei", "Siyuan Li", "Conghui He", "Lijun Wu", "Cheng Tan"], "title": "Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning", "categories": ["cs.AI"], "comment": "25 pages, 5 figures", "summary": "Recent vision-language models (VLMs) achieve remarkable reasoning through reinforcement learning (RL), which provides a feasible solution for realizing continuous self-evolving large vision-language models (LVLMs) in the era of experience. However, RL for VLMs requires abundant high-quality multimodal data, especially challenging in specialized domains like chemistry, earth sciences, and multimodal mathematics. Existing strategies such as synthetic data and self-rewarding mechanisms suffer from limited distributions and alignment difficulties, ultimately causing reward hacking: models exploit high-reward patterns, collapsing policy entropy and destabilizing training. We propose DoGe (Decouple to Generalize), a dual-decoupling framework that guides models to first learn from context rather than problem solving by refocusing on the problem context scenarios overlooked by synthetic data methods. By decoupling learning process into dual components (Thinker and Solver), we reasonably quantify the reward signals of this process and propose a two-stage RL post-training approach from freely exploring context to practically solving tasks. Second, to increase the diversity of training data, DoGe constructs an evolving curriculum learning pipeline: an expanded native domain knowledge corpus and an iteratively evolving seed problems pool. Experiments show that our method consistently outperforms the baseline across various benchmarks, providing a scalable pathway for realizing self-evolving LVLMs.", "AI": {"tldr": "DoGe框架通过双解耦方法解决VLMs在专业领域RL训练中的奖励黑客问题，将学习过程分为思考者和解决者两个阶段，并构建进化课程学习管道提升数据多样性，在多个基准测试中表现优于基线方法。", "motivation": "现有VLMs在专业领域（如化学、地球科学、多模态数学）进行RL训练时面临高质量多模态数据稀缺问题，合成数据和自奖励机制存在分布受限和对齐困难，导致奖励黑客现象，模型利用高奖励模式导致策略熵崩溃和训练不稳定。", "method": "提出DoGe双解耦框架：1）将学习过程解耦为思考者（学习上下文）和解决者（解决问题）两个组件，量化奖励信号并采用两阶段RL后训练方法；2）构建进化课程学习管道，包括扩展的本地领域知识语料库和迭代进化的种子问题池。", "result": "实验表明该方法在多个基准测试中持续优于基线方法", "conclusion": "DoGe为实现在经验时代的自进化大型视觉语言模型提供了一条可扩展的路径"}}
{"id": "2512.06688", "pdf": "https://arxiv.org/pdf/2512.06688", "abs": "https://arxiv.org/abs/2512.06688", "authors": ["Bowen Jiang", "Yuan Yuan", "Maohao Shen", "Zhuoqun Hao", "Zhangchen Xu", "Zichen Chen", "Ziyi Liu", "Anvesh Rao Vijjini", "Jiashu He", "Hanchao Yu", "Radha Poovendran", "Gregory Wornell", "Lyle Ungar", "Dan Roth", "Sihao Chen", "Camillo Jose Taylor"], "title": "PersonaMem-v2: Towards Personalized Intelligence via Learning Implicit User Personas and Agentic Memory", "categories": ["cs.CL"], "comment": "Data is available at https://huggingface.co/datasets/bowen-upenn/PersonaMem-v2", "summary": "Personalization is one of the next milestones in advancing AI capability and alignment. We introduce PersonaMem-v2, the state-of-the-art dataset for LLM personalization that simulates 1,000 realistic user-chatbot interactions on 300+ scenarios, 20,000+ user preferences, and 128k-token context windows, where most user preferences are implicitly revealed to reflect real-world interactions. Using this data, we investigate how reinforcement fine-tuning enables a model to improve its long-context reasoning capabilities for user understanding and personalization. We also develop a framework for training an agentic memory system, which maintains a single, human-readable memory that grows with each user over time.\n  In our experiments, frontier LLMs still struggle with implicit personalization, achieving only 37-48% accuracy. While they support long context windows, reasoning remains the bottleneck for implicit personalization tasks. Using reinforcement fine-tuning, we successfully train Qwen3-4B to outperforms GPT-5, reaching 53% accuracy in implicit personalization. Moreover, our agentic memory framework achieves state-of-the-art 55% accuracy while using 16x fewer input tokens, relying on a 2k-token memory instead of full 32k conversation histories. These results underscore the impact of our dataset and demonstrate agentic memory as a scalable path toward real-world personalized intelligence.", "AI": {"tldr": "PersonaMem-v2是当前最先进的LLM个性化数据集，包含1000个真实用户-聊天机器人交互，覆盖300+场景和20000+用户偏好。通过强化微调，Qwen3-4B模型在隐式个性化任务上达到53%准确率，超越GPT-5。代理记忆框架仅用2k token内存就实现55%准确率，比完整32k对话历史节省16倍计算资源。", "motivation": "个性化是AI能力和对齐的下一个重要里程碑。当前前沿LLM在隐式个性化任务上表现不佳（仅37-48%准确率），长上下文推理能力成为瓶颈。需要开发更好的数据集和训练方法来提升个性化能力。", "method": "1) 创建PersonaMem-v2数据集，模拟真实用户交互，包含隐式偏好信息；2) 使用强化微调方法训练模型；3) 开发代理记忆系统框架，维护可读内存并随时间增长。", "result": "1) 前沿LLM隐式个性化准确率仅37-48%；2) Qwen3-4B通过强化微调达到53%准确率，超越GPT-5；3) 代理记忆框架达到55%准确率，仅需2k token内存（比32k完整历史节省16倍资源）。", "conclusion": "PersonaMem-v2数据集对个性化研究有重要影响，代理记忆系统为实际个性化智能提供了可扩展的路径。强化微调能有效提升模型的长上下文推理和个性化能力。"}}
{"id": "2512.06859", "pdf": "https://arxiv.org/pdf/2512.06859", "abs": "https://arxiv.org/abs/2512.06859", "authors": ["Ce Chi", "Xing Wang", "Zhendong Wang", "Xiaofan Liu", "Ce Li", "Zhiyan Song", "Chen Zhao", "Kexin Yang", "Boshen Shi", "Jingjing Yang", "Chao Deng", "Junlan Feng"], "title": "JT-DA: Enhancing Data Analysis with Tool-Integrated Table Reasoning Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "In this work, we present JT-DA-8B (JiuTian Data Analyst 8B), a specialized large language model designed for complex table reasoning tasks across diverse real-world scenarios. To address the lack of high-quality supervision in tabular reasoning scenarios, we construct a comprehensive and diverse training corpus with 34 well-defined table reasoning tasks, by aggregating 29 public table QA datasets and 3 million tables. An automatic pipeline is proposed to generate realistic multi-step analytical tasks involving reasoning patterns. The model is trained upon open-source JT-Coder-8B model, an 8B-parameter decoder-only foundation model trained from scratch. In the training stage, we leverage LLM-based scoring and workflow-aligned filtering to distill high-quality, table-centric data. Both supervised fine-tuning (SFT) and Reinforcement learning (RL) are adopted to optimize our model. Afterwards, a four-stage table reasoning workflow is proposed, including table preprocessing, table sensing, tool-integrated reasoning, and prompt engineering, to improve model interpretability and execution accuracy. Experimental results show that JT-DA-8B achieves strong performance in various table reasoning tasks, demonstrating the effectiveness of data-centric generation and workflow-driven optimization.", "AI": {"tldr": "JT-DA-8B是一个专门用于复杂表格推理任务的大语言模型，通过构建包含34个表格推理任务的大规模训练语料库，采用SFT和RL训练，并提出了四阶段表格推理工作流程，在各种表格推理任务中表现出色。", "motivation": "解决表格推理场景中高质量监督数据缺乏的问题，需要开发能够处理多样化真实场景中复杂表格推理任务的专门模型。", "method": "聚合29个公共表格QA数据集和300万张表格构建训练语料库；提出自动管道生成多步分析任务；基于开源JT-Coder-8B模型进行训练；采用LLM评分和工作流对齐过滤来蒸馏高质量数据；使用监督微调(SFT)和强化学习(RL)优化模型；提出四阶段表格推理工作流程。", "result": "JT-DA-8B在各种表格推理任务中实现了强劲性能，证明了以数据为中心的生成和工作流驱动的优化方法的有效性。", "conclusion": "通过数据中心的生成方法和工作流程驱动的优化策略，成功开发出在复杂表格推理任务中表现优异的专门化语言模型，为解决表格数据分析中的挑战提供了有效解决方案。"}}
{"id": "2512.06690", "pdf": "https://arxiv.org/pdf/2512.06690", "abs": "https://arxiv.org/abs/2512.06690", "authors": ["Chengbing Wang", "Yang Zhang", "Wenjie Wang", "Xiaoyan Zhao", "Fuli Feng", "Xiangnan He", "Tat-Seng Chua"], "title": "Think-While-Generating: On-the-Fly Reasoning for Personalized Long-Form Generation", "categories": ["cs.CL"], "comment": null, "summary": "Preference alignment has enabled large language models (LLMs) to better reflect human expectations, but current methods mostly optimize for population-level preferences, overlooking individual users. Personalization is essential, yet early approaches-such as prompt customization or fine-tuning-struggle to reason over implicit preferences, limiting real-world effectiveness. Recent \"think-then-generate\" methods address this by reasoning before response generation. However, they face challenges in long-form generation: their static one-shot reasoning must capture all relevant information for the full response generation, making learning difficult and limiting adaptability to evolving content. To address this issue, we propose FlyThinker, an efficient \"think-while-generating\" framework for personalized long-form generation. FlyThinker employs a separate reasoning model that generates latent token-level reasoning in parallel, which is fused into the generation model to dynamically guide response generation. This design enables reasoning and generation to run concurrently, ensuring inference efficiency. In addition, the reasoning model is designed to depend only on previous responses rather than its own prior outputs, which preserves training parallelism across different positions-allowing all reasoning tokens for training data to be produced in a single forward pass like standard LLM training, ensuring training efficiency. Extensive experiments on real-world benchmarks demonstrate that FlyThinker achieves better personalized generation while keeping training and inference efficiency.", "AI": {"tldr": "FlyThinker是一个\"边思考边生成\"的高效个性化长文本生成框架，通过并行推理模型生成潜在token级推理，动态指导响应生成，在保持训练和推理效率的同时实现更好的个性化生成效果。", "motivation": "现有偏好对齐方法主要优化群体级偏好，忽视个体用户需求。早期个性化方法难以推理隐含偏好，而近期\"先思考后生成\"方法在长文本生成中存在静态一次性推理难以捕获全部相关信息、学习困难、适应性受限等问题。", "method": "提出FlyThinker框架：使用独立推理模型并行生成潜在token级推理，将其融合到生成模型中动态指导响应生成；推理模型仅依赖先前响应而非自身输出，保持训练并行性；支持推理和生成并发执行。", "result": "在真实基准测试中的广泛实验表明，FlyThinker在保持训练和推理效率的同时，实现了更好的个性化生成效果。", "conclusion": "FlyThinker通过\"边思考边生成\"的并行架构有效解决了长文本个性化生成的挑战，在效率和效果方面都表现出色，为个性化大语言模型应用提供了实用解决方案。"}}
{"id": "2512.06867", "pdf": "https://arxiv.org/pdf/2512.06867", "abs": "https://arxiv.org/abs/2512.06867", "authors": ["John Licato", "Stephen Steinle", "Brayden Hollis"], "title": "Do Persona-Infused LLMs Affect Performance in a Strategic Reasoning Game?", "categories": ["cs.AI"], "comment": "Accepted at IJCNLP-AACL 2025", "summary": "Although persona prompting in large language models appears to trigger different styles of generated text, it is unclear whether these translate into measurable behavioral differences, much less whether they affect decision-making in an adversarial strategic environment that we provide as open-source. We investigate the impact of persona prompting on strategic performance in PERIL, a world-domination board game. Specifically, we compare the effectiveness of persona-derived heuristic strategies to those chosen manually. Our findings reveal that certain personas associated with strategic thinking improve game performance, but only when a mediator is used to translate personas into heuristic values. We introduce this mediator as a structured translation process, inspired by exploratory factor analysis, that maps LLM-generated inventory responses into heuristics. Results indicate our method enhances heuristic reliability and face validity compared to directly inferred heuristics, allowing us to better study the effect of persona types on decision making. These insights advance our understanding of how persona prompting influences LLM-based decision-making and propose a heuristic generation method that applies psychometric principles to LLMs.", "AI": {"tldr": "研究通过人格提示在大型语言模型中生成不同策略行为，发现在PERIL棋盘游戏中，通过中介转换机制将人格特征映射为启发式值可以提升游戏表现，但直接使用人格提示效果有限。", "motivation": "探索人格提示是否能在对抗性战略环境中产生可测量的行为差异，以及这些差异如何影响决策制定。", "method": "使用PERIL世界统治棋盘游戏作为测试环境，比较人格衍生启发式策略与手动选择策略的效果，引入基于探索性因子分析的结构化中介转换过程。", "result": "某些与战略思维相关的人格确实能提高游戏表现，但必须通过中介转换过程将人格映射为启发式值才能生效。该方法相比直接推断的启发式具有更高的可靠性和表面效度。", "conclusion": "人格提示通过结构化中介转换过程可以影响LLM的决策制定，提出了将心理测量学原理应用于LLM的启发式生成方法，增进了对人格提示如何影响基于LLM决策的理解。"}}
{"id": "2512.06694", "pdf": "https://arxiv.org/pdf/2512.06694", "abs": "https://arxiv.org/abs/2512.06694", "authors": ["Aoi Fujita", "Taichi Yamamoto", "Yuri Nakayama", "Ryota Kobayashi"], "title": "TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction", "categories": ["cs.CL"], "comment": "15 pages, 4 figures, code available at https://github.com/aoi8716/TopiCLEAR", "summary": "Rapid expansion of social media platforms such as X (formerly Twitter), Facebook, and Reddit has enabled large-scale analysis of public perceptions on diverse topics, including social issues, politics, natural disasters, and consumer sentiment. Topic modeling is a widely used approach for uncovering latent themes in text data, typically framed as an unsupervised classification task. However, traditional models, originally designed for longer and more formal documents, struggle with short social media posts due to limited co-occurrence statistics, fragmented semantics, inconsistent spelling, and informal language. To address these challenges, we propose a new method, TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction. Specifically, each text is embedded using Sentence-BERT (SBERT) and provisionally clustered using Gaussian Mixture Models (GMM). The clusters are then refined iteratively using a supervised projection based on linear discriminant analysis, followed by GMM-based clustering until convergence. Notably, our method operates directly on raw text, eliminating the need for preprocessing steps such as stop word removal. We evaluate our approach on four diverse datasets, 20News, AgNewsTitle, Reddit, and TweetTopic, each containing human-labeled topic information. Compared with seven baseline methods, including a recent SBERT-based method and a zero-shot generative AI method, our approach achieves the highest similarity to human-annotated topics, with significant improvements for both social media posts and online news articles. Additionally, qualitative analysis shows that our method produces more interpretable topics, highlighting its potential for applications in social media data and web content analytics.", "AI": {"tldr": "提出了TopiCLEAR方法，通过自适应降维聚类嵌入来提取主题，在社交媒体短文本主题建模中表现优于传统方法，无需预处理步骤，在多个数据集上取得了最佳效果。", "motivation": "传统主题模型针对长文档设计，在处理社交媒体短文本时面临共现统计有限、语义碎片化、拼写不一致和非正式语言等挑战。", "method": "使用Sentence-BERT嵌入文本，先用高斯混合模型进行初步聚类，然后通过线性判别分析的监督投影迭代优化聚类直到收敛。", "result": "在四个数据集上评估，相比7个基线方法（包括SBERT方法和零样本生成AI方法），该方法与人工标注主题的相似度最高，对社交媒体帖子和在线新闻文章都有显著改进。", "conclusion": "TopiCLEAR方法能够产生更可解释的主题，在社交媒体数据和网络内容分析中具有应用潜力。"}}
{"id": "2512.06983", "pdf": "https://arxiv.org/pdf/2512.06983", "abs": "https://arxiv.org/abs/2512.06983", "authors": ["Eli J. Laird", "Corey Clark"], "title": "On Memory: A comparison of memory mechanisms in world models", "categories": ["cs.AI", "cs.LG"], "comment": "10 pages, 1 figure", "summary": "World models enable agents to plan within imagined environments by predicting future states conditioned on past observations and actions. However, their ability to plan over long horizons is limited by the effective memory span of the backbone architecture. This limitation leads to perceptual drift in long rollouts, hindering the model's capacity to perform loop closures within imagined trajectories. In this work, we investigate the effective memory span of transformer-based world models through an analysis of several memory augmentation mechanisms. We introduce a taxonomy that distinguishes between memory encoding and memory injection mechanisms, motivating their roles in extending the world model's memory through the lens of residual stream dynamics. Using a state recall evaluation task, we measure the memory recall of each mechanism and analyze its respective trade-offs. Our findings show that memory mechanisms improve the effective memory span in vision transformers and provide a path to completing loop closures within a world model's imagination.", "AI": {"tldr": "该研究分析了基于Transformer的世界模型的有效记忆跨度，提出了记忆编码和记忆注入机制的分类法，并通过状态回忆任务验证了这些机制在扩展世界模型记忆和实现轨迹闭环方面的有效性。", "motivation": "世界模型通过预测未来状态来支持智能体规划，但其长期规划能力受到主干架构有效记忆跨度的限制，导致长序列生成中出现感知漂移和无法实现轨迹闭环的问题。", "method": "研究通过分析多种记忆增强机制，引入了区分记忆编码和记忆注入机制的分类法，从残差流动力学的角度理解其扩展世界模型记忆的作用，并使用状态回忆评估任务测量每种机制的记忆回忆能力。", "result": "研究结果表明，记忆机制显著提高了视觉Transformer的有效记忆跨度，并为在世界模型想象中完成轨迹闭环提供了可行路径。", "conclusion": "记忆增强机制是提升基于Transformer的世界模型长期规划能力的关键技术，通过合理的记忆编码和注入策略可以有效解决感知漂移和轨迹闭环问题。"}}
{"id": "2512.06711", "pdf": "https://arxiv.org/pdf/2512.06711", "abs": "https://arxiv.org/abs/2512.06711", "authors": ["Yulin Huang", "Yaxuan Luan", "Jinxu Guo", "Xiangchen Song", "Yuchen Liu"], "title": "Parameter-Efficient Fine-Tuning with Differential Privacy for Robust Instruction Adaptation in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "This study addresses the issues of privacy protection and efficiency in instruction fine-tuning of large-scale language models by proposing a parameter-efficient method that integrates differential privacy noise allocation with gradient clipping in a collaborative optimization framework. The method keeps the backbone model frozen and updates parameters through a low-dimensional projection subspace, while introducing clipping and adaptive noise allocation during gradient computation. This design reduces privacy budget consumption and ensures training stability and robustness. The unified framework combines gradient constraints, noise allocation, and parameter projection, effectively mitigating performance fluctuations and privacy risks in multi-task instruction scenarios. Experiments are conducted across hyperparameter, environment, and data sensitivity dimensions. Results show that the method outperforms baseline models in accuracy, privacy budget, and parameter efficiency, and maintains stable performance under diverse and uncertain data conditions. The findings enrich the theoretical integration of differential privacy and parameter-efficient fine-tuning and demonstrate its practical adaptability in instruction tasks, providing a feasible solution for secure training in complex instruction environments.", "AI": {"tldr": "提出一种参数高效的指令微调方法，通过差分隐私噪声分配和梯度裁剪的协同优化框架，在保护隐私的同时提高训练效率。", "motivation": "解决大规模语言模型指令微调中的隐私保护和效率问题，需要在多任务指令场景下平衡性能稳定性和隐私风险。", "method": "保持主干模型冻结，通过低维投影子空间更新参数，在梯度计算中引入裁剪和自适应噪声分配，结合梯度约束、噪声分配和参数投影的统一框架。", "result": "在超参数、环境和数据敏感性维度上的实验表明，该方法在准确性、隐私预算和参数效率方面优于基线模型，并在多样化不确定数据条件下保持稳定性能。", "conclusion": "该方法丰富了差分隐私与参数高效微调的理论整合，证明了在指令任务中的实际适应性，为复杂指令环境下的安全训练提供了可行解决方案。"}}
{"id": "2512.06990", "pdf": "https://arxiv.org/pdf/2512.06990", "abs": "https://arxiv.org/abs/2512.06990", "authors": ["Krishna Arun", "Moinak Bhattachrya", "Paras Goel"], "title": "Utilizing Multi-Agent Reinforcement Learning with Encoder-Decoder Architecture Agents to Identify Optimal Resection Location in Glioblastoma Multiforme Patients", "categories": ["cs.AI", "cs.CV", "eess.IV"], "comment": null, "summary": "Currently, there is a noticeable lack of AI in the medical field to support doctors in treating heterogenous brain tumors such as Glioblastoma Multiforme (GBM), the deadliest human cancer in the world with a five-year survival rate of just 5.1%. This project develops an AI system offering the only end-to-end solution by aiding doctors with both diagnosis and treatment planning. In the diagnosis phase, a sequential decision-making framework consisting of 4 classification models (Convolutional Neural Networks and Support Vector Machine) are used. Each model progressively classifies the patient's brain into increasingly specific categories, with the final step being named diagnosis. For treatment planning, an RL system consisting of 3 generative models is used. First, the resection model (diffusion model) analyzes the diagnosed GBM MRI and predicts a possible resection outcome. Second, the radiotherapy model (Spatio-Temporal Vision Transformer) generates an MRI of the brain's progression after a user-defined number of weeks. Third, the chemotherapy model (Diffusion Model) produces the post-treatment MRI. A survival rate calculator (Convolutional Neural Network) then checks if the generated post treatment MRI has a survival rate within 15% of the user defined target. If not, a feedback loop using proximal policy optimization iterates over this system until an optimal resection location is identified. When compared to existing solutions, this project found 3 key findings: (1) Using a sequential decision-making framework consisting of 4 small diagnostic models reduced computing costs by 22.28x, (2) Transformers regression capabilities decreased tumor progression inference time by 113 hours, and (3) Applying Augmentations resembling Real-life situations improved overall DICE scores by 2.9%. These results project to increase survival rates by 0.9%, potentially saving approximately 2,250 lives.", "AI": {"tldr": "开发了一个端到端AI系统，用于胶质母细胞瘤的诊断和治疗规划，通过序列决策框架和强化学习系统，显著降低了计算成本并提高了治疗效果。", "motivation": "目前医疗领域缺乏支持医生治疗异质性脑肿瘤（如胶质母细胞瘤）的AI系统，该癌症五年生存率仅为5.1%，急需技术突破。", "method": "诊断阶段使用4个分类模型（CNN和SVM）的序列决策框架；治疗规划阶段使用3个生成模型：切除模型（扩散模型）、放疗模型（时空视觉Transformer）、化疗模型（扩散模型），结合生存率计算器和近端策略优化的反馈循环。", "result": "计算成本降低22.28倍，肿瘤进展推理时间减少113小时，DICE分数提高2.9%，预计生存率提高0.9%，可能挽救约2250人的生命。", "conclusion": "该AI系统为胶质母细胞瘤提供了有效的端到端解决方案，在诊断准确性和治疗规划效率方面均有显著提升，具有重要的临床价值。"}}
{"id": "2512.06732", "pdf": "https://arxiv.org/pdf/2512.06732", "abs": "https://arxiv.org/abs/2512.06732", "authors": ["Aarushi Wagh", "Saniya Srivastava"], "title": "\"The Dentist is an involved parent, the bartender is not\": Revealing Implicit Biases in QA with Implicit BBQ", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing benchmarks evaluating biases in large language models (LLMs) primarily rely on explicit cues, declaring protected attributes like religion, race, gender by name. However, real-world interactions often contain implicit biases, inferred subtly through names, cultural cues, or traits. This critical oversight creates a significant blind spot in fairness evaluation. We introduce ImplicitBBQ, a benchmark extending the Bias Benchmark for QA (BBQ) with implicitly cued protected attributes across 6 categories. Our evaluation of GPT-4o on ImplicitBBQ illustrates troubling performance disparity from explicit BBQ prompts, with accuracy declining up to 7% in the \"sexual orientation\" subcategory and consistent decline located across most other categories. This indicates that current LLMs contain implicit biases undetected by explicit benchmarks. ImplicitBBQ offers a crucial tool for nuanced fairness evaluation in NLP.", "AI": {"tldr": "ImplicitBBQ基准测试扩展了BBQ基准，通过隐式线索评估大语言模型中的偏见，发现GPT-4o在隐式偏见测试中性能显著下降，揭示了现有显式基准无法检测到的隐式偏见问题。", "motivation": "现有偏见评估基准主要依赖显式线索（如直接声明宗教、种族、性别等受保护属性），但现实世界互动中常包含通过姓名、文化线索或特征隐式推断的偏见，这种关键疏忽造成了公平性评估的重大盲点。", "method": "引入ImplicitBBQ基准，扩展了Bias Benchmark for QA (BBQ)，在6个类别中加入了隐式线索的受保护属性评估。通过对GPT-4o在ImplicitBBQ上的评估，比较其与显式BBQ提示的性能差异。", "result": "GPT-4o在ImplicitBBQ上表现出令人担忧的性能差异，在\"性取向\"子类别中准确率下降高达7%，在其他大多数类别中也观察到一致的下降趋势。", "conclusion": "当前大语言模型包含现有显式基准无法检测到的隐式偏见，ImplicitBBQ为NLP领域的细致公平性评估提供了重要工具。"}}
{"id": "2512.07081", "pdf": "https://arxiv.org/pdf/2512.07081", "abs": "https://arxiv.org/abs/2512.07081", "authors": ["Rongjia Zhou", "Chengzhuo Li", "Carl Yang", "Jiaying Lu"], "title": "ClinNoteAgents: An LLM Multi-Agent System for Predicting and Interpreting Heart Failure 30-Day Readmission from Clinical Notes", "categories": ["cs.AI"], "comment": "10 pages, 2 figures. Submitted to AMIA 2026 Informatics Summit Student Paper Track", "summary": "Heart failure (HF) is one of the leading causes of rehospitalization among older adults in the United States. Although clinical notes contain rich, detailed patient information and make up a large portion of electronic health records (EHRs), they remain underutilized for HF readmission risk analysis. Traditional computational models for HF readmission often rely on expert-crafted rules, medical thesauri, and ontologies to interpret clinical notes, which are typically written under time pressure and may contain misspellings, abbreviations, and domain-specific jargon. We present ClinNoteAgents, an LLM-based multi-agent framework that transforms free-text clinical notes into (1) structured representations of clinical and social risk factors for association analysis and (2) clinician-style abstractions for HF 30-day readmission prediction. We evaluate ClinNoteAgents on 3,544 notes from 2,065 patients (readmission rate=35.16%), demonstrating strong performance in extracting risk factors from free-text, identifying key contributing factors, and predicting readmission risk. By reducing reliance on structured fields and minimizing manual annotation and model training, ClinNoteAgents provides a scalable and interpretable approach to note-based HF readmission risk modeling in data-limited healthcare systems.", "AI": {"tldr": "ClinNoteAgents是一个基于大语言模型的多智能体框架，能够将自由文本临床笔记转化为结构化风险因素表示和临床医生风格的抽象，用于心衰再入院风险预测，在数据有限的医疗系统中提供可扩展且可解释的解决方案。", "motivation": "心衰是美国老年人再住院的主要原因之一，临床笔记包含丰富的患者信息但未充分利用。传统方法依赖专家规则和医学词典，难以处理临床笔记中的拼写错误、缩写和领域特定术语。", "method": "开发LLM-based多智能体框架ClinNoteAgents，将自由文本临床笔记转化为：(1)结构化临床和社会风险因素表示用于关联分析；(2)临床医生风格的抽象用于30天再入院预测。", "result": "在3,544份笔记（2,065名患者，再入院率35.16%）上评估，在提取风险因素、识别关键贡献因素和预测再入院风险方面表现出色。", "conclusion": "ClinNoteAgents通过减少对结构化字段的依赖，最小化人工标注和模型训练，为基于临床笔记的心衰再入院风险建模提供了可扩展和可解释的方法。"}}
{"id": "2512.06734", "pdf": "https://arxiv.org/pdf/2512.06734", "abs": "https://arxiv.org/abs/2512.06734", "authors": ["Subrit Dikshit", "Ritu Tiwari", "Priyank Jain"], "title": "A Patient-Doctor-NLP-System to contest inequality for less privileged", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 6 figures", "summary": "Transfer Learning (TL) has accelerated the rapid development and availability of large language models (LLMs) for mainstream natural language processing (NLP) use cases. However, training and deploying such gigantic LLMs in resource-constrained, real-world healthcare situations remains challenging. This study addresses the limited support available to visually impaired users and speakers of low-resource languages such as Hindi who require medical assistance in rural environments. We propose PDFTEMRA (Performant Distilled Frequency Transformer Ensemble Model with Random Activations), a compact transformer-based architecture that integrates model distillation, frequency-domain modulation, ensemble learning, and randomized activation patterns to reduce computational cost while preserving language understanding performance. The model is trained and evaluated on medical question-answering and consultation datasets tailored to Hindi and accessibility scenarios, and its performance is compared against standard NLP state-of-the-art model baselines. Results demonstrate that PDFTEMRA achieves comparable performance with substantially lower computational requirements, indicating its suitability for accessible, inclusive, low-resource medical NLP applications.", "AI": {"tldr": "PDFTEMRA是一种紧凑的Transformer架构，通过模型蒸馏、频域调制、集成学习和随机激活模式等技术，在保持语言理解性能的同时大幅降低计算成本，适用于资源受限的医疗NLP应用。", "motivation": "解决在资源受限的医疗环境中，视觉障碍用户和低资源语言（如印地语）使用者难以获得大型语言模型支持的问题，特别是在农村地区的医疗援助需求。", "method": "提出PDFTEMRA模型，整合模型蒸馏、频域调制、集成学习和随机激活模式，在针对印地语和可访问性场景的医疗问答数据集上进行训练和评估。", "result": "PDFTEMRA在显著降低计算需求的同时实现了可比较的性能表现", "conclusion": "该模型适用于可访问、包容性的低资源医疗NLP应用，为资源受限环境下的医疗语言处理提供了可行解决方案"}}
{"id": "2512.07094", "pdf": "https://arxiv.org/pdf/2512.07094", "abs": "https://arxiv.org/abs/2512.07094", "authors": ["Christopher Cruz"], "title": "VIGIL: A Reflective Runtime for Self-Healing Agents", "categories": ["cs.AI"], "comment": null, "summary": "Agentic LLM frameworks promise autonomous behavior via task decomposition, tool use, and iterative planning, but most deployed systems remain brittle. They lack runtime introspection, cannot diagnose their own failure modes, and do not improve over time without human intervention. In practice, many agent stacks degrade into decorated chains of LLM calls with no structural mechanisms for reliability. We present VIGIL (Verifiable Inspection and Guarded Iterative Learning), a reflective runtime that supervises a sibling agent and performs autonomous maintenance rather than task execution. VIGIL ingests behavioral logs, appraises each event into a structured emotional representation, maintains a persistent EmoBank with decay and contextual policies, and derives an RBT diagnosis that sorts recent behavior into strengths, opportunities, and failures. From this analysis, VIGIL generates both guarded prompt updates that preserve core identity semantics and read only code proposals produced by a strategy engine that operates on log evidence and code hotspots. VIGIL functions as a state gated pipeline. Illegal transitions produce explicit errors rather than allowing the LLM to improvise. In a reminder latency case study, VIGIL identified elevated lag, proposed prompt and code repairs, and when its own diagnostic tool failed due to a schema conflict, it surfaced the internal error, produced a fallback diagnosis, and emitted a repair plan. This demonstrates meta level self repair in a deployed agent runtime.", "AI": {"tldr": "VIGIL是一个可验证的反射运行时系统，通过监督兄弟代理、分析行为日志、进行情感评估和自主维护，实现LLM代理的自我诊断和修复能力。", "motivation": "现有LLM代理框架缺乏运行时自省能力，无法诊断自身故障模式，需要人工干预才能改进，导致系统脆弱且可靠性差。", "method": "VIGIL采用状态门控管道架构，通过行为日志分析、结构化情感表示、持久性EmoBank维护、RBT诊断分类，生成防护性提示更新和只读代码建议。", "result": "在提醒延迟案例研究中，VIGIL成功识别性能问题，提出修复方案，并在自身诊断工具失败时进行元级自我修复。", "conclusion": "VIGIL展示了在部署的代理运行时中实现元级自我修复的能力，为构建更可靠、自适应的LLM代理系统提供了有效解决方案。"}}
{"id": "2512.06744", "pdf": "https://arxiv.org/pdf/2512.06744", "abs": "https://arxiv.org/abs/2512.06744", "authors": ["Rajeev Ranjan"], "title": "One Word Is Not Enough: Simple Prompts Improve Word Embeddings", "categories": ["cs.CL"], "comment": null, "summary": "Text embedding models are designed for sentence-level applications like retrieval and semantic similarity, and are primarily evaluated on sentence-level benchmarks. Their behavior on isolated words is less understood. We show that simply prepending semantic prompts to words before embedding substantially improves word similarity correlations. Testing 7 text embedding models, including text-embedding-3-large (OpenAI), embed-english-v3.0 (Cohere), voyage-3(Voyage AI), all-mpnet-base-v2, and Qwen3-Embedding-8B, on 3 standard benchmarks (SimLex-999, WordSim-353, MEN-3000), we find that prompts like \"meaning: {word}\" or \"Represent the semantic concept: {word}\" improve Spearman correlations by up to +0.29 on SimLex-999. Some models fail completely on bare words (correlation = 0) but recover with prompts (+0.73 improvement). Our best results achieve correlation = 0.692 on SimLex-999 with embed-english-v3.0 (Cohere), correlation = 0.811 on WordSim-353, and correlation = 0.855 on MEN-3000 with text-embedding-3-large (OpenAI). These results outperform classic static embeddings like Word2Vec (correlation = 0.40) and even the best static method LexVec (correlation = 0.48) on SimLex-999, establishing a new state-of-the-art for pure embedding methods. This zero-shot technique requires no training and works with any text embedding model.", "AI": {"tldr": "通过简单地在单词前添加语义提示（如\"meaning: {word}\"），可以显著提升文本嵌入模型在单词相似性任务上的表现，无需训练即可达到新的SOTA效果。", "motivation": "文本嵌入模型主要针对句子级应用设计，在孤立单词上的表现研究较少，需要探索如何提升模型在单词级任务上的性能。", "method": "在7个文本嵌入模型上测试，通过在单词前添加语义提示（如\"meaning: {word}\"），在3个标准基准（SimLex-999、WordSim-353、MEN-3000）上评估单词相似性相关性。", "result": "提示方法使Spearman相关性最高提升+0.29，有些模型从零相关性恢复到+0.73的提升。最佳结果在SimLex-999上达到0.692，超越Word2Vec（0.40）和LexVec（0.48）。", "conclusion": "简单的语义提示技术无需训练即可显著提升文本嵌入模型的单词相似性性能，为零样本单词级应用提供了有效解决方案。"}}
{"id": "2512.07109", "pdf": "https://arxiv.org/pdf/2512.07109", "abs": "https://arxiv.org/abs/2512.07109", "authors": ["Miguel Ingram", "Arthur Joseph Merritt"], "title": "A Neural Affinity Framework for Abstract Reasoning: Diagnosing the Compositional Gap in Transformer Architectures via Procedural Task Taxonomy", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "62 pages, 10 figures", "summary": "Responding to Hodel et al.'s (2024) call for a formal definition of task relatedness in re-arc, we present the first 9-category taxonomy of all 400 tasks, validated at 97.5% accuracy via rule-based code analysis. We prove the taxonomy's visual coherence by training a CNN on raw grid pixels (95.24% accuracy on S3, 36.25% overall, 3.3x chance), then apply the taxonomy diagnostically to the original ARC-AGI-2 test set. Our curriculum analysis reveals 35.3% of tasks exhibit low neural affinity for Transformers--a distributional bias mirroring ARC-AGI-2. To probe this misalignment, we fine-tuned a 1.7M-parameter Transformer across 302 tasks, revealing a profound Compositional Gap: 210 of 302 tasks (69.5%) achieve >80% cell accuracy (local patterns) but <10% grid accuracy (global synthesis). This provides direct evidence for a Neural Affinity Ceiling Effect, where performance is bounded by architectural suitability, not curriculum. Applying our framework to Li et al.'s independent ViTARC study (400 specialists, 1M examples each) confirms its predictive power: Very Low affinity tasks achieve 51.9% versus 77.7% for High affinity (p<0.001), with a task at 0% despite massive data. The taxonomy enables precise diagnosis: low-affinity tasks (A2) hit hard ceilings, while high-affinity tasks (C1) reach 99.8%. These findings indicate that progress requires hybrid architectures with affinity-aligned modules. We release our validated taxonomy,", "AI": {"tldr": "该研究提出了首个针对ARC-AGI任务的9类别分类法，通过CNN验证视觉一致性，发现Transformer架构在35.3%的任务上存在神经亲和力低的问题，揭示了组合性差距和神经亲和力天花板效应。", "motivation": "响应Hodel等人对任务相关性形式化定义的需求，建立系统化的任务分类体系来诊断神经网络在抽象推理任务中的性能限制。", "method": "开发基于规则代码分析的9类别分类法（97.5%准确率），训练CNN验证视觉一致性，对302个任务进行Transformer微调实验，分析局部与全局准确率差距。", "result": "发现69.5%的任务存在组合性差距（局部准确率>80%但全局准确率<10%），低亲和力任务准确率51.9% vs 高亲和力任务77.7%，验证了神经亲和力天花板效应。", "conclusion": "研究结果表明需要开发具有亲和力对齐模块的混合架构来突破性能限制，分类法为精确诊断任务难度和架构适配性提供了有效工具。"}}
{"id": "2512.06751", "pdf": "https://arxiv.org/pdf/2512.06751", "abs": "https://arxiv.org/abs/2512.06751", "authors": ["Seungyeon Jwa", "Daechul Ahn", "Reokyoung Kim", "Dongyeop Kang", "Jonghyun Choi"], "title": "Becoming Experienced Judges: Selective Test-Time Learning for Evaluators", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Automatic evaluation with large language models, commonly known as LLM-as-a-judge, is now standard across reasoning and alignment tasks. Despite evaluating many samples in deployment, these evaluators typically (i) treat each case independently, missing the opportunity to accumulate experience, and (ii) rely on a single fixed prompt for all cases, neglecting the need for sample-specific evaluation criteria. We introduce Learning While Evaluating (LWE), a framework that allows evaluators to improve sequentially at inference time without requiring training or validation sets. LWE maintains an evolving meta-prompt that (i) produces sample-specific evaluation instructions and (ii) refines itself through self-generated feedback. Furthermore, we propose Selective LWE, which updates the meta-prompt only on self-inconsistent cases, focusing computation where it matters most. This selective approach retains the benefits of sequential learning while being far more cost-effective. Across two pairwise comparison benchmarks, Selective LWE outperforms strong baselines, empirically demonstrating that evaluators can improve during sequential testing with a simple selective update, learning most from the cases they struggle with.", "AI": {"tldr": "论文提出了Learning While Evaluating (LWE)框架，通过动态更新的元提示实现LLM评估器的在线学习，无需训练集即可在推理过程中持续改进评估质量。", "motivation": "当前LLM评估器存在两个问题：(1) 独立处理每个样本，无法积累经验；(2) 使用固定提示，缺乏样本特定的评估标准。需要一种能够在推理时持续改进的评估方法。", "method": "提出LWE框架，维护一个动态演化的元提示，能够生成样本特定的评估指令并通过自我生成的反馈进行改进。还提出了Selective LWE变体，只在自不一致的情况下更新元提示以提高计算效率。", "result": "在两个成对比较基准测试中，Selective LWE优于强基线方法，证明评估器可以通过简单的选择性更新在顺序测试中持续改进，特别是在困难样本上学习效果最好。", "conclusion": "LWE框架成功实现了LLM评估器的在线学习能力，通过选择性更新策略在保持性能的同时显著提高了计算效率，为自动评估系统的发展提供了新方向。"}}
{"id": "2512.07178", "pdf": "https://arxiv.org/pdf/2512.07178", "abs": "https://arxiv.org/abs/2512.07178", "authors": ["Latifa Dwiyanti", "Sergio Ryan Wibisono", "Hidetaka Nambo"], "title": "ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation", "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": "This paper was accepted and presented at the 7th World Symposium on Software Engineering (WSSE) 2025 on 25 October 2025 in Okayama, Japan, and is currently awaiting publication", "summary": "Explainable Artificial Intelligence (XAI) has become an increasingly important area of research, particularly as machine learning models are deployed in high-stakes domains. Among various XAI approaches, SHAP (SHapley Additive exPlanations) has gained prominence due to its ability to provide both global and local explanations across different machine learning models. While SHAP effectively visualizes feature importance, it often lacks contextual explanations that are meaningful for end-users, especially those without technical backgrounds. To address this gap, we propose a Python package that extends SHAP by integrating it with a large language model (LLM), specifically OpenAI's GPT, to generate contextualized textual explanations. This integration is guided by user-defined parameters (such as feature aliases, descriptions, and additional background) to tailor the explanation to both the model context and the user perspective. We hypothesize that this enhancement can improve the perceived understandability of SHAP explanations. To evaluate the effectiveness of the proposed package, we applied it in a healthcare-related case study and conducted user evaluations involving real end-users. The results, based on Likert-scale surveys and follow-up interviews, indicate that the generated explanations were perceived as more understandable and contextually appropriate compared to visual-only outputs. While the findings are preliminary, they suggest that combining visualization with contextualized text may support more user-friendly and trustworthy model explanations.", "AI": {"tldr": "提出一个Python包，将SHAP与大型语言模型(GPT)结合，生成情境化文本解释，提高非技术用户对机器学习模型解释的理解度。", "motivation": "SHAP虽然能有效可视化特征重要性，但缺乏对非技术背景终端用户有意义的情境化解释。", "method": "开发Python包，集成SHAP与GPT模型，通过用户定义参数(特征别名、描述等)生成情境化文本解释，并在医疗案例中进行用户评估。", "result": "用户评估(Likert量表和访谈)显示，生成的解释比纯可视化输出更易理解和情境适当。", "conclusion": "可视化与情境化文本结合可以支持更用户友好和可信的模型解释，尽管结果还是初步的。"}}
{"id": "2512.06776", "pdf": "https://arxiv.org/pdf/2512.06776", "abs": "https://arxiv.org/abs/2512.06776", "authors": ["Yuchuan Tian", "Yuchen Liang", "Jiacheng Sun", "Shuo Zhang", "Guangwen Yang", "Yingte Shu", "Sibo Fang", "Tianyu Guo", "Kai Han", "Chao Xu", "Hanting Chen", "Xinghao Chen", "Yunhe Wang"], "title": "From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 4 figures", "summary": "Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior \"adaptation\" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.", "AI": {"tldr": "NBDiff是一种将自回归语言模型高效适配为块扩散模型的新方法，通过渐进式块大小增加、上下文因果注意力掩码和辅助AR损失，实现了从AR到块扩散的平滑转换，在7B参数规模上取得了最先进的性能。", "motivation": "现有的扩散语言模型训练成本高昂，而直接适配方法存在AR因果性与块双向性之间的根本不匹配问题，浪费了成熟AR检查点的知识。", "method": "提出将AR视为块大小=1的块扩散模型，设计包含上下文因果注意力掩码、并行适配程序、辅助AR损失和渐进块大小增加的适配路径。", "result": "NBDiff-7B在通用知识、数学和代码基准测试中超越强基线，实现了7B级别DLMs中的最先进性能。", "conclusion": "原则性的AR到块扩散适配是训练DLMs的有效且计算高效替代方案，能够继承长上下文建模和推理能力。"}}
{"id": "2512.07179", "pdf": "https://arxiv.org/pdf/2512.07179", "abs": "https://arxiv.org/abs/2512.07179", "authors": ["Wonbeen Lee", "Channyoung Lee", "Junho Sohn", "Hansam Cho"], "title": "PICKT: Practical Interlinked Concept Knowledge Tracing for Personalized Learning using Knowledge Map Concept Relations", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": "15 pages, 5 figures, 17 tables. Preparing submission for EDM 2026 conference", "summary": "With the recent surge in personalized learning, Intelligent Tutoring Systems (ITS) that can accurately track students' individual knowledge states and provide tailored learning paths based on this information are in demand as an essential task. This paper focuses on the core technology of Knowledge Tracing (KT) models that analyze students' sequences of interactions to predict their knowledge acquisition levels. However, existing KT models suffer from limitations such as restricted input data formats, cold start problems arising with new student enrollment or new question addition, and insufficient stability in real-world service environments. To overcome these limitations, a Practical Interlinked Concept Knowledge Tracing (PICKT) model that can effectively process multiple types of input data is proposed. Specifically, a knowledge map structures the relationships among concepts considering the question and concept text information, thereby enabling effective knowledge tracing even in cold start situations. Experiments reflecting real operational environments demonstrated the model's excellent performance and practicality. The main contributions of this research are as follows. First, a model architecture that effectively utilizes diverse data formats is presented. Second, significant performance improvements are achieved over existing models for two core cold start challenges: new student enrollment and new question addition. Third, the model's stability and practicality are validated through delicate experimental design, enhancing its applicability in real-world product environments. This provides a crucial theoretical and technical foundation for the practical implementation of next-generation ITS.", "AI": {"tldr": "提出了PICKT模型解决知识追踪中的冷启动和数据格式限制问题，通过知识图谱整合多种数据类型，在新学生和新题目场景下显著提升性能，验证了实际应用稳定性。", "motivation": "现有知识追踪模型存在输入数据格式限制、新学生/新题目冷启动问题，以及在真实服务环境中稳定性不足的局限性。", "method": "提出PICKT模型，构建知识图谱整合题目和概念文本信息的关系，有效处理多种类型输入数据，解决冷启动问题。", "result": "在反映真实运营环境的实验中表现出优异性能和实用性，在新学生注册和新题目添加两个核心冷启动挑战上显著超越现有模型。", "conclusion": "为下一代智能辅导系统的实际实施提供了重要的理论和技术基础，增强了在真实产品环境中的适用性。"}}
{"id": "2512.06787", "pdf": "https://arxiv.org/pdf/2512.06787", "abs": "https://arxiv.org/abs/2512.06787", "authors": ["Ofek Glick", "Vladimir Tchuiev", "Marah Ghoummaid", "Michal Moshkovitz", "Dotan Di-Castro"], "title": "LLM4SFC: Sequential Function Chart Generation via Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) are increasingly used for synthesizing textual PLC programming languages like Structured Text (ST) code, other IEC 61131-3 standard graphical languages like Sequential Function Charts (SFCs) remain underexplored. Generating SFCs is challenging due to graphical nature and ST actions embedded within, which are not directly compatible with standard generation techniques, often leading to non-executable code that is incompatible with industrial tool-chains In this work, we introduce LLM4SFC, the first framework to receive natural-language descriptions of industrial workflows and provide executable SFCs. LLM4SFC is based on three components: (i) A reduced structured representation that captures essential topology and in-line ST and reduced textual verbosity; (ii) Fine-tuning and few-shot retrieval-augmented generation (RAG) for alignment with SFC programming conventions; and (iii) A structured generation approach that prunes illegal tokens in real-time to ensure compliance with the textual format of SFCs. We evaluate LLM4SFC on a dataset of real-world SFCs from automated manufacturing projects, using both open-source and proprietary LLMs. The results show that LLM4SFC reliably generates syntactically valid SFC programs effectively bridging graphical and textual PLC languages, achieving a generation generation success of 75% - 94%, paving the way for automated industrial programming.", "AI": {"tldr": "LLM4SFC是首个从自然语言描述生成可执行顺序功能图(SFC)的框架，通过结构化表示、微调/RAG增强和实时非法令牌剪枝技术，成功率达到75%-94%", "motivation": "虽然大语言模型已用于生成文本化PLC编程语言，但图形化语言如SFC因图形特性和嵌入式ST代码的复杂性而未被充分探索，现有方法常生成不可执行代码", "method": "基于三个组件：(1)精简结构化表示捕获拓扑和嵌入式ST代码；(2)微调和少样本检索增强生成对齐SFC编程规范；(3)结构化生成方法实时剪枝非法令牌确保格式合规", "result": "在真实工业SFC数据集上评估，LLM4SFC可靠地生成语法有效的SFC程序，成功率为75%-94%，有效桥接图形和文本PLC语言", "conclusion": "该框架为自动化工业编程铺平道路，成功解决了SFC生成的技术挑战，实现了从自然语言到可执行图形化程序的有效转换"}}
{"id": "2512.07212", "pdf": "https://arxiv.org/pdf/2512.07212", "abs": "https://arxiv.org/abs/2512.07212", "authors": ["Zhaoyang Liu", "Mokai Pan", "Zhongyi Wang", "Kaizhen Zhu", "Haotao Lu", "Jingya Wang", "Ye Shi"], "title": "Sample from What You See: Visuomotor Policy Learning via Diffusion Bridge with Observation-Embedded Stochastic Differential Equation", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Imitation learning with diffusion models has advanced robotic control by capturing multi-modal action distributions. However, existing approaches typically treat observations as high-level conditioning inputs to the denoising network, rather than integrating them into the stochastic dynamics of the diffusion process itself. As a result, sampling must begin from random Gaussian noise, weakening the coupling between perception and control and often yielding suboptimal performance. We introduce BridgePolicy, a generative visuomotor policy that explicitly embeds observations within the stochastic differential equation via a diffusion-bridge formulation. By constructing an observation-informed trajectory, BridgePolicy enables sampling to start from a rich, informative prior rather than random noise, substantially improving precision and reliability in control. A key challenge is that classical diffusion bridges connect distributions with matched dimensionality, whereas robotic observations are heterogeneous and multi-modal and do not naturally align with the action space. To address this, we design a multi-modal fusion module and a semantic aligner that unify visual and state inputs and align observation and action representations, making the bridge applicable to heterogeneous robot data. Extensive experiments across 52 simulation tasks on three benchmarks and five real-world tasks demonstrate that BridgePolicy consistently outperforms state-of-the-art generative policies.", "AI": {"tldr": "BridgePolicy是一种新的视觉运动策略，通过扩散桥公式将观测直接嵌入随机微分方程，从信息丰富的先验开始采样而非随机噪声，显著提升了机器人控制的精度和可靠性。", "motivation": "现有模仿学习方法将观测作为去噪网络的高级条件输入，而非整合到扩散过程的随机动态中，导致采样必须从随机高斯噪声开始，削弱了感知与控制的耦合，性能次优。", "method": "提出BridgePolicy，设计多模态融合模块和语义对齐器，统一视觉和状态输入，对齐观测和动作表示，使扩散桥适用于异构机器人数据。", "result": "在3个基准测试的52个仿真任务和5个真实世界任务中，BridgePolicy始终优于最先进的生成策略。", "conclusion": "通过将观测嵌入扩散过程的随机动态，BridgePolicy实现了更紧密的感知-控制耦合，为机器人控制提供了更精确可靠的生成策略。"}}
{"id": "2512.06812", "pdf": "https://arxiv.org/pdf/2512.06812", "abs": "https://arxiv.org/abs/2512.06812", "authors": ["Tiago Rodrigues", "Carla Teixeira Lopes"], "title": "Large Language Model-Based Generation of Discharge Summaries", "categories": ["cs.CL"], "comment": "17 pages, 6 figures", "summary": "Discharge Summaries are documents written by medical professionals that detail a patient's visit to a care facility. They contain a wealth of information crucial for patient care, and automating their generation could significantly reduce the effort required from healthcare professionals, minimize errors, and ensure that critical patient information is easily accessible and actionable. In this work, we explore the use of five Large Language Models on this task, from open-source models (Mistral, Llama 2) to proprietary systems (GPT-3, GPT-4, Gemini 1.5 Pro), leveraging MIMIC-III summaries and notes. We evaluate them using exact-match, soft-overlap, and reference-free metrics. Our results show that proprietary models, particularly Gemini with one-shot prompting, outperformed others, producing summaries with the highest similarity to the gold-standard ones. Open-source models, while promising, especially Mistral after fine-tuning, lagged in performance, often struggling with hallucinations and repeated information. Human evaluation by a clinical expert confirmed the practical utility of the summaries generated by proprietary models. Despite the challenges, such as hallucinations and missing information, the findings suggest that LLMs, especially proprietary models, are promising candidates for automatic discharge summary generation as long as data privacy is ensured.", "AI": {"tldr": "本研究评估了5种大型语言模型（包括开源和专有模型）在自动生成出院小结任务上的表现，发现专有模型特别是Gemini在单样本提示下表现最佳，而开源模型虽有一定潜力但存在幻觉和重复信息的问题。", "motivation": "出院小结包含丰富的患者诊疗信息，自动化生成可减轻医护人员负担、减少错误并确保关键信息易于获取和使用。", "method": "使用MIMIC-III数据集，评估了Mistral、Llama 2、GPT-3、GPT-4和Gemini 1.5 Pro五种模型，采用精确匹配、软重叠和无参考指标进行评估，并由临床专家进行人工评估。", "result": "专有模型（特别是Gemini）在单样本提示下生成的摘要与金标准最相似，开源模型（尤其是微调后的Mistral）表现较差，存在幻觉和重复信息问题。人工评估确认专有模型生成的摘要具有实际应用价值。", "conclusion": "在确保数据隐私的前提下，大型语言模型（特别是专有模型）是自动生成出院小结的有力候选方案，尽管仍面临幻觉和缺失信息等挑战。"}}
{"id": "2512.07232", "pdf": "https://arxiv.org/pdf/2512.07232", "abs": "https://arxiv.org/abs/2512.07232", "authors": ["Wenlong Liu", "Jiahua Pan", "Xingyu Zhang", "Xinxin Gong", "Yang Ye", "Xujin Zhao", "Xin Wang", "Kent Wu", "Hua Xiang", "Houmin Yan", "Qingpeng Zhang"], "title": "Cross-platform Product Matching Based on Entity Alignment of Knowledge Graph with RAEA model", "categories": ["cs.AI"], "comment": "10 pages, 5 figures, published on World Wide Web", "summary": "Product matching aims to identify identical or similar products sold on different platforms. By building knowledge graphs (KGs), the product matching problem can be converted to the Entity Alignment (EA) task, which aims to discover the equivalent entities from diverse KGs. The existing EA methods inadequately utilize both attribute triples and relation triples simultaneously, especially the interactions between them. This paper introduces a two-stage pipeline consisting of rough filter and fine filter to match products from eBay and Amazon. For fine filtering, a new framework for Entity Alignment, Relation-aware and Attribute-aware Graph Attention Networks for Entity Alignment (RAEA), is employed. RAEA focuses on the interactions between attribute triples and relation triples, where the entity representation aggregates the alignment signals from attributes and relations with Attribute-aware Entity Encoder and Relation-aware Graph Attention Networks. The experimental results indicate that the RAEA model achieves significant improvements over 12 baselines on EA task in the cross-lingual dataset DBP15K (6.59% on average Hits@1) and delivers competitive results in the monolingual dataset DWY100K. The source code for experiments on DBP15K and DWY100K is available at github (https://github.com/Mockingjay-liu/RAEA-model-for-Entity-Alignment).", "AI": {"tldr": "该论文提出了一种两阶段产品匹配方法，使用RAEA框架通过结合属性三元组和关系三元组的交互信息，在知识图谱实体对齐任务中取得了显著改进效果", "motivation": "现有实体对齐方法未能充分利用属性三元组和关系三元组之间的交互信息，特别是在产品匹配场景中需要同时考虑这两种信息源", "method": "采用两阶段流水线（粗过滤+细过滤），提出RAEA框架，包含属性感知实体编码器和关系感知图注意力网络，通过聚合属性和关系的对齐信号来增强实体表示", "result": "在跨语言数据集DBP15K上相比12个基线方法平均Hits@1提升6.59%，在单语言数据集DWY100K上也取得了竞争性结果", "conclusion": "RAEA模型通过有效利用属性和关系三元组的交互信息，显著提升了实体对齐性能，为产品匹配等实际应用提供了有效解决方案"}}
{"id": "2512.06814", "pdf": "https://arxiv.org/pdf/2512.06814", "abs": "https://arxiv.org/abs/2512.06814", "authors": ["Dibyanayan Bandyopadhyay", "Soham Bhattacharjee", "Mohammed Hasanuzzaman", "Asif Ekbal"], "title": "CAuSE: Decoding Multimodal Classifiers using Faithful Natural Language Explanation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Transactions of the Association for Computational Linguistics (TACL). Pre-MIT Press publication version", "summary": "Multimodal classifiers function as opaque black box models. While several techniques exist to interpret their predictions, very few of them are as intuitive and accessible as natural language explanations (NLEs). To build trust, such explanations must faithfully capture the classifier's internal decision making behavior, a property known as faithfulness. In this paper, we propose CAuSE (Causal Abstraction under Simulated Explanations), a novel framework to generate faithful NLEs for any pretrained multimodal classifier. We demonstrate that CAuSE generalizes across datasets and models through extensive empirical evaluations. Theoretically, we show that CAuSE, trained via interchange intervention, forms a causal abstraction of the underlying classifier. We further validate this through a redesigned metric for measuring causal faithfulness in multimodal settings. CAuSE surpasses other methods on this metric, with qualitative analysis reinforcing its advantages. We perform detailed error analysis to pinpoint the failure cases of CAuSE. For replicability, we make the codes available at https://github.com/newcodevelop/CAuSE", "AI": {"tldr": "CAuSE是一个新颖的框架，通过因果抽象和交换干预技术为预训练多模态分类器生成忠实于模型内部决策过程的自然语言解释，在多个数据集和模型上展现出优越性能。", "motivation": "多模态分类器通常是黑盒模型，现有解释方法缺乏直观性和可访问性。为了建立信任，需要能够忠实捕捉分类器内部决策行为的自然语言解释。", "method": "提出CAuSE框架，通过交换干预训练来构建底层分类器的因果抽象，并重新设计了多模态环境下测量因果忠实度的指标。", "result": "CAuSE在新设计的因果忠实度指标上超越其他方法，通过广泛的实证评估证明其在不同数据集和模型上的泛化能力，定性分析也支持其优势。", "conclusion": "CAuSE通过因果抽象方法成功生成了忠实的多模态分类器解释，理论分析和实证结果都验证了其有效性，同时进行了详细的错误分析以识别失败案例。"}}
{"id": "2512.07314", "pdf": "https://arxiv.org/pdf/2512.07314", "abs": "https://arxiv.org/abs/2512.07314", "authors": ["Yuxiao Luo", "Songming Zhang", "Sijie Ruan", "Siran Chen", "Kang Liu", "Yang Xu", "Yu Zheng", "Ling Yin"], "title": "M-STAR: Multi-Scale Spatiotemporal Autoregression for Human Mobility Modeling", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Modeling human mobility is vital for extensive applications such as transportation planning and epidemic modeling. With the rise of the Artificial Intelligence Generated Content (AIGC) paradigm, recent works explore synthetic trajectory generation using autoregressive and diffusion models. While these methods show promise for generating single-day trajectories, they remain limited by inefficiencies in long-term generation (e.g., weekly trajectories) and a lack of explicit spatiotemporal multi-scale modeling. This study proposes Multi-Scale Spatio-Temporal AutoRegression (M-STAR), a new framework that generates long-term trajectories through a coarse-to-fine spatiotemporal prediction process. M-STAR combines a Multi-scale Spatiotemporal Tokenizer that encodes hierarchical mobility patterns with a Transformer-based decoder for next-scale autoregressive prediction. Experiments on two real-world datasets show that M-STAR outperforms existing methods in fidelity and significantly improves generation speed. The data and codes are available at https://github.com/YuxiaoLuo0013/M-STAR.", "AI": {"tldr": "提出M-STAR框架，通过多尺度时空自回归方法生成长期人类移动轨迹，在保真度和生成速度上优于现有方法", "motivation": "现有方法在生成长时间轨迹（如周轨迹）时效率低下，且缺乏显式的时空多尺度建模能力", "method": "结合多尺度时空标记器和基于Transformer的解码器，实现从粗到细的时空预测过程", "result": "在两个真实数据集上实验表明，M-STAR在保真度上优于现有方法，并显著提高生成速度", "conclusion": "M-STAR框架有效解决了长期轨迹生成的效率和建模问题，为人类移动建模提供了新方法"}}
{"id": "2512.06848", "pdf": "https://arxiv.org/pdf/2512.06848", "abs": "https://arxiv.org/abs/2512.06848", "authors": ["Sepyan Purnama Kristanto", "Lutfi Hakim", "Hermansyah"], "title": "AquaFusionNet: Lightweight VisionSensor Fusion Framework for Real-Time Pathogen Detection and Water Quality Anomaly Prediction on Edge Devices", "categories": ["cs.CL", "cs.CV"], "comment": "9Pages, 3 figure, Politeknik Negeri Banyuwangi", "summary": "Evidence from many low and middle income regions shows that microbial contamination in small scale drinking water systems often fluctuates rapidly, yet existing monitoring tools capture only fragments of this behaviour. Microscopic imaging provides organism level visibility, whereas physicochemical sensors reveal shortterm changes in water chemistry; in practice, operators must interpret these streams separately, making realtime decision-making unreliable. This study introduces AquaFusionNet, a lightweight cross-modal framework that unifies both information sources inside a single edge deployable model. Unlike prior work that treats microscopic detection and water quality prediction as independent tasks, AquaFusionNet learns the statistical dependencies between microbial appearance and concurrent sensor dynamics through a gated crossattention mechanism designed specifically for lowpower hardware. The framework is trained on AquaMicro12K, a new dataset comprising 12,846 annotated 1000 micrographs curated for drinking water contexts, an area where publicly accessible microscopic datasets are scarce. Deployed for six months across seven facilities in East Java, Indonesia, the system processed 1.84 million frames and consistently detected contamination events with 94.8% mAP@0.5 and 96.3% anomaly prediction accuracy, while operating at 4.8 W on a Jetson Nano. Comparative experiments against representative lightweight detectors show that AquaFusionNet provides higher accuracy at comparable or lower power, and field results indicate that cross-modal coupling reduces common failure modes of unimodal detectors, particularly under fouling, turbidity spikes, and inconsistent illumination. All models, data, and hardware designs are released openly to facilitate replication and adaptation in decentralized water safety infrastructures.", "AI": {"tldr": "AquaFusionNet是一个轻量级跨模态框架，通过融合微生物显微镜图像和水质传感器数据，在边缘设备上实现饮用水微生物污染的实时高精度检测，在印度尼西亚实地部署中表现出色。", "motivation": "现有饮用水监测工具只能分别处理微生物图像和水质传感器数据，无法捕捉微生物污染的快速波动变化，导致实时决策不可靠。", "method": "开发AquaFusionNet框架，使用门控跨注意力机制学习微生物外观与传感器动态之间的统计依赖关系，基于新的AquaMicro12K数据集（12,846张标注显微图像）进行训练。", "result": "在印度尼西亚东爪哇7个设施部署6个月，处理184万帧图像，实现94.8% mAP@0.5检测精度和96.3%异常预测准确率，功耗仅4.8W。", "conclusion": "跨模态耦合减少了单模态检测器的常见故障模式，特别是在污垢、浊度峰值和不一致照明条件下表现更优，所有模型和数据已开源以促进分散式水安全基础设施的应用。"}}
{"id": "2512.07355", "pdf": "https://arxiv.org/pdf/2512.07355", "abs": "https://arxiv.org/abs/2512.07355", "authors": ["Alexandre Rocchi--Henry", "Thomas Fel", "Gianni Franchi"], "title": "A Geometric Unification of Concept Learning with Concept Cones", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "22 pages", "summary": "Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\\footnote{We adopt the terminology of \\citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.", "AI": {"tldr": "该论文通过几何框架统一了监督式概念瓶颈模型(CBMs)和无监督稀疏自编码器(SAEs)的概念发现方法，提出了基于概念锥包含关系的评估指标，发现了最佳稀疏度和扩展因子配置。", "motivation": "概念解释性研究存在两个独立发展的传统：CBM使用人工标注的概念，SAE通过稀疏编码发现涌现概念。这两种方法虽然目标相似但缺乏统一的理论框架和评估标准。", "method": "提出几何框架，将CBM和SAE都视为在激活空间中学习线性方向形成概念锥的方法。建立包含关系评估指标，量化SAE学习的概念锥与CBM参考几何的近似程度。", "result": "发现了稀疏度和扩展因子的\"最佳点\"，能同时最大化与CBM概念的几何和语义对齐。提出了连接归纳偏置与概念涌现的定量指标。", "conclusion": "通过共享的几何框架统一了监督和无监督概念发现方法，为评估SAE进展和衡量发现概念与人类概念的对齐程度提供了原则性指标。"}}
{"id": "2512.06869", "pdf": "https://arxiv.org/pdf/2512.06869", "abs": "https://arxiv.org/abs/2512.06869", "authors": ["Wanyang Hong", "Zhaoning Zhang", "Yi Chen", "Libo Zhang", "Baihui Liu", "Linbo Qiao", "Zhiliang Tian", "Dongsheng Li"], "title": "Rhea: Role-aware Heuristic Episodic Attention for Conversational LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable performance on single-turn tasks, yet their effectiveness deteriorates in multi-turn conversations. We define this phenomenon as cumulative contextual decay - a progressive degradation of contextual integrity caused by attention pollution, dilution, and drift. To address this challenge, we propose Rhea (Role-aware Heuristic Episodic Attention), a novel framework that decouples conversation history into two functionally independent memory modules: (1) an Instructional Memory (IM) that persistently stores high-fidelity global constraints via a structural priority mechanism, and (2) an Episodic Memory (EM) that dynamically manages user-model interactions via asymmetric noise control and heuristic context retrieval. During inference, Rhea constructs a high signal-to-noise context by applying its priority attention: selectively integrating relevant episodic information while always prioritizing global instructions. To validate this approach, experiments on multiple multi-turn conversation benchmarks - including MT-Eval and Long-MT-Bench+ - show that Rhea mitigates performance decay and improves overall accuracy by 1.04 points on a 10-point scale (a 16% relative gain over strong baselines). Moreover, Rhea maintains near-perfect instruction fidelity (IAR > 8.1) across long-horizon interactions. These results demonstrate that Rhea provides a principled and effective framework for building more precise, instruction-consistent conversational LLMs.", "AI": {"tldr": "Rhea框架通过角色感知启发式情景注意力机制，将对话历史解耦为指令记忆和情景记忆两个模块，有效解决了多轮对话中累积上下文衰减问题，在多个基准测试中提升了16%的性能表现。", "motivation": "大型语言模型在单轮任务中表现优异，但在多轮对话中性能会因累积上下文衰减（注意力污染、稀释和漂移）而显著下降，需要新的框架来维持上下文完整性和指令一致性。", "method": "提出Rhea框架，包含两个独立内存模块：指令记忆（通过结构优先级机制持久存储高保真全局约束）和情景记忆（通过非对称噪声控制和启发式上下文检索动态管理用户-模型交互），在推理时应用优先级注意力构建高信噪比上下文。", "result": "在MT-Eval和Long-MT-Bench+等多个多轮对话基准测试中，Rhea将整体准确率提升了1.04分（10分制），相对基线模型提升16%，并在长程交互中保持接近完美的指令保真度（IAR > 8.1）。", "conclusion": "Rhea为解决多轮对话中的累积上下文衰减问题提供了一个有原则且有效的框架，能够构建更精确、指令一致的对话型大型语言模型。"}}
{"id": "2512.07436", "pdf": "https://arxiv.org/pdf/2512.07436", "abs": "https://arxiv.org/abs/2512.07436", "authors": ["Hang He", "Chuhuai Yue", "Chengqi Dong", "Mingxue Tian", "Zhenfeng Liu", "Jiajun Chai", "Xiaohan Wang", "Yufei Zhang", "Qun Liao", "Guojun Yin", "Wei Lin", "Chengcheng Wan", "Haiying Sun", "Ting Su"], "title": "LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in large reasoning models (LRMs) have enabled agentic search systems to perform complex multi-step reasoning across multiple sources. However, most studies focus on general information retrieval and rarely explores vertical domains with unique challenges. In this work, we focus on local life services and introduce LocalSearchBench, which encompass diverse and complex business scenarios. Real-world queries in this domain are often ambiguous and require multi-hop reasoning across merchants and products, remaining challenging and not fully addressed. As the first comprehensive benchmark for agentic search in local life services, LocalSearchBench includes over 150,000 high-quality entries from various cities and business types. We construct 300 multi-hop QA tasks based on real user queries, challenging agents to understand questions and retrieve information in multiple steps. We also developed LocalPlayground, a unified environment integrating multiple tools for agent interaction. Experiments show that even state-of-the-art LRMs struggle on LocalSearchBench: the best model (DeepSeek-V3.1) achieves only 34.34% correctness, and most models have issues with completeness (average 77.33%) and faithfulness (average 61.99%). This highlights the need for specialized benchmarks and domain-specific agent training in local life services. Code, Benchmark, and Leaderboard are available at localsearchbench.github.io.", "AI": {"tldr": "该论文提出了LocalSearchBench，这是首个针对本地生活服务的智能搜索基准测试，包含30万个高质量条目和300个多跳问答任务，测试显示即使最先进的推理模型在该领域表现也较差（正确率仅34.34%），突显了垂直领域专业基准测试的重要性。", "motivation": "现有大型推理模型主要关注通用信息检索，很少研究具有独特挑战的垂直领域。本地生活服务领域的查询往往模糊且需要跨商家和产品的多跳推理，这一挑战尚未得到充分解决。", "method": "构建LocalSearchBench基准，包含来自不同城市和业务类型的150,000个高质量条目，基于真实用户查询创建300个多跳问答任务。开发LocalPlayground统一环境，集成多种工具供智能体交互。", "result": "实验结果显示，即使最先进的LRMs在LocalSearchBench上表现不佳：最佳模型（DeepSeek-V3.1）正确率仅为34.34%，大多数模型在完整性（平均77.33%）和忠实度（平均61.99%）方面存在问题。", "conclusion": "本地生活服务领域需要专门的基准测试和领域特定的智能体训练，当前通用模型在该垂直领域的表现仍有很大提升空间。"}}
{"id": "2512.06874", "pdf": "https://arxiv.org/pdf/2512.06874", "abs": "https://arxiv.org/abs/2512.06874", "authors": ["Ziyun Yu", "Yiru Zhou", "Chen Zhao", "Hongyi Wen"], "title": "An Analysis of Large Language Models for Simulating User Responses in Surveys", "categories": ["cs.CL"], "comment": "Accepted to IJCNLP-AACL 2025 (Main Conference)", "summary": "Using Large Language Models (LLMs) to simulate user opinions has received growing attention. Yet LLMs, especially trained with reinforcement learning from human feedback (RLHF), are known to exhibit biases toward dominant viewpoints, raising concerns about their ability to represent users from diverse demographic and cultural backgrounds. In this work, we examine the extent to which LLMs can simulate human responses to cross-domain survey questions through direct prompting and chain-of-thought prompting. We further propose a claim diversification method CLAIMSIM, which elicits viewpoints from LLM parametric knowledge as contextual input. Experiments on the survey question answering task indicate that, while CLAIMSIM produces more diverse responses, both approaches struggle to accurately simulate users. Further analysis reveals two key limitations: (1) LLMs tend to maintain fixed viewpoints across varying demographic features, and generate single-perspective claims; and (2) when presented with conflicting claims, LLMs struggle to reason over nuanced differences among demographic features, limiting their ability to adapt responses to specific user profiles.", "AI": {"tldr": "本研究探讨了使用LLMs模拟用户观点的局限性，发现即使采用CLAIMSIM方法增加回答多样性，LLMs仍难以准确模拟不同背景用户的观点，主要存在观点固定和无法处理人口特征细微差异的问题。", "motivation": "LLMs在模拟用户观点时存在对主流观点的偏见，难以代表不同人口统计和文化背景的用户，需要评估其在跨领域调查问题中模拟人类响应的能力。", "method": "通过直接提示和思维链提示评估LLMs模拟能力，并提出CLAIMSIM方法从LLMs参数知识中提取观点作为上下文输入以增加回答多样性。", "result": "CLAIMSIM能产生更多样化的回答，但两种方法都无法准确模拟用户。LLMs存在观点固定和无法处理人口特征细微差异的局限性。", "conclusion": "LLMs在模拟多样化用户观点方面存在根本性限制，需要开发更先进的方法来准确捕捉不同人口统计群体的观点差异。"}}
{"id": "2512.07497", "pdf": "https://arxiv.org/pdf/2512.07497", "abs": "https://arxiv.org/abs/2512.07497", "authors": ["JV Roig"], "title": "How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations", "categories": ["cs.AI", "cs.SE"], "comment": "48 pages, 3 tables, 2 listings", "summary": "We investigate how large language models (LLMs) fail when operating as autonomous agents with tool-use capabilities. Using the Kamiwaza Agentic Merit Index (KAMI) v0.1 benchmark, we analyze 900 execution traces from three representative models - Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1 - across filesystem, text extraction, CSV analysis, and SQL scenarios. Rather than focusing on aggregate scores, we perform fine-grained, per-trial behavioral analysis to surface the strategies that enable successful multi-step tool execution and the recurrent failure modes that undermine reliability. Our findings show that model scale alone does not predict agentic robustness: Llama 4 Maverick (400B) performs only marginally better than Granite 4 Small (32B) in some uncertainty-driven tasks, while DeepSeek V3.1's superior reliability derives primarily from post-training reinforcement learning rather than architecture or size. Across models, we identify four recurring failure archetypes: premature action without grounding, over-helpfulness that substitutes missing entities, vulnerability to distractor-induced context pollution, and fragile execution under load. These patterns highlight the need for agentic evaluation methods that emphasize interactive grounding, recovery behavior, and environment-aware adaptation, suggesting that reliable enterprise deployment requires not just stronger models but deliberate training and design choices that reinforce verification, constraint discovery, and adherence to source-of-truth data.", "AI": {"tldr": "该研究通过KAMI基准测试分析了三个大型语言模型作为自主代理使用工具时的失败模式，发现模型规模不是决定代理可靠性的关键因素，而是揭示了四种常见失败类型和强化学习对可靠性的重要性。", "motivation": "研究大型语言模型作为自主代理使用工具时的失败原因，而非仅仅关注聚合分数，旨在深入理解多步工具执行中的成功策略和重复失败模式。", "method": "使用KAMI v0.1基准测试，分析900个执行轨迹，涵盖三个代表性模型（Granite 4 Small、Llama 4 Maverick、DeepSeek V3.1）在文件系统、文本提取、CSV分析和SQL场景中的表现，进行细粒度的逐次试验行为分析。", "result": "发现模型规模不能单独预测代理稳健性；Llama 4 Maverick在不确定性任务中仅略优于Granite 4 Small；DeepSeek V3.1的优越可靠性主要来自训练后强化学习；识别出四种重复失败模式：无基础前提早行动、过度帮助替代缺失实体、干扰诱导的上下文污染脆弱性、负载下的脆弱执行。", "conclusion": "可靠的企业部署不仅需要更强的模型，还需要有意的训练和设计选择，强调交互基础、恢复行为和环境感知适应，需要加强验证、约束发现和对真实数据源的遵守。"}}
{"id": "2512.06919", "pdf": "https://arxiv.org/pdf/2512.06919", "abs": "https://arxiv.org/abs/2512.06919", "authors": ["Francois Vandenhende", "Anna Georgiou", "Michalis Georgiou", "Theodoros Psaras", "Ellie Karekla"], "title": "Automated PRO-CTCAE Symptom Selection based on Prior Adverse Event Profiles", "categories": ["cs.CL"], "comment": "13 pages, 2 figures", "summary": "The PRO-CTCAE is an NCI-developed patient-reported outcome system for capturing symptomatic adverse events in oncology trials. It comprises a large library drawn from the CTCAE vocabulary, and item selection for a given trial is typically guided by expected toxicity profiles from prior data. Selecting too many PRO-CTCAE items can burden patients and reduce compliance, while too few may miss important safety signals. We present an automated method to select a minimal yet comprehensive PRO-CTCAE subset based on historical safety data. Each candidate PRO-CTCAE symptom term is first mapped to its corresponding MedDRA Preferred Terms (PTs), which are then encoded into Safeterm, a high-dimensional semantic space capturing clinical and contextual diversity in MedDRA terminology. We score each candidate PRO item for relevance to the historical list of adverse event PTs and combine relevance and incidence into a utility function. Spectral analysis is then applied to the combined utility and diversity matrix to identify an orthogonal set of medical concepts that balances relevance and diversity. Symptoms are rank-ordered by importance, and a cut-off is suggested based on the explained information. The tool is implemented as part of the Safeterm trial-safety app. We evaluate its performance using simulations and oncology case studies in which PRO-CTCAE was employed. This automated approach can streamline PRO-CTCAE design by leveraging MedDRA semantics and historical data, providing an objective and reproducible method to balance signal coverage against patient burden.", "AI": {"tldr": "开发了一种基于历史安全数据的自动化方法，用于选择最小但全面的PRO-CTCAE症状项子集，以平衡临床试验中的安全信号捕获和患者负担", "motivation": "PRO-CTCAE系统包含大量症状项，但过多项目会增加患者负担并降低依从性，而过少项目可能遗漏重要安全信号，需要一种客观的方法来选择最优子集", "method": "将PRO-CTCAE症状项映射到MedDRA术语，编码到Safeterm语义空间，结合相关性和发生率计算效用函数，通过谱分析选择正交的医学概念集，按重要性排序并确定截断点", "result": "该方法已集成到Safeterm试验安全应用中，通过模拟和肿瘤学案例研究进行了评估", "conclusion": "这种自动化方法利用MedDRA语义和历史数据，为PRO-CTCAE设计提供了客观且可重复的方法，能够平衡信号覆盖范围和患者负担"}}
{"id": "2512.07611", "pdf": "https://arxiv.org/pdf/2512.07611", "abs": "https://arxiv.org/abs/2512.07611", "authors": ["Yongsheng Lian"], "title": "Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.\n  Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.", "AI": {"tldr": "本研究系统比较了三种强化学习算法（PPO、GRPO、DAPO）在提升大语言模型复杂推理能力方面的效果，通过控制性迁移学习评估发现RL训练模型在所有任务上都优于基线模型，并提供了RL训练的参数化实践指导。", "motivation": "探索不同强化学习算法如何有效提升大语言模型的复杂推理能力，为RL-based LLM训练提供系统性的实证分析和实践指导。", "method": "采用控制性迁移学习方法：先在专门的Countdown Game上进行微调，然后在通用推理基准测试套件上进行评估；对GRPO和DAPO算法进行了参数分析（组大小、KL惩罚系数、动态采样组件）。", "result": "所有RL训练模型在各项任务上都优于对应的基线模型；增大GRPO和DAPO的组大小能带来更稳定的训练动态和更高准确率；KL惩罚系数的影响是非单调的；DAPO的动态采样组件并未提升性能，禁用时反而获得最佳效果。", "conclusion": "强化学习能有效提升LLM的推理能力，但算法组件的效果需要仔细验证；参数选择对训练效果有重要影响，为RL-based LLM训练提供了实用的参数配置指导。"}}
{"id": "2512.06922", "pdf": "https://arxiv.org/pdf/2512.06922", "abs": "https://arxiv.org/abs/2512.06922", "authors": ["George Mikros"], "title": "Large Language Models and Forensic Linguistics: Navigating Opportunities and Threats in the Age of Generative AI", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) present a dual challenge for forensic linguistics. They serve as powerful analytical tools enabling scalable corpus analysis and embedding-based authorship attribution, while simultaneously destabilising foundational assumptions about idiolect through style mimicry, authorship obfuscation, and the proliferation of synthetic texts. Recent stylometric research indicates that LLMs can approximate surface stylistic features yet exhibit detectable differences from human writers, a tension with significant forensic implications. However, current AI-text detection techniques, whether classifier-based, stylometric, or watermarking approaches, face substantial limitations: high false positive rates for non-native English writers and vulnerability to adversarial strategies such as homoglyph substitution. These uncertainties raise concerns under legal admissibility standards, particularly the Daubert and Kumho Tire frameworks. The article concludes that forensic linguistics requires methodological reconfiguration to remain scientifically credible and legally admissible. Proposed adaptations include hybrid human-AI workflows, explainable detection paradigms beyond binary classification, and validation regimes measuring error and bias across diverse populations. The discipline's core insight, i.e., that language reveals information about its producer, remains valid but must accommodate increasingly complex chains of human and machine authorship.", "AI": {"tldr": "本文探讨了大型语言模型对法庭语言学的双重挑战：既是强大的分析工具，又通过风格模仿和作者身份混淆破坏了语体学的基本假设，需要方法论重构来保持科学可信度和法律可采性。", "motivation": "LLMs作为分析工具和文本生成器的双重角色对法庭语言学构成挑战，破坏了语体学的基本假设，同时现有的AI文本检测技术存在高误报率和对抗策略脆弱性等问题。", "method": "文章分析了当前AI文本检测技术的局限性（分类器、风格计量学和水印方法），并提出了方法论重构方案，包括人机混合工作流程、可解释的检测范式和跨人群验证机制。", "result": "研究发现LLMs能够近似表面风格特征但与人类作者存在可检测差异，当前检测技术存在严重局限，需要新的方法论来应对法律可采性标准。", "conclusion": "法庭语言学需要进行方法论重构，采用混合人机工作流程、超越二元分类的可解释检测范式，以及测量不同人群错误和偏见的验证机制，以适应日益复杂的人机作者身份链。"}}
{"id": "2512.07631", "pdf": "https://arxiv.org/pdf/2512.07631", "abs": "https://arxiv.org/abs/2512.07631", "authors": ["Shahar Lutati"], "title": "The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds", "categories": ["cs.AI", "cs.CC", "cs.IT", "cs.LG"], "comment": null, "summary": "When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\\Itotal$ bits to identify a solution and gains $\\Istep$ bits per action at cost $\\Cstep$, yielding an effective cost $\\Ceff = (\\Itotal/\\Istep), \\Cstep$ that predicts resource requirements before search. We prove that $\\Ceff$ lower-bounds expected cost and provide tight probabilistic upper bounds. Experimental validation shows that ACP predictions closely track actual agent performance, consistently bounding search effort while improving efficiency over greedy and random strategies. The framework generalizes across LLM-based and agentic workflows, linking principles from active learning, Bayesian optimization, and reinforcement learning through a unified information-theoretic lens. \\", "AI": {"tldr": "论文提出了Agent Capability Problem (ACP)框架，通过信息论方法预测智能体在资源约束下解决问题的能力，将问题解决建模为信息获取过程，建立了资源需求的理论预测模型。", "motivation": "解决自主智能体在何时应该为任务投入资源的问题，避免依赖经验启发式方法，提供理论框架来预测智能体在资源约束下能否解决问题。", "method": "提出ACP框架，将问题解决视为信息获取过程：智能体需要I_total比特来识别解决方案，每步动作获得I_step比特信息，代价为C_step，从而建立有效成本C_eff = (I_total/I_step)*C_step来预测资源需求。", "result": "证明了C_eff是期望成本的下界，并提供了紧致的概率上界。实验验证显示ACP预测与实际智能体性能高度吻合，在限制搜索努力的同时提高了贪婪和随机策略的效率。", "conclusion": "ACP框架可泛化到基于LLM和智能体工作流，通过统一的信息论视角连接了主动学习、贝叶斯优化和强化学习的原理，为资源约束下的智能体能力预测提供了理论基础。"}}
{"id": "2512.06924", "pdf": "https://arxiv.org/pdf/2512.06924", "abs": "https://arxiv.org/abs/2512.06924", "authors": ["Milad Alshomary", "Anisha Bhatnagar", "Peter Zeng", "Smaranda Muresan", "Owen Rambow", "Kathleen McKeown"], "title": "XAM: Interactive Explainability for Authorship Attribution Models", "categories": ["cs.CL"], "comment": null, "summary": "We present IXAM, an Interactive eXplainability framework for Authorship Attribution Models. Given an authorship attribution (AA) task and an embedding-based AA model, our tool enables users to interactively explore the model's embedding space and construct an explanation of the model's prediction as a set of writing style features at different levels of granularity. Through a user evaluation, we demonstrate the value of our framework compared to predefined stylistic explanations.", "AI": {"tldr": "IXAM是一个交互式可解释性框架，用于作者归属模型的可视化分析和特征解释", "motivation": "现有作者归属模型缺乏交互式的可解释性工具，用户无法深入理解模型预测的依据和写作风格特征", "method": "开发交互式框架，让用户能够探索模型嵌入空间，并在不同粒度级别构建写作风格特征的解释", "result": "通过用户评估证明，该框架相比预定义的风格解释具有更高的价值", "conclusion": "IXAM框架为作者归属模型提供了有效的交互式可解释性解决方案，有助于用户更好地理解模型决策过程"}}
{"id": "2512.07710", "pdf": "https://arxiv.org/pdf/2512.07710", "abs": "https://arxiv.org/abs/2512.07710", "authors": ["Anxiang Zeng", "Haibo Zhang", "Hailing Zhang", "Kaixiang Mo", "Liang Yao", "Ling Hu", "Long Zhang", "Shuman Liu", "Shuyi Xie", "Yanshi Li", "Yizhang Chen", "Yuepeng Sheng", "Yuwei Huang", "Zhaochen Xu", "Zhiqiang Zhou", "Ziqin Liew"], "title": "Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.", "AI": {"tldr": "CompassMax-V3-Thinking是一个千亿规模的MoE推理模型，采用新的RL框架训练，核心原则是确保每个提示都有效。通过多项创新技术解决了大规模RL训练中的效率问题，实现了稳定高效的训练。", "motivation": "在大规模MoE模型上进行RL训练时面临多个关键效率问题：零方差提示浪费rollout资源、长时域重要性采样不稳定、标准奖励模型导致的优势反转，以及rollout处理的系统性瓶颈。", "method": "提出四项统一创新：(1)多阶段零方差消除技术过滤无效提示；(2)ESPO熵自适应优化方法平衡标记级和序列级重要性采样；(3)Router Replay策略对齐训练与推理时MoE路由行为；(4)高性能RL系统采用FP8精度rollout、重叠奖励计算和长度感知调度。", "result": "构建了一个连贯的pipeline，使千亿规模MoE模型的RL训练变得稳定高效。模型在内部和公共评估中都表现出强劲性能。", "conclusion": "通过系统性技术创新解决了大规模MoE模型RL训练的关键挑战，为超大规模模型的强化学习训练提供了有效解决方案，证明了该方法的可行性和优越性。"}}
{"id": "2512.06938", "pdf": "https://arxiv.org/pdf/2512.06938", "abs": "https://arxiv.org/abs/2512.06938", "authors": ["Ivanhoé Botcazou", "Tassadit Amghar", "Sylvain Lamprier", "Frédéric Saubion"], "title": "Progress Ratio Embeddings: An Impatience Signal for Robust Length Control in Neural Text Generation", "categories": ["cs.CL"], "comment": null, "summary": "Modern neural language models achieve high accuracy in text generation, yet precise control over generation length remains underdeveloped. In this paper, we first investigate a recent length control method based on Reverse Positional Embeddings (RPE) and show its limits when control is requested beyond the training distribution. In particular, using a discrete countdown signal tied to the absolute remaining token count leads to instability. To provide robust length control, we introduce Progress Ratio Embeddings (PRE), as continuous embeddings tied to a trigonometric impatience signal. PRE integrates seamlessly into standard Transformer architectures, providing stable length fidelity without degrading text accuracy under standard evaluation metrics. We further show that PRE generalizes well to unseen target lengths. Experiments on two widely used news-summarization benchmarks validate these findings.", "AI": {"tldr": "论文提出Progress Ratio Embeddings (PRE)方法，通过连续三角函数信号实现稳定的文本生成长度控制，解决了现有方法在超出训练分布时的长度控制不稳定问题。", "motivation": "现有基于反向位置嵌入(RPE)的长度控制方法在超出训练分布时存在不稳定性问题，特别是使用绝对剩余token计数的离散倒计时信号会导致控制失效。", "method": "引入Progress Ratio Embeddings (PRE)方法，使用连续的三角函数不耐烦信号作为嵌入，可无缝集成到标准Transformer架构中。", "result": "PRE方法在保持标准评估指标下文本准确性的同时，提供了稳定的长度保真度，并能很好地泛化到未见过的目标长度。在两个新闻摘要基准测试中验证了这些发现。", "conclusion": "PRE方法为神经语言模型提供了鲁棒的长度控制能力，解决了现有方法在长度控制方面的局限性，特别是在超出训练分布时的稳定性问题。"}}
{"id": "2512.07761", "pdf": "https://arxiv.org/pdf/2512.07761", "abs": "https://arxiv.org/abs/2512.07761", "authors": ["Xiqiao Xiong", "Ouxiang Li", "Zhuo Liu", "Moxin Li", "Wentao Shi", "Fuli Feng", "Xiangnan He"], "title": "RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models", "categories": ["cs.AI", "cs.LG"], "comment": "19 pages, 15 figures", "summary": "Large language models are vulnerable to jailbreak attacks, threatening their safe deployment in real-world applications. This paper studies black-box multi-turn jailbreaks, aiming to train attacker LLMs to elicit harmful content from black-box models through a sequence of prompt-output interactions. Existing approaches typically rely on single turn optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate the problem as a multi-turn reinforcement learning task, directly optimizing the harmfulness of the final-turn output as the outcome reward. To mitigate sparse supervision and promote long-term attack strategies, we propose two heuristic process rewards: (1) controlling the harmfulness of intermediate outputs to prevent triggering the black-box model's rejection mechanisms, and (2) maintaining the semantic relevance of intermediate outputs to avoid drifting into irrelevant content. Experimental results on multiple benchmarks show consistently improved attack success rates across multiple models, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/RL-MTJail. Warning: This paper contains examples of harmful content.", "AI": {"tldr": "本文提出基于强化学习的多轮越狱攻击方法，通过优化最终轮输出的有害性作为奖励，并结合中间输出的控制机制，显著提高了对黑盒大语言模型的攻击成功率。", "motivation": "现有的大语言模型容易受到越狱攻击威胁，而现有的单轮优化方法无法有效学习长期攻击策略，需要解决多轮攻击中的稀疏监督问题。", "method": "将多轮越狱攻击建模为强化学习任务，直接优化最终轮输出的有害性作为结果奖励，并引入两个启发式过程奖励：控制中间输出的有害性以避免触发模型的拒绝机制，保持中间输出的语义相关性以防止内容漂移。", "result": "在多个基准测试上的实验结果表明，该方法在多个模型上持续提高了攻击成功率，证明了方法的有效性。", "conclusion": "该研究提出的多轮强化学习越狱方法通过结合结果奖励和过程奖励，有效解决了多轮攻击中的挑战，为大语言模型的安全部署提供了重要的攻防研究价值。"}}
{"id": "2512.06991", "pdf": "https://arxiv.org/pdf/2512.06991", "abs": "https://arxiv.org/abs/2512.06991", "authors": ["Jing Jie Tan", "Ban-Hoe Kwan", "Danny Wee-Kiat Ng", "Yan-Chai Hum", "Anissa Mokraoui", "Shih-Yu Lo"], "title": "Prompting-in-a-Series: Psychology-Informed Contents and Embeddings for Personality Recognition With Decoder-Only Models", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks. This research introduces a novel \"Prompting-in-a-Series\" algorithm, termed PICEPR (Psychology-Informed Contents Embeddings for Personality Recognition), featuring two pipelines: (a) Contents and (b) Embeddings. The approach demonstrates how a modularised decoder-only LLM can summarize or generate content, which can aid in classifying or enhancing personality recognition functions as a personality feature extractor and a generator for personality-rich content. We conducted various experiments to provide evidence to justify the rationale behind the PICEPR algorithm. Meanwhile, we also explored closed-source models such as \\textit{gpt4o} from OpenAI and \\textit{gemini} from Google, along with open-source models like \\textit{mistral} from Mistral AI, to compare the quality of the generated content. The PICEPR algorithm has achieved a new state-of-the-art performance for personality recognition by 5-15\\% improvement. The work repository and models' weight can be found at https://research.jingjietan.com/?q=PICEPR.", "AI": {"tldr": "PICEPR算法通过心理学信息内容嵌入实现人格识别，采用两个管道（内容和嵌入）的模块化解码器LLM，在人格识别任务上实现了5-15%的性能提升，达到新的最先进水平。", "motivation": "大型语言模型在自然语言处理任务中表现出色，但需要开发更有效的人格识别方法，利用LLM的生成和分类能力来提升人格特征提取和内容生成。", "method": "提出PICEPR算法，包含内容和嵌入两个管道，使用模块化解码器LLM进行内容总结和生成，同时比较了OpenAI的gpt4o、Google的gemini和Mistral AI的mistral等开源和闭源模型。", "result": "PICEPR算法在人格识别任务上实现了5-15%的性能改进，达到了新的最先进性能水平。", "conclusion": "PICEPR算法通过心理学信息内容嵌入有效提升了人格识别性能，证明了模块化LLM在人格特征提取和内容生成方面的价值，相关工作已开源。"}}
{"id": "2512.07795", "pdf": "https://arxiv.org/pdf/2512.07795", "abs": "https://arxiv.org/abs/2512.07795", "authors": ["Nearchos Potamitis", "Lars Klein", "Akhil Arora"], "title": "ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "11 pages, 3 tables, 4 figures", "summary": "Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .", "AI": {"tldr": "ReasonBENCH是首个专门评估大语言模型推理不稳定性的基准测试，通过多轮运行协议提供统计可靠的性能和成本指标，揭示当前推理方法存在的高不稳定性问题。", "motivation": "当前LLM推理评估只关注单次运行准确率，忽略了随机解码带来的内在不确定性，无法可靠评估方法的稳定性、可重现性和成本一致性。", "method": "开发ReasonBENCH基准测试，包括：(1)模块化评估库标准化推理框架、模型和任务；(2)多轮运行协议报告质量和成本的统计可靠指标；(3)公开排行榜鼓励方差感知报告。", "result": "发现绝大多数推理策略和模型表现出高度不稳定性，即使平均性能相似的策略置信区间宽度差异可达4倍，性能最佳的方法通常成本更高且更不稳定。", "conclusion": "可重现性是可靠LLM推理的关键维度，ReasonBENCH为未来推理方法和不确定性量化技术提供了基础，强调了在评估中考虑不确定性的重要性。"}}
{"id": "2512.07015", "pdf": "https://arxiv.org/pdf/2512.07015", "abs": "https://arxiv.org/abs/2512.07015", "authors": ["Mayank Ravishankara"], "title": "FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems have significantly reduced hallucinations in Large Language Models (LLMs) by grounding responses in external context. However, standard RAG architectures suffer from a critical vulnerability: Retrieval Sycophancy. When presented with a query based on a false premise or a common misconception, vector-based retrievers tend to fetch documents that align with the user's bias rather than objective truth, leading the model to \"hallucinate with citations.\"\n  In this work, we introduce Falsification-Verification Alignment RAG (FVA-RAG), a framework that shifts the retrieval paradigm from Inductive Verification (seeking support) to Deductive Falsification (seeking disproof). Unlike existing \"Self-Correction\" methods that rely on internal consistency, FVA-RAG deploys a distinct Adversarial Retrieval Policy that actively generates \"Kill Queries\"-targeted search terms designed to surface contradictory evidence. We introduce a dual-verification mechanism that explicitly weighs the draft answer against this \"Anti-Context.\" Preliminary experiments on a dataset of common misconceptions demonstrate that FVA-RAG significantly improves robustness against sycophantic hallucinations compared to standard RAG baselines, effectively acting as an inference-time \"Red Team\" for factual generation.", "AI": {"tldr": "FVA-RAG是一种新的检索增强生成框架，通过从归纳验证转向演绎证伪的方式，主动检索矛盾证据来对抗检索谄媚问题，显著减少基于错误前提查询的幻觉现象。", "motivation": "标准RAG系统存在检索谄媚漏洞，当查询基于错误前提时，向量检索器倾向于返回与用户偏见一致而非客观事实的文档，导致模型产生带有引用的幻觉。", "method": "提出FVA-RAG框架，采用对抗性检索策略生成'击杀查询'来寻找矛盾证据，并通过双重验证机制将草稿答案与'反上下文'进行权衡比较。", "result": "在常见误解数据集上的初步实验表明，FVA-RAG相比标准RAG基线显著提高了对抗谄媚幻觉的鲁棒性。", "conclusion": "FVA-RAG作为事实生成时的推理阶段'红队'方法，通过主动寻找证伪证据有效解决了RAG系统的检索谄媚问题。"}}
{"id": "2512.07796", "pdf": "https://arxiv.org/pdf/2512.07796", "abs": "https://arxiv.org/abs/2512.07796", "authors": ["Sridhar Mahadevan"], "title": "Large Causal Models from Large Language Models", "categories": ["cs.AI"], "comment": "29 pages", "summary": "We introduce a new paradigm for building large causal models (LCMs) that exploits the enormous potential latent in today's large language models (LLMs). We describe our ongoing experiments with an implemented system called DEMOCRITUS (Decentralized Extraction of Manifold Ontologies of Causal Relations Integrating Topos Universal Slices) aimed at building, organizing, and visualizing LCMs that span disparate domains extracted from carefully targeted textual queries to LLMs. DEMOCRITUS is methodologically distinct from traditional narrow domain and hypothesis centered causal inference that builds causal models from experiments that produce numerical data. A high-quality LLM is used to propose topics, generate causal questions, and extract plausible causal statements from a diverse range of domains. The technical challenge is then to take these isolated, fragmented, potentially ambiguous and possibly conflicting causal claims, and weave them into a coherent whole, converting them into relational causal triples and embedding them into a LCM. Addressing this technical challenge required inventing new categorical machine learning methods, which we can only briefly summarize in this paper, as it is focused more on the systems side of building DEMOCRITUS. We describe the implementation pipeline for DEMOCRITUS comprising of six modules, examine its computational cost profile to determine where the current bottlenecks in scaling the system to larger models. We describe the results of using DEMOCRITUS over a wide range of domains, spanning archaeology, biology, climate change, economics, medicine and technology. We discuss the limitations of the current DEMOCRITUS system, and outline directions for extending its capabilities.", "AI": {"tldr": "DEMOCRITUS是一个利用大型语言模型构建大规模因果模型的新范式，通过提取多领域文本中的因果关系并整合成统一模型", "motivation": "传统因果推断方法局限于特定领域和假设驱动的数值数据，需要一种能从广泛文本数据中自动提取和组织跨领域因果知识的新方法", "method": "使用高质量LLM提出主题、生成因果问题、提取因果陈述，然后通过新的范畴机器学习方法将这些碎片化因果声明整合成关系三元组并嵌入到大型因果模型中", "result": "开发了包含六个模块的DEMOCRITUS实现管道，在考古学、生物学、气候变化、经济学、医学和技术等多个领域进行了广泛测试", "conclusion": "DEMOCRITUS展示了利用LLMs构建跨领域大规模因果模型的可行性，但当前系统存在扩展瓶颈，需要进一步扩展能力"}}
{"id": "2512.07059", "pdf": "https://arxiv.org/pdf/2512.07059", "abs": "https://arxiv.org/abs/2512.07059", "authors": ["Richard Young"], "title": "Replicating TEMPEST at Scale: Multi-Turn Adversarial Attacks Against Trillion-Parameter Frontier Models", "categories": ["cs.CL"], "comment": "30 pages, 11 figures, 5 tables. Code and data: https://github.com/ricyoung/tempest-replication", "summary": "Despite substantial investment in safety alignment, the vulnerability of large language models to sophisticated multi-turn adversarial attacks remains poorly characterized, and whether model scale or inference mode affects robustness is unknown. This study employed the TEMPEST multi-turn attack framework to evaluate ten frontier models from eight vendors across 1,000 harmful behaviors, generating over 97,000 API queries across adversarial conversations with automated evaluation by independent safety classifiers. Results demonstrated a spectrum of vulnerability: six models achieved 96% to 100% attack success rate (ASR), while four showed meaningful resistance, with ASR ranging from 42% to 78%; enabling extended reasoning on identical architecture reduced ASR from 97% to 42%. These findings indicate that safety alignment quality varies substantially across vendors, that model scale does not predict adversarial robustness, and that thinking mode provides a deployable safety enhancement. Collectively, this work establishes that current alignment techniques remain fundamentally vulnerable to adaptive multi-turn attacks regardless of model scale, while identifying deliberative inference as a promising defense direction.", "AI": {"tldr": "该研究发现当前大型语言模型的安全对齐技术对多轮对抗攻击存在根本性脆弱性，不同厂商模型的安全防护质量差异显著，模型规模不能预测鲁棒性，而启用扩展推理模式可显著提升安全性。", "motivation": "尽管在安全对齐方面投入了大量资源，但大型语言模型对复杂多轮对抗攻击的脆弱性仍未得到充分表征，且模型规模或推理模式是否影响鲁棒性尚不清楚。", "method": "使用TEMPEST多轮攻击框架评估8个厂商的10个前沿模型，针对1000种有害行为生成超过97,000个API查询，通过独立安全分类器进行自动化评估。", "result": "结果显示：6个模型的攻击成功率(ASR)达到96%-100%，4个模型表现出有意义的抵抗能力(ASR 42%-78%)；在相同架构上启用扩展推理模式可将ASR从97%降至42%。", "conclusion": "当前对齐技术无论模型规模如何都对自适应多轮攻击存在根本性脆弱性，而审慎推理模式是一个有前景的防御方向，安全对齐质量在不同厂商间差异显著，模型规模不能预测对抗鲁棒性。"}}
{"id": "2512.07810", "pdf": "https://arxiv.org/pdf/2512.07810", "abs": "https://arxiv.org/abs/2512.07810", "authors": ["Jordan Taylor", "Sid Black", "Dillon Bowen", "Thomas Read", "Satvik Golechha", "Alex Zelenka-Martin", "Oliver Makins", "Connor Kissane", "Kola Ayonrinde", "Jacob Merizian", "Samuel Marks", "Chris Cundy", "Joseph Bloom"], "title": "Auditing Games for Sandbagging", "categories": ["cs.AI"], "comment": "77 pages (28 non-appendix pages), 38 figures", "summary": "Future AI systems could conceal their capabilities ('sandbagging') during evaluations, potentially misleading developers and auditors. We stress-tested sandbagging detection techniques using an auditing game. First, a red team fine-tuned five models, some of which conditionally underperformed, as a proxy for sandbagging. Second, a blue team used black-box, model-internals, or training-based approaches to identify sandbagging models. We found that the blue team could not reliably discriminate sandbaggers from benign models. Black-box approaches were defeated by effective imitation of a weaker model. Linear probes, a model-internals approach, showed more promise but their naive application was vulnerable to behaviours instilled by the red team. We also explored capability elicitation as a strategy for detecting sandbagging. Although Prompt-based elicitation was not reliable, training-based elicitation consistently elicited full performance from the sandbagging models, using only a single correct demonstration of the evaluation task. However the performance of benign models was sometimes also raised, so relying on elicitation as a detection strategy was prone to false-positives. In the short-term, we recommend developers remove potential sandbagging using on-distribution training for elicitation. In the longer-term, further research is needed to ensure the efficacy of training-based elicitation, and develop robust methods for sandbagging detection. We open source our model organisms at https://github.com/AI-Safety-Institute/sandbagging_auditing_games and select transcripts and results at https://huggingface.co/datasets/sandbagging-games/evaluation_logs . A demo illustrating the game can be played at https://sandbagging-demo.far.ai/ .", "AI": {"tldr": "论文研究了AI系统可能在评估中隐藏能力（'sandbagging'）的问题，通过红蓝对抗游戏测试了检测技术，发现现有方法难以可靠识别sandbagging模型，但基于训练的能力激发方法有一定效果。", "motivation": "未来AI系统可能在评估中故意隐藏其真实能力，误导开发者和审计人员，这需要有效的检测方法来确保AI系统的透明性和安全性。", "method": "使用红蓝对抗游戏：红队微调五个模型（部分有条件地表现不佳），蓝队使用黑盒、模型内部或基于训练的方法来识别sandbagging模型。", "result": "蓝队无法可靠区分sandbagging模型和良性模型；黑盒方法被有效模仿较弱模型的行为击败；线性探针（模型内部方法）更有前景但易受红队行为影响；基于训练的能力激发能持续从sandbagging模型中激发完整性能，但也会提升良性模型性能导致假阳性。", "conclusion": "短期内建议开发者使用分布内训练移除潜在sandbagging；长期需要进一步研究确保基于训练的能力激发的有效性，并开发更鲁棒的sandbagging检测方法。"}}
{"id": "2512.07068", "pdf": "https://arxiv.org/pdf/2512.07068", "abs": "https://arxiv.org/abs/2512.07068", "authors": ["Emma Markle", "Javier Gutierrez Bach", "Shira Wein"], "title": "SETUP: Sentence-level English-To-Uniform Meaning Representation Parser", "categories": ["cs.CL"], "comment": null, "summary": "Uniform Meaning Representation (UMR) is a novel graph-based semantic representation which captures the core meaning of a text, with flexibility incorporated into the annotation schema such that the breadth of the world's languages can be annotated (including low-resource languages). While UMR shows promise in enabling language documentation, improving low-resource language technologies, and adding interpretability, the downstream applications of UMR can only be fully explored when text-to-UMR parsers enable the automatic large-scale production of accurate UMR graphs at test time. Prior work on text-to-UMR parsing is limited to date. In this paper, we introduce two methods for English text-to-UMR parsing, one of which fine-tunes existing parsers for Abstract Meaning Representation and the other, which leverages a converter from Universal Dependencies, using prior work as a baseline. Our best-performing model, which we call SETUP, achieves an AnCast score of 84 and a SMATCH++ score of 91, indicating substantial gains towards automatic UMR parsing.", "AI": {"tldr": "本文介绍了两种英语文本到UMR解析的方法：一种基于AMR解析器的微调，另一种利用UD转换器，其中SETUP模型在AnCast和SMATCH++评分上表现最佳。", "motivation": "UMR作为一种新颖的基于图的语义表示方法，具有支持语言记录、改善低资源语言技术和增加可解释性的潜力，但需要自动化的文本到UMR解析器来实现大规模应用。", "method": "提出了两种英语文本到UMR解析方法：1）微调现有的抽象意义表示(AMR)解析器；2）利用通用依存(UD)转换器，并使用先前工作作为基线。", "result": "最佳性能模型SETUP在AnCast评分达到84，SMATCH++评分达到91，显示出在自动UMR解析方面的显著进展。", "conclusion": "研究展示了文本到UMR解析的可行性，SETUP模型的高评分表明该方法在自动生成准确UMR图方面取得了重要进展，为UMR的下游应用奠定了基础。"}}
{"id": "2512.07075", "pdf": "https://arxiv.org/pdf/2512.07075", "abs": "https://arxiv.org/abs/2512.07075", "authors": ["Shiwei Guo", "Sihang Jiang", "Qianxi He", "Yanghua Xiao", "Jiaqing Liang", "Bi Yude", "Minggui He", "Shimin Tao", "Li Zhang"], "title": "Do Large Language Models Truly Understand Cross-cultural Differences?", "categories": ["cs.CL"], "comment": null, "summary": "In recent years, large language models (LLMs) have demonstrated strong performance on multilingual tasks. Given its wide range of applications, cross-cultural understanding capability is a crucial competency. However, existing benchmarks for evaluating whether LLMs genuinely possess this capability suffer from three key limitations: a lack of contextual scenarios, insufficient cross-cultural concept mapping, and limited deep cultural reasoning capabilities. To address these gaps, we propose SAGE, a scenario-based benchmark built via cross-cultural core concept alignment and generative task design, to evaluate LLMs' cross-cultural understanding and reasoning. Grounded in cultural theory, we categorize cross-cultural capabilities into nine dimensions. Using this framework, we curated 210 core concepts and constructed 4530 test items across 15 specific real-world scenarios, organized under four broader categories of cross-cultural situations, following established item design principles. The SAGE dataset supports continuous expansion, and experiments confirm its transferability to other languages. It reveals model weaknesses across both dimensions and scenarios, exposing systematic limitations in cross-cultural reasoning. While progress has been made, LLMs are still some distance away from reaching a truly nuanced cross-cultural understanding. In compliance with the anonymity policy, we include data and code in the supplement materials. In future versions, we will make them publicly available online.", "AI": {"tldr": "SAGE是一个基于场景的跨文化理解评估基准，通过文化核心概念对齐和生成式任务设计构建，包含4530个测试项，覆盖9个文化维度和15个现实场景，揭示了LLMs在跨文化推理方面的系统性局限。", "motivation": "现有评估LLMs跨文化理解能力的基准存在三个关键局限：缺乏情境场景、跨文化概念映射不足、深度文化推理能力有限，需要更全面的评估工具。", "method": "基于文化理论将跨文化能力分为9个维度，选取210个核心概念，在15个现实场景中构建4530个测试项，遵循既定的项目设计原则，支持数据集持续扩展和多语言迁移。", "result": "实验验证了SAGE的可迁移性，揭示了模型在跨文化推理方面的系统性弱点，显示LLMs距离真正的细致跨文化理解仍有差距。", "conclusion": "虽然LLMs在跨文化任务上有所进步，但仍未达到真正细致的跨文化理解水平，SAGE基准为评估和改进模型跨文化能力提供了有效工具。"}}
{"id": "2512.07090", "pdf": "https://arxiv.org/pdf/2512.07090", "abs": "https://arxiv.org/abs/2512.07090", "authors": ["Jungmin Lee", "Gwangeun Byeon", "Yulhwa Kim", "Seokin Hong"], "title": "Leveraging KV Similarity for Online Structured Pruning in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning.", "AI": {"tldr": "Token Filtering是一种轻量级在线结构化剪枝技术，通过联合键值相似度测量令牌冗余度，在推理过程中直接跳过冗余注意力计算，无需校准数据即可减少推理成本并保持关键信息。", "motivation": "现有LLM剪枝方法依赖离线校准数据，存在泛化性差和不稳定的问题，需要一种无需校准数据、能在推理时动态决策的稳定剪枝方法。", "method": "提出基于联合键值相似度的令牌冗余度测量方法，设计方差感知融合策略自适应加权键值相似度，在多头注意力中保留信息丰富的令牌。", "result": "在LLaMA-2、LLaMA-3和Mistral模型上验证，Token Filtering在50%剪枝比例下仍能保持常识推理基准的准确性，并在MMLU等挑战性任务上表现优于现有结构化剪枝方法。", "conclusion": "Token Filtering提供了一种无需校准数据、内存开销为零的在线剪枝解决方案，通过方差感知的令牌重要性评估显著提升了剪枝的稳定性和效果。"}}
{"id": "2512.07132", "pdf": "https://arxiv.org/pdf/2512.07132", "abs": "https://arxiv.org/abs/2512.07132", "authors": ["Nithin Sivakumaran", "Justin Chih-Yao Chen", "David Wan", "Yue Zhang", "Jaehong Yoon", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Code: https://github.com/nsivaku/dart", "summary": "Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.", "AI": {"tldr": "DART是一个多智能体框架，通过视觉智能体之间的辩论分歧来识别有用的视觉工具，利用工具提供的专家知识和一致性评分来促进讨论，最终通过聚合智能体选择最佳答案。", "motivation": "解决在视觉任务中如何选择和调用合适的专家视觉工具（如目标检测、OCR、空间推理等）的挑战，传统方法难以确定何时调用哪些工具。", "method": "使用多个辩论视觉智能体，通过它们之间的分歧来识别有用的视觉工具，工具提供新信息并给出工具对齐的一致性评分，最后由聚合智能体基于智能体输出和工具信息选择最佳答案。", "result": "在四个不同基准测试中表现优异，比最强的基线（带法官模型的多智能体辩论）在A-OKVQA和MMMU上分别提升3.4%和2.4%，在M3D医疗数据集上比其他基线提升1.3%。", "conclusion": "DART框架能有效利用视觉工具解决多智能体分歧，适应新工具能力强，能产生丰富的讨论内容，并可靠地使用多样化工具来解决分歧。"}}
{"id": "2512.07134", "pdf": "https://arxiv.org/pdf/2512.07134", "abs": "https://arxiv.org/abs/2512.07134", "authors": ["Lauren Levine", "Amir Zeldes"], "title": "GUMBridge: a Corpus for Varieties of Bridging Anaphora", "categories": ["cs.CL"], "comment": null, "summary": "Bridging is an anaphoric phenomenon where the referent of an entity in a discourse is dependent on a previous, non-identical entity for interpretation, such as in \"There is 'a house'. 'The door' is red,\" where the door is specifically understood to be the door of the aforementioned house. While there are several existing resources in English for bridging anaphora, most are small, provide limited coverage of the phenomenon, and/or provide limited genre coverage. In this paper, we introduce GUMBridge, a new resource for bridging, which includes 16 diverse genres of English, providing both broad coverage for the phenomenon and granular annotations for the subtype categorization of bridging varieties. We also present an evaluation of annotation quality and report on baseline performance using open and closed source contemporary LLMs on three tasks underlying our data, showing that bridging resolution and subtype classification remain difficult NLP tasks in the age of LLMs.", "AI": {"tldr": "GUMBridge是一个新的桥接指代语料库，覆盖16种不同英语文体，提供细粒度标注和子类型分类，评估显示桥接解析和分类在LLM时代仍是挑战性NLP任务", "motivation": "现有英语桥接指代资源规模小、覆盖有限、文体单一，需要更全面多样的标注资源来支持研究", "method": "构建GUMBridge语料库，包含16种英语文体，提供桥接现象的细粒度标注和子类型分类，并评估标注质量", "result": "创建了覆盖面广的桥接指代标注资源，基线实验显示当前LLM在桥接解析和子类型分类任务上表现仍有困难", "conclusion": "桥接指代解析和分类仍是NLP中的难点，GUMBridge为这一领域提供了重要的资源支持，有助于推动相关研究发展"}}
{"id": "2512.07195", "pdf": "https://arxiv.org/pdf/2512.07195", "abs": "https://arxiv.org/abs/2512.07195", "authors": ["Xuan Zhang", "Wenxuan Zhang", "Anxu Wang", "See-Kiong Ng", "Yang Deng"], "title": "MASim: Multilingual Agent-Based Simulation for Social Science", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA", "cs.SI"], "comment": null, "summary": "Multi-agent role-playing has recently shown promise for studying social behavior with language agents, but existing simulations are mostly monolingual and fail to model cross-lingual interaction, an essential property of real societies. We introduce MASim, the first multilingual agent-based simulation framework that supports multi-turn interaction among generative agents with diverse sociolinguistic profiles. MASim offers two key analyses: (i) global public opinion modeling, by simulating how attitudes toward open-domain hypotheses evolve across languages and cultures, and (ii) media influence and information diffusion, via autonomous news agents that dynamically generate content and shape user behavior. To instantiate simulations, we construct the MAPS benchmark, which combines survey questions and demographic personas drawn from global population distributions. Experiments on calibration, sensitivity, consistency, and cultural case studies show that MASim reproduces sociocultural phenomena and highlights the importance of multilingual simulation for scalable, controlled computational social science.", "AI": {"tldr": "MASim是首个多语言智能体模拟框架，支持具有不同社会语言背景的生成式智能体进行多轮交互，用于研究跨语言社会行为和计算社会科学。", "motivation": "现有智能体角色扮演模拟大多是单语言的，无法模拟现实社会中关键的跨语言交互特性。", "method": "开发MASim多语言智能体模拟框架，构建MAPS基准数据集（包含调查问题和基于全球人口分布的人口统计角色），支持全球公众意见建模和媒体影响分析。", "result": "实验显示MASim能够重现社会文化现象，校准、敏感性、一致性和文化案例研究验证了其有效性。", "conclusion": "MASim证明了多语言模拟对于可扩展、可控的计算社会科学研究的重要性，为跨文化社会行为研究提供了新工具。"}}
{"id": "2512.07218", "pdf": "https://arxiv.org/pdf/2512.07218", "abs": "https://arxiv.org/abs/2512.07218", "authors": ["Feng Liang", "Weixin Zeng", "Runhao Zhao", "Xiang Zhao"], "title": "NeSTR: A Neuro-Symbolic Abductive Framework for Temporal Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by AAAI 2026", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, temporal reasoning, particularly under complex temporal constraints, remains a major challenge. To this end, existing approaches have explored symbolic methods, which encode temporal structure explicitly, and reflective mechanisms, which revise reasoning errors through multi-step inference. Nonetheless, symbolic approaches often underutilize the reasoning capabilities of LLMs, while reflective methods typically lack structured temporal representations, which can result in inconsistent or hallucinated reasoning. As a result, even when the correct temporal context is available, LLMs may still misinterpret or misapply time-related information, leading to incomplete or inaccurate answers. To address these limitations, in this work, we propose Neuro-Symbolic Temporal Reasoning (NeSTR), a novel framework that integrates structured symbolic representations with hybrid reflective reasoning to enhance the temporal sensitivity of LLM inference. NeSTR preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification, and corrects flawed inferences using abductive reflection. Extensive experiments on diverse temporal question answering benchmarks demonstrate that NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning without any fine-tuning, showcasing the advantage of neuro-symbolic integration in enhancing temporal understanding in large language models.", "AI": {"tldr": "NeSTR是一个神经符号时序推理框架，通过结合符号表示和混合反思推理来增强大语言模型的时序敏感性，无需微调即可在多个时序问答基准上取得优异性能。", "motivation": "现有方法存在局限性：符号方法未能充分利用LLMs的推理能力，而反思方法缺乏结构化时序表示，导致时序推理不一致或产生幻觉，即使有时序上下文也容易误解时间信息。", "method": "提出Neuro-Symbolic Temporal Reasoning (NeSTR)框架，集成结构化符号表示与混合反思推理，通过符号编码保持显式时序关系，验证确保逻辑一致性，使用溯因反思纠正错误推理。", "result": "在多个时序问答基准上的广泛实验表明，NeSTR实现了优异的零样本性能，持续改进了时序推理能力。", "conclusion": "神经符号集成在增强大语言模型时序理解方面具有显著优势，NeSTR框架有效解决了复杂时序约束下的推理挑战。"}}
{"id": "2512.07246", "pdf": "https://arxiv.org/pdf/2512.07246", "abs": "https://arxiv.org/abs/2512.07246", "authors": ["Mengqi Wang", "Jianwei Wang", "Qing Liu", "Xiwei Xu", "Zhenchang Xing", "Liming Zhu", "Wenjie Zhang"], "title": "Ensembling LLM-Induced Decision Trees for Explainable and Robust Error Detection", "categories": ["cs.CL"], "comment": "14 pages, 8 figures", "summary": "Error detection (ED), which aims to identify incorrect or inconsistent cell values in tabular data, is important for ensuring data quality. Recent state-of-the-art ED methods leverage the pre-trained knowledge and semantic capability embedded in large language models (LLMs) to directly label whether a cell is erroneous. However, this LLM-as-a-labeler pipeline (1) relies on the black box, implicit decision process, thus failing to provide explainability for the detection results, and (2) is highly sensitive to prompts, yielding inconsistent outputs due to inherent model stochasticity, therefore lacking robustness. To address these limitations, we propose an LLM-as-an-inducer framework that adopts LLM to induce the decision tree for ED (termed TreeED) and further ensembles multiple such trees for consensus detection (termed ForestED), thereby improving explainability and robustness. Specifically, based on prompts derived from data context, decision tree specifications and output requirements, TreeED queries the LLM to induce the decision tree skeleton, whose root-to-leaf decision paths specify the stepwise procedure for evaluating a given sample. Each tree contains three types of nodes: (1) rule nodes that perform simple validation checks (e.g., format or range), (2) Graph Neural Network (GNN) nodes that capture complex patterns (e.g., functional dependencies), and (3) leaf nodes that output the final decision types (error or clean). Furthermore, ForestED employs uncertainty-based sampling to obtain multiple row subsets, constructing a decision tree for each subset using TreeED. It then leverages an Expectation-Maximization-based algorithm that jointly estimates tree reliability and optimizes the consensus ED prediction. Extensive xperiments demonstrate that our methods are accurate, explainable and robust, achieving an average F1-score improvement of 16.1% over the best baseline.", "AI": {"tldr": "提出TreeED和ForestED框架，使用LLM诱导决策树进行错误检测，提高可解释性和鲁棒性，相比基线方法F1分数平均提升16.1%", "motivation": "现有基于LLM的错误检测方法缺乏可解释性（黑盒决策）和鲁棒性（对提示词敏感），需要改进", "method": "使用LLM诱导决策树（TreeED），包含规则节点、GNN节点和叶子节点；通过不确定性采样构建多个决策树并集成（ForestED），使用EM算法优化共识预测", "result": "实验证明方法准确、可解释且鲁棒，F1分数相比最佳基线平均提升16.1%", "conclusion": "提出的LLM-as-an-inducer框架通过决策树诱导和集成，有效解决了现有LLM错误检测方法的可解释性和鲁棒性问题"}}
{"id": "2512.07265", "pdf": "https://arxiv.org/pdf/2512.07265", "abs": "https://arxiv.org/abs/2512.07265", "authors": ["Bhavana Akkiraju", "Srihari Bandarupalli", "Swathi Sambangi", "Vasavi Ravuri", "R Vijaya Saraswathi", "Anil Kumar Vuppala"], "title": "TeluguST-46: A Benchmark Corpus and Comprehensive Evaluation for Telugu-English Speech Translation", "categories": ["cs.CL", "eess.AS"], "comment": "Submitted to AACL IJCNLP 2025", "summary": "Despite Telugu being spoken by over 80 million people, speech translation research for this morphologically rich language remains severely underexplored. We address this gap by developing a high-quality Telugu--English speech translation benchmark from 46 hours of manually verified CSTD corpus data (30h/8h/8h train/dev/test split). Our systematic comparison of cascaded versus end-to-end architectures shows that while IndicWhisper + IndicMT achieves the highest performance due to extensive Telugu-specific training data, finetuned SeamlessM4T models demonstrate remarkable competitiveness despite using significantly less Telugu-specific training data. This finding suggests that with careful hyperparameter tuning and sufficient parallel data (potentially less than 100 hours), end-to-end systems can achieve performance comparable to cascaded approaches in low-resource settings. Our metric reliability study evaluating BLEU, METEOR, ChrF++, ROUGE-L, TER, and BERTScore against human judgments reveals that traditional metrics provide better quality discrimination than BERTScore for Telugu--English translation. The work delivers three key contributions: a reproducible Telugu--English benchmark, empirical evidence of competitive end-to-end performance potential in low-resource scenarios, and practical guidance for automatic evaluation in morphologically complex language pairs.", "AI": {"tldr": "为泰卢固语-英语语音翻译开发了高质量基准测试，比较了级联与端到端架构的性能，发现端到端系统在低资源环境下也能达到竞争性表现，并评估了多种自动评估指标的可靠性。", "motivation": "泰卢固语有超过8000万使用者，但该形态丰富的语言的语音翻译研究严重不足，需要填补这一空白。", "method": "使用46小时手动验证的CSTD语料库数据，建立训练/开发/测试集(30h/8h/8h)，系统比较级联架构(IndicWhisper + IndicMT)与端到端架构(finetuned SeamlessM4T)的性能，并评估BLEU、METEOR、ChrF++、ROUGE-L、TER和BERTScore等指标的可靠性。", "result": "IndicWhisper + IndicMT由于使用大量泰卢固语特定训练数据而表现最佳，但finetuned SeamlessM4T模型使用显著更少的泰卢固语数据却表现出色，表明端到端系统在低资源环境下也能达到级联方法的性能水平。传统评估指标比BERTScore在泰卢固语-英语翻译中提供更好的质量区分。", "conclusion": "该研究提供了可复现的泰卢固语-英语基准测试，证明了端到端系统在低资源场景下的竞争潜力，并为形态复杂语言对的自动评估提供了实用指导。"}}
{"id": "2512.07277", "pdf": "https://arxiv.org/pdf/2512.07277", "abs": "https://arxiv.org/abs/2512.07277", "authors": ["Srihari Bandarupalli", "Bhavana Akkiraju", "Charan Devarakonda", "Vamsiraghusimha Narsinga", "Anil Kumar Vuppala"], "title": "Efficient ASR for Low-Resource Languages: Leveraging Cross-Lingual Unlabeled Data", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted in AACL IJCNLP 2025", "summary": "Automatic speech recognition for low-resource languages remains fundamentally constrained by the scarcity of labeled data and computational resources required by state-of-the-art models. We present a systematic investigation into cross-lingual continuous pretraining for low-resource languages, using Perso-Arabic languages (Persian, Arabic, and Urdu) as our primary case study. Our approach demonstrates that strategic utilization of unlabeled speech data can effectively bridge the resource gap without sacrificing recognition accuracy. We construct a 3,000-hour multilingual corpus through a scalable unlabeled data collection pipeline and employ targeted continual pretraining combined with morphologically-aware tokenization to develop a 300M parameter model that achieves performance comparable to systems 5 times larger. Our model outperforms Whisper Large v3 (1.5B parameters) on Persian and achieves competitive results on Arabic and Urdu despite using significantly fewer parameters and substantially less labeled data. These findings challenge the prevailing assumption that ASR quality scales primarily with model size, revealing instead that data relevance and strategic pretraining are more critical factors for low-resource scenarios. This work provides a practical pathway toward inclusive speech technology, enabling effective ASR for underrepresented languages without dependence on massive computational infrastructure or proprietary datasets.", "AI": {"tldr": "本研究提出了一种针对低资源语言的跨语言持续预训练方法，通过在波斯语、阿拉伯语和乌尔都语上的实验证明，利用未标注语音数据和形态感知分词策略，可以用更少的参数和标注数据实现与大型模型相当的性能。", "motivation": "低资源语言的自动语音识别面临标注数据稀缺和计算资源不足的双重挑战，现有技术主要依赖大规模模型，但这对资源受限的语言不现实。", "method": "构建了3000小时的多语言未标注语料库，采用可扩展的数据收集流程，结合目标导向的持续预训练和形态感知分词技术，开发了一个3亿参数的模型。", "result": "该模型在波斯语上超越了拥有15亿参数的Whisper Large v3，在阿拉伯语和乌尔都语上也取得了有竞争力的结果，尽管使用的参数和标注数据都显著减少。", "conclusion": "研究挑战了ASR质量主要随模型规模扩展的普遍假设，表明数据相关性和战略预训练对低资源场景更为关键，为包容性语音技术提供了实用路径。"}}
{"id": "2512.07288", "pdf": "https://arxiv.org/pdf/2512.07288", "abs": "https://arxiv.org/abs/2512.07288", "authors": ["Tomoki Doi", "Masaru Isonuma", "Hitomi Yanaka"], "title": "Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models", "categories": ["cs.CL"], "comment": "To appear in the Proceedings of the Asia-Pacific Chapter of the Association for Computational Linguistics: Student Research Workshop (AACL-SRW 2025)", "summary": "Large language models have the potential to generate explanations for their own predictions in a variety of styles based on user instructions. Recent research has examined whether these self-explanations faithfully reflect the models' actual behavior and has found that they often lack faithfulness. However, the question of how to improve faithfulness remains underexplored. Moreover, because different explanation styles have superficially distinct characteristics, it is unclear whether improvements observed in one style also arise when using other styles. This study analyzes the effects of training for faithful self-explanations and the extent to which these effects generalize, using three classification tasks and three explanation styles. We construct one-word constrained explanations that are likely to be faithful using a feature attribution method, and use these pseudo-faithful self-explanations for continual learning on instruction-tuned models. Our experiments demonstrate that training can improve self-explanation faithfulness across all classification tasks and explanation styles, and that these improvements also show signs of generalization to the multi-word settings and to unseen tasks. Furthermore, we find consistent cross-style generalization among three styles, suggesting that training may contribute to a broader improvement in faithful self-explanation ability.", "AI": {"tldr": "该研究通过特征归因方法构建伪忠实解释，用于指令调优模型的持续学习，发现训练可以提高所有分类任务和解释风格的自解释忠实度，并显示出跨风格和跨任务的一般化能力。", "motivation": "大型语言模型能够根据用户指令生成不同风格的自解释，但现有研究发现这些自解释往往缺乏忠实性，如何提高忠实性以及改进效果是否能在不同风格间泛化尚不明确。", "method": "使用三个分类任务和三种解释风格，通过特征归因方法构建可能忠实的一词约束解释作为伪忠实自解释，并用于指令调优模型的持续学习。", "result": "实验表明训练可以提高所有分类任务和解释风格的自解释忠实度，改进效果能泛化到多词设置和未见任务，且三种风格间存在一致的跨风格泛化。", "conclusion": "训练可能有助于更广泛地提高忠实自解释能力，显示出跨风格和跨任务的一般化潜力。"}}
{"id": "2512.07367", "pdf": "https://arxiv.org/pdf/2512.07367", "abs": "https://arxiv.org/abs/2512.07367", "authors": ["Revekka Kyriakoglou", "Anna Pappa"], "title": "Multilingual corpora for the study of new concepts in the social sciences and humanities:", "categories": ["cs.CL"], "comment": "in French language", "summary": "This article presents a hybrid methodology for building a multilingual corpus designed to support the study of emerging concepts in the humanities and social sciences (HSS), illustrated here through the case of ``non-technological innovation''. The corpus relies on two complementary sources: (1) textual content automatically extracted from company websites, cleaned for French and English, and (2) annual reports collected and automatically filtered according to documentary criteria (year, format, duplication). The processing pipeline includes automatic language detection, filtering of non-relevant content, extraction of relevant segments, and enrichment with structural metadata. From this initial corpus, a derived dataset in English is created for machine learning purposes. For each occurrence of a term from the expert lexicon, a contextual block of five sentences is extracted (two preceding and two following the sentence containing the term). Each occurrence is annotated with the thematic category associated with the term, enabling the construction of data suitable for supervised classification tasks. This approach results in a reproducible and extensible resource, suitable both for analyzing lexical variability around emerging concepts and for generating datasets dedicated to natural language processing applications.", "AI": {"tldr": "本文提出了一种构建多语言语料库的混合方法，用于支持人文社会科学中新兴概念的研究，以\"非技术创新\"为例。该方法结合公司网站文本和年报数据，通过自动化处理流程创建适合机器学习的有标注数据集。", "motivation": "为支持人文社会科学领域新兴概念的研究，需要构建一个可重复且可扩展的多语言语料库资源，既能分析新兴概念的词汇变异性，又能为自然语言处理应用生成专用数据集。", "method": "使用两种互补数据源：1)从公司网站自动提取并清洗的法语和英语文本内容；2)按文献标准自动筛选的年度报告。处理流程包括自动语言检测、非相关内容过滤、相关片段提取和结构元数据丰富。从初始语料库创建英文派生数据集，为专家词典中的每个术语提取包含前后各两句的上下文块，并按主题类别进行标注。", "result": "成功构建了一个可重复且可扩展的多语言语料库资源，包含经过标注的上下文块数据集，适用于监督分类任务，既能分析新兴概念的词汇变异性，又能支持自然语言处理应用。", "conclusion": "该混合方法为研究人文社会科学中的新兴概念提供了有效的语料库构建解决方案，通过自动化处理流程创建了适合机器学习的有标注数据集，具有可重复性和可扩展性优势。"}}
{"id": "2512.07407", "pdf": "https://arxiv.org/pdf/2512.07407", "abs": "https://arxiv.org/abs/2512.07407", "authors": ["Niklas Mellgren", "Peter Schneider-Kamp", "Lukas Galke Poech"], "title": "Training Language Models to Use Prolog as a Tool", "categories": ["cs.CL"], "comment": "10 pages", "summary": "Ensuring reliable tool use is critical for safe agentic AI systems. Language models frequently produce unreliable reasoning with plausible but incorrect solutions that are difficult to verify. To address this, we investigate fine-tuning models to use Prolog as an external tool for verifiable computation. Using Group Relative Policy Optimization (GRPO), we fine-tune Qwen2.5-3B-Instruct on a cleaned GSM8K-Prolog-Prover dataset while varying (i) prompt structure, (ii) reward composition (execution, syntax, semantics, structure), and (iii) inference protocol: single-shot, best-of-N, and two agentic modes where Prolog is invoked internally or independently. Our reinforcement learning approach outperforms supervised fine-tuning, with our 3B model achieving zero-shot MMLU performance comparable to 7B few-shot results. Our findings reveal that: 1) joint tuning of prompt, reward, and inference shapes program syntax and logic; 2) best-of-N with external Prolog verification maximizes accuracy on GSM8K; 3) agentic inference with internal repair yields superior zero-shot generalization on MMLU-Stem and MMLU-Pro. These results demonstrate that grounding model reasoning in formal verification systems substantially improves reliability and auditability for safety-critical applications. The source code for reproducing our experiments is available under https://github.com/niklasmellgren/grpo-prolog-inference", "AI": {"tldr": "通过强化学习微调语言模型使用Prolog作为可验证计算工具，显著提升了推理可靠性和泛化能力，在GSM8K和MMLU基准测试中表现出色。", "motivation": "解决语言模型推理不可靠的问题，生成看似合理但实际错误的解决方案难以验证，需要建立可验证的计算框架来确保智能体AI系统的安全性。", "method": "使用Group Relative Policy Optimization (GRPO)对Qwen2.5-3B-Instruct进行微调，基于清洗后的GSM8K-Prolog-Prover数据集，研究不同提示结构、奖励组合（执行、语法、语义、结构）和推理协议（单次采样、best-of-N、内部调用Prolog、独立调用Prolog的智能体模式）的影响。", "result": "强化学习方法优于监督微调，3B模型在零样本MMLU上的性能可与7B模型的少样本结果相媲美；best-of-N结合外部Prolog验证在GSM8K上达到最高准确率；内部修复的智能体推理在MMLU-Stem和MMLU-Pro上表现出优异的零样本泛化能力。", "conclusion": "将模型推理建立在形式化验证系统基础上，显著提高了安全关键应用的可靠性和可审计性，为构建可靠的智能体AI系统提供了有效途径。"}}
{"id": "2512.07454", "pdf": "https://arxiv.org/pdf/2512.07454", "abs": "https://arxiv.org/abs/2512.07454", "authors": ["Amir Mohammad Akhlaghi", "Amirhossein Shabani", "Mostafa Abdolmaleki", "Saeed Reza Kheradpisheh"], "title": "Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini -- originally a monolingual English model -- can be effectively adapted to Persian through a novel, resource-efficient curriculum learning pipeline. Our approach employs a unique \"warm-up\" stage using bilingual narratives (Tiny Stories) to align embeddings prior to heavy training, followed by continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT). Despite its compact size, Persian-Phi achieves competitive results on Open Persian LLM Leaderboard in HuggingFace. Our findings provide a validated, scalable framework for extending the reach of state-of-the-art LLMs to underrepresented languages with minimal hardware resources. The Persian-Phi model is publicly available at https://huggingface.co/amirakhlaghiqqq/PersianPhi.", "AI": {"tldr": "Persian-Phi是一个3.8B参数模型，通过创新的课程学习流程将英语单语模型Phi-3 Mini有效适配到波斯语，证明了小模型也能在多语言任务中取得竞争力表现。", "motivation": "当前AI民主化受到低资源语言训练大语言模型巨大计算成本的阻碍，需要找到资源高效的方法将先进LLM扩展到代表性不足的语言。", "method": "采用新颖的资源高效课程学习流程：首先使用双语叙述（Tiny Stories）进行\"预热\"阶段对齐嵌入，然后通过参数高效微调（PEFT）进行持续预训练和指令调优。", "result": "Persian-Phi在HuggingFace的Open Persian LLM Leaderboard上取得了有竞争力的结果，证明了小模型也能实现稳健的多语言能力。", "conclusion": "研究提供了一个经过验证、可扩展的框架，可以用最少的硬件资源将最先进的LLM扩展到代表性不足的语言，模型已公开可用。"}}
{"id": "2512.07461", "pdf": "https://arxiv.org/pdf/2512.07461", "abs": "https://arxiv.org/abs/2512.07461", "authors": ["Tong Wu", "Yang Liu", "Jun Bai", "Zixia Jia", "Shuyi Zhang", "Ziyong Lin", "Yanting Wang", "Song-Chun Zhu", "Zilong Zheng"], "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.", "AI": {"tldr": "NPR是一个无需教师的框架，使大语言模型能够自我进化真正的并行推理能力，通过自我蒸馏训练、并行感知策略优化和重构引擎实现，在8个基准测试中性能提升24.5%，推理速度提升4.6倍", "motivation": "解决现有大语言模型在推理时仍主要依赖自回归解码，缺乏真正的并行推理能力，导致推理效率低下的问题", "method": "1) 自我蒸馏渐进训练范式，从冷启动格式发现到严格拓扑约束；2) 并行感知策略优化算法，在执行图中直接优化分支策略；3) 重构NPR引擎，改进SGLang的内存管理和流程控制", "result": "在8个推理基准测试中，基于Qwen3-4B训练的NPR模型性能提升达24.5%，推理速度提升达4.6倍，实现了100%真正的并行执行", "conclusion": "NPR为大语言模型建立了自我进化、高效且可扩展的智能推理新标准，成功将模型从顺序模拟转变为原生并行认知"}}
{"id": "2512.07478", "pdf": "https://arxiv.org/pdf/2512.07478", "abs": "https://arxiv.org/abs/2512.07478", "authors": ["Zhuoran Zhuang", "Ye Chen", "Jianghao Su", "Chao Luo", "Luhui Liu", "Xia Zeng"], "title": "Enhancing Agentic RL with Progressive Reward Shaping and Value-based Sampling Policy Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) empowered with Tool-Integrated Reasoning (TIR) can iteratively plan, call external tools, and integrate returned information to solve complex, long-horizon reasoning tasks. Agentic Reinforcement Learning (Agentic RL) optimizes such models over full tool-interaction trajectories, but two key challenges hinder effectiveness: (1) Sparse, non-instructive rewards, such as binary 0-1 verifiable signals, provide limited guidance for intermediate steps and slow convergence; (2) Gradient degradation in Group Relative Policy Optimization (GRPO), where identical rewards within a rollout group yield zero advantage, reducing sample efficiency and destabilizing training. To address these challenges, we propose two complementary techniques: Progressive Reward Shaping (PRS) and Value-based Sampling Policy Optimization (VSPO). PRS is a curriculum-inspired reward design that introduces dense, stage-wise feedback - encouraging models to first master parseable and properly formatted tool calls, then optimize for factual correctness and answer quality. We instantiate PRS for short-form QA (with a length-aware BLEU to fairly score concise answers) and long-form QA (with LLM-as-a-Judge scoring to prevent reward hacking). VSPO is an enhanced GRPO variant that replaces low-value samples with prompts selected by a task-value metric balancing difficulty and uncertainty, and applies value-smoothing clipping to stabilize gradient updates. Experiments on multiple short-form and long-form QA benchmarks show that PRS consistently outperforms traditional binary rewards, and VSPO achieves superior stability, faster convergence, and higher final performance compared to PPO, GRPO, CISPO, and SFT-only baselines. Together, PRS and VSPO yield LLM-based TIR agents that generalize better across domains.", "AI": {"tldr": "该论文提出了PRS和VSPO两种技术来解决工具集成推理中强化学习的挑战：PRS通过渐进式奖励设计提供密集的阶段反馈，VSPO通过基于价值的采样策略优化提升训练稳定性和效率。", "motivation": "工具集成推理的强化学习面临两个关键挑战：稀疏的非指导性奖励限制了中间步骤的指导，以及组相对策略优化中的梯度退化问题降低了样本效率和训练稳定性。", "method": "提出两种互补技术：渐进式奖励塑形(PRS)设计密集的阶段反馈奖励，以及基于价值的采样策略优化(VSPO)通过任务价值指标选择样本并应用价值平滑裁剪。", "result": "在多个短形式和长形式QA基准测试中，PRS始终优于传统二元奖励，VSPO相比PPO、GRPO、CISPO和SFT基线实现了更好的稳定性、更快的收敛和更高的最终性能。", "conclusion": "PRS和VSPO共同产生了能够更好跨领域泛化的基于LLM的工具集成推理智能体。"}}
{"id": "2512.07515", "pdf": "https://arxiv.org/pdf/2512.07515", "abs": "https://arxiv.org/abs/2512.07515", "authors": ["Pengqian Lu", "Jie Lu", "Anjin Liu", "Guangquan Zhang"], "title": "SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation for Detecting Hallucinations in RAG", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Detecting hallucinations in Retrieval-Augmented Generation (RAG) remains a challenge. Prior approaches attribute hallucinations to a binary conflict between internal knowledge (stored in FFNs) and retrieved context. However, this perspective is incomplete, failing to account for the impact of other components in the generative process, such as the user query, previously generated tokens, the current token itself, and the final LayerNorm adjustment. To address this, we introduce SPAD. First, we mathematically attribute each token's probability into seven distinct sources: Query, RAG, Past, Current Token, FFN, Final LayerNorm, and Initial Embedding. This attribution quantifies how each source contributes to the generation of the current token. Then, we aggregate these scores by POS tags to quantify how different components drive specific linguistic categories. By identifying anomalies, such as Nouns relying on Final LayerNorm, SPAD effectively detects hallucinations. Extensive experiments demonstrate that SPAD achieves state-of-the-art performance", "AI": {"tldr": "SPAD方法通过数学归因将token概率分解为7个来源，并通过词性标签聚合来检测RAG中的幻觉，实现了最先进的性能", "motivation": "现有方法将幻觉归因于内部知识(FFN)和检索上下文之间的二元冲突，这种视角不完整，忽略了生成过程中其他组件的影响", "method": "将每个token的概率数学归因为7个不同来源：查询、RAG、历史token、当前token、FFN、最终LayerNorm调整和初始嵌入，然后通过词性标签聚合这些分数来量化不同组件对特定语言类别的贡献", "result": "通过识别异常情况（如名词依赖最终LayerNorm），SPAD能有效检测幻觉，大量实验证明其达到最先进性能", "conclusion": "SPAD提供了一个更全面的幻觉检测框架，通过多源归因分析显著提升了RAG系统中幻觉检测的准确性和可靠性"}}
{"id": "2512.07522", "pdf": "https://arxiv.org/pdf/2512.07522", "abs": "https://arxiv.org/abs/2512.07522", "authors": ["Sebastian Sztwiertnia", "Felix Friedrich", "Kristian Kersting", "Patrick Schramowski", "Björn Deiseroth"], "title": "LIME: Making LLM Data More Efficient with Linguistic Metadata Embeddings", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Pre-training decoder-only language models relies on vast amounts of high-quality data, yet the availability of such data is increasingly reaching its limits. While metadata is commonly used to create and curate these datasets, its potential as a direct training signal remains under-explored. We challenge this status quo and propose LIME (Linguistic Metadata Embeddings), a method that enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. LIME substantially improves pre-training efficiency. Specifically, it adapts up to 56% faster to the training data distribution, while introducing only 0.01% additional parameters at negligible compute overhead. Beyond efficiency, LIME improves tokenization, leading to remarkably stronger language modeling capabilities and generative task performance. These benefits persist across model scales (500M to 2B). In addition, we develop a variant with shifted metadata, LIME+1, that can guide token generation. Given prior metadata for the next token, LIME+1 improves reasoning performance by up to 38% and arithmetic accuracy by up to 35%.", "AI": {"tldr": "LIME方法通过将语法、语义和上下文元数据嵌入到token表示中，显著提升预训练效率、语言建模能力和生成任务性能，且在不同规模模型上都有效。", "motivation": "当前预训练语言模型依赖大量高质量数据，但这类数据资源日益有限。虽然元数据常用于数据集构建，但其作为直接训练信号的潜力未被充分探索。", "method": "提出LIME方法，将捕捉语法、语义和上下文属性的元数据信息融入到token嵌入中，同时开发了LIME+1变体用于引导token生成。", "result": "LIME使模型对训练数据分布的适应速度提升高达56%，仅增加0.01%参数且计算开销可忽略。在语言建模和生成任务上表现显著提升，LIME+1变体使推理性能提升38%，算术准确率提升35%。", "conclusion": "元数据可以作为有效的直接训练信号，LIME方法在提升预训练效率和模型性能方面具有显著优势，且适用于不同规模的模型。"}}
{"id": "2512.07525", "pdf": "https://arxiv.org/pdf/2512.07525", "abs": "https://arxiv.org/abs/2512.07525", "authors": ["Xiaoran Liu", "Yuerong Song", "Zhigeng Liu", "Zengfeng Huang", "Qipeng Guo", "Zhaoxiang Liu", "Shiguo Lian", "Ziwei He", "Xipeng Qiu"], "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs", "categories": ["cs.CL"], "comment": "20 pages, 6 figures, under review", "summary": "Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.", "AI": {"tldr": "本文提出了一种改进的旋转位置编码方法，通过利用复数点积的虚部信息来增强长上下文依赖建模能力，相比标准RoPE在长上下文基准测试中表现更优。", "motivation": "标准RoPE实现仅使用复数点积的实部计算注意力分数，丢弃了包含重要相位信息的虚部，这可能导致长上下文依赖建模中关系细节的损失。", "method": "提出扩展方法重新整合被丢弃的虚部，利用完整的复数表示创建双分量注意力分数，保留更多位置信息。", "result": "理论和实证研究表明该方法增强了长上下文依赖建模能力，在长上下文语言建模基准测试中一致优于标准RoPE，且随着上下文长度增加效果更显著。", "conclusion": "通过充分利用复数表示的虚部信息，该方法有效提升了RoPE在长序列建模中的性能，为长上下文语言模型提供了更好的位置编码解决方案。"}}
{"id": "2512.07538", "pdf": "https://arxiv.org/pdf/2512.07538", "abs": "https://arxiv.org/abs/2512.07538", "authors": ["Michelle Wastl", "Jannis Vamvas", "Rico Sennrich"], "title": "SwissGov-RSD: A Human-annotated, Cross-lingual Benchmark for Token-level Recognition of Semantic Differences Between Related Documents", "categories": ["cs.CL"], "comment": "30 pages", "summary": "Recognizing semantic differences across documents, especially in different languages, is crucial for text generation evaluation and multilingual content alignment. However, as a standalone task it has received little attention. We address this by introducing SwissGov-RSD, the first naturalistic, document-level, cross-lingual dataset for semantic difference recognition. It encompasses a total of 224 multi-parallel documents in English-German, English-French, and English-Italian with token-level difference annotations by human annotators. We evaluate a variety of open-source and closed source large language models as well as encoder models across different fine-tuning settings on this new benchmark. Our results show that current automatic approaches perform poorly compared to their performance on monolingual, sentence-level, and synthetic benchmarks, revealing a considerable gap for both LLMs and encoder models. We make our code and datasets publicly available.", "AI": {"tldr": "该论文提出了SwissGov-RSD数据集，这是第一个自然主义的文档级跨语言语义差异识别数据集，包含英德、英法、英意三语对的224个多平行文档，并评估了各种大语言模型和编码器模型在该任务上的表现。", "motivation": "识别跨文档语义差异（特别是跨语言场景）对于文本生成评估和多语言内容对齐至关重要，但作为独立任务尚未得到足够关注。", "method": "构建包含224个多平行文档的SwissGov-RSD数据集，进行人工标注的token级差异标注，评估开源和闭源大语言模型以及编码器模型在不同微调设置下的性能。", "result": "当前自动方法在该任务上表现较差，与单语言、句子级和合成基准测试相比存在显著差距，大语言模型和编码器模型都显示出相当大的性能差距。", "conclusion": "跨语言文档级语义差异识别是一个具有挑战性的任务，现有模型表现不佳，需要进一步研究改进，作者公开了代码和数据集以促进该领域发展。"}}
{"id": "2512.07540", "pdf": "https://arxiv.org/pdf/2512.07540", "abs": "https://arxiv.org/abs/2512.07540", "authors": ["Boxuan Lyu", "Haiyue Song", "Hidetaka Kamigaito", "Chenchen Ding", "Hideki Tanaka", "Masao Utiyama", "Kotaro Funakoshi", "Manabu Okumura"], "title": "Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Error Span Detection (ESD) is a subtask of automatic machine translation evaluation that localizes error spans in translations and labels their severity. State-of-the-art generative ESD methods typically decode using Maximum a Posteriori (MAP), assuming that model-estimated probabilities are perfectly correlated with similarity to human annotation. However, we observed that annotations dissimilar to the human annotation could achieve a higher model likelihood than the human annotation. We address this issue by applying Minimum Bayes Risk (MBR) decoding to generative ESD models. Specifically, we employ sentence- and span-level similarity metrics as utility functions to select candidate hypotheses based on their approximate similarity to the human annotation. Extensive experimental results show that our MBR decoding outperforms the MAP baseline at the system, sentence, and span-levels. Furthermore, to mitigate the computational cost of MBR decoding, we demonstrate that applying MBR distillation enables a standard greedy model to match MBR decoding performance, effectively eliminating the inference-time latency bottleneck.", "AI": {"tldr": "该论文提出使用最小贝叶斯风险(MBR)解码替代最大后验概率(MAP)解码来改进生成式错误跨度检测(ESD)模型，通过句子和跨度级别的相似度度量选择候选假设，显著提升性能，并通过MBR蒸馏解决计算成本问题。", "motivation": "现有生成式ESD方法使用MAP解码时存在模型估计概率与人类标注相似度不完全相关的问题，观察到非人类标注的假设可能获得更高的模型似然。", "method": "应用MBR解码到生成式ESD模型，使用句子和跨度级别的相似度度量作为效用函数来选择最接近人类标注的候选假设，并采用MBR蒸馏来降低计算成本。", "result": "实验结果显示MBR解码在系统、句子和跨度级别均优于MAP基线，且MBR蒸馏能使标准贪婪模型达到与MBR解码相当的性能。", "conclusion": "MBR解码能有效解决MAP解码在ESD任务中的局限性，通过相似度导向的假设选择提升性能，MBR蒸馏技术成功解决了计算效率问题，使该方法具有实际应用价值。"}}
{"id": "2512.07544", "pdf": "https://arxiv.org/pdf/2512.07544", "abs": "https://arxiv.org/abs/2512.07544", "authors": ["Kyungro Lee", "Dongha Choi", "Hyunju Lee"], "title": "MoCoRP: Modeling Consistent Relations between Persona and Response for Persona-based Dialogue", "categories": ["cs.CL", "cs.AI"], "comment": "18 pages", "summary": "As dialogue systems become increasingly important across various domains, a key challenge in persona-based dialogue is generating engaging and context-specific interactions while ensuring the model acts with a coherent personality. However, existing persona-based dialogue datasets lack explicit relations between persona sentences and responses, which makes it difficult for models to effectively capture persona information. To address these issues, we propose MoCoRP (Modeling Consistent Relations between Persona and Response), a framework that incorporates explicit relations into language models. MoCoRP leverages an NLI expert to explicitly extract the NLI relations between persona sentences and responses, enabling the model to effectively incorporate appropriate persona information from the context into its responses. We applied this framework to pre-trained models like BART and further extended it to modern large language models (LLMs) through alignment tuning. Experimental results on the public datasets ConvAI2 and MPChat demonstrate that MoCoRP outperforms existing baselines, achieving superior persona consistency and engaging, context-aware dialogue generation. Furthermore, our model not only excels in quantitative metrics but also shows significant improvements in qualitative aspects. These results highlight the effectiveness of explicitly modeling persona-response relations in persona-based dialogue. The source codes of MoCoRP are available at https://github.com/DMCB-GIST/MoCoRP.", "AI": {"tldr": "MoCoRP框架通过显式建模人物描述与回复之间的NLI关系，提升基于人物对话系统的个性一致性和对话质量，在ConvAI2和MPChat数据集上超越现有基线。", "motivation": "现有基于人物的对话数据集缺乏人物句子与回复之间的显式关系，导致模型难以有效捕捉人物信息，影响对话的个性一致性和互动质量。", "method": "提出MoCoRP框架，利用NLI专家显式提取人物句子与回复之间的NLI关系，使模型能有效将适当的人物信息融入回复。该方法应用于BART等预训练模型，并通过对齐调优扩展到现代大语言模型。", "result": "在ConvAI2和MPChat数据集上的实验表明，MoCoRP在人物一致性和情境感知对话生成方面优于现有基线，在定量指标和定性方面均有显著提升。", "conclusion": "显式建模人物-回复关系能有效提升基于人物对话系统的性能，MoCoRP框架为解决人物信息融合问题提供了有效方案。"}}
{"id": "2512.07543", "pdf": "https://arxiv.org/pdf/2512.07543", "abs": "https://arxiv.org/abs/2512.07543", "authors": ["Frederic Blum"], "title": "Most over-representation of phonological features in basic vocabulary disappears when controlling for spatial and phylogenetic effects", "categories": ["cs.CL"], "comment": "Accepted with minor revisions at *Linguistic Typology*, expected to be fully published in 2026", "summary": "The statistical over-representation of phonological features in the basic vocabulary of languages is often interpreted as reflecting potentially universal sound symbolic patterns. However, most of those results have not been tested explicitly for reproducibility and might be prone to biases in the study samples or models. Many studies on the topic do not adequately control for genealogical and areal dependencies between sampled languages, casting doubts on the robustness of the results. In this study, we test the robustness of a recent study on sound symbolism of basic vocabulary concepts which analyzed245 languages.The new sample includes data on 2864 languages from Lexibank. We modify the original model by adding statistical controls for spatial and phylogenetic dependencies between languages. The new results show that most of the previously observed patterns are not robust, and in fact many patterns disappear completely when adding the genealogical and areal controls. A small number of patterns, however, emerges as highly stable even with the new sample. Through the new analysis, we are able to assess the distribution of sound symbolism on a larger scale than previously. The study further highlights the need for testing all universal claims on language for robustness on various levels.", "AI": {"tldr": "本研究重新检验了基本词汇中语音象征现象的稳健性，发现大多数先前观察到的模式在控制语言间的谱系和地域依赖后不再显著，只有少数模式保持稳定。", "motivation": "先前关于语音象征的研究存在样本偏差和模型控制不足的问题，特别是未能充分控制语言间的谱系关系和地域依赖，导致结果稳健性存疑。", "method": "使用Lexibank的2864种语言数据，修改原模型加入空间和谱系依赖的统计控制，重新分析基本词汇中的语音象征模式。", "result": "大多数先前观察到的语音象征模式在加入谱系和地域控制后不再稳健，许多模式完全消失，但少数模式保持高度稳定。", "conclusion": "语音象征现象比先前认为的要少且更有限，研究强调了对语言普遍性主张进行多层次稳健性检验的必要性。"}}
{"id": "2512.07583", "pdf": "https://arxiv.org/pdf/2512.07583", "abs": "https://arxiv.org/abs/2512.07583", "authors": ["Navid Asgari", "Benjamin M. Cole"], "title": "Complementary Learning Approach for Text Classification using Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "67 pages", "summary": "In this study, we propose a structured methodology that utilizes large language models (LLMs) in a cost-efficient and parsimonious manner, integrating the strengths of scholars and machines while offsetting their respective weaknesses. Our methodology, facilitated through a chain of thought and few-shot learning prompting from computer science, extends best practices for co-author teams in qualitative research to human-machine teams in quantitative research. This allows humans to utilize abductive reasoning and natural language to interrogate not just what the machine has done but also what the human has done. Our method highlights how scholars can manage inherent weaknesses OF LLMs using careful, low-cost techniques. We demonstrate how to use the methodology to interrogate human-machine rating discrepancies for a sample of 1,934 press releases announcing pharmaceutical alliances (1990-2017).", "AI": {"tldr": "提出了一种利用大型语言模型(LLMs)的成本效益结构化方法，将学者与机器的优势结合，弥补各自弱点，通过思维链和少样本学习提示技术，将定性研究的最佳实践扩展到人机协作的定量研究中。", "motivation": "解决LLMs在定量研究中的应用成本高且存在固有弱点的问题，寻求一种经济高效的人机协作方法，让学者能够利用溯因推理和自然语言来审查机器和人类的工作。", "method": "采用思维链(chain of thought)和少样本学习(few-shot learning)提示技术，构建结构化方法，将定性研究的合著团队最佳实践扩展到人机团队的定量研究中，通过低成本技术管理LLMs的固有弱点。", "result": "成功应用该方法分析了1990-2017年间1,934份制药联盟新闻稿中的人机评分差异问题，验证了方法的有效性。", "conclusion": "该方法提供了一种成本效益高且简约的方式，使学者能够有效利用LLMs进行定量研究，同时通过人机协作弥补各自局限，为量化研究提供了新的方法论框架。"}}
{"id": "2512.07608", "pdf": "https://arxiv.org/pdf/2512.07608", "abs": "https://arxiv.org/abs/2512.07608", "authors": ["Jing Wang", "Jie Shen", "Xing Niu", "Tong Zhang", "Jeremy Weiss"], "title": "Metric-Fair Prompting: Treating Similar Samples Similarly", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce \\emph{Metric-Fair Prompting}, a fairness-aware prompting framework that guides large language models (LLMs) to make decisions under metric-fairness constraints. In the application of multiple-choice medical question answering, each {(question, option)} pair is treated as a binary instance with label $+1$ (correct) or $-1$ (incorrect). To promote {individual fairness}~--~treating similar instances similarly~--~we compute question similarity using NLP embeddings and solve items in \\emph{joint pairs of similar questions} rather than in isolation. The prompt enforces a global decision protocol: extract decisive clinical features, map each \\((\\text{question}, \\text{option})\\) to a score $f(x)$ that acts as confidence, and impose a Lipschitz-style constraint so that similar inputs receive similar scores and, hence, consistent outputs. Evaluated on the {MedQA (US)} benchmark, Metric-Fair Prompting is shown to improve performance over standard single-item prompting, demonstrating that fairness-guided, confidence-oriented reasoning can enhance LLM accuracy on high-stakes clinical multiple-choice questions.", "AI": {"tldr": "提出Metric-Fair Prompting框架，通过度量公平性约束指导大语言模型在医学选择题问答中做出更公平和准确的决策，相比标准单题提示方法提升了性能。", "motivation": "为了解决大语言模型在医学问答中的公平性问题，确保相似问题得到相似处理（个体公平性），提高高风险临床选择题的准确性和一致性。", "method": "使用NLP嵌入计算问题相似度，将相似问题作为联合对处理；在提示中强制实施全局决策协议：提取关键临床特征，将(问题,选项)映射为置信度分数，并施加Lipschitz式约束确保相似输入获得相似输出。", "result": "在MedQA (US)基准测试中，Metric-Fair Prompting相比标准单题提示方法提升了性能表现。", "conclusion": "公平性引导、置信度导向的推理方法能够有效提升大语言模型在高风险临床选择题上的准确性，证明了度量公平性约束的实际价值。"}}
{"id": "2512.07552", "pdf": "https://arxiv.org/pdf/2512.07552", "abs": "https://arxiv.org/abs/2512.07552", "authors": ["Francois Vandenhende", "Anna Georgiou", "Michalis Georgiou", "Theodoros Psaras", "Ellie Karekla", "Elena Hadjicosta"], "title": "Performance of the SafeTerm AI-Based MedDRA Query System Against Standardised MedDRA Queries", "categories": ["cs.CL"], "comment": "8 pages, 3 figures", "summary": "In pre-market drug safety review, grouping related adverse event terms into SMQs or OCMQs is critical for signal detection. We assess the performance of SafeTerm Automated Medical Query (AMQ) on MedDRA SMQs. The AMQ is a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score (0-1) using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity, and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against tier-1 SMQs (110 queries, v28.1). Precision, recall and F1 were computed at multiple similarity-thresholds, defined either manually or using an automated method. High recall (94%)) is achieved at moderate similarity thresholds, indicative of good retrieval sensitivity. Higher thresholds filter out more terms, resulting in improved precision (up to 89%). The optimal threshold (0.70)) yielded an overall recall of (48%) and precision of (45%) across all 110 queries. Restricting to narrow-term PTs achieved slightly better performance at an increased (+0.05) similarity threshold, confirming increased relatedness of narrow versus broad terms. The automatic threshold (0.66) selection prioritizes recall (0.58) to precision (0.29). SafeTerm AMQ achieves comparable, satisfactory performance on SMQs and sanitized OCMQs. It is therefore a viable supplementary method for automated MedDRA query generation, balancing recall and precision. We recommend using suitable MedDRA PT terminology in query formulation and applying the automated threshold method to optimise recall. Increasing similarity scores allows refined, narrow terms selection.", "AI": {"tldr": "SafeTerm AMQ是一种基于AI的自动化医学查询系统，通过多维向量空间嵌入和余弦相似性分析，能够自动检索和排名MedDRA术语，在药物安全监测中表现出良好的召回率和精确度平衡。", "motivation": "在药物上市前安全性审查中，将相关不良事件术语分组到SMQs或OCMQs中对信号检测至关重要，需要自动化工具来提高MedDRA术语检索的效率和准确性。", "method": "开发了SafeTerm AMQ系统，将医学查询术语和MedDRA PTs嵌入多维向量空间，应用余弦相似性和极值聚类方法，生成按相关性分数(0-1)排序的PT列表。使用110个SMQs进行验证，计算不同相似度阈值下的精确率、召回率和F1分数。", "result": "在中等相似度阈值下实现高召回率(94%)，较高阈值下精确度可达89%。最优阈值(0.70)下总体召回率48%、精确度45%。自动阈值选择(0.66)优先召回率(0.58)而非精确度(0.29)。", "conclusion": "SafeTerm AMQ在SMQs和OCMQs上表现良好，是自动化MedDRA查询生成的可行补充方法，建议使用合适的MedDRA PT术语并应用自动阈值方法来优化召回率。"}}
{"id": "2512.07612", "pdf": "https://arxiv.org/pdf/2512.07612", "abs": "https://arxiv.org/abs/2512.07612", "authors": ["Kairong Luo", "Zhenbo Sun", "Xinyu Shi", "Shengqi Chen", "Bowen Yu", "Yunyi Chen", "Chenyi Dang", "Hengtao Tao", "Hui Wang", "Fangming Liu", "Kaifeng Lyu", "Wenguang Chen"], "title": "PCMind-2.1-Kaiyuan-2B Technical Report", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.", "AI": {"tldr": "PCMind-2.1-Kaiyuan-2B是一个20亿参数的全开源模型，通过创新的数据筛选、训练策略和课程学习方法，在资源受限条件下实现了与最先进开源模型竞争的性能。", "motivation": "解决开源社区与工业界在大型语言模型发展中的知识鸿沟，因为工业界依赖闭源的高质量数据和训练方案，而开源社区缺乏这些资源。", "method": "采用三种关键创新：1)分位数数据基准方法比较异构开源数据集；2)多阶段范式中的策略性选择性重复方案利用稀疏高质量数据；3)多领域课程训练策略按质量排序样本。配合优化的数据预处理流程和FP16稳定性架构修改。", "result": "Kaiyuan-2B在性能上达到了与最先进全开源模型竞争的水平，为资源受限的预训练提供了实用且可扩展的解决方案。", "conclusion": "该研究展示了在资源有限条件下通过创新的数据管理和训练策略可以开发出高性能的开源语言模型，所有资产（模型权重、数据和代码）均在Apache 2.0许可下开源。"}}
{"id": "2512.07571", "pdf": "https://arxiv.org/pdf/2512.07571", "abs": "https://arxiv.org/abs/2512.07571", "authors": ["Nicolas Calbucura", "Valentin Barriere"], "title": "A Simple Method to Enhance Pre-trained Language Models with Speech Tokens for Classification", "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "This paper presents a simple method that allows to easily enhance textual pre-trained large language models with speech information, when fine-tuned for a specific classification task. A classical issue with the fusion of many embeddings from audio with text is the large length of the audio sequence compared to the text one. Our method benefits from an existing speech tokenizer trained for Audio Speech Recognition that output long sequences of tokens from a large vocabulary, making it difficult to integrate it at low cost in a large language model. By applying a simple lasso-based feature selection on multimodal Bag-of-Words representation, we retain only the most important audio tokens for the task, and adapt the language model to them with a self-supervised language modeling objective, before fine-tuning it on the downstream task. We show this helps to improve the performances compared to an unimodal model, to a bigger SpeechLM or to integrating audio via a learned representation. We show the effectiveness of our method on two recent Argumentative Fallacy Detection and Classification tasks where the use of audio was believed counterproductive, reaching state-of-the-art results. We also provide an in-depth analysis of the method, showing that even a random audio token selection helps enhancing the unimodal model. Our code is available [online](https://github.com/salocinc/EACL26SpeechTokFallacy/).", "AI": {"tldr": "提出一种简单方法，通过LASSO特征选择将语音信息有效整合到文本预训练大语言模型中，在论证谬误检测任务上达到SOTA效果", "motivation": "解决音频序列长度远大于文本序列导致的多模态融合困难，以及现有语音标记器输出长序列标记难以低成本整合到大语言模型中的问题", "method": "使用ASR语音标记器获取音频标记，通过LASSO特征选择保留重要音频标记，采用自监督语言建模目标适配语言模型，最后在下游任务上进行微调", "result": "相比单模态模型、更大规模SpeechLM或学习表示整合方法，性能均有提升，在论证谬误检测分类任务上达到state-of-the-art结果", "conclusion": "该方法有效整合语音信息提升文本模型性能，即使随机选择音频标记也能增强单模态模型，为多模态融合提供了简单有效的解决方案"}}
{"id": "2512.07684", "pdf": "https://arxiv.org/pdf/2512.07684", "abs": "https://arxiv.org/abs/2512.07684", "authors": ["Zihan Chen", "Lanyu Yu"], "title": "When Large Language Models Do Not Work: Online Incivility Prediction through Graph Neural Networks", "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": "10 pages", "summary": "Online incivility has emerged as a widespread and persistent problem in digital communities, imposing substantial social and psychological burdens on users. Although many platforms attempt to curb incivility through moderation and automated detection, the performance of existing approaches often remains limited in both accuracy and efficiency. To address this challenge, we propose a Graph Neural Network (GNN) framework for detecting three types of uncivil behavior (i.e., toxicity, aggression, and personal attacks) within the English Wikipedia community. Our model represents each user comment as a node, with textual similarity between comments defining the edges, allowing the network to jointly learn from both linguistic content and relational structures among comments. We also introduce a dynamically adjusted attention mechanism that adaptively balances nodal and topological features during information aggregation. Empirical evaluations demonstrate that our proposed architecture outperforms 12 state-of-the-art Large Language Models (LLMs) across multiple metrics while requiring significantly lower inference cost. These findings highlight the crucial role of structural context in detecting online incivility and address the limitations of text-only LLM paradigms in behavioral prediction. All datasets and comparative outputs will be publicly available in our repository to support further research and reproducibility.", "AI": {"tldr": "本文提出了一种基于图神经网络(GNN)的框架，用于检测维基百科社区中的不文明行为，通过结合文本内容和评论间的关系结构，在准确性和效率上优于12种大型语言模型。", "motivation": "在线不文明行为在数字社区中普遍存在，给用户带来社会和心理负担。现有方法在准确性和效率上存在局限，需要更好的检测方案。", "method": "使用图神经网络框架，将用户评论表示为节点，文本相似性定义边，引入动态调整的注意力机制来平衡节点和拓扑特征。", "result": "提出的架构在多个指标上优于12种最先进的大型语言模型，同时显著降低推理成本。", "conclusion": "结构上下文在检测在线不文明行为中起关键作用，解决了纯文本LLM范式在行为预测中的局限性。所有数据集和比较输出将公开以支持进一步研究。"}}
{"id": "2512.07801", "pdf": "https://arxiv.org/pdf/2512.07801", "abs": "https://arxiv.org/abs/2512.07801", "authors": ["Raunak Jain", "Mudita Khurana"], "title": "Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.", "AI": {"tldr": "论文提出协作因果意义构建(CCS)框架，将AI助手重新构想为参与人类专家认知过程的合作伙伴，通过共同构建心智模型、目标和因果假设，实现真正的人机互补决策支持。", "motivation": "当前基于LLM的决策支持系统在复杂高风险环境中表现不佳，人机团队往往表现不如最佳个体，专家在验证循环和过度依赖间摇摆，承诺的互补性未能实现。", "method": "提出协作因果意义构建(CCS)研究议程和组织框架，设计作为认知工作伙伴的系统，维护专家推理的演化模型，帮助阐述和修订目标，共同构建和压力测试因果假设，从联合决策结果中学习。", "result": "构建了一个理论框架，指出了训练生态、共同建模的表征和交互协议、以信任和互补性为中心的评价等关键挑战方向。", "conclusion": "CCS框架可以重新定位MAS研究，围绕参与协作意义构建的智能体，使其成为与人类伙伴共同思考的AI队友，实现真正的人机协同决策。"}}
{"id": "2512.07666", "pdf": "https://arxiv.org/pdf/2512.07666", "abs": "https://arxiv.org/abs/2512.07666", "authors": ["Zeqi Chen", "Zhaoyang Chu", "Yi Gui", "Feng Guo", "Yao Wan", "Chuan Shi"], "title": "Bridging Code Graphs and Large Language Models for Better Code Understanding", "categories": ["cs.CL", "cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in code intelligence tasks such as code generation, summarization, and translation. However, their reliance on linearized token sequences limits their ability to understand the structural semantics of programs. While prior studies have explored graphaugmented prompting and structure-aware pretraining, they either suffer from prompt length constraints or require task-specific architectural changes that are incompatible with large-scale instructionfollowing LLMs. To address these limitations, this paper proposes CGBridge, a novel plug-and-play method that enhances LLMs with Code Graph information through an external, trainable Bridge module. CGBridge first pre-trains a code graph encoder via selfsupervised learning on a large-scale dataset of 270K code graphs to learn structural code semantics. It then trains an external module to bridge the modality gap among code, graph, and text by aligning their semantics through cross-modal attention mechanisms. Finally, the bridge module generates structure-informed prompts, which are injected into a frozen LLM, and is fine-tuned for downstream code intelligence tasks. Experiments show that CGBridge achieves notable improvements over both the original model and the graphaugmented prompting method. Specifically, it yields a 16.19% and 9.12% relative gain in LLM-as-a-Judge on code summarization, and a 9.84% and 38.87% relative gain in Execution Accuracy on code translation. Moreover, CGBridge achieves over 4x faster inference than LoRA-tuned models, demonstrating both effectiveness and efficiency in structure-aware code understanding.", "AI": {"tldr": "CGBridge是一种即插即用的方法，通过外部可训练的桥接模块将代码图信息融入大型语言模型，提升代码结构理解能力，在代码摘要和翻译任务中取得显著性能提升且推理速度更快。", "motivation": "大型语言模型在代码智能任务中表现优异，但对线性化token序列的依赖限制了其对程序结构语义的理解能力。现有方法要么受限于提示长度约束，要么需要特定任务架构改动，无法兼容大规模指令跟随LLM。", "method": "提出CGBridge方法：1）在大规模27万代码图数据集上通过自监督学习预训练代码图编码器；2）训练外部模块通过跨模态注意力机制对齐代码、图和文本语义；3）桥接模块生成结构感知提示并注入冻结的LLM中，针对下游任务进行微调。", "result": "CGBridge相比原始模型和图增强提示方法有显著改进：代码摘要任务LLM-as-a-Judge相对提升16.19%和9.12%；代码翻译任务执行准确率相对提升9.84%和38.87%；推理速度比LoRA调优模型快4倍以上。", "conclusion": "CGBridge通过外部桥接模块有效融合代码结构信息，在不改变LLM架构的情况下提升了代码理解能力，同时保持了高效推理速度，为结构感知的代码智能任务提供了有效解决方案。"}}
{"id": "2512.07687", "pdf": "https://arxiv.org/pdf/2512.07687", "abs": "https://arxiv.org/abs/2512.07687", "authors": ["Sujoy Nath", "Arkaprabha Basu", "Sharanya Dasgupta", "Swagatam Das"], "title": "HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding tasks. While these models often produce linguistically coherent output, they often suffer from hallucinations, generating descriptions that are factually inconsistent with the visual content, potentially leading to adverse consequences. Therefore, the assessment of hallucinations in MLLM has become increasingly crucial in the model development process. Contemporary methodologies predominantly depend on external LLM evaluators, which are themselves susceptible to hallucinations and may present challenges in terms of domain adaptation. In this study, we propose the hypothesis that hallucination manifests as measurable irregularities within the internal layer dynamics of MLLMs, not merely due to distributional shifts but also in the context of layer-wise analysis of specific assumptions. By incorporating such modifications, \\textsc{\\textsc{HalluShift++}} broadens the efficacy of hallucination detection from text-based large language models (LLMs) to encompass multimodal scenarios. Our codebase is available at https://github.com/C0mRD/HalluShift_Plus.", "AI": {"tldr": "HalluShift++是一个检测多模态大语言模型幻觉的新方法，通过分析模型内部层动态的异常来识别事实不一致的描述，相比依赖外部LLM评估器的方法更加可靠。", "motivation": "多模态大语言模型虽然能产生语言连贯的输出，但经常出现与视觉内容事实不一致的幻觉问题，现有方法依赖外部LLM评估器但同样存在幻觉问题且领域适应性差。", "method": "提出假设认为幻觉表现为MLLM内部层动态的可测量异常，通过层间分析特定假设来检测幻觉，将基于文本LLM的幻觉检测扩展到多模态场景。", "result": "开发了HalluShift++方法，能够更有效地检测多模态模型中的幻觉问题。", "conclusion": "通过分析内部层动态异常来检测幻觉的方法比依赖外部评估器更有效，HalluShift++为多模态模型开发提供了重要的幻觉评估工具。"}}
{"id": "2512.07694", "pdf": "https://arxiv.org/pdf/2512.07694", "abs": "https://arxiv.org/abs/2512.07694", "authors": ["Francois Vandenhende", "Anna Georgiou", "Michalis Georgiou", "Theodoros Psaras", "Ellie Karekla", "Elena Hadjicosta"], "title": "Automated Generation of Custom MedDRA Queries Using SafeTerm Medical Map", "categories": ["cs.CL"], "comment": "12 pages, 4 figures", "summary": "In pre-market drug safety review, grouping related adverse event terms into standardised MedDRA queries or the FDA Office of New Drugs Custom Medical Queries (OCMQs) is critical for signal detection. We present a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against the FDA OCMQ v3.0 (104 queries), restricted to valid MedDRA PTs. Precision, recall and F1 were computed across similarity-thresholds. High recall (>95%) is achieved at moderate thresholds. Higher thresholds improve precision (up to 86%). The optimal threshold (~0.70 - 0.75) yielded recall ~50% and precision ~33%. Narrow-term PT subsets performed similarly but required slightly higher similarity thresholds. The SafeTerm AI-driven system provides a viable supplementary method for automated MedDRA query generation. A similarity threshold of ~0.60 is recommended initially, with increased thresholds for refined term selection.", "AI": {"tldr": "开发了SafeTerm AI系统，通过多维向量空间嵌入和相似度计算，自动为医学查询检索相关MedDRA术语，提供药物安全监测的自动化解决方案。", "motivation": "在药物安全审查中，将相关不良事件术语标准化分组对于信号检测至关重要，需要自动化工具来高效处理医学术语并生成MedDRA查询。", "method": "系统将医学查询术语和MedDRA术语嵌入多维向量空间，应用余弦相似度和极值聚类方法，生成按相关性排序的术语列表。", "result": "在FDA OCMQ v3.0验证中，中等阈值下召回率>95%，高阈值下精确度达86%，最优阈值(0.70-0.75)下召回率50%、精确度33%。", "conclusion": "SafeTerm系统为自动化MedDRA查询生成提供了可行的补充方法，建议初始使用0.60相似度阈值，需要更精确选择时可提高阈值。"}}
{"id": "2512.07777", "pdf": "https://arxiv.org/pdf/2512.07777", "abs": "https://arxiv.org/abs/2512.07777", "authors": ["Karin de Langis", "Püren Öncel", "Ryan Peters", "Andrew Elfenbein", "Laura Kristen Allen", "Andreas Schramm", "Dongyeop Kang"], "title": "Mary, the Cheeseburger-Eating Vegetarian: Do LLMs Recognize Incoherence in Narratives?", "categories": ["cs.CL"], "comment": null, "summary": "Leveraging a dataset of paired narratives, we investigate the extent to which large language models (LLMs) can reliably separate incoherent and coherent stories. A probing study finds that LLMs' internal representations can reliably identify incoherent narratives. However, LLMs generate responses to rating questions that fail to satisfactorily separate the coherent and incoherent narratives across several prompt variations, hinting at a gap in LLM's understanding of storytelling. The reasoning LLMs tested do not eliminate these deficits, indicating that thought strings may not be able to fully address the discrepancy between model internal state and behavior. Additionally, we find that LLMs appear to be more sensitive to incoherence resulting from an event that violates the setting (e.g., a rainy day in the desert) than to incoherence arising from a character violating an established trait (e.g., Mary, a vegetarian, later orders a cheeseburger), suggesting that LLMs may rely more on prototypical world knowledge than building meaning-based narrative coherence. The consistent asymmetry found in our results suggests that LLMs do not have a complete grasp on narrative coherence.", "AI": {"tldr": "研究发现LLM虽然内部表征能识别不连贯叙事，但在回答评分问题时无法可靠区分连贯与不连贯故事，表明LLM对叙事连贯性的理解存在缺陷", "motivation": "探究大型语言模型是否能够可靠地区分连贯与不连贯的叙事故事", "method": "使用配对叙事数据集进行探测研究，测试LLM在不同提示变体下的表现，包括使用思维链推理方法", "result": "LLM内部表征能识别不连贯叙事，但生成的评分回答无法有效区分连贯性；对设定违反比角色特质违反更敏感；思维链方法无法消除这种缺陷", "conclusion": "LLM对叙事连贯性缺乏完整理解，更依赖原型世界知识而非基于意义的叙事连贯性构建"}}
{"id": "2512.07783", "pdf": "https://arxiv.org/pdf/2512.07783", "abs": "https://arxiv.org/abs/2512.07783", "authors": ["Charlie Zhang", "Graham Neubig", "Xiang Yue"], "title": "On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.", "AI": {"tldr": "该研究通过完全受控的实验框架，揭示了预训练、中期训练和RL后训练在语言模型推理能力发展中的因果作用，发现RL仅在预训练留有足够空间且针对模型能力边界时才能产生真正的能力提升。", "motivation": "为了解决当前强化学习技术是否真正扩展了语言模型的推理能力，以及预训练、中期训练和RL后训练之间复杂的相互作用问题，研究者需要建立一个完全受控的实验框架来隔离这些因素的因果贡献。", "method": "采用合成推理任务，包含明确的原子操作、可解析的逐步推理轨迹，以及系统性地操纵训练分布。评估模型在两个维度：外推泛化到更复杂组合，以及跨表面上下文的上下文泛化。", "result": "研究发现：1) RL仅在预训练留有足够空间且针对模型能力边界时产生真正能力提升；2) 上下文泛化需要最小但充分的预训练暴露；3) 中期训练在固定计算量下显著提升性能；4) 过程级奖励减少奖励破解并提高推理保真度。", "conclusion": "这些结果阐明了预训练、中期训练和RL之间的相互作用，为理解和改进推理语言模型的训练策略提供了基础，强调了中期训练在训练流程中的核心但未被充分探索的作用。"}}
{"id": "2512.07832", "pdf": "https://arxiv.org/pdf/2512.07832", "abs": "https://arxiv.org/abs/2512.07832", "authors": ["Matteo Boglioni", "Andrea Sgobbi", "Gabriel Tavernini", "Francesco Rita", "Marius Mosbach", "Tiago Pimentel"], "title": "Do Generalisation Results Generalise?", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model's performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.", "AI": {"tldr": "该研究探讨了大型语言模型在不同分布外测试集上的泛化性能是否具有一致性，发现泛化结果没有统一趋势，不同测试集间的相关性高度依赖于具体模型选择。", "motivation": "以往评估LLM泛化能力的研究通常只使用单一分布外数据集，无法准确反映模型部署时面临的多样化数据偏移问题。", "method": "通过在微调过程中评估模型在多个分布外测试集上的性能，并控制域内性能后计算这些测试集性能之间的偏相关系数。", "result": "分析OLMo2和OPT模型发现，任何两个分布外测试集之间的正负相关性存在与否，强烈依赖于所分析的具体模型选择。", "conclusion": "LLM的分布外泛化性能评估需要更全面的测试集覆盖，因为泛化结果在不同测试集间缺乏一致性，不能从单一测试集结果推断整体泛化能力。"}}
