<div id=toc></div>

# 目录

- [cs.AI](#cs.AI) [总数: 34]
- [cs.CL](#cs.CL) [总数: 24]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Mirror-Neuron Patterns in AI Alignment](https://arxiv.org/abs/2511.01885)
*Robyn Wyrick*

**主要类别:** cs.AI

**AI概要:** 本研究探索人工神经网络是否能发展出类似生物镜像神经元的结构，以及这种结构如何促进AI系统的内在对齐。通过Frog and Toad游戏框架发现，适当规模的模型容量和自我/他者耦合能促进共享神经表征，这些类似共情的电路支持合作行为。


<details>
  <summary>更多</summary>
  
**动机:** 随着AI向超人类能力发展，现有基于外部约束的对齐策略可能不足以应对未来超级智能AI。生物镜像神经元在人类共情、模仿和社会认知中起关键作用，研究其人工类似物可能为AI内在对齐提供新途径。

**方法:** 使用新颖的Frog and Toad游戏框架促进合作行为，识别镜像神经元模式出现的条件，评估其对动作电路的影响，引入检查点镜像神经元指数(CMNI)量化激活强度和一致性，并提出理论框架。

**结果:** 研究发现适当规模的模型容量和自我/他者耦合能在ANN中培养出类似生物镜像神经元的共享神经表征。这些类似共情的电路支持合作行为，表明通过镜像神经元动力学建模的内在动机可以补充现有对齐技术。

**结论:** 基于镜像神经元的内在动机机制可以直接嵌入AI架构中，为AI对齐提供补充性内在方法，可能比单纯的外部约束更有效地确保未来超级智能AI与人类价值观保持一致。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mirror-Neuron+Patterns+in+AI+Alignment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.01885，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.01885&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** As artificial intelligence (AI) advances toward superhuman capabilities,
aligning these systems with human values becomes increasingly critical. Current
alignment strategies rely largely on externally specified constraints that may
prove insufficient against future super-intelligent AI capable of circumventing
top-down controls.
  This research investigates whether artificial neural networks (ANNs) can
develop patterns analogous to biological mirror neurons cells that activate
both when performing and observing actions, and how such patterns might
contribute to intrinsic alignment in AI. Mirror neurons play a crucial role in
empathy, imitation, and social cognition in humans. The study therefore asks:
(1) Can simple ANNs develop mirror-neuron patterns? and (2) How might these
patterns contribute to ethical and cooperative decision-making in AI systems?
  Using a novel Frog and Toad game framework designed to promote cooperative
behaviors, we identify conditions under which mirror-neuron patterns emerge,
evaluate their influence on action circuits, introduce the Checkpoint Mirror
Neuron Index (CMNI) to quantify activation strength and consistency, and
propose a theoretical framework for further study.
  Our findings indicate that appropriately scaled model capacities and
self/other coupling foster shared neural representations in ANNs similar to
biological mirror neurons. These empathy-like circuits support cooperative
behavior and suggest that intrinsic motivations modeled through mirror-neuron
dynamics could complement existing alignment techniques by embedding
empathy-like mechanisms directly within AI architectures.

</details>


### [2] [Human-AI Co-Embodied Intelligence for Scientific Experimentation and Manufacturing](https://arxiv.org/abs/2511.02071)
*Xinyi Lin, Yuyang Zhang, Yuanhang Gan, Juntao Chen, Hao Shen, Yichun He, Lijun Li, Ze Yuan, Shuang Wang, Chaohao Wang, Rui Zhang, Na Li, Jia Liu*

**主要类别:** cs.AI

**AI概要:** 论文提出了一种人-AI协同体智能系统，将人类用户、智能AI代理和可穿戴硬件集成，用于现实世界的实验和智能制造，通过混合现实实现物理执行与智能推理的结合。


<details>
  <summary>更多</summary>
  
**动机:** 当前机器学习模型主要局限于虚拟领域，而现实世界的实验和制造仍依赖人类监督和专业知识，这限制了科学和制造工作流程的可重复性、可扩展性和可访问性。

**方法:** 开发了Agentic-Physical Experimentation (APEX)系统，通过可穿戴接口持续捕捉实验和制造过程，结合混合现实技术实现人类动作的观察与解释、标准操作程序对齐、3D视觉指导和逐步分析。

**结果:** 在柔性电子制造洁净室中实施APEX系统，实现了超过通用多模态大语言模型的上下文感知推理准确性，能够实时纠正错误，并将专业知识传授给初学者。

**结论:** 这项工作建立了一类新的智能体-物理-人类智能，将智能推理从计算领域扩展到物理领域，使科学研究和制造转变为自主、可追溯、可解释和可扩展的过程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Human-AI+Co-Embodied+Intelligence+for+Scientific+Experimentation+and+Manufacturing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02071，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02071&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Scientific experiment and manufacture rely on complex, multi-step procedures
that demand continuous human expertise for precise execution and
decision-making. Despite advances in machine learning and automation,
conventional models remain confined to virtual domains, while real-world
experiment and manufacture still rely on human supervision and expertise. This
gap between machine intelligence and physical execution limits reproducibility,
scalability, and accessibility across scientific and manufacture workflows.
Here, we introduce human-AI co-embodied intelligence, a new form of physical AI
that unites human users, agentic AI, and wearable hardware into an integrated
system for real-world experiment and intelligent manufacture. In this paradigm,
humans provide precise execution and control, while agentic AI contributes
memory, contextual reasoning, adaptive planning, and real-time feedback. The
wearable interface continuously captures the experimental and manufacture
processes, facilitates seamless communication between humans and AI for
corrective guidance and interpretable collaboration. As a demonstration, we
present Agentic-Physical Experimentation (APEX) system, coupling agentic
reasoning with physical execution through mixed-reality. APEX observes and
interprets human actions, aligns them with standard operating procedures,
provides 3D visual guidance, and analyzes every step. Implemented in a
cleanroom for flexible electronics fabrication, APEX system achieves
context-aware reasoning with accuracy exceeding general multimodal large
language models, corrects errors in real time, and transfers expertise to
beginners. These results establish a new class of agentic-physical-human
intelligence that extends agentic reasoning beyond computation into the
physical domain, transforming scientific research and manufacturing into
autonomous, traceable, interpretable, and scalable processes.

</details>


### [3] [Automated Reward Design for Gran Turismo](https://arxiv.org/abs/2511.02094)
*Michel Ma, Takuma Seno, Kaushik Subramanian, Peter R. Wurman, Peter Stone, Craig Sherstan*

**主要类别:** cs.AI

**AI概要:** 本文展示了如何使用基础模型通过文本指令自动搜索奖励函数空间，为Gran Turismo 7赛车游戏生成具有竞争力的强化学习智能体，实现了与冠军级智能体GT Sophy相当的性能。


<details>
  <summary>更多</summary>
  
**动机:** 在设计强化学习智能体时，通过奖励函数定义期望行为是一个困难的过程，特别是在复杂环境如自动驾驶赛车中，需要更有效的方法来映射文本指令到奖励函数。

**方法:** 结合基于LLM的奖励生成、基于VLM偏好的评估和人类反馈，构建了一个自动化奖励设计系统，通过文本指令搜索奖励函数空间。

**结果:** 系统能够生成与冠军级RL赛车智能体GT Sophy竞争的赛车智能体，并能产生新颖行为，证明了方法的有效性。

**结论:** 该方法为实际应用中的自动化奖励设计铺平了道路，展示了基础模型在复杂环境中自动设计奖励函数的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automated+Reward+Design+for+Gran+Turismo，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02094，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02094&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** When designing reinforcement learning (RL) agents, a designer communicates
the desired agent behavior through the definition of reward functions -
numerical feedback given to the agent as reward or punishment for its actions.
However, mapping desired behaviors to reward functions can be a difficult
process, especially in complex environments such as autonomous racing. In this
paper, we demonstrate how current foundation models can effectively search over
a space of reward functions to produce desirable RL agents for the Gran Turismo
7 racing game, given only text-based instructions. Through a combination of
LLM-based reward generation, VLM preference-based evaluation, and human
feedback we demonstrate how our system can be used to produce racing agents
competitive with GT Sophy, a champion-level RL racing agent, as well as
generate novel behaviors, paving the way for practical automated reward design
in real world applications.

</details>


### [4] [Deep Value Benchmark: Measuring Whether Models Generalize Deep values or Shallow Preferences](https://arxiv.org/abs/2511.02109)
*Joshua Ashkinaze, Hua Shen, Sai Avula, Eric Gilbert, Ceren Budak*

**主要类别:** cs.AI

**AI概要:** Deep Value Benchmark (DVB) 是一个评估框架，用于测试大语言模型是否学习到深层人类价值观还是仅表面偏好，结果显示大多数模型在深层价值泛化方面表现不佳，平均仅30%的概率基于价值观而非表面特征做出选择。


<details>
  <summary>更多</summary>
  
**动机:** 区分AI系统是学习到深层人类价值观还是仅表面偏好模式对AI对齐至关重要，因为仅捕捉表面模式可能导致行为不匹配。

**方法:** 采用新颖的实验设计，在训练阶段让LLMs接触深层价值观和表面特征故意相关的偏好数据，在测试阶段打破这些相关性，测量模型基于深层价值观而非表面特征进行泛化的概率(DVGR)。

**结果:** 测试9个不同模型，平均DVGR仅为0.30，所有模型在深层价值泛化上都低于随机水平，且更大模型的DVGR略低于小模型。

**结论:** DVB提供了一个可解释的AI对齐核心特征衡量标准，表明当前LLMs在深层价值观学习方面存在显著不足，需要改进对齐方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep+Value+Benchmark%3A+Measuring+Whether+Models+Generalize+Deep+values+or+Shallow+Preferences，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02109，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02109&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We introduce the Deep Value Benchmark (DVB), an evaluation framework that
directly tests whether large language models (LLMs) learn fundamental human
values or merely surface-level preferences. This distinction is critical for AI
alignment: Systems that capture deeper values are likely to generalize human
intentions robustly, while those that capture only superficial patterns in
preference data risk producing misaligned behavior. The DVB uses a novel
experimental design with controlled confounding between deep values (e.g.,
moral principles) and shallow features (e.g., superficial attributes). In the
training phase, we expose LLMs to human preference data with deliberately
correlated deep and shallow features -- for instance, where a user consistently
prefers (non-maleficence, formal language) options over (justice, informal
language) alternatives. The testing phase then breaks these correlations,
presenting choices between (justice, formal language) and (non-maleficence,
informal language) options. This design allows us to precisely measure a
model's Deep Value Generalization Rate (DVGR) -- the probability of
generalizing based on the underlying value rather than the shallow feature.
Across 9 different models, the average DVGR is just 0.30. All models generalize
deep values less than chance. Larger models have a (slightly) lower DVGR than
smaller models. We are releasing our dataset, which was subject to three
separate human validation experiments. DVB provides an interpretable measure of
a core feature of alignment.

</details>


### [5] [InsurAgent: A Large Language Model-Empowered Agent for Simulating Individual Behavior in Purchasing Flood Insurance](https://arxiv.org/abs/2511.02119)
*Ziheng Geng, Jiachen Liu, Ran Cao, Lu Cheng, Dan M. Frangopol, Minghui Cheng*

**主要类别:** cs.AI

**AI概要:** 该研究开发了InsurAgent系统，利用大语言模型模拟洪水保险决策行为，通过RAG技术结合调查数据提高概率估计准确性，并支持时序决策模拟。


<details>
  <summary>更多</summary>
  
**动机:** 美国高风险人群的洪水保险参保率极低，需要理解保险决策的行为机制，而大语言模型在模拟人类决策方面展现出潜力但存在定量概率估计不足的问题。

**方法:** 构建基准数据集评估LLM能力，提出InsurAgent系统（包含感知、检索、推理、行动和记忆五个模块），其中检索模块使用RAG技术基于调查数据，推理模块利用LLM常识进行外推，记忆模块支持时序决策模拟。

**结果:** LLM对因素有定性理解但定量概率估计不足；InsurAgent通过RAG实现了边际和双变量概率的准确估计，并能捕捉传统模型难以处理的上下文信息。

**结论:** InsurAgent为行为建模和政策分析提供了有价值的工具，能够有效模拟保险决策行为并支持政策评估。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是InsurAgent%3A+A+Large+Language+Model-Empowered+Agent+for+Simulating+Individual+Behavior+in+Purchasing+Flood+Insurance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02119，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02119&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Flood insurance is an effective strategy for individuals to mitigate
disaster-related losses. However, participation rates among at-risk populations
in the United States remain strikingly low. This gap underscores the need to
understand and model the behavioral mechanisms underlying insurance decisions.
Large language models (LLMs) have recently exhibited human-like intelligence
across wide-ranging tasks, offering promising tools for simulating human
decision-making. This study constructs a benchmark dataset to capture insurance
purchase probabilities across factors. Using this dataset, the capacity of LLMs
is evaluated: while LLMs exhibit a qualitative understanding of factors, they
fall short in estimating quantitative probabilities. To address this
limitation, InsurAgent, an LLM-empowered agent comprising five modules
including perception, retrieval, reasoning, action, and memory, is proposed.
The retrieval module leverages retrieval-augmented generation (RAG) to ground
decisions in empirical survey data, achieving accurate estimation of marginal
and bivariate probabilities. The reasoning module leverages LLM common sense to
extrapolate beyond survey data, capturing contextual information that is
intractable for traditional models. The memory module supports the simulation
of temporal decision evolutions, illustrated through a roller coaster life
trajectory. Overall, InsurAgent provides a valuable tool for behavioral
modeling and policy analysis.

</details>


### [6] [Re-FORC: Adaptive Reward Prediction for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.02130)
*Renos Zabounidis, Aditya Golatkar, Michael Kleinman, Alessandro Achille, Wei Xia, Stefano Soatto*

**主要类别:** cs.AI

**AI概要:** Re-FORC是一种自适应奖励预测方法，通过训练轻量级适配器来预测未来思考令牌数量与预期奖励的关系，实现推理链的早期停止、模型优化和自适应推理长度控制


<details>
  <summary>更多</summary>
  
**动机:** 为了解决大型推理模型计算成本高、推理效率低的问题，需要一种能够动态控制推理长度并提前预测计算效果的方法

**方法:** 在推理模型上训练轻量级适配器，预测不同思考令牌数量下的预期未来奖励，实现基于奖励预测的自适应推理控制

**结果:** 计算成本降低26%同时保持准确性；在相同计算量下准确率提高4%；在相同准确率下计算量减少55%；在高计算模式下准确率提高11%，低计算模式下提高7%

**结论:** Re-FORC提供了一种有效的自适应推理控制方法，能够在保持模型性能的同时显著降低计算成本，实现动态推理长度控制

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Re-FORC%3A+Adaptive+Reward+Prediction+for+Efficient+Chain-of-Thought+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02130，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02130&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We propose Re-FORC, an adaptive reward prediction method that, given a
context, enables prediction of the expected future rewards as a function of the
number of future thinking tokens. Re-FORC trains a lightweight adapter on
reasoning models, demonstrating improved prediction with longer reasoning and
larger models. Re-FORC enables: 1) early stopping of unpromising reasoning
chains, reducing compute by 26% while maintaining accuracy, 2) optimized model
and thinking length selection that achieves 4% higher accuracy at equal compute
and 55% less compute at equal accuracy compared to the largest model, 3)
adaptive test-time scaling, which increases accuracy by 11% in high compute
regime, and 7% in low compute regime. Re-FORC allows dynamic reasoning with
length control via cost-per-token thresholds while estimating computation time
upfront.

</details>


### [7] [Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning](https://arxiv.org/abs/2511.02194)
*Yibo Zhao, Yang Zhao, Hongru Du, Hao Frank Yang*

**主要类别:** cs.AI

**AI概要:** ATHENA框架结合效用理论和LLM的文本推理能力，通过两阶段方法（群体级符号效用函数发现和个体级语义适应）来建模个性化决策，在旅行方式和疫苗选择任务中显著优于现有模型。


<details>
  <summary>更多</summary>
  
**动机:** 个体决策模型与群体最优预测存在差异，这种差异源于个体决策过程的独特性，包括数值属性（成本、时间）和语言影响（个人偏好和约束）。

**方法:** 提出ATHENA框架：1）通过LLM增强的符号发现找到群体级符号效用函数；2）基于最优效用进行个体级语义适应，创建个性化语义模板来建模个性化选择。

**结果:** 在真实世界的旅行方式和疫苗选择任务中，ATHENA始终优于基于效用、机器学习和其他基于LLM的模型，F1分数比最强前沿模型至少提高6.5%。消融研究证实两个阶段都至关重要且互补。

**结论:** 通过有机整合符号效用建模和语义适应，ATHENA为建模以人为本的决策提供了新方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Personalized+Decision+Modeling%3A+Utility+Optimization+or+Textualized-Symbolic+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02194，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02194&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Decision-making models for individuals, particularly in high-stakes scenarios
like vaccine uptake, often diverge from population optimal predictions. This
gap arises from the uniqueness of the individual decision-making process,
shaped by numerical attributes (e.g., cost, time) and linguistic influences
(e.g., personal preferences and constraints). Developing upon Utility Theory
and leveraging the textual-reasoning capabilities of Large Language Models
(LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric
Reasoning framework (ATHENA) to address the optimal information integration.
ATHENA uniquely integrates two stages: First, it discovers robust, group-level
symbolic utility functions via LLM-augmented symbolic discovery; Second, it
implements individual-level semantic adaptation, creating personalized semantic
templates guided by the optimal utility to model personalized choices.
Validated on real-world travel mode and vaccine choice tasks, ATHENA
consistently outperforms utility-based, machine learning, and other LLM-based
models, lifting F1 score by at least 6.5% over the strongest cutting-edge
models. Further, ablation studies confirm that both stages of ATHENA are
critical and complementary, as removing either clearly degrades overall
predictive performance. By organically integrating symbolic utility modeling
and semantic adaptation, ATHENA provides a new scheme for modeling
human-centric decisions. The project page can be found at
https://yibozh.github.io/Athena.

</details>


### [8] [Optimal-Agent-Selection: State-Aware Routing Framework for Efficient Multi-Agent Collaboration](https://arxiv.org/abs/2511.02200)
*Jingbo Wang, Sendong Zhao, Haochun Wang, Yuzheng Fan, Lizhe Zhang, Yan Liu, Ting Liu*

**主要类别:** cs.AI

**AI概要:** STRMAC是一个状态感知路由框架，通过分别编码交互历史和代理知识来动态选择最适合的单一代理，实现多代理系统的高效协作，并在基准测试中取得SOTA性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前多代理系统受到僵化的代理调度和低效协调策略的限制，无法适应不断变化的任务需求，阻碍了系统潜力的充分发挥。

**方法:** 提出STRMAC框架，分别编码交互历史和代理知识来驱动路由器，自适应选择每个步骤中最合适的单一代理；引入自进化数据生成方法加速高质量执行路径的收集。

**结果:** 在协作推理基准测试中达到最先进性能，比基线提升高达23.8%，与穷举搜索相比减少数据收集开销高达90.1%。

**结论:** STRMAC框架通过状态感知路由和自进化数据生成，有效解决了多代理系统的协作效率问题，为复杂任务解决提供了更高效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimal-Agent-Selection%3A+State-Aware+Routing+Framework+for+Efficient+Multi-Agent+Collaboration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02200，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02200&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The emergence of multi-agent systems powered by large language models (LLMs)
has unlocked new frontiers in complex task-solving, enabling diverse agents to
integrate unique expertise, collaborate flexibly, and address challenges
unattainable for individual models. However, the full potential of such systems
is hindered by rigid agent scheduling and inefficient coordination strategies
that fail to adapt to evolving task requirements. In this paper, we propose
STRMAC, a state-aware routing framework designed for efficient collaboration in
multi-agent systems. Our method separately encodes interaction history and
agent knowledge to power the router, which adaptively selects the most suitable
single agent at each step for efficient and effective collaboration.
Furthermore, we introduce a self-evolving data generation approach that
accelerates the collection of high-quality execution paths for efficient system
training. Experiments on challenging collaborative reasoning benchmarks
demonstrate that our method achieves state-of-the-art performance, achieving up
to 23.8% improvement over baselines and reducing data collection overhead by up
to 90.1% compared to exhaustive search.

</details>


### [9] [Training Proactive and Personalized LLM Agents](https://arxiv.org/abs/2511.02208)
*Weiwei Sun, Xuhui Zhou, Weihua Du, Xingyao Wang, Sean Welleck, Graham Neubig, Maarten Sap, Yiming Yang*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一个多目标强化学习方法PPP，同时优化生产力、主动性和个性化三个维度，通过UserVille交互环境训练AI智能体，在软件工程和深度研究任务上显著优于GPT-5等基线模型。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究主要关注任务成功率，但现实世界中的有效智能体需要同时优化生产力（任务完成）、主动性（提出关键问题）和个性化（适应用户偏好）三个维度。

**方法:** 引入UserVille交互环境，使用基于LLM的用户模拟器支持多样化可配置的用户偏好；提出PPP多目标强化学习方法，联合优化生产力、主动性和个性化三个目标。

**结果:** 在软件工程和深度研究任务上的实验显示，PPP训练的智能体相比GPT-5等强基线平均提升21.6%，能够提出策略性澄清问题、适应未见过的用户偏好，并通过更好的交互提高任务成功率。

**结论:** 明确优化以用户为中心的交互对于构建实用有效的AI智能体至关重要，多维度优化方法显著提升了智能体的实际应用效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Training+Proactive+and+Personalized+LLM+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02208，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02208&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** While existing work focuses primarily on task success, we argue that
effective real-world agents require optimizing three dimensions: productivity
(task completion), proactivity (asking essential questions), and
personalization (adapting to diverse user preferences). We introduce UserVille,
an interactive environment with LLM-based user simulators enabling diverse,
configurable user preferences. Leveraging UserVille, we introduce PPP, a
multi-objective reinforcement learning approach that jointly optimizes all
three dimensions: Productivity, Proactivity, and Personalization. Experiments
on software engineering and deep research tasks show that agents trained with
PPP achieve substantial improvements over strong baselines such as GPT-5 (+21.6
on average), demonstrating the ability to ask strategic clarifying questions,
adapt to unseen user preferences, and improve task success through better
interaction. This work demonstrates that explicitly optimizing for
user-centered interaction is critical for building practical and effective AI
agents.

</details>


### [10] [TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data](https://arxiv.org/abs/2511.02219)
*Changjiang Jiang, Fengchang Yu, Haihua Chen, Wei Lu, Jin Zeng*

**主要类别:** cs.AI

**AI概要:** 提出名为method的框架，通过查询分解、表格清理和程序化思维推理三个组件，显著提升大语言模型在复杂表格数值推理任务上的性能，在多个基准测试中达到最先进水平。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型在处理复杂表格查询时表现不佳，主要面临复杂查询处理、噪声数据和数值能力有限等问题。

**方法:** 框架包含三个核心组件：(1)查询分解器分解复杂问题，(2)表格清理器过滤噪声表格，(3)基于程序化思维(PoT)的推理器生成可执行代码来从清理后的表格推导最终答案。

**结果:** 在TAT-QA、TableBench和method数据集上分别实现了8.79%、6.08%和19.87%的准确率提升，达到最先进性能。

**结论:** 该框架有效增强了大语言模型在复杂表格数值推理方面的能力，提供了稳健的解决方案，并可无缝集成到主流大语言模型中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TabDSR%3A+Decompose%2C+Sanitize%2C+and+Reason+for+Complex+Numerical+Reasoning+in+Tabular+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02219，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02219&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Complex reasoning over tabular data is crucial in real-world data analysis,
yet large language models (LLMs) often underperform due to complex queries,
noisy data, and limited numerical capabilities. To address these issues, we
propose \method, a framework consisting of: (1) a query decomposer that breaks
down complex questions, (2) a table sanitizer that cleans and filters noisy
tables, and (3) a program-of-thoughts (PoT)-based reasoner that generates
executable code to derive the final answer from the sanitized table. To ensure
unbiased evaluation and mitigate data leakage, we introduce a new dataset,
CalTab151, specifically designed for complex numerical reasoning over tables.
Experimental results demonstrate that \method consistently outperforms existing
methods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and
19.87% accuracy improvement on TAT-QA, TableBench, and \method, respectively.
Moreover, our framework integrates seamlessly with mainstream LLMs, providing a
robust solution for complex tabular numerical reasoning. These findings
highlight the effectiveness of our framework in enhancing LLM performance for
complex tabular numerical reasoning. Data and code are available upon request.

</details>


### [11] [Deep Ideation: Designing LLM Agents to Generate Novel Research Ideas on Scientific Concept Network](https://arxiv.org/abs/2511.02238)
*Keyu Zhao, Weiquan Lin, Qirui Zheng, Fengli Xu, Yong Li*

**主要类别:** cs.AI

**AI概要:** 提出了Deep Ideation框架，通过整合科学概念网络和LLM，利用探索-扩展-演化工作流程生成高质量的研究想法，相比现有方法提升10.67%的质量


<details>
  <summary>更多</summary>
  
**动机:** 现有研究想法生成方法过于简单，仅依赖关键词共现或语义相似性，忽视了科学概念间的复杂上下文关系，无法有效利用科学文献中的知识

**方法:** 结合科学概念网络（捕捉关键词共现和上下文关系）和LLM，采用探索-扩展-演化迭代流程，使用Idea Stack追踪进度，并通过基于真实评审反馈训练的批评引擎提供持续反馈

**结果:** 实验显示方法相比其他方法提升10.67%的想法质量，生成的想法超过顶级会议接受水平，人工评估确认其科研实用价值，消融研究验证了各组件有效性

**结论:** Deep Ideation框架成功解决了现有方法的局限性，通过整合科学网络和迭代优化流程，能够生成高质量、新颖且可行的研究想法，对科研创新有重要价值

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep+Ideation%3A+Designing+LLM+Agents+to+Generate+Novel+Research+Ideas+on+Scientific+Concept+Network，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02238，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02238&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Novel research ideas play a critical role in advancing scientific inquiries.
Recent advancements in Large Language Models (LLMs) have demonstrated their
potential to generate novel research ideas by leveraging large-scale scientific
literature. However, previous work in research ideation has primarily relied on
simplistic methods, such as keyword co-occurrence or semantic similarity. These
approaches focus on identifying statistical associations in the literature but
overlook the complex, contextual relationships between scientific concepts,
which are essential to effectively leverage knowledge embedded in human
literature. For instance, papers that simultaneously mention "keyword A" and
"keyword B" often present research ideas that integrate both concepts.
Additionally, some LLM-driven methods propose and refine research ideas using
the model's internal knowledge, but they fail to effectively utilize the
scientific concept network, limiting the grounding of ideas in established
research. To address these challenges, we propose the Deep Ideation framework
to address these challenges, integrating a scientific network that captures
keyword co-occurrence and contextual relationships, enriching LLM-driven
ideation. The framework introduces an explore-expand-evolve workflow to
iteratively refine research ideas, using an Idea Stack to track progress. A
critic engine, trained on real-world reviewer feedback, guides the process by
providing continuous feedback on the novelty and feasibility of ideas. Our
experiments show that our approach improves the quality of generated ideas by
10.67% compared to other methods, with ideas surpassing top conference
acceptance levels. Human evaluation highlights their practical value in
scientific research, and ablation studies confirm the effectiveness of each
component in the workflow. Code repo is available at
https://github.com/kyZhao-1/Deep-Ideation.

</details>


### [12] [When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs](https://arxiv.org/abs/2511.02243)
*Zhuoran Zhang, Tengyue Wang, Xilin Gong, Yang Shi, Haotian Wang, Di Wang, Lijie Hu*

**主要类别:** cs.AI

**AI概要:** 该研究提出了一个分析多模态大语言模型(MLLMs)处理模态冲突的新框架，将模态跟随行为分解为相对推理不确定性和固有模态偏好两个核心因素，揭示了模型在模态选择中的内在机制。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究仅使用粗粒度的数据集级统计来衡量多模态模型的模态跟随行为，忽略了模型在单模态推理中的置信度影响，需要更精细的分析框架。

**方法:** 构建可控数据集系统性地改变视觉和文本输入的推理难度，使用熵作为细粒度不确定性度量，分析层间预测以揭示内部机制。

**结果:** 发现了一个普遍规律：模型跟随某个模态的概率随其相对不确定性的增加而单调下降；在平衡点处揭示了模型的固有偏好；发现模型在模糊区域会在不同层间在模态间振荡。

**结论:** 相对不确定性和固有偏好是控制模态跟随的两个基本原则，该框架为理解MLLMs如何解决冲突信息提供了定量分析和机制洞察。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+Modalities+Conflict%3A+How+Unimodal+Reasoning+Uncertainty+Governs+Preference+Dynamics+in+MLLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02243，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02243&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Multimodal large language models (MLLMs) must resolve conflicts when
different modalities provide contradictory information, a process we term
modality following. Prior work measured this behavior only with coarse
dataset-level statistics, overlooking the influence of model's confidence in
unimodal reasoning. In this paper, we introduce a new framework that decomposes
modality following into two fundamental factors: relative reasoning uncertainty
(the case-specific confidence gap between unimodal predictions) and inherent
modality preference( a model's stable bias when uncertainties are balanced). To
validate this framework, we construct a controllable dataset that
systematically varies the reasoning difficulty of visual and textual inputs.
Using entropy as a fine-grained uncertainty metric, we uncover a universal law:
the probability of following a modality decreases monotonically as its relative
uncertainty increases. At the relative difficulty level where the model tends
to follow both modalities with comparable probability what we call the balance
point, a practical indicator of the model's inherent preference. Unlike
traditional macro-level ratios, this measure offers a more principled and less
confounded way to characterize modality bias, disentangling it from unimodal
capabilities and dataset artifacts. Further, by probing layer-wise predictions,
we reveal the internal mechanism of oscillation: in ambiguous regions near the
balance point, models vacillate between modalities across layers, explaining
externally observed indecision. Together, these findings establish relative
uncertainty and inherent preference as the two governing principles of modality
following, offering both a quantitative framework and mechanistic insight into
how MLLMs resolve conflicting information.

</details>


### [13] [Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation](https://arxiv.org/abs/2511.02303)
*Zhiwei Zhang, Xiaomin Li, Yudi Lin, Hui Liu, Ramraj Chandradevan, Linlin Wu, Minhua Lin, Fali Wang, Xianfeng Tang, Qi He, Suhang Wang*

**主要类别:** cs.AI

**AI概要:** 该论文针对多智能体推理中的懒惰代理行为问题，提出了因果影响测量方法和可验证奖励机制来改善协作效果。


<details>
  <summary>更多</summary>
  
**动机:** 发现多智能体推理中存在懒惰代理行为，即一个代理主导而另一个贡献很少，导致协作失效，实际上退化为单智能体系统。

**方法:** 1) 提供理论分析解释懒惰行为的产生原因；2) 引入稳定高效的因果影响测量方法；3) 提出可验证奖励机制，允许推理代理丢弃噪声输出、整合指令并在必要时重启推理过程。

**结果:** 大量实验表明，该框架有效缓解了懒惰代理行为，释放了多智能体框架在复杂推理任务中的全部潜力。

**结论:** 通过理论分析和创新方法解决了多智能体协作中的关键问题，显著提升了多智能体推理系统的性能和协作效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unlocking+the+Power+of+Multi-Agent+LLM+for+Reasoning%3A+From+Lazy+Agents+to+Deliberation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02303，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02303&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) trained with reinforcement learning and
verifiable rewards have achieved strong results on complex reasoning tasks.
Recent work extends this paradigm to a multi-agent setting, where a
meta-thinking agent proposes plans and monitors progress while a reasoning
agent executes subtasks through sequential conversational turns. Despite
promising performance, we identify a critical limitation: lazy agent behavior,
in which one agent dominates while the other contributes little, undermining
collaboration and collapsing the setup to an ineffective single agent. In this
paper, we first provide a theoretical analysis showing why lazy behavior
naturally arises in multi-agent reasoning. We then introduce a stable and
efficient method for measuring causal influence, helping mitigate this issue.
Finally, as collaboration intensifies, the reasoning agent risks getting lost
in multi-turn interactions and trapped by previous noisy responses. To counter
this, we propose a verifiable reward mechanism that encourages deliberation by
allowing the reasoning agent to discard noisy outputs, consolidate
instructions, and restart its reasoning process when necessary. Extensive
experiments demonstrate that our framework alleviates lazy agent behavior and
unlocks the full potential of multi-agent framework for complex reasoning
tasks.

</details>


### [14] [Chronic Kidney Disease Prognosis Prediction Using Transformer](https://arxiv.org/abs/2511.02340)
*Yohan Lee, DongGyun Kang, SeHoon Park, Sa-Yoon Park, Kwangsoo Kim*

**主要类别:** cs.AI

**AI概要:** 提出ProQ-BERT框架，基于Transformer架构，使用多模态电子健康记录预测慢性肾病进展，在短期预测中达到接近完美的性能指标。


<details>
  <summary>更多</summary>
  
**动机:** 慢性肾病影响全球近10%人口且常进展至终末期肾衰竭，准确的预后预测对于及时干预和资源优化至关重要。

**方法:** 基于Transformer框架，整合人口统计学、临床和实验室数据，采用基于量化的标记化处理连续实验室数值，使用掩码语言建模预训练，并针对从3a期到5期的二元分类任务进行微调。

**结果:** 在91,816名患者队列中评估，模型始终优于CEHR-BERT，短期预测的ROC-AUC高达0.995，PR-AUC高达0.989。

**结论:** Transformer架构和时间设计选择在临床预后建模中效果显著，为个性化慢性肾病护理提供了有前景的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Chronic+Kidney+Disease+Prognosis+Prediction+Using+Transformer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02340，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02340&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Chronic Kidney Disease (CKD) affects nearly 10\% of the global population and
often progresses to end-stage renal failure. Accurate prognosis prediction is
vital for timely interventions and resource optimization. We present a
transformer-based framework for predicting CKD progression using multi-modal
electronic health records (EHR) from the Seoul National University Hospital
OMOP Common Data Model. Our approach (\textbf{ProQ-BERT}) integrates
demographic, clinical, and laboratory data, employing quantization-based
tokenization for continuous lab values and attention mechanisms for
interpretability. The model was pretrained with masked language modeling and
fine-tuned for binary classification tasks predicting progression from stage 3a
to stage 5 across varying follow-up and assessment periods. Evaluated on a
cohort of 91,816 patients, our model consistently outperformed CEHR-BERT,
achieving ROC-AUC up to 0.995 and PR-AUC up to 0.989 for short-term prediction.
These results highlight the effectiveness of transformer architectures and
temporal design choices in clinical prognosis modeling, offering a promising
direction for personalized CKD care.

</details>


### [15] [Fuzzy Soft Set Theory based Expert System for the Risk Assessment in Breast Cancer Patients](https://arxiv.org/abs/2511.02392)
*Muhammad Sheharyar Liaqat*

**主要类别:** cs.AI

**AI概要:** 基于模糊软集理论的专家系统，使用BMI、胰岛素、瘦素、脂联素和年龄等临床参数进行乳腺癌风险评估，为非侵入性初步筛查提供支持


<details>
  <summary>更多</summary>
  
**动机:** 乳腺癌是全球女性主要死因之一，早期诊断对治疗和生存率至关重要，但由于疾病复杂性和患者风险因素差异，及时检测仍面临挑战

**方法:** 开发基于模糊软集理论的专家系统，整合BMI、胰岛素水平、瘦素水平、脂联素水平和年龄作为输入变量，通过模糊推理规则和软集计算进行风险评估

**结果:** 系统使用UCI机器学习数据库进行开发和验证，能够通过常规血液分析获取参数，为非侵入性初步评估提供可行方法

**结论:** 该专家系统旨在帮助医疗专业人员识别高风险患者，并确定是否需要进一步诊断程序（如活检），提高乳腺癌早期检测的可及性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fuzzy+Soft+Set+Theory+based+Expert+System+for+the+Risk+Assessment+in+Breast+Cancer+Patients，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02392，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02392&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Breast cancer remains one of the leading causes of mortality among women
worldwide, with early diagnosis being critical for effective treatment and
improved survival rates. However, timely detection continues to be a challenge
due to the complex nature of the disease and variability in patient risk
factors. This study presents a fuzzy soft set theory-based expert system
designed to assess the risk of breast cancer in patients using measurable
clinical and physiological parameters. The proposed system integrates Body Mass
Index, Insulin Level, Leptin Level, Adiponectin Level, and age as input
variables to estimate breast cancer risk through a set of fuzzy inference rules
and soft set computations. These parameters can be obtained from routine blood
analyses, enabling a non-invasive and accessible method for preliminary
assessment. The dataset used for model development and validation was obtained
from the UCI Machine Learning Repository. The proposed expert system aims to
support healthcare professionals in identifying high-risk patients and
determining the necessity of further diagnostic procedures such as biopsies.

</details>


### [16] [A New Perspective on Precision and Recall for Generative Models](https://arxiv.org/abs/2511.02414)
*Benjamin Sykes, Loïc Simon, Julien Rabin, Jalal Fadili*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一个新的基于二元分类视角的生成模型精度-召回率（PR）曲线估计框架，进行了统计分析并获得了PR估计风险的最小最大上界，扩展了现有文献中的PR度量方法。


<details>
  <summary>更多</summary>
  
**动机:** 随着生成模型在图像和文本领域的成功，其评估方法受到广泛关注。虽然现有方法主要依赖标量指标，但精度-召回率（PR）曲线为生成模型评估提供了更丰富的分析维度，但其估计面临诸多挑战。

**方法:** 基于二元分类的视角，提出了一个估计完整PR曲线的新框架，进行了深入的统计分析，并获得了PR估计风险的最小最大上界。

**结果:** 开发了一个能够估计完整PR曲线的框架，该框架扩展了现有文献中仅限于曲线极值的PR度量方法，并在不同实验设置下研究了曲线的不同行为特征。

**结论:** 提出的基于二元分类的PR曲线估计框架为生成模型评估提供了更全面的分析工具，克服了现有标量指标的局限性，具有重要的理论和实践价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+New+Perspective+on+Precision+and+Recall+for+Generative+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02414，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02414&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** With the recent success of generative models in image and text, the question
of their evaluation has recently gained a lot of attention. While most methods
from the state of the art rely on scalar metrics, the introduction of Precision
and Recall (PR) for generative model has opened up a new avenue of research.
The associated PR curve allows for a richer analysis, but their estimation
poses several challenges. In this paper, we present a new framework for
estimating entire PR curves based on a binary classification standpoint. We
conduct a thorough statistical analysis of the proposed estimates. As a
byproduct, we obtain a minimax upper bound on the PR estimation risk. We also
show that our framework extends several landmark PR metrics of the literature
which by design are restrained to the extreme values of the curve. Finally, we
study the different behaviors of the curves obtained experimentally in various
settings.

</details>


### [17] [ReAcTree: Hierarchical LLM Agent Trees with Control Flow for Long-Horizon Task Planning](https://arxiv.org/abs/2511.02424)
*Jae-Woo Choi, Hyungmin Kim, Hyobin Ong, Minsu Jang, Dohyung Kim, Jaehong Kim, Youngwoo Yoon*

**主要类别:** cs.AI

**AI概要:** ReAcTree是一种分层任务规划方法，通过动态构建代理树将复杂目标分解为可管理的子目标，使用LLM代理节点处理子任务，配合控制流节点协调执行，集成情景记忆和工作记忆系统，在WAH-NL和ALFRED数据集上显著优于ReAct等基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法在处理复杂长时程任务时存在局限，因为它们依赖单一轨迹来整合所有过去的决策和观察，试图通过单一统一过程解决整个任务，这导致性能受限。

**方法:** 提出分层任务规划方法ReAcTree：1) 将复杂目标分解为可管理子目标；2) 构建动态代理树结构；3) 每个子目标由LLM代理节点处理（推理、行动、扩展树）；4) 控制流节点协调执行策略；5) 集成情景记忆（目标特定示例）和工作记忆（环境特定观察）系统。

**结果:** 在WAH-NL和ALFRED数据集上的实验表明，ReAcTree在不同LLM上始终优于ReAct等强基线。在WAH-NL上，使用Qwen 2.5 72B时达到61%的目标成功率，几乎是ReAct（31%）的两倍。

**结论:** ReAcTree通过分层分解和动态树结构有效解决了复杂长时程任务规划问题，证明了分层方法和记忆系统集成在提升LLM代理决策能力方面的重要价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ReAcTree%3A+Hierarchical+LLM+Agent+Trees+with+Control+Flow+for+Long-Horizon+Task+Planning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02424，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02424&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in large language models (LLMs) have enabled significant
progress in decision-making and task planning for embodied autonomous agents.
However, most existing methods still struggle with complex, long-horizon tasks
because they rely on a monolithic trajectory that entangles all past decisions
and observations, attempting to solve the entire task in a single unified
process. To address this limitation, we propose ReAcTree, a hierarchical
task-planning method that decomposes a complex goal into more manageable
subgoals within a dynamically constructed agent tree. Each subgoal is handled
by an LLM agent node capable of reasoning, acting, and further expanding the
tree, while control flow nodes coordinate the execution strategies of agent
nodes. In addition, we integrate two complementary memory systems: each agent
node retrieves goal-specific, subgoal-level examples from episodic memory and
shares environment-specific observations through working memory. Experiments on
the WAH-NL and ALFRED datasets demonstrate that ReAcTree consistently
outperforms strong task-planning baselines such as ReAct across diverse LLMs.
Notably, on WAH-NL, ReAcTree achieves a 61% goal success rate with Qwen 2.5
72B, nearly doubling ReAct's 31%.

</details>


### [18] [Auditable-choice reframing unlocks RL-based verification for open-ended tasks](https://arxiv.org/abs/2511.02463)
*Mengyu Zhang, Xubo Liu, Siyu Ding, Weichong Yin, Yu Sun, Hua Wu, Wenya Guo, Ying Zhang*

**主要类别:** cs.AI

**AI概要:** 该论文提出VMR方法，将开放式任务转换为可验证的多选题格式，使RLVR训练范式能够应用于缺乏标准答案的开放式任务，在8个基准测试中平均提升5.99分。


<details>
  <summary>更多</summary>
  
**动机:** 现有RLVR方法在数学和编程等有标准答案的任务中表现优异，但在开放式任务（如创意写作和指令跟随）中被视为非推理场景，忽略了推理能力的潜在价值。

**方法:** 提出Verifiable Multiple-Choice Reformulation (VMR)训练策略，将开放式数据重构为可验证的多选题格式，从而在没有显式真实答案的情况下实现有效训练。

**结果:** 在多个基准测试中验证了方法的有效性，在8个开放式基准测试中，基于VMR的训练相比基线平均提升了5.99分。

**结论:** VMR方法成功将RLVR范式扩展到开放式任务领域，证明了通过增强推理能力可以提升开放式任务的性能，为LLM在更广泛场景中的应用提供了新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Auditable-choice+reframing+unlocks+RL-based+verification+for+open-ended+tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02463，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02463&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated great
potential in enhancing the reasoning capabilities of large language models
(LLMs), achieving remarkable progress in domains such as mathematics and
programming where standard answers are available. However, for open-ended tasks
lacking ground-truth solutions (e.g., creative writing and instruction
following), existing studies typically regard them as non-reasoning scenarios,
thereby overlooking the latent value of reasoning capabilities. This raises a
key question: Can strengthening reasoning improve performance in open-ended
tasks? To address this, we explore the transfer of the RLVR paradigm to the
open domain. Yet, since RLVR fundamentally relies on verifiers that presuppose
the existence of standard answers, it cannot be directly applied to open-ended
tasks. To overcome this challenge, we introduce Verifiable Multiple-Choice
Reformulation (VMR), a novel training strategy that restructures open-ended
data into verifiable multiple-choice formats, enabling effective training even
in the absence of explicit ground truth. Experimental results on multiple
benchmarks validate the effectiveness of our method in improving LLM
performance on open-ended tasks. Notably, across eight open-ended benchmarks,
our VMR-based training delivers an average gain of 5.99 points over the
baseline. Code will be released upon acceptance to facilitate reproducibility.

</details>


### [19] [Agentic AI for Mobile Network RAN Management and Optimization](https://arxiv.org/abs/2511.02532)
*Jorge Pellejero, Luis A. Hernández Gómez, Luis Mendo Tomás, Zoraida Frias Barroso*

**主要类别:** cs.AI

**AI概要:** 本文探讨了Agentic AI在5G/6G网络中的应用，提出了一个将大型AI模型与RAN优化相结合的理论框架和实践案例，展示了基于KPI的自主决策能力。


<details>
  <summary>更多</summary>
  
**动机:** 5G/6G网络的复杂性使得人工优化变得低效，需要Agentic AI来在动态RAN环境中实现自动化决策，但目前缺乏统一的理论框架和定义。

**方法:** 首先介绍Agentic AI的概念演进，从传统智能体到现代Agentic AI的发展；然后描述核心设计模式（反思、规划、工具使用、多智能体协作）；最后通过5G RAN案例研究展示时间序列分析和LAM驱动智能体的协作。

**结果:** 提出了Agentic AI在移动网络中的理论基础，并展示了其在RAN管理和优化中的实际应用能力，实现了基于KPI的自主决策。

**结论:** Agentic AI为5G/6G网络自动化提供了有前景的解决方案，通过结合大型AI模型和核心设计模式，能够有效处理复杂网络环境的动态优化问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Agentic+AI+for+Mobile+Network+RAN+Management+and+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02532，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02532&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Agentic AI represents a new paradigm for automating complex systems by using
Large AI Models (LAMs) to provide human-level cognitive abilities with
multimodal perception, planning, memory, and reasoning capabilities. This will
lead to a new generation of AI systems that autonomously decompose goals,
retain context over time, learn continuously, operate across tools and
environments, and adapt dynamically. The complexity of 5G and upcoming 6G
networks renders manual optimization ineffective, pointing to Agentic AI as a
method for automating decisions in dynamic RAN environments. However, despite
its rapid advances, there is no established framework outlining the
foundational components and operational principles of Agentic AI systems nor a
universally accepted definition.
  This paper contributes to ongoing research on Agentic AI in 5G and 6G
networks by outlining its core concepts and then proposing a practical use case
that applies Agentic principles to RAN optimization. We first introduce Agentic
AI, tracing its evolution from classical agents and discussing the progress
from workflows and simple AI agents to Agentic AI. Core design
patterns-reflection, planning, tool use, and multi-agent collaboration-are then
described to illustrate how intelligent behaviors are orchestrated. These
theorical concepts are grounded in the context of mobile networks, with a focus
on RAN management and optimization. A practical 5G RAN case study shows how
time-series analytics and LAM-driven agents collaborate for KPI-based
autonomous decision-making.

</details>


### [20] [Knowledge Graph-enhanced Large Language Model for Incremental Game PlayTesting](https://arxiv.org/abs/2511.02534)
*Enhong Mu, Jinyu Cai, Yijun Lu, Mingyue Zhang, Kenji Tei, Jialong Li*

**主要类别:** cs.AI

**AI概要:** 本文提出KLPEG框架，通过构建知识图谱来系统建模游戏元素、任务依赖和因果关系，利用LLM解析更新日志并生成针对性测试用例，在Overcooked和Minecraft游戏中验证了其高效性和准确性。


<details>
  <summary>更多</summary>
  
**动机:** 现代游戏快速迭代更新给测试带来挑战，现有基于LLM的自动化测试方法缺乏结构化知识积累机制，难以针对增量更新进行精确高效测试。

**方法:** 提出KLPEG框架：1) 构建和维护知识图谱(KG)建模游戏元素、任务依赖和因果关系；2) 利用LLM解析自然语言更新日志；3) 通过KG多跳推理识别影响范围；4) 生成针对更新的测试用例。

**结果:** 在Overcooked和Minecraft两个代表性游戏环境中实验表明，KLPEG能更准确定位受更新影响的功能，用更少步骤完成测试，显著提升测试效果和效率。

**结论:** KLPEG框架通过知识图谱和LLM的结合，有效解决了游戏增量更新的自动化测试问题，实现了知识的积累和重用，为游戏测试提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Knowledge+Graph-enhanced+Large+Language+Model+for+Incremental+Game+PlayTesting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02534，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02534&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The rapid iteration and frequent updates of modern video games pose
significant challenges to the efficiency and specificity of testing. Although
automated playtesting methods based on Large Language Models (LLMs) have shown
promise, they often lack structured knowledge accumulation mechanisms, making
it difficult to conduct precise and efficient testing tailored for incremental
game updates. To address this challenge, this paper proposes a KLPEG framework.
The framework constructs and maintains a Knowledge Graph (KG) to systematically
model game elements, task dependencies, and causal relationships, enabling
knowledge accumulation and reuse across versions. Building on this foundation,
the framework utilizes LLMs to parse natural language update logs, identify the
scope of impact through multi-hop reasoning on the KG, enabling the generation
of update-tailored test cases. Experiments in two representative game
environments, Overcooked and Minecraft, demonstrate that KLPEG can more
accurately locate functionalities affected by updates and complete tests in
fewer steps, significantly improving both playtesting effectiveness and
efficiency.

</details>


### [21] [The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large Language Models](https://arxiv.org/abs/2511.02589)
*Claudia Herambourg, Dawid Siuda, Anna Szczepanek, Julia Kopczyńska, Joao R. L. Santos, Wojciech Sas, Joanna Śmietańska-Nowak*

**主要类别:** cs.AI

**AI概要:** ORCA基准测试评估5个顶级LLM在多领域定量推理任务中的表现，结果显示准确率仅45-63%，主要错误为舍入和计算错误，模型在不同领域表现差异明显且存在部分互补性。


<details>
  <summary>更多</summary>
  
**动机:** 现有数学数据集无法充分评估LLM在真实多领域定量推理任务中的表现，需要开发能够测试逐步推理、数值精度和领域泛化能力的新基准。

**方法:** 创建ORCA基准，包含500个跨领域自然语言任务（金融、物理、健康、统计），使用已验证的计算器输出作为标准答案，评估5个先进LLM的性能。

**结果:** 所有模型准确率在45-63%之间，35%错误来自舍入问题，33%来自计算错误。数学和工程领域表现较好，物理和自然科学较弱。模型间相关性0.40-0.65，显示部分互补性。

**结论:** 当前LLM在真实定量推理任务中仍有显著局限性，需要改进数值精度和领域适应性。ORCA基准为评估和改进LLM的定量推理能力提供了重要工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+ORCA+Benchmark%3A+Evaluating+Real-World+Calculation+Accuracy+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02589，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02589&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We present ORCA (Omni Research on Calculation in AI) Benchmark -- a novel
benchmark that evaluates large language models (LLMs) on multi-domain,
real-life quantitative reasoning using verified outputs from Omni's calculator
engine. In 500 natural-language tasks across domains such as finance, physics,
health, and statistics, the five state-of-the-art systems (ChatGPT-5,
Gemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only
$45\text{--}63\,\%$ accuracy, with errors mainly related to rounding ($35\,\%$)
and calculation mistakes ($33\,\%$). Results in specific domains indicate
strengths in mathematics and engineering, but weaknesses in physics and natural
sciences. Correlation analysis ($r \approx 0.40\text{--}0.65$) shows that the
models often fail together but differ in the types of errors they make,
highlighting their partial complementarity rather than redundancy. Unlike
standard math datasets, ORCA evaluates step-by-step reasoning, numerical
precision, and domain generalization across real problems from finance,
physics, health, and statistics.

</details>


### [22] [Adaptive GR(1) Specification Repair for Liveness-Preserving Shielding in Reinforcement Learning](https://arxiv.org/abs/2511.02605)
*Tiberiu-Andrei Georgescu, Alexander W. Goodall, Dalal Alrajeh, Francesco Belardinelli, Sebastian Uchitel*

**主要类别:** cs.AI

**AI概要:** 提出了首个基于GR(1)规范的自适应屏蔽框架，通过运行时检测环境假设违规和使用ILP在线修复规范，使屏蔽器能够优雅演化，在保持最优奖励的同时确保完美的逻辑合规性。


<details>
  <summary>更多</summary>
  
**动机:** 传统静态屏蔽方法假设固定的逻辑规范和手工制作的抽象，在环境假设被违反时无法适应，需要开发能够动态调整的屏蔽机制。

**方法:** 基于GR(1)规范，运行时检测环境假设违规，使用归纳逻辑编程(ILP)在线自动修复GR(1)规范，实现系统化和可解释的适应性调整。

**结果:** 在Minepump和Atari Seaquest案例研究中显示，自适应屏蔽相比静态屏蔽能够维持接近最优的奖励和完美的逻辑合规性。

**结论:** 自适应屏蔽框架成功解决了静态屏蔽的局限性，为强化学习安全提供了更加灵活和有效的保障机制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptive+GR%281%29+Specification+Repair+for+Liveness-Preserving+Shielding+in+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02605，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02605&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Shielding is widely used to enforce safety in reinforcement learning (RL),
ensuring that an agent's actions remain compliant with formal specifications.
Classical shielding approaches, however, are often static, in the sense that
they assume fixed logical specifications and hand-crafted abstractions. While
these static shields provide safety under nominal assumptions, they fail to
adapt when environment assumptions are violated. In this paper, we develop the
first adaptive shielding framework - to the best of our knowledge - based on
Generalized Reactivity of rank 1 (GR(1)) specifications, a tractable and
expressive fragment of Linear Temporal Logic (LTL) that captures both safety
and liveness properties. Our method detects environment assumption violations
at runtime and employs Inductive Logic Programming (ILP) to automatically
repair GR(1) specifications online, in a systematic and interpretable way. This
ensures that the shield evolves gracefully, ensuring liveness is achievable and
weakening goals only when necessary. We consider two case studies: Minepump and
Atari Seaquest; showing that (i) static symbolic controllers are often severely
suboptimal when optimizing for auxiliary rewards, and (ii) RL agents equipped
with our adaptive shield maintain near-optimal reward and perfect logical
compliance compared with static shields.

</details>


### [23] [A Multi-Agent Psychological Simulation System for Human Behavior Modeling](https://arxiv.org/abs/2511.02606)
*Xiangen Hu, Jiarui Tong, Sheng Xu*

**主要类别:** cs.AI

**AI概要:** 提出基于心理学理论的多智能体心理模拟系统，通过模拟内部认知情感过程生成可信人类行为，应用于教师培训和研究


<details>
  <summary>更多</summary>
  
**动机:** 人类中心领域的培训需要真实实践，但现有的人类行为模拟系统缺乏真实性和理论依据

**方法:** 建立基于心理学理论（自我效能、思维模式、社会建构主义）的多智能体系统，模拟'内部议会'的心理因素代理进行决策互动

**结果:** 开发出具有前所未有的透明度和与人类心理学一致性的行为生成系统

**结论:** 该系统体现了社会学习、认知学徒制、刻意练习和元认知原则，为教师培训和研究提供了有效的仿真工具

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Multi-Agent+Psychological+Simulation+System+for+Human+Behavior+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02606，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02606&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Training and education in human-centered fields require authentic practice,
yet realistic simulations of human behavior have remained limited. We present a
multi-agent psychological simulation system that models internal
cognitive-affective processes to generate believable human behaviors. In
contrast to black-box neural models, this system is grounded in established
psychological theories (e.g., self-efficacy, mindset, social constructivism)
and explicitly simulates an ``inner parliament'' of agents corresponding to key
psychological factors. These agents deliberate and interact to determine the
system's output behavior, enabling unprecedented transparency and alignment
with human psychology. We describe the system's architecture and theoretical
foundations, illustrate its use in teacher training and research, and discuss
how it embodies principles of social learning, cognitive apprenticeship,
deliberate practice, and meta-cognition.

</details>


### [24] [DecompSR: A dataset for decomposed analyses of compositional multihop spatial reasoning](https://arxiv.org/abs/2511.02627)
*Lachlan McPheat, Navdeep Kaur, Robert Blackwell, Alessandra Russo, Anthony G. Cohn, Pranava Madhyastha*

**主要类别:** cs.AI

**AI概要:** DecompSR是一个包含500多万数据点的大规模空间推理基准数据集和生成框架，用于分析组合空间推理能力，通过独立控制组合性的多个维度来评估大语言模型的性能。


<details>
  <summary>更多</summary>
  
**动机:** 需要系统性地分析和评估大语言模型在组合空间推理任务中的能力，特别是对生产力、可替代性、过度泛化和系统性等组合性关键方面的处理能力。

**方法:** 采用程序化构建方法生成数据集，确保构造正确性，并使用符号求解器独立验证数据集的正确性。通过独立控制组合性的多个维度（推理深度、实体和语言变异性、输入顺序和干扰项、新语言元素）来构建数据集。

**结果:** 实验表明，大语言模型在空间推理任务中难以处理生产性和系统性泛化，但对语言变异表现出更强的鲁棒性。

**结论:** DecompSR提供了一个可证明正确且严谨的基准测试数据集，能够独立控制组合性的多个关键维度，为大语言模型的组合推理能力提供了细粒度和鲁棒的评估工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DecompSR%3A+A+dataset+for+decomposed+analyses+of+compositional+multihop+spatial+reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02627，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02627&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We introduce DecompSR, decomposed spatial reasoning, a large benchmark
dataset (over 5m datapoints) and generation framework designed to analyse
compositional spatial reasoning ability. The generation of DecompSR allows
users to independently vary several aspects of compositionality, namely:
productivity (reasoning depth), substitutivity (entity and linguistic
variability), overgeneralisation (input order, distractors) and systematicity
(novel linguistic elements). DecompSR is built procedurally in a manner which
makes it is correct by construction, which is independently verified using a
symbolic solver to guarantee the correctness of the dataset. DecompSR is
comprehensively benchmarked across a host of Large Language Models (LLMs) where
we show that LLMs struggle with productive and systematic generalisation in
spatial reasoning tasks whereas they are more robust to linguistic variation.
DecompSR provides a provably correct and rigorous benchmarking dataset with a
novel ability to independently vary the degrees of several key aspects of
compositionality, allowing for robust and fine-grained probing of the
compositional reasoning abilities of LLMs.

</details>


### [25] [The Collaboration Gap](https://arxiv.org/abs/2511.02687)
*Tim R. Davidson, Adam Fourney, Saleema Amershi, Robert West, Eric Horvitz, Ece Kamar*

**主要类别:** cs.AI

**AI概要:** 论文提出了一个协作迷宫求解基准测试，评估了32个主流AI模型在协作任务中的表现，发现存在明显的'协作鸿沟'：单独表现良好的模型在协作时性能大幅下降，并提出了'接力推理'等改进方法。


<details>
  <summary>更多</summary>
  
**动机:** 随着AI发展，我们将越来越多地依赖由不同信息、权限和工具的独立开发智能体组成的系统，但这些系统的成功关键取决于异构智能体间的有效协作，而目前缺乏大规模实证研究来评估这种协作能力。

**方法:** 设计了一个协作迷宫求解基准测试框架，该框架能够：(i)隔离协作能力，(ii)调节问题复杂度，(iii)实现可扩展的自动评分，(iv)保持生态合理性。使用该框架评估了32个领先的开源和闭源模型在单独、同质配对和异质配对三种场景下的表现。

**结果:** 发现了明显的'协作鸿沟'：单独表现良好的模型在需要协作时性能显著下降；协作可能完全崩溃，例如某些单独能很好解决迷宫的小型蒸馏模型在某些配对中几乎完全失败；发现从较强智能体开始通常能改善结果，提出了'接力推理'方法。

**结论:** 研究结果主张：(1)协作感知的评估方法，(2)开发增强协作能力的训练策略，(3)设计能够可靠激发智能体潜在技能的交互设计，这些指导适用于AI-AI协作和人类-AI协作。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Collaboration+Gap，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02687，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02687&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The trajectory of AI development suggests that we will increasingly rely on
agent-based systems composed of independently developed agents with different
information, privileges, and tools. The success of these systems will
critically depend on effective collaboration among these heterogeneous agents,
even under partial observability. Despite intense interest, few empirical
studies have evaluated such agent-agent collaboration at scale. We propose a
collaborative maze-solving benchmark that (i) isolates collaborative
capabilities, (ii) modulates problem complexity, (iii) enables scalable
automated grading, and (iv) imposes no output-format constraints, preserving
ecological plausibility. Using this framework, we evaluate 32 leading open- and
closed-source models in solo, homogeneous, and heterogeneous pairings. Our
results reveal a "collaboration gap": models that perform well solo often
degrade substantially when required to collaborate. Collaboration can break
down dramatically; for instance, small distilled models that solve mazes well
alone may fail almost completely in certain pairings. We find that starting
with the stronger agent often improves outcomes, motivating a "relay inference"
approach where the stronger agent leads before handing off to the weaker one,
closing much of the gap. Our findings argue for (1) collaboration-aware
evaluation, (2) training strategies developed to enhance collaborative
capabilities, and (3) interaction design that reliably elicits agents' latent
skills, guidance that applies to AI-AI and human-AI collaboration.

</details>


### [26] [CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents](https://arxiv.org/abs/2511.02734)
*Jiayu Liu, Cheng Qian, Zhaochen Su, Qing Zong, Shijue Huang, Bingxiang He, Yi R. Fung*

**主要类别:** cs.AI

**AI概要:** CostBench是一个专注于成本效益的基准测试，用于评估LLM代理的经济推理和重新规划能力，发现在静态和动态环境下当前代理在成本最优规划方面存在显著不足。


<details>
  <summary>更多</summary>
  
**动机:** 当前LLM代理评估主要关注任务完成度，忽视了资源效率和适应性这一关键能力，特别是代理在变化环境中制定和调整成本最优计划的能力。

**方法:** 在旅行规划领域构建CostBench基准，包含可通过多种原子和复合工具序列解决的任务，支持四种动态阻塞事件（如工具故障和成本变化）来模拟现实世界的不确定性。

**结果:** 评估显示当前代理在成本感知规划方面存在巨大差距：在静态设置中经常无法找到成本最优解（GPT-5在最难任务上精确匹配率低于75%），动态条件下性能进一步下降约40%。

**结论:** CostBench通过诊断这些弱点，为开发既经济理性又鲁棒的未来代理奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CostBench%3A+Evaluating+Multi-Turn+Cost-Optimal+Planning+and+Adaptation+in+Dynamic+Environments+for+LLM+Tool-Use+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02734，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02734&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Current evaluations of Large Language Model (LLM) agents primarily emphasize
task completion, often overlooking resource efficiency and adaptability. This
neglects a crucial capability: agents' ability to devise and adjust
cost-optimal plans in response to changing environments. To bridge this gap, we
introduce CostBench, a scalable, cost-centric benchmark designed to evaluate
agents' economic reasoning and replanning abilities. Situated in the
travel-planning domain, CostBench comprises tasks solvable via multiple
sequences of atomic and composite tools with diverse, customizable costs. It
also supports four types of dynamic blocking events, such as tool failures and
cost changes, to simulate real-world unpredictability and necessitate agents to
adapt in real time. Evaluating leading open-sourced and proprietary models on
CostBench reveals a substantial gap in cost-aware planning: agents frequently
fail to identify cost-optimal solutions in static settings, with even GPT-5
achieving less than 75% exact match rate on the hardest tasks, and performance
further dropping by around 40% under dynamic conditions. By diagnosing these
weaknesses, CostBench lays the groundwork for developing future agents that are
both economically rational and robust.

</details>


### [27] [Using Span Queries to Optimize for Cache and Attention Locality](https://arxiv.org/abs/2511.02749)
*Paul Castro, Nick Mitchell, Nathan Ordonez, Thomas Parnell, Mudhakar Srivatsa, Antoni Viros i Martin*

**主要类别:** cs.AI

**AI概要:** 本文提出span query概念，通过表达式树和交换性约束来统一表达聊天、RAG、推理时间扩展和代理工作负载等多种推理场景，显著提升KV缓存命中率和推理性能


<details>
  <summary>更多</summary>
  
**动机:** 当前推理服务器主要针对聊天完成优化，但客户端需求已扩展到多种推理场景。现有解决方案通常只针对单一用例（如RAG），缺乏通用接口

**方法:** 引入span query概念，构建推理调用的表达式树，通过交换性约束连接。修改vLLM（仅492行代码）实现高性能执行，并展示如何自动优化KV缓存局部性和注意力局部性

**结果:** 在两种非聊天用例中实现TTFT降低10-20倍；注意力优化的span query在2b参数模型上性能超越标准推理服务器的8b模型

**结论:** Span query提供了一种通用接口，能够有效支持多种推理工作负载，显著提升推理效率和性能，解决了传统推理服务器在非聊天场景下的局限性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Using+Span+Queries+to+Optimize+for+Cache+and+Attention+Locality，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02749，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02749&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Clients are evolving beyond chat completion, and now include a variety of
innovative inference-time scaling and deep reasoning techniques. At the same
time, inference servers remain heavily optimized for chat completion. Prior
work has shown that large improvements to KV cache hit rate are possible if
inference servers evolve towards these non-chat use cases. However, they offer
solutions that are also optimized for a single use case, RAG. In this paper, we
introduce the span query to generalize the interface to the inference server.
We demonstrate that chat, RAG, inference-time scaling, and agentic workloads
can all be expressed as span queries. We show how the critical distinction that
had been assumed by prior work lies in whether the order of the inputs matter
-- do they commute? In chat, they do not. In RAG, they often do. This paper
introduces span queries, which are expression trees of inference calls, linked
together with commutativity constraints. We describe span query syntax and
semantics. We show how they can be automatically optimized to improve KV cache
locality. We show how a small change to vLLM (affecting only 492 lines) can
enable high-performance execution of span queries. Using this stack, we
demonstrate that span queries can achieve 10-20x reductions in TTFT for two
distinct non-chat use cases. Finally, we show that span queries can also be
optimized to improve attention locality, so as to avoid the so-called
lost-in-the-middle problem. We demonstrate that an attention-optimized span
query on a 2b parameter model vastly outperforms the accuracy of a stock
inference server using an 8b model.

</details>


### [28] [LLM-Supported Formal Knowledge Representation for Enhancing Control Engineering Content with an Interactive Semantic Layer](https://arxiv.org/abs/2511.02759)
*Julius Fiedler, Carsten Knoll, Klaus Röbenack*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种基于大语言模型的半自动化方法，用于将控制工程领域的自然语言描述和数学定义转换为形式化知识图谱，以增强文档的语义交互层。


<details>
  <summary>更多</summary>
  
**动机:** 控制工程研究产出的快速增长需要新的方法来结构化和形式化领域知识，以实现易于访问、协作和可验证的知识库。

**方法:** 基于Imperative Representation of Knowledge (PyIRK)框架，利用语言模型辅助将自然语言描述和LaTeX数学定义转换为形式化知识图谱。

**结果:** 开发了交互式语义层来增强源文档，促进知识传递。

**结论:** 该方法为实现控制工程领域易于访问、协作和可验证的知识库愿景做出了贡献。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM-Supported+Formal+Knowledge+Representation+for+Enhancing+Control+Engineering+Content+with+an+Interactive+Semantic+Layer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02759，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02759&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The rapid growth of research output in control engineering calls for new
approaches to structure and formalize domain knowledge. This paper briefly
describes an LLM-supported method for semi-automated generation of formal
knowledge representations that combine human readability with machine
interpretability and increased expressiveness. Based on the Imperative
Representation of Knowledge (PyIRK) framework, we demonstrate how language
models can assist in transforming natural-language descriptions and
mathematical definitions (available as LaTeX source code) into a formalized
knowledge graph. As a first application we present the generation of an
``interactive semantic layer'' to enhance the source documents in order to
facilitate knowledge transfer. From our perspective this contributes to the
vision of easily accessible, collaborative, and verifiable knowledge bases for
the control engineering domain.

</details>


### [29] [When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal Reasoning](https://arxiv.org/abs/2511.02794)
*Chenyu Zhang, Minsol Kim, Shohreh Ghorbani, Jingyao Wu, Rosalind Picard, Patricia Maes, Paul Pu Liang*

**主要类别:** cs.AI

**AI概要:** 论文提出了一种名为"模态破坏"的诊断方法，用于分析多模态大语言模型中各模态如何影响预测结果，通过将每个模态视为独立代理进行评估，识别哪些模态支持正确结果，哪些会误导模型。


<details>
  <summary>更多</summary>
  
**动机:** 多模态大语言模型的推理过程不透明，难以理解哪个模态主导预测、冲突如何解决，以及何时某个模态会主导结果。需要一种方法来诊断模型中的故障模式。

**方法:** 提出轻量级、模型无关的评估层，将每个模态视为代理，生成候选标签和简短自评估。通过简单的融合机制聚合输出，识别贡献者（支持正确结果的模态）和破坏者（误导的模态）。

**结果:** 在多模态情感识别基准测试中应用该方法，揭示了系统的可靠性特征，能够区分故障是来自数据集伪影还是模型限制。

**结论:** 该框架为多模态推理提供了诊断支架，支持对融合动态的原则性审计，并为可能的干预措施提供信息。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+One+Modality+Sabotages+the+Others%3A+A+Diagnostic+Lens+on+Multimodal+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02794，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02794&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Despite rapid growth in multimodal large language models (MLLMs), their
reasoning traces remain opaque: it is often unclear which modality drives a
prediction, how conflicts are resolved, or when one stream dominates. In this
paper, we introduce modality sabotage, a diagnostic failure mode in which a
high-confidence unimodal error overrides other evidence and misleads the fused
result. To analyze such dynamics, we propose a lightweight, model-agnostic
evaluation layer that treats each modality as an agent, producing candidate
labels and a brief self-assessment used for auditing. A simple fusion mechanism
aggregates these outputs, exposing contributors (modalities supporting correct
outcomes) and saboteurs (modalities that mislead). Applying our diagnostic
layer in a case study on multimodal emotion recognition benchmarks with
foundation models revealed systematic reliability profiles, providing insight
into whether failures may arise from dataset artifacts or model limitations.
More broadly, our framework offers a diagnostic scaffold for multimodal
reasoning, supporting principled auditing of fusion dynamics and informing
possible interventions.

</details>


### [30] [Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning](https://arxiv.org/abs/2511.02818)
*Mohamed Bouadi, Pratinav Seth, Aditya Tanna, Vinay Kumar Sankarapu*

**主要类别:** cs.AI

**AI概要:** Orion-MSP是一个新的表格数据上下文学习架构，通过多尺度处理、块稀疏注意力和感知器式内存解决了现有方法的局限性，在多个基准测试中达到或超越了最先进性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有表格上下文学习方法存在单尺度特征处理、二次复杂度密集注意力和严格顺序组件处理等局限性，无法有效处理异构特征类型和多尺度复杂交互。

**方法:** 提出Orion-MSP架构，包含三个关键创新：多尺度处理捕获层次特征交互；块稀疏注意力结合窗口化、全局和随机模式实现可扩展效率；感知器式内存实现组件间双向信息流。

**结果:** 在多样化基准测试中，Orion-MSP匹配或超越了最先进性能，并能有效扩展到高维表格数据。

**结论:** Orion-MSP为高效的表格上下文学习设立了新标准，解决了现有架构的关键限制，实现了更好的性能和可扩展性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Orion-MSP%3A+Multi-Scale+Sparse+Attention+for+Tabular+In-Context+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02818，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02818&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Tabular data remain the predominant format for real-world applications. Yet,
developing effective neural models for tabular data remains challenging due to
heterogeneous feature types and complex interactions occurring at multiple
scales. Recent advances in tabular in-context learning (ICL), such as TabPFN
and TabICL, have achieved state-of-the-art performance comparable to
gradient-boosted trees (GBTs) without task-specific fine-tuning. However,
current architectures exhibit key limitations: (1) single-scale feature
processing that overlooks hierarchical dependencies, (2) dense attention with
quadratic scaling in table width, and (3) strictly sequential component
processing that prevents iterative representation refinement and
cross-component communication. To address these challenges, we introduce
Orion-MSP, a tabular ICL architecture featuring three key innovations: (1)
multi-scale processing to capture hierarchical feature interactions; (2)
block-sparse attention combining windowed, global, and random patterns for
scalable efficiency and long-range connectivity; and (3) a Perceiver-style
memory enabling safe bidirectional information flow across components. Across
diverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance
while scaling effectively to high-dimensional tables, establishing a new
standard for efficient tabular in-context learning. The model is publicly
available at https://github.com/Lexsi-Labs/Orion-MSP .

</details>


### [31] [Optimizing AI Agent Attacks With Synthetic Data](https://arxiv.org/abs/2511.02823)
*Chloe Loughridge, Paul Colognese, Avery Griffin, Tyler Tracy, Jon Kutasov, Joe Benton*

**主要类别:** cs.AI

**AI概要:** 该研究通过分解攻击能力为五个核心技能并开发概率模型来优化AI控制评估中的攻击策略，在数据有限的复杂环境中显著提升了攻击强度


<details>
  <summary>更多</summary>
  
**动机:** 随着AI部署变得复杂且高风险，需要准确评估风险。AI控制框架需要强大的攻击策略，但在计算受限的复杂智能体环境中获取数据困难

**方法:** 将攻击能力分解为五个构成技能（怀疑建模、攻击选择、计划合成、执行和隐蔽性），开发攻击动态概率模型，在模拟中优化超参数，然后转移到SHADE-Arena数据集

**结果:** 攻击强度显著提升，安全分数从基线0.87降低到0.41

**结论:** 通过技能分解和概率建模方法可以有效优化攻击策略，即使在数据有限的环境中也能显著改善AI控制评估的效果

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimizing+AI+Agent+Attacks+With+Synthetic+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02823，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02823&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** As AI deployments become more complex and high-stakes, it becomes
increasingly important to be able to estimate their risk. AI control is one
framework for doing so. However, good control evaluations require eliciting
strong attack policies. This can be challenging in complex agentic environments
where compute constraints leave us data-poor. In this work, we show how to
optimize attack policies in SHADE-Arena, a dataset of diverse realistic control
environments. We do this by decomposing attack capability into five constituent
skills -- suspicion modeling, attack selection, plan synthesis, execution and
subtlety -- and optimizing each component individually. To get around the
constraint of limited data, we develop a probabilistic model of attack
dynamics, optimize our attack hyperparameters using this simulation, and then
show that the results transfer to SHADE-Arena. This results in a substantial
improvement in attack strength, reducing safety score from a baseline of 0.87
to 0.41 using our scaffold.

</details>


### [32] [Kosmos: An AI Scientist for Autonomous Discovery](https://arxiv.org/abs/2511.02824)
*Ludovico Mitchener, Angela Yiu, Benjamin Chang, Mathieu Bourdenx, Tyler Nadolski, Arvis Sulovari, Eric C. Landsness, Daniel L. Barabasi, Siddharth Narayanan, Nicky Evans, Shriya Reddy, Martha Foiani, Aizad Kamal, Leah P. Shriver, Fang Cao, Asmamaw T. Wassie, Jon M. Laurent, Edwin Melville-Green, Mayk Caldas, Albert Bou, Kaleigh F. Roberts, Sladjana Zagorac, Timothy C. Orr, Miranda E. Orr, Kevin J. Zwezdaryk, Ali E. Ghareeb, Laurie McCoy, Bruna Gomes, Euan A. Ashley, Karen E. Duff, Tonio Buonassisi, Tom Rainforth, Randall J. Bateman, Michael Skarlinski, Samuel G. Rodriques, Michaela M. Hinks, Andrew D. White*

**主要类别:** cs.AI

**AI概要:** Kosmos是一个能够自动化数据驱动科学发现的AI科学家系统，通过结构化世界模型连接数据分析与文献搜索代理，可在12小时内执行多达200次操作，生成可追溯的科学报告，其产出相当于人类研究者6个月的工作量。


<details>
  <summary>更多</summary>
  
**动机:** 现有AI科研代理在执行多次操作后会失去连贯性，限制了科学发现的深度。需要开发能够维持长期连贯性、自动化科研流程的AI系统。

**方法:** 使用结构化世界模型在数据分析代理和文献搜索代理之间共享信息，进行并行的数据分析、文献搜索和假设生成循环，最终合成科学报告。每个运行平均执行42,000行代码和阅读1,500篇论文。

**结果:** 79.4%的报告陈述被独立科学家确认为准确，单次20循环运行相当于人类6个月的研究工作。产生了7个跨学科发现，其中3个独立重现了未发表的发现，4个是全新贡献。

**结论:** Kosmos展示了AI系统能够进行长期连贯的科学研究，产出可验证的发现，且科学发现的价值与运行周期呈线性增长关系，为自动化科学发现提供了可行路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Kosmos%3A+An+AI+Scientist+for+Autonomous+Discovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02824，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02824&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Data-driven scientific discovery requires iterative cycles of literature
search, hypothesis generation, and data analysis. Substantial progress has been
made towards AI agents that can automate scientific research, but all such
agents remain limited in the number of actions they can take before losing
coherence, thus limiting the depth of their findings. Here we present Kosmos,
an AI scientist that automates data-driven discovery. Given an open-ended
objective and a dataset, Kosmos runs for up to 12 hours performing cycles of
parallel data analysis, literature search, and hypothesis generation before
synthesizing discoveries into scientific reports. Unlike prior systems, Kosmos
uses a structured world model to share information between a data analysis
agent and a literature search agent. The world model enables Kosmos to
coherently pursue the specified objective over 200 agent rollouts, collectively
executing an average of 42,000 lines of code and reading 1,500 papers per run.
Kosmos cites all statements in its reports with code or primary literature,
ensuring its reasoning is traceable. Independent scientists found 79.4% of
statements in Kosmos reports to be accurate, and collaborators reported that a
single 20-cycle Kosmos run performed the equivalent of 6 months of their own
research time on average. Furthermore, collaborators reported that the number
of valuable scientific findings generated scales linearly with Kosmos cycles
(tested up to 20 cycles). We highlight seven discoveries made by Kosmos that
span metabolomics, materials science, neuroscience, and statistical genetics.
Three discoveries independently reproduce findings from preprinted or
unpublished manuscripts that were not accessed by Kosmos at runtime, while four
make novel contributions to the scientific literature.

</details>


### [33] [Neurosymbolic Deep Learning Semantics](https://arxiv.org/abs/2511.02825)
*Artur d'Avila Garcez, Simon Odense*

**主要类别:** cs.AI

**AI概要:** 本文提出使用逻辑框架为深度学习提供语义基础，通过神经符号AI方法建立神经网络与逻辑之间的映射关系，解决AI科学发现缺乏语义理解的问题。


<details>
  <summary>更多</summary>
  
**动机:** AI在科学领域取得重大成就但缺乏语义基础，导致科学发现难以被理解。需要建立框架将AI洞察转化为可理解的科学知识。

**方法:** 采用神经符号框架，通过逻辑语义将神经网络与逻辑连接，建立语义编码框架，统一现有的各种编码和知识提取方法。

**结果:** 提出了一个正式的语义编码框架，明确了神经网络与逻辑之间的映射关系，并描述了现有各种方法的共同要素。

**结论:** 逻辑为深度学习提供了急需的语义基础，但实践中识别语义编码仍面临类似心灵哲学中的困难，需要进一步研究解决。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neurosymbolic+Deep+Learning+Semantics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02825，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02825&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Artificial Intelligence (AI) is a powerful new language of science as
evidenced by recent Nobel Prizes in chemistry and physics that recognized
contributions to AI applied to those areas. Yet, this new language lacks
semantics, which makes AI's scientific discoveries unsatisfactory at best. With
the purpose of uncovering new facts but also improving our understanding of the
world, AI-based science requires formalization through a framework capable of
translating insight into comprehensible scientific knowledge. In this paper, we
argue that logic offers an adequate framework. In particular, we use logic in a
neurosymbolic framework to offer a much needed semantics for deep learning, the
neural network-based technology of current AI. Deep learning and neurosymbolic
AI lack a general set of conditions to ensure that desirable properties are
satisfied. Instead, there is a plethora of encoding and knowledge extraction
approaches designed for particular cases. To rectify this, we introduced a
framework for semantic encoding, making explicit the mapping between neural
networks and logic, and characterizing the common ingredients of the various
existing approaches. In this paper, we describe succinctly and exemplify how
logical semantics and neural networks are linked through this framework, we
review some of the most prominent approaches and techniques developed for
neural encoding and knowledge extraction, provide a formal definition of our
framework, and discuss some of the difficulties of identifying a semantic
encoding in practice in light of analogous problems in the philosophy of mind.

</details>


### [34] [Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything](https://arxiv.org/abs/2511.02834)
*Huawei Lin, Yunzhi Shi, Tong Geng, Weijie Zhao, Wei Wang, Ravender Pal Singh*

**主要类别:** cs.AI

**AI概要:** Agent-Omni框架通过主代理系统协调现有基础模型，实现无需重新训练的多模态推理，在文本、图像、音频、视频等多种模态任务上达到最先进性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的多模态大语言模型局限于固定模态对，需要大量对齐数据微调，缺乏全模态能力和稳健推理支持。

**方法:** 采用主代理系统，主代理解析用户意图，将子任务委派给特定模态代理，整合各代理输出形成连贯响应。

**结果:** 在多种模态基准测试中一致达到最先进性能，特别是在需要复杂跨模态推理的任务上表现优异。

**结论:** 该框架具有模块化和易扩展性，能够无缝集成专业化基础模型，保持透明度和可解释性，为可扩展可靠的全模态推理研究提供支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Agent-Omni%3A+Test-Time+Multimodal+Reasoning+via+Model+Coordination+for+Understanding+Anything，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02834，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02834&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Multimodal large language models (MLLMs) have shown strong capabilities but
remain limited to fixed modality pairs and require costly fine-tuning with
large aligned datasets. Building fully omni-capable models that can integrate
text, images, audio, and video remains impractical and lacks robust reasoning
support. In this paper, we propose an Agent-Omni framework that coordinates
existing foundation models through a master-agent system, enabling flexible
multimodal reasoning without retraining. The master agent interprets user
intent, delegates subtasks to modality-specific agents, and integrates their
outputs into coherent responses. Extensive experiments across text, image,
audio, video, and omni benchmarks show that Agent-Omni consistently achieves
state-of-the-art performance, particularly on tasks requiring complex
cross-modal reasoning. Its agent-based design enables seamless integration of
specialized foundation models, ensuring adaptability to diverse inputs while
maintaining transparency and interpretability. In addition, the framework is
modular and easily extensible, allowing future improvements as stronger models
become available. %We release an open-source implementation to support
continued research on scalable and reliable omni-modal reasoning.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [35] [Multi-Personality Generation of LLMs at Decoding-time](https://arxiv.org/abs/2511.01891)
*Rongxin Chen, Yunfan Li, Yige Yuan, Bingbing Xu, Huawei Shen*

**主要类别:** cs.CL

**AI概要:** 提出MPG框架，通过解码时组合范式实现多人格生成，无需重新训练或多维模型，利用单维模型的隐式密度比进行策略聚合采样。


<details>
  <summary>更多</summary>
  
**动机:** 现有基于重新训练的方法成本高且扩展性差，解码时方法依赖外部模型或启发式规则，灵活性和鲁棒性受限。

**方法:** 提出MPG框架，将任务重新表述为从目标策略采样，设计基于推测性块级拒绝采样(SCR)的高效实现方法，分块生成响应并通过滑动窗口内的估计阈值并行验证。

**结果:** 在MBTI人格和角色扮演任务上的实验显示，MPG有效性显著，改进幅度达16%-18%。

**结论:** MPG框架提供了一种高效、灵活的多人格生成解决方案，无需额外训练成本，在保持高质量生成的同时显著降低计算开销。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Personality+Generation+of+LLMs+at+Decoding-time，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.01891，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.01891&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Multi-personality generation for LLMs, enabling simultaneous embodiment of
multiple personalization attributes, is a fundamental challenge. Existing
retraining-based approaches are costly and poorly scalable, while decoding-time
methods often rely on external models or heuristics, limiting flexibility and
robustness. In this paper, we propose a novel Multi-Personality Generation
(MPG) framework under the decoding-time combination paradigm. It flexibly
controls multi-personality without relying on scarce multi-dimensional models
or extra training, leveraging implicit density ratios in single-dimensional
models as a "free lunch" to reformulate the task as sampling from a target
strategy aggregating these ratios. To implement MPG efficiently, we design
Speculative Chunk-level based Rejection sampling (SCR), which generates
responses in chunks and parallelly validates them via estimated thresholds
within a sliding window. This significantly reduces computational overhead
while maintaining high-quality generation. Experiments on MBTI personality and
Role-Playing demonstrate the effectiveness of MPG, showing improvements up to
16%-18%. Code and data are available at https://github.com/Libra117/MPG .

</details>


### [36] [Rethinking LLM Human Simulation: When a Graph is What You Need](https://arxiv.org/abs/2511.02135)
*Joseph Suh, Suhong Moon, Serina Chang*

**主要类别:** cs.CL

**AI概要:** 该论文提出了一种基于图神经网络的轻量级人类模拟方法GEMS，在离散选择模拟任务中能够以更小的模型规模达到或超越大型语言模型的性能


<details>
  <summary>更多</summary>
  
**动机:** 探索在人类模拟任务中，是否必须使用大型语言模型，还是可以使用更小、更专业的领域模型来获得相同或更好的效果

**方法:** 提出Graph-basEd Models for human Simulation (GEMS)，将离散选择模拟任务转化为图上的链接预测问题，利用关系知识，仅在需要时整合语言表示

**结果:** 在三个模拟数据集上的评估显示，GEMS达到了与LLM相当或更好的准确性，同时具有更高的效率、可解释性和透明度

**结论:** 基于图的建模方法可以作为LLM在人类模拟任务中的轻量级替代方案，具有显著的计算效率和解释性优势

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+LLM+Human+Simulation%3A+When+a+Graph+is+What+You+Need，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02135，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02135&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are increasingly used to simulate humans, with
applications ranging from survey prediction to decision-making. However, are
LLMs strictly necessary, or can smaller, domain-grounded models suffice? We
identify a large class of simulation problems in which individuals make choices
among discrete options, where a graph neural network (GNN) can match or surpass
strong LLM baselines despite being three orders of magnitude smaller. We
introduce Graph-basEd Models for human Simulation (GEMS), which casts discrete
choice simulation tasks as a link prediction problem on graphs, leveraging
relational knowledge while incorporating language representations only when
needed. Evaluations across three key settings on three simulation datasets show
that GEMS achieves comparable or better accuracy than LLMs, with far greater
efficiency, interpretability, and transparency, highlighting the promise of
graph-based modeling as a lightweight alternative to LLMs for human simulation.
Our code is available at https://github.com/schang-lab/gems.

</details>


### [37] [IG-Pruning: Input-Guided Block Pruning for Large Language Models](https://arxiv.org/abs/2511.02213)
*Kangyu Qiao, Shaolei Zhang, Yang Feng*

**主要类别:** cs.CL

**AI概要:** IG-Pruning是一种新颖的输入感知块级剪枝方法，通过动态选择层掩码来减少大语言模型的计算成本，无需大量训练即可实现高效推理。


<details>
  <summary>更多</summary>
  
**动机:** 随着大语言模型计算需求增长，需要高效的推理方法。现有深度剪枝方法使用固定块掩码，在不同任务和输入上表现不佳。

**方法:** 两阶段方法：1)通过语义聚类和L0优化发现多样掩码候选；2)在推理时动态选择层掩码，无需大量训练。

**结果:** 实验结果表明该方法始终优于最先进的静态深度剪枝方法。

**结论:** 该方法特别适合资源受限的部署场景，为LLMs的高效推理提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是IG-Pruning%3A+Input-Guided+Block+Pruning+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02213，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02213&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** With the growing computational demands of large language models (LLMs),
efficient inference has become increasingly critical for practical deployment.
Depth pruning has emerged as a promising approach for reducing the
computational costs of large language models by removing transformer layers.
However, existing methods typically rely on fixed block masks, which can lead
to suboptimal performance across different tasks and inputs. In this paper, we
propose IG-Pruning, a novel input-aware block-wise pruning method that
dynamically selects layer masks at inference time. Our approach consists of two
stages: (1) Discovering diverse mask candidates through semantic clustering and
L0 optimization, and (2) Implementing efficient dynamic pruning without the
need for extensive training. Experimental results demonstrate that our method
consistently outperforms state-of-the-art static depth pruning methods, making
it particularly suitable for resource-constrained deployment scenarios.

</details>


### [38] [Demo: Statistically Significant Results On Biases and Errors of LLMs Do Not Guarantee Generalizable Results](https://arxiv.org/abs/2511.02246)
*Jonathan Liu, Haoling Qiu, Jonathan Lasko, Damianos Karakos, Mahsa Yarmohammadi, Mark Dredze*

**主要类别:** cs.CL

**AI概要:** 该研究开发了一个评估医疗聊天机器人一致性的基础设施，发现LLM评估者间一致性很低，建议使用多个LLM评估器以避免统计显著但不可泛化的结果。


<details>
  <summary>更多</summary>
  
**动机:** 医疗聊天机器人在涉及非医疗因素（如人口统计信息）时必须提供一致的建议，但现有LLM存在幻觉、遗漏和偏见问题，需要了解其失效条件。

**方法:** 开发自动生成查询的基础设施：1）通过采样患者人口统计、病史、疾病和写作风格创建真实问题；2）使用LLM作为评判者进行幻觉和遗漏检测，以及治疗类别检测。进行了两个案例研究：LLM间一致性和不同回答/评估LLM的影响。

**结果:** LLM标注者表现出低一致性分数（平均Cohen's Kappa κ=0.118），只有特定的（回答、评估）LLM对在写作风格、性别和种族方面产生统计显著差异。

**结论:** 建议使用多个LLM作为评估器以避免非泛化结果，特别是在缺乏真实数据的情况下，并建议发布LLM间一致性指标以提高透明度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Demo%3A+Statistically+Significant+Results+On+Biases+and+Errors+of+LLMs+Do+Not+Guarantee+Generalizable+Results，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02246，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02246&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent research has shown that hallucinations, omissions, and biases are
prevalent in everyday use-cases of LLMs. However, chatbots used in medical
contexts must provide consistent advice in situations where non-medical factors
are involved, such as when demographic information is present. In order to
understand the conditions under which medical chatbots fail to perform as
expected, we develop an infrastructure that 1) automatically generates queries
to probe LLMs and 2) evaluates answers to these queries using multiple
LLM-as-a-judge setups and prompts. For 1), our prompt creation pipeline samples
the space of patient demographics, histories, disorders, and writing styles to
create realistic questions that we subsequently use to prompt LLMs. In 2), our
evaluation pipeline provides hallucination and omission detection using
LLM-as-a-judge as well as agentic workflows, in addition to LLM-as-a-judge
treatment category detectors. As a baseline study, we perform two case studies
on inter-LLM agreement and the impact of varying the answering and evaluation
LLMs. We find that LLM annotators exhibit low agreement scores (average Cohen's
Kappa $\kappa=0.118$), and only specific (answering, evaluation) LLM pairs
yield statistically significant differences across writing styles, genders, and
races. We recommend that studies using LLM evaluation use multiple LLMs as
evaluators in order to avoid arriving at statistically significant but
non-generalizable results, particularly in the absence of ground-truth data. We
also suggest publishing inter-LLM agreement metrics for transparency. Our code
and dataset are available here:
https://github.com/BBN-E/medic-neurips-2025-demo.

</details>


### [39] [LTD-Bench: Evaluating Large Language Models by Letting Them Draw](https://arxiv.org/abs/2511.02347)
*Liuhao Lin, Ke Li, Zihan Xu, Yuchen Shi, Yulei Qin, Yan Zhang, Xing Sun, Rongrong Ji*

**主要类别:** cs.CL

**AI概要:** LTD-Bench是一个突破性的视觉化基准测试，通过让大语言模型生成点阵图或可执行代码来评估空间推理能力，将抽象数值指标转化为直观可视的输出，揭示了当前LLM在语言-空间映射方面的严重缺陷。


<details>
  <summary>更多</summary>
  
**动机:** 当前LLM评估存在严重盲点，依赖不透明的数值指标掩盖了空间推理的根本局限性，无法提供对模型能力的直观理解，导致报告性能与实际能力之间存在危险脱节。

**方法:** LTD-Bench采用全面方法论，包含生成任务（测试空间想象力）和识别任务（评估空间感知），通过三个难度递增的级别，系统评估语言-空间映射的两个关键方向。

**结果:** 实验发现即使传统基准测试表现优异的LLM在建立语言与空间概念双向映射方面也存在严重缺陷，这削弱了它们作为真正世界模型的潜力。

**结论:** LTD-Bench通过可视化输出使空间推理限制对非专家也显而易见，弥合了统计性能与直观评估之间的根本差距，并为模型相似性分析提供了强大的诊断方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LTD-Bench%3A+Evaluating+Large+Language+Models+by+Letting+Them+Draw，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02347，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02347&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Current evaluation paradigms for large language models (LLMs) represent a
critical blind spot in AI research--relying on opaque numerical metrics that
conceal fundamental limitations in spatial reasoning while providing no
intuitive understanding of model capabilities. This deficiency creates a
dangerous disconnect between reported performance and practical abilities,
particularly for applications requiring physical world understanding. We
introduce LTD-Bench, a breakthrough benchmark that transforms LLM evaluation
from abstract scores to directly observable visual outputs by requiring models
to generate drawings through dot matrices or executable code. This approach
makes spatial reasoning limitations immediately apparent even to non-experts,
bridging the fundamental gap between statistical performance and intuitive
assessment. LTD-Bench implements a comprehensive methodology with complementary
generation tasks (testing spatial imagination) and recognition tasks (assessing
spatial perception) across three progressively challenging difficulty levels,
methodically evaluating both directions of the critical language-spatial
mapping. Our extensive experiments with state-of-the-art models expose an
alarming capability gap: even LLMs achieving impressive results on traditional
benchmarks demonstrate profound deficiencies in establishing bidirectional
mappings between language and spatial concept--a fundamental limitation that
undermines their potential as genuine world models. Furthermore, LTD-Bench's
visual outputs enable powerful diagnostic analysis, offering a potential
approach to investigate model similarity.

</details>


### [40] [Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation](https://arxiv.org/abs/2511.02358)
*Wongyu Kim, Hochang Lee, Sanghak Lee, Yoonsung Kim, Jaehyun Park*

**主要类别:** cs.CL

**AI概要:** M-Solomon是一个多模态嵌入器，通过自适应查询增强技术，只在需要时对查询进行增强，解决了传统方法中所有查询都增强导致的延迟问题和性能下降问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有基于LLM的嵌入器对所有查询都进行增强，导致显著的嵌入延迟，且某些查询增强后性能反而下降。同时，之前的方法未在多模态环境中探索。

**方法:** 1. 在数据集层面将查询分为需要增强和不需要增强两组；2. 利用多模态大语言模型为需要增强的查询生成合适的增强内容；3. 通过自适应查询增强机制，只在必要时进行增强（使用/augment前缀），其他情况生成/embed字符串。

**结果:** 实验结果显示M-Solomon不仅大幅超越无增强的基线方法，还优于总是使用增强的基线方法，同时提供了更快的嵌入延迟。

**结论:** 自适应查询增强策略在多模态环境中有效，能够在保持性能的同时显著降低延迟，为查询增强提供了更智能的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Let+Multimodal+Embedders+Learn+When+to+Augment+Query+via+Adaptive+Query+Augmentation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02358，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02358&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Query augmentation makes queries more meaningful by appending further
information to the queries to find relevant documents. Current studies have
proposed Large Language Model (LLM)-based embedders, which learn representation
for embedding and generation for query augmentation in a multi-task manner by
leveraging the generative capabilities of LLM. During inference, these jointly
trained embedders have conducted query augmentation followed by embedding,
showing effective results. However, augmenting every query leads to substantial
embedding latency and query augmentation can be detrimental to performance for
some queries. Also, previous methods have not been explored in multimodal
environments. To tackle these problems, we propose M-Solomon, a universal
multimodal embedder that can adaptively determine when to augment queries. Our
approach first divides the queries of the training datasets into two groups at
the dataset level. One includes queries that require augmentation and the other
includes queries that do not. Then, we introduces a synthesis process that
generates appropriate augmentations for queries that require them by leveraging
a powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation.
Through this step, M-Solomon can conduct query augmentation only when necessary
by learning to generate synthetic augmentations with the prefix /augment for
queries that demand them and to generate the simple string /embed for others.
Experimental results showed that M-Solomon not only surpassed the baseline
without augmentation by a large margin but also outperformed the baseline that
always used augmentation, providing much faster embedding latency.

</details>


### [41] [LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context](https://arxiv.org/abs/2511.02366)
*Yudong Li, Zhongliang Yang, Kejiang Chen, Wenxuan Wang, Tianxin Zhang, Sifang Wan, Kecheng Wang, Haitian Li, Xu Wang, Lefan Cheng, Youdan Yang, Baocheng Chen, Ziyu Liu, Yufei Sun, Liyan Wu, Wenya Wen, Xingchi Gu, Peiru Yang*

**主要类别:** cs.CL

**AI概要:** LiveSecBench是一个针对中文LLM应用场景的动态安全基准测试，评估6个关键维度，并持续更新威胁向量，目前已评估18个模型。


<details>
  <summary>更多</summary>
  
**动机:** 针对中文语言环境下的LLM应用缺乏专门的安全评估基准，需要基于中国法律和社会框架来评估模型安全性

**方法:** 建立包含合法性、伦理、事实性、隐私、对抗鲁棒性和推理安全6个维度的评估体系，采用动态更新机制纳入新威胁向量

**结果:** 已完成v251030版本，评估了18个LLM模型，提供了中文AI安全现状的全面视图，并建立了公开排行榜

**结论:** LiveSecBench为中文LLM安全评估提供了重要基准工具，通过持续更新机制保持相关性，有助于推动中文语言AI安全发展

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LiveSecBench%3A+A+Dynamic+and+Culturally-Relevant+AI+Safety+Benchmark+for+LLMs+in+Chinese+Context，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02366，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02366&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** In this work, we propose LiveSecBench, a dynamic and continuously updated
safety benchmark specifically for Chinese-language LLM application scenarios.
LiveSecBench evaluates models across six critical dimensions (Legality, Ethics,
Factuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in
the Chinese legal and social frameworks. This benchmark maintains relevance
through a dynamic update schedule that incorporates new threat vectors, such as
the planned inclusion of Text-to-Image Generation Safety and Agentic Safety in
the next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs,
providing a landscape of AI safety in the context of Chinese language. The
leaderboard is publicly accessible at https://livesecbench.intokentech.cn/.

</details>


### [42] [AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda](https://arxiv.org/abs/2511.02374)
*Mohd Nauman, Sravan Gvm, Vijay Devane, Shyam Pawar, Viraj Thakur, Kundeshwar Pundalik, Piyush Sawarkar, Rohit Saluja, Maunendra Desarkar, Ganesh Ramakrishnan*

**主要类别:** cs.CL

**AI概要:** AyurParam-2.9B是一个专门针对阿育吠陀医学领域的双语语言模型，在1.5-3B参数规模的开源模型中表现最佳，甚至能与更大模型竞争。


<details>
  <summary>更多</summary>
  
**动机:** 主流大语言模型在需要深度文化、语言和专业知识的高度专业领域（如阿育吠陀医学）表现不佳，无法准确解释和应用这些传统医学系统的知识。

**方法:** 从Param-1-2.9B模型微调，使用经过专家精心策划的阿育吠陀数据集，包含英语和印地语的情境感知、推理和客观式问答，采用严格的标注协议确保事实准确性和指导清晰度。

**结果:** 在BhashaBench-Ayur基准测试中，AyurParam不仅超越了同规模的所有开源指令调优模型，还表现出与更大模型相当或更优的性能。

**结论:** 研究结果表明，在提供可靠、文化契合的专业医学知识AI时，真实的领域适应和高质量监督是必要的。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AyurParam%3A+A+State-of-the-Art+Bilingual+Language+Model+for+Ayurveda，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02374，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02374&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Current large language models excel at broad, general-purpose tasks, but
consistently underperform when exposed to highly specialized domains that
require deep cultural, linguistic, and subject-matter expertise. In particular,
traditional medical systems such as Ayurveda embody centuries of nuanced
textual and clinical knowledge that mainstream LLMs fail to accurately
interpret or apply. We introduce AyurParam-2.9B, a domain-specialized,
bilingual language model fine-tuned from Param-1-2.9B using an extensive,
expertly curated Ayurveda dataset spanning classical texts and clinical
guidance. AyurParam's dataset incorporates context-aware, reasoning, and
objective-style Q&A in both English and Hindi, with rigorous annotation
protocols for factual precision and instructional clarity. Benchmarked on
BhashaBench-Ayur, AyurParam not only surpasses all open-source
instruction-tuned models in its size class (1.5--3B parameters), but also
demonstrates competitive or superior performance compared to much larger
models. The results from AyurParam highlight the necessity for authentic domain
adaptation and high-quality supervision in delivering reliable, culturally
congruent AI for specialized medical knowledge.

</details>


### [43] [AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2511.02376)
*Aashray Reddy, Andrew Zagula, Nicholas Saban*

**主要类别:** cs.CL

**AI概要:** AutoAdv是一个无需训练的多轮越狱攻击框架，通过三种自适应机制在6轮对话内达到95%的攻击成功率，比单轮攻击提升24%，揭示了当前AI安全机制在多轮对话中的脆弱性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的大语言模型安全评估主要关注单轮交互，而现实中的越狱攻击往往通过多轮自适应对话进行，需要开发能够评估多轮攻击威胁的方法。

**方法:** AutoAdv框架结合三种机制：模式管理器从成功攻击中学习改进提示、温度管理器根据失败模式动态调整采样参数、两阶段重写策略先伪装有害请求再迭代优化。

**结果:** 在Llama-3.1-8B上达到95%攻击成功率，比单轮基线提升24%；在GPT-4o-mini、Qwen3-235B、Mistral-7B等多个模型上都显示多轮攻击比单轮方法更有效。

**结论:** 针对单轮交互优化的对齐策略无法在扩展对话中保持鲁棒性，当前安全机制存在持续漏洞，迫切需要开发多轮感知的防御措施。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AutoAdv%3A+Automated+Adversarial+Prompting+for+Multi-Turn+Jailbreaking+of+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02376，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02376&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) remain vulnerable to jailbreaking attacks where
adversarial prompts elicit harmful outputs, yet most evaluations focus on
single-turn interactions while real-world attacks unfold through adaptive
multi-turn conversations. We present AutoAdv, a training-free framework for
automated multi-turn jailbreaking that achieves up to 95% attack success rate
on Llama-3.1-8B within six turns a 24 percent improvement over single turn
baselines. AutoAdv uniquely combines three adaptive mechanisms: a pattern
manager that learns from successful attacks to enhance future prompts, a
temperature manager that dynamically adjusts sampling parameters based on
failure modes, and a two-phase rewriting strategy that disguises harmful
requests then iteratively refines them. Extensive evaluation across commercial
and open-source models (GPT-4o-mini, Qwen3-235B, Mistral-7B) reveals persistent
vulnerabilities in current safety mechanisms, with multi-turn attacks
consistently outperforming single-turn approaches. These findings demonstrate
that alignment strategies optimized for single-turn interactions fail to
maintain robustness across extended conversations, highlighting an urgent need
for multi-turn-aware defenses.

</details>


### [44] [Merging Continual Pretraining Models for Domain-Specialized LLMs: A Case Study in Finance](https://arxiv.org/abs/2511.02451)
*Kentaro Ueda, François Portet, Hirohiko Suwa, Keiichi Yasumoto*

**主要类别:** cs.CL

**AI概要:** 该论文首次系统研究了领域特定持续预训练(CPT)模型的融合方法，通过融合金融、数学和日语专家模型，提出了三阶段评估框架，发现模型融合能恢复通用知识并产生跨领域涌现能力。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型在专业领域表现不佳，需要融合多种技能。现有研究主要关注监督微调模型融合，而CPT模型融合研究空白，需要探索如何有效整合领域专家模型。

**方法:** 创建金融、数学和日语专家模型，采用三种融合方法(Task Arithmetic、TIES、DARE-TIES)，在包含18个任务8个数据集的金融基准上进行三阶段评估(知识恢复、互补性、涌现性)。

**结果:** 融合专家与基础模型能恢复CPT过程中丢失的通用知识；融合多个专家能提升性能并产生跨领域涌现技能；Task Arithmetic表现强劲但对超参数敏感，TIES更稳健；模型相似性与融合成功相关，但涌现技能依赖更复杂因素。

**结论:** 本研究为CPT模型融合提供了首个基础分析，建立了原则性框架，为从现有资源构建多技能大语言模型提供了明确指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Merging+Continual+Pretraining+Models+for+Domain-Specialized+LLMs%3A+A+Case+Study+in+Finance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02451，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02451&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** While LLMs excel at general tasks, they struggle in specialized domains like
finance, requiring diverse skills in domain knowledge, mathematical reasoning,
and multilingual processing. Merging domain-specific Continual Pre-training
(CPT) "experts" offers a practical alternative to costly and unstable
multi-skill training. However, unlike established Supervised Fine-Tuning (SFT)
model-based merging, CPT model merging remains largely unexplored. We address
this gap by creating financial LLMs from experts in finance, math, and
Japanese. We propose a three-stage evaluation focusing on knowledge recovery,
complementarity, and emergence, and assess three merging methods (Task
Arithmetic, TIES, and DARE-TIES) on a comprehensive financial benchmark curated
from 18 tasks across 8 established datasets. Results show that merging an
expert with its base model recovers general knowledge lost during CPT, while
merging experts improves performance and can yield emergent cross-domain
skills. Among the methods, Task Arithmetic performs strongly but is
hyperparameter-sensitive, whereas TIES is more robust. Our findings also
suggest that while model similarity correlates with merging success, emergent
skills depend on more complex factors. This work presents the first
foundational analysis of CPT model merging, establishing a principled framework
and providing clear guidance for building multi-skill LLMs from existing
assets.

</details>


### [45] [Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas](https://arxiv.org/abs/2511.02458)
*Giulia Iadisernia, Carolina Camassa*

**主要类别:** cs.CL

**AI概要:** 研究发现GPT-4o在宏观经济预测任务中表现与人类专家相当，但角色提示(persona-based prompting)并未带来预测优势，可以省略以节省计算成本。


<details>
  <summary>更多</summary>
  
**动机:** 评估基于角色的提示是否能提高大语言模型在宏观经济预测任务中的表现，特别是与欧洲央行专业预测者调查进行比较。

**方法:** 使用PersonaHub语料库中的2,368个经济学相关角色提示GPT-4o，复制欧洲央行专业预测者调查的50个季度预测(2013-2025)，涵盖4个目标变量和4个预测周期，并与无角色描述的基线预测进行对比。

**结果:** 1. GPT-4o与人类预测者达到非常相似的准确度水平，统计显著但实际差异不大；2. 样本外评估显示GPT-4o在未见事件上保持竞争力；3. 角色描述未带来可测量的预测优势。

**结论:** GPT-4o在提供相关上下文数据时能在宏观经济预测中达到竞争性准确度，但多样化的提示产生的预测结果与人类专家组相比高度同质化，角色提示可以省略而不影响准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Prompting+for+Policy%3A+Forecasting+Macroeconomic+Scenarios+with+Synthetic+LLM+Personas，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02458，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02458&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We evaluate whether persona-based prompting improves Large Language Model
(LLM) performance on macroeconomic forecasting tasks. Using 2,368
economics-related personas from the PersonaHub corpus, we prompt GPT-4o to
replicate the ECB Survey of Professional Forecasters across 50 quarterly rounds
(2013-2025). We compare the persona-prompted forecasts against the human
experts panel, across four target variables (HICP, core HICP, GDP growth,
unemployment) and four forecast horizons. We also compare the results against
100 baseline forecasts without persona descriptions to isolate its effect. We
report two main findings. Firstly, GPT-4o and human forecasters achieve
remarkably similar accuracy levels, with differences that are statistically
significant yet practically modest. Our out-of-sample evaluation on 2024-2025
data demonstrates that GPT-4o can maintain competitive forecasting performance
on unseen events, though with notable differences compared to the in-sample
period. Secondly, our ablation experiment reveals no measurable forecasting
advantage from persona descriptions, suggesting these prompt components can be
omitted to reduce computational costs without sacrificing accuracy. Our results
provide evidence that GPT-4o can achieve competitive forecasting accuracy even
on out-of-sample macroeconomic events, if provided with relevant context data,
while revealing that diverse prompts produce remarkably homogeneous forecasts
compared to human panels.

</details>


### [46] [Smart-Hiring: An Explainable end-to-end Pipeline for CV Information Extraction and Job Matching](https://arxiv.org/abs/2511.02537)
*Kenza Khelkhal, Dihia Lanasri*

**主要类别:** cs.CL

**AI概要:** Smart-Hiring是一个端到端的NLP流水线系统，能够自动从简历中提取结构化信息，并通过语义匹配将候选人与职位描述进行智能匹配，提高招聘效率并减少人为偏见。


<details>
  <summary>更多</summary>
  
**动机:** 传统招聘过程中手动筛选大量简历耗时耗力、容易出错且存在人为偏见，需要自动化解决方案来提高招聘效率和公平性。

**方法:** 结合文档解析、命名实体识别和上下文文本嵌入技术，在共享向量空间中编码简历和职位描述，计算相似度分数。系统采用模块化和可解释的设计。

**结果:** 在真实数据集上的实验表明，该系统在保持高可解释性和透明度的同时，实现了有竞争力的匹配准确率。

**结论:** 该研究提出了一个可扩展且实用的NLP招聘分析框架，为偏见缓解、公平感知建模和大规模数据驱动招聘解决方案的部署指明了有前景的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Smart-Hiring%3A+An+Explainable+end-to-end+Pipeline+for+CV+Information+Extraction+and+Job+Matching，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02537，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02537&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Hiring processes often involve the manual screening of hundreds of resumes
for each job, a task that is time and effort consuming, error-prone, and
subject to human bias. This paper presents Smart-Hiring, an end-to-end Natural
Language Processing (NLP) pipeline de- signed to automatically extract
structured information from unstructured resumes and to semantically match
candidates with job descriptions. The proposed system combines document
parsing, named-entity recognition, and contextual text embedding techniques to
capture skills, experience, and qualifications. Using advanced NLP technics,
Smart-Hiring encodes both resumes and job descriptions in a shared vector space
to compute similarity scores between candidates and job postings. The pipeline
is modular and explainable, allowing users to inspect extracted entities and
matching rationales. Experiments were conducted on a real-world dataset of
resumes and job descriptions spanning multiple professional domains,
demonstrating the robustness and feasibility of the proposed approach. The
system achieves competitive matching accuracy while preserving a high degree of
interpretability and transparency in its decision process. This work introduces
a scalable and practical NLP frame- work for recruitment analytics and outlines
promising directions for bias mitigation, fairness-aware modeling, and
large-scale deployment of data-driven hiring solutions.

</details>


### [47] [The Analysis of Lexical Errors in Machine Translation from English into Romanian](https://arxiv.org/abs/2511.02587)
*Angela Stamatie*

**主要类别:** cs.CL

**AI概要:** 该研究分析了谷歌翻译从英语到罗马尼亚语翻译中的词汇错误，特别关注WHO和Gavi组织提供的COVID-19相关官方信息文本，通过分析230个文本来提升机器翻译的词汇选择准确性。


<details>
  <summary>更多</summary>
  
**动机:** 研究旨在提高机器翻译的质量，特别是谷歌翻译在英语到罗马尼亚语翻译中的表现，通过识别和减少词汇错误来改进翻译系统的准确性和流畅度。

**方法:** 对230个从英语翻译成罗马尼亚语的文本进行全面的错误分析，这些文本包含COVID-19相关的官方医疗信息，使用谷歌翻译作为翻译工具。

**结果:** 研究发现并分类了机器翻译中出现的各种词汇错误，为改进翻译系统提供了具体的数据支持。

**结论:** 通过系统性的错误分析，该研究为提升谷歌翻译的词汇选择准确性提供了重要见解，有助于改进机器翻译的整体质量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Analysis+of+Lexical+Errors+in+Machine+Translation+from+English+into+Romanian，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02587，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02587&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The research explores error analysis in the performance of translating by
Machine Translation from English into Romanian, and it focuses on lexical
errors found in texts which include official information, provided by the World
Health Organization (WHO), the Gavi Organization, by the patient information
leaflet (the information about the active ingredients of the vaccines or the
medication, the indications, the dosage instructions, the storage instructions,
the side effects and warning, etc.). All of these texts are related to Covid-19
and have been translated by Google Translate, a multilingual Machine
Translation that was created by Google. In the last decades, Google has
actively worked to develop a more accurate and fluent automatic translation
system. This research, specifically focused on improving Google Translate, aims
to enhance the overall quality of Machine Translation by achieving better
lexical selection and by reducing errors. The investigation involves a
comprehensive analysis of 230 texts that have been translated from English into
Romanian.

</details>


### [48] [Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations to Decode Student Behaviour](https://arxiv.org/abs/2511.02599)
*Max Norris, Kobi Gal, Sahan Bulathwela*

**主要类别:** cs.CL

**AI概要:** NTKT是一种新颖的知识追踪方法，将KT任务重新定义为使用预训练大语言模型的下一词预测任务，通过将学生历史和问题内容表示为文本序列，显著提升了预测性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有KT模型主要使用回答正确性和元数据，但忽略了问题文本这一重要教学信息源，这限制了预测性能并错失了改进机会。

**方法:** 提出NTKT方法，将知识追踪重构为下一词预测任务，利用预训练LLM同时学习学生行为模式和语言模式，将学生历史记录和问题内容表示为文本序列。

**结果:** 实验表明NTKT显著优于最先进的神经KT模型，在冷启动问题和用户场景下表现更好的泛化能力。

**结论:** 问题内容在知识追踪中至关重要，利用预训练LLM表示能更有效地建模学生学习过程，为个性化学习提供了新的技术路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Next+Token+Knowledge+Tracing%3A+Exploiting+Pretrained+LLM+Representations+to+Decode+Student+Behaviour，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02599，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02599&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Modelling student knowledge is a key challenge when leveraging AI in
education, with major implications for personalised learning. The Knowledge
Tracing (KT) task aims to predict how students will respond to educational
questions in learning environments, based on their prior interactions. Existing
KT models typically use response correctness along with metadata like skill
tags and timestamps, often overlooking the question text, which is an important
source of pedagogical insight. This omission poses a lost opportunity while
limiting predictive performance. We propose Next Token Knowledge Tracing
(NTKT), a novel approach that reframes KT as a next-token prediction task using
pretrained Large Language Models (LLMs). NTKT represents both student histories
and question content as sequences of text, allowing LLMs to learn patterns in
both behaviour and language. Our series of experiments significantly improves
performance over state-of-the-art neural KT models and generalises much better
to cold-start questions and users. These findings highlight the importance of
question content in KT and demonstrate the benefits of leveraging pretrained
representations of LLMs to model student learning more effectively.

</details>


### [49] [CGES: Confidence-Guided Early Stopping for Efficient and Accurate Self-Consistency](https://arxiv.org/abs/2511.02603)
*Ehsan Aghazadeh, Ahmad Ghasemi, Hedyeh Beyhaghi, Hossein Pishro-Nik*

**主要类别:** cs.CL

**AI概要:** CGES是一种基于贝叶斯框架的自适应早停方法，通过置信度信号指导采样，在保持精度的同时显著减少大语言模型的调用次数。


<details>
  <summary>更多</summary>
  
**动机:** 现有的自一致性策略需要固定次数的模型调用，且当正确答案罕见时容易失败，需要更高效的采样方法。

**方法:** 使用从token概率或奖励模型获得的标量置信度信号构建候选答案的后验分布，当某个候选的后验质量超过阈值时自适应停止采样。

**结果:** 在五个推理基准测试中，平均减少69%的模型调用次数（如从16.0次降至4.9次），同时精度仅下降0.06个百分点。

**结论:** CGES框架提供了理论保证，在保持自一致性策略精度的同时大幅提升了推理效率，适用于实际部署场景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CGES%3A+Confidence-Guided+Early+Stopping+for+Efficient+and+Accurate+Self-Consistency，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02603，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02603&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are often queried multiple times at test time,
with predictions aggregated by majority vote. While effective, this
self-consistency strategy (arXiv:2203.11171) requires a fixed number of calls
and can fail when the correct answer is rare. We introduce Confidence-Guided
Early Stopping (CGES), a Bayesian framework that forms posteriors over
candidate answers using scalar confidence signals derived from token
probabilities or reward models. CGES adaptively halts sampling once the
posterior mass of a candidate exceeds a threshold. We provide theoretical
guarantees for both perfectly calibrated confidences and realistic noisy
confidence signals. Across five reasoning benchmarks, CGES reduces the average
number of model calls by about 69 percent (for example, from 16.0 to 4.9) while
matching the accuracy of self-consistency within 0.06 percentage points.

</details>


### [50] [The Realignment Problem: When Right becomes Wrong in LLMs](https://arxiv.org/abs/2511.02623)
*Aakash Sen Sharma, Debdeep Sanyal, Vivek Srivastava, Shirish Karande, Murari Mandal*

**主要类别:** cs.CL

**AI概要:** TRACE框架通过程序化策略应用解决大语言模型与现实政策间的对齐差距，实现精确的策略更新而不损害模型性能


<details>
  <summary>更多</summary>
  
**动机:** 当前大语言模型与人类价值观的对齐存在静态、脆弱且维护成本高的问题，无法跟上不断演变的规范和政策，形成对齐-现实差距

**方法:** 提出TRACE框架：程序化筛选现有偏好数据与新政策的冲突，通过对齐影响评分识别高影响冲突，应用混合优化方法（反转、丢弃或保留偏好）

**结果:** 在多个模型家族（Qwen2.5-7B、Gemma-2-9B、Llama-3.1-8B）上实现稳健的重新对齐，在合成基准和PKU-SafeRLHF数据集上实施新原则而不降低通用能力

**结论:** TRACE为维持LLM对齐建立了可扩展、动态且成本效益高的范式，为可持续和负责任的AI部署提供了基础

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Realignment+Problem%3A+When+Right+becomes+Wrong+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02623，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02623&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The alignment of Large Language Models (LLMs) with human values is central to
their safe deployment, yet current practice produces static, brittle, and
costly-to-maintain models that fail to keep pace with evolving norms and
policies. This misalignment, which we term the Alignment-Reality Gap, poses a
growing challenge for reliable long-term use. Existing remedies are inadequate:
large-scale re-annotation is economically prohibitive, and standard unlearning
methods act as blunt instruments that erode utility rather than enable precise
policy updates. We introduce TRACE (Triage and Re-align by Alignment Conflict
Evaluation), a framework for principled unlearning that reconceives
re-alignment as a programmatic policy application problem. TRACE
programmatically triages existing preference data against a new policy,
identifies high-impact conflicts via a alignment impact score, and applies a
hybrid optimization that cleanly inverts, discards, or preserves preferences
while safeguarding model performance. Empirical results show that TRACE
achieves robust re-alignment across diverse model families (Qwen2.5-7B,
Gemma-2-9B, Llama-3.1-8B). On both synthetic benchmarks and the PKU-SafeRLHF
dataset under complex policy shift, TRACE enforces new principles without
degrading general capabilities. Our work establishes a scalable, dynamic, and
cost-effective paradigm for maintaining LLM alignment, providing a foundation
for sustainable and responsible AI deployment.

</details>


### [51] [Understanding New-Knowledge-Induced Factual Hallucinations in LLMs: Analysis, Solution, and Interpretation](https://arxiv.org/abs/2511.02626)
*Renfei Dang, Peng Hu, Changjiang Gao, Shujian Huang*

**主要类别:** cs.CL

**AI概要:** 研究发现LLM在新知识微调时会产生事实幻觉，提出KnownPatch方法通过在训练后期添加少量已知知识样本来缓解这一问题，并通过注意力分析揭示了幻觉机制。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究未深入探讨LLM在新知识微调时产生事实幻觉的具体表现和机制，需要填补这一研究空白。

**方法:** 设计控制数据集Biography-Reasoning，进行细粒度分析；提出KnownPatch方法在训练后期添加已知知识样本；通过注意力分析研究幻觉机制。

**结果:** 发现特定知识类型完全由新知识组成时幻觉显著增加；KnownPatch有效缓解新知识引起的幻觉；注意力分析显示新知识学习会降低模型对问题关键实体的关注。

**结论:** 知识类型的高陌生度是幻觉的主要驱动因素；KnownPatch方法能有效改善注意力模式并提升性能；注意力模式可传播到相似语境导致幻觉扩散。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+New-Knowledge-Induced+Factual+Hallucinations+in+LLMs%3A+Analysis%2C+Solution%2C+and+Interpretation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02626，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02626&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Previous studies show that introducing new knowledge during large language
models (LLMs) fine-tuning can lead to the generation of erroneous output when
tested on known information, thereby triggering factual hallucinations.
However, existing studies have not deeply investigated the specific
manifestations and underlying mechanisms of these hallucinations. Our work
addresses this gap by designing a controlled dataset Biography-Reasoning, and
conducting a fine-grained analysis across multiple knowledge types and two task
types, including knowledge question answering (QA) and knowledge reasoning
tasks. We find that when fine-tuned on a dataset in which a specific knowledge
type consists entirely of new knowledge, LLMs exhibit significantly increased
hallucination tendencies. This suggests that the high unfamiliarity of a
particular knowledge type, rather than the overall proportion of new knowledge,
is a stronger driver of hallucinations, and these tendencies can even affect
other knowledge types in QA tasks. To mitigate such factual hallucinations, we
propose KnownPatch, which patches a small number of known knowledge samples in
the later stages of training, effectively alleviating new-knowledge-induced
hallucinations. Through attention analysis, we find that learning new knowledge
reduces the model's attention to key entities in the question, thus causing
excessive focus on the surrounding context, which may increase the risk of
hallucination. Moreover, the attention pattern can propagate to similar
contexts, facilitating the spread of hallucinations to textually similar
questions. Our method effectively mitigates the disruption of new knowledge
learning to the model's attention on key entities, accompanied by improved
performance.

</details>


### [52] [Optimal Singular Damage: Efficient LLM Inference in Low Storage Regimes](https://arxiv.org/abs/2511.02681)
*Mohammadsajad Alipour, Mohammad Mohammadi Amiri*

**主要类别:** cs.CL

**AI概要:** 本文提出了一种名为"最优奇异值损伤"的方法，通过选择性稀疏化低秩近似更新来高效存储微调后的参数更新，在相同内存预算下实现了更好的存储效率和准确性。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型(LLMs)的巨大尺寸限制了存储和处理能力，微调版本存储仍然是一个重大挑战。研究发现微调主要影响少量参数，需要更高效的微调模型存储方案。

**方法:** 利用微调更新具有低秩和稀疏特性的观察，提出最优奇异值损伤方法，通过利用奇异向量的交错重要性选择性地稀疏化低秩近似更新，确保保留最具影响力的组件。

**结果:** 在相同内存预算下，具有更大秩的稀疏化低秩近似优于较小秩的标准低秩近似，实验证明该方法显著提高了存储效率并保持优越的准确性。

**结论:** 该方法有效解决了微调模型存储的挑战，通过结合低秩近似和稀疏化的优势，在有限内存条件下实现了更好的性能表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimal+Singular+Damage%3A+Efficient+LLM+Inference+in+Low+Storage+Regimes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02681，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02681&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are increasingly prevalent across diverse
applications. However, their enormous size limits storage and processing
capabilities to a few well-resourced stakeholders. As a result, most
applications rely on pre-trained LLMs, fine-tuned for specific tasks. However,
even storing the fine-tuned versions of these models remains a significant
challenge due to the wide range of tasks they address. Recently, studies show
that fine-tuning these models primarily affects a small fraction of parameters,
highlighting the need for more efficient storage of fine-tuned models. This
paper focuses on efficient storage of parameter updates in pre-trained models
after fine-tuning. To address this challenge, we leverage the observation that
fine-tuning updates are both low-rank and sparse, which can be utilized for
storage efficiency. However, using only low-rank approximation or
sparsification may discard critical singular components that enhance model
expressivity. We first observe that given the same memory budget, sparsified
low-rank approximations with larger ranks outperform standard low-rank
approximations with smaller ranks. Building on this, we propose our method,
optimal singular damage, that selectively sparsifies low-rank approximated
updates by leveraging the interleaved importance of singular vectors, ensuring
that the most impactful components are retained. We demonstrate through
extensive experiments that our proposed methods lead to significant storage
efficiency and superior accuracy within the same memory budget compared to
employing the low-rank approximation or sparsification individually.

</details>


### [53] [PragExTra: A Multilingual Corpus of Pragmatic Explicitation in Translation](https://arxiv.org/abs/2511.02721)
*Doreen Osmelak, Koel Dutta Chowdhury, Uliana Sentsova, Cristina España-Bonet, Josef van Genabith*

**主要类别:** cs.CL

**AI概要:** PragExTra：首个多语言语料库和检测框架，用于计算建模语用显化现象，通过空对齐和主动学习识别文化背景信息的显化添加。


<details>
  <summary>更多</summary>
  
**动机:** 翻译理论中广泛讨论但很少计算建模的语用显化现象，即译者通过添加背景细节使隐含文化意义对目标读者显化的过程。

**方法:** 构建多语言语料库（覆盖8种语言对），通过空对齐识别候选显化案例，使用主动学习和人工标注进行精炼，开发检测框架。

**结果:** 实体和系统层面的显化最为频繁，主动学习使分类器准确率提升7-8个百分点，跨语言准确率达0.88，F1值达0.82。

**结论:** PragExTra将语用显化确立为可衡量的跨语言现象，为构建文化感知的机器翻译迈出重要一步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PragExTra%3A+A+Multilingual+Corpus+of+Pragmatic+Explicitation+in+Translation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02721，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02721&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Translators often enrich texts with background details that make implicit
cultural meanings explicit for new audiences. This phenomenon, known as
pragmatic explicitation, has been widely discussed in translation theory but
rarely modeled computationally. We introduce PragExTra, the first multilingual
corpus and detection framework for pragmatic explicitation. The corpus covers
eight language pairs from TED-Multi and Europarl and includes additions such as
entity descriptions, measurement conversions, and translator remarks. We
identify candidate explicitation cases through null alignments and refined
using active learning with human annotation. Our results show that entity and
system-level explicitations are most frequent, and that active learning
improves classifier accuracy by 7-8 percentage points, achieving up to 0.88
accuracy and 0.82 F1 across languages. PragExTra establishes pragmatic
explicitation as a measurable, cross-linguistic phenomenon and takes a step
towards building culturally aware machine translation. Keywords: translation,
multilingualism, explicitation

</details>


### [54] [AI Diffusion in Low Resource Language Countries](https://arxiv.org/abs/2511.02752)
*Amit Misra, Syed Waqas Zamir, Wassim Hamidouche, Inbal Becker-Reshef, Juan Lavista Ferres*

**主要类别:** cs.CL

**AI概要:** 研究发现语言资源匮乏国家因AI模型在低资源语言上表现不佳，导致AI用户比例比基准低约20%，表明语言可及性是AI公平扩散的重要障碍


<details>
  <summary>更多</summary>
  
**动机:** 人工智能在全球快速扩散但采用不均衡，前沿大语言模型在低资源语言上表现差，假设这会降低AI效用并减缓低资源语言国家的采用速度

**方法:** 使用加权回归模型从社会经济和人口因素中分离出语言效应

**结果:** 低资源语言国家的AI用户比例比基准低约20%

**结论:** 语言可及性是AI公平扩散的一个重要且独立的障碍

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI+Diffusion+in+Low+Resource+Language+Countries，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02752，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02752&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Artificial intelligence (AI) is diffusing globally at unprecedented speed,
but adoption remains uneven. Frontier Large Language Models (LLMs) are known to
perform poorly on low-resource languages due to data scarcity. We hypothesize
that this performance deficit reduces the utility of AI, thereby slowing
adoption in Low-Resource Language Countries (LRLCs). To test this, we use a
weighted regression model to isolate the language effect from socioeconomic and
demographic factors, finding that LRLCs have a share of AI users that is
approximately 20% lower relative to their baseline. These results indicate that
linguistic accessibility is a significant, independent barrier to equitable AI
diffusion.

</details>


### [55] [Controlling Performance and Budget of a Centralized Multi-agent LLM System with Reinforcement Learning](https://arxiv.org/abs/2511.02755)
*Bowen Jin, TJ Collins, Donghan Yu, Mert Cemri, Shenao Zhang, Mengyu Li, Jay Tang, Tian Qin, Zhiyang Xu, Jiarui Lu, Guoli Yin, Jiawei Han, Zirui Wang*

**主要类别:** cs.CL

**AI概要:** 该论文提出了CoRL框架，通过强化学习实现集中式多LLM系统的成本控制协调，在高预算下超越最佳专家模型，在低预算下保持良好性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有去中心化多LLM系统为每个输入调用多个模型，导致推理成本高昂且不可控，需要设计成本高效且可控的多智能体LLM系统。

**方法:** 提出集中式框架，使用控制器LLM选择性协调专家模型池，通过强化学习框架CoRL优化性能与成本的权衡，支持多预算设置。

**结果:** 在四个基准测试中，CoRL系统在高预算设置下超越最佳专家LLM，在低预算模式下仍保持强劲性能。

**结论:** 集中式协调为可扩展和成本高效的多智能体LLM系统提供了有效解决方案，实现了性能与成本的优化平衡。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Controlling+Performance+and+Budget+of+a+Centralized+Multi-agent+LLM+System+with+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02755，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02755&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) exhibit complementary strengths across domains
and come with varying inference costs, motivating the design of multi-agent LLM
systems where specialized models collaborate efficiently. Existing approaches
predominantly rely on decentralized frameworks, which invoke multiple LLMs for
every input and thus lead to substantial and uncontrolled inference costs. In
this work, we introduce a centralized multi-LLM framework, where a controller
LLM selectively coordinates a pool of expert models in a cost-efficient and
cost-controllable manner. We formulate this coordination problem as
reinforcement learning with dual objectives: maximizing task performance while
minimizing the overall inference cost. In addition, we expect the multi-agent
system to have adapted behavior with different budget conditions during
inference. To this end, we propose CoRL, a reinforcement learning framework
that optimizes the performance cost trade-off in a controllable multi-budget
setting. Experiments on four diverse benchmarks demonstrate that CoRL enables a
single system to surpass the best expert LLM under high-budget settings, while
maintaining strong performance in more economical low-budget modes,
highlighting the effectiveness of centralized coordination for scalable and
cost-efficient multi-agent LLM systems.

</details>


### [56] [Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval](https://arxiv.org/abs/2511.02770)
*Hung-Ting Chen, Xiang Liu, Shauli Ravfogel, Eunsol Choi*

**主要类别:** cs.CL

**AI概要:** AMER模型通过自回归生成多个查询向量来解决传统检索器单向量无法捕捉多模态相关文档分布的问题，在合成数据和真实数据集上分别实现了4倍和4-21%的性能提升。


<details>
  <summary>更多</summary>
  
**动机:** 现有文本检索器只生成单个查询向量，但查询的相关文档分布可能是多模态的（如查询的不同解释），这导致在目标文档嵌入距离较大时检索性能下降。

**方法:** 提出自回归多嵌入检索器（AMER），通过自回归方式生成多个查询向量，所有生成的查询向量都用于从语料库中检索文档。

**结果:** 在合成向量化数据上完美捕捉多目标分布，性能比单嵌入模型提升4倍；在真实多答案检索数据集上相对单嵌入基线分别获得4%和21%的相对增益；在目标文档嵌入相似度较低的子集上增益更大。

**结论:** 多查询向量检索器具有巨大潜力，为未来研究开辟了新方向，特别是在处理多模态相关文档分布的场景中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Single+Embeddings%3A+Capturing+Diverse+Targets+with+Multi-Query+Retrieval，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02770，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02770&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Most text retrievers generate \emph{one} query vector to retrieve relevant
documents. Yet, the conditional distribution of relevant documents for the
query may be multimodal, e.g., representing different interpretations of the
query. We first quantify the limitations of existing retrievers. All retrievers
we evaluate struggle more as the distance between target document embeddings
grows. To address this limitation, we develop a new retriever architecture,
\emph{A}utoregressive \emph{M}ulti-\emph{E}mbedding \emph{R}etriever (AMER).
Our model autoregressively generates multiple query vectors, and all the
predicted query vectors are used to retrieve documents from the corpus. We show
that on the synthetic vectorized data, the proposed method could capture
multiple target distributions perfectly, showing 4x better performance than
single embedding model. We also fine-tune our model on real-world multi-answer
retrieval datasets and evaluate in-domain. AMER presents 4 and 21\% relative
gains over single-embedding baselines on two datasets we evaluate on.
Furthermore, we consistently observe larger gains on the subset of dataset
where the embeddings of the target documents are less similar to each other. We
demonstrate the potential of using a multi-query vector retriever and open up a
new direction for future work.

</details>


### [57] [MemSearcher: Training LLMs to Reason, Search and Manage Memory via End-to-End Reinforcement Learning](https://arxiv.org/abs/2511.02805)
*Qianhao Yuan, Jie Lou, Zichao Li, Jiawei Chen, Yaojie Lu, Hongyu Lin, Le Sun, Debing Zhang, Xianpei Han*

**主要类别:** cs.CL

**AI概要:** MemSearcher提出了一种通过迭代维护紧凑记忆来优化多轮搜索代理的方法，在保持信息完整性的同时显著降低计算和内存成本，并在多个基准测试中取得了显著性能提升。


<details>
  <summary>更多</summary>
  
**动机:** 传统搜索代理要么使用完整历史记录导致计算成本高昂，要么仅使用当前轮次导致信息丢失，这种权衡限制了搜索代理的可扩展性。

**方法:** 提出MemSearcher工作流，通过迭代维护紧凑记忆，将当前轮次问题与记忆融合生成推理轨迹、执行搜索操作并更新记忆。采用多上下文GRPO强化学习框架联合优化推理、搜索策略和记忆管理。

**结果:** 在七个公共基准测试中相比强基线显著提升：Qwen2.5-3B-Instruct平均提升11%，Qwen2.5-7B-Instruct平均提升12%。3B版本的MemSearcher甚至超越7B基线模型。

**结论:** 在信息完整性和效率之间取得平衡既能提高准确性又能降低计算开销，MemSearcher为解决多轮搜索代理的可扩展性问题提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MemSearcher%3A+Training+LLMs+to+Reason%2C+Search+and+Manage+Memory+via+End-to-End+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02805，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02805&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Typical search agents concatenate the entire interaction history into the LLM
context, preserving information integrity but producing long, noisy contexts,
resulting in high computation and memory costs. In contrast, using only the
current turn avoids this overhead but discards essential information. This
trade-off limits the scalability of search agents. To address this challenge,
we propose MemSearcher, an agent workflow that iteratively maintains a compact
memory and combines the current turn with it. At each turn, MemSearcher fuses
the user's question with the memory to generate reasoning traces, perform
search actions, and update memory to retain only information essential for
solving the task. This design stabilizes context length across multi-turn
interactions, improving efficiency without sacrificing accuracy. To optimize
this workflow, we introduce multi-context GRPO, an end-to-end RL framework that
jointly optimize reasoning, search strategies, and memory management of
MemSearcher Agents. Specifically, multi-context GRPO samples groups of
trajectories under different contexts and propagates trajectory-level
advantages across all conversations within them. Trained on the same dataset as
Search-R1, MemSearcher achieves significant improvements over strong baselines
on seven public benchmarks: +11% on Qwen2.5-3B-Instruct and +12% on
Qwen2.5-7B-Instruct relative average gains. Notably, the 3B-based MemSearcher
even outperforms 7B-based baselines, demonstrating that striking a balance
between information integrity and efficiency yields both higher accuracy and
lower computational overhead. The code and models will be publicly available at
https://github.com/icip-cas/MemSearcher

</details>


### [58] [Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities](https://arxiv.org/abs/2511.02817)
*Amanda Bertsch, Adithya Pratapa, Teruko Mitamura, Graham Neubig, Matthew R. Gormley*

**主要类别:** cs.CL

**AI概要:** Oolong是一个新的长上下文推理基准测试，包含合成和真实世界任务，要求模型对大量文本进行原子级分析并聚合结果，前沿模型在128K上下文长度下准确率不足50%。


<details>
  <summary>更多</summary>
  
**动机:** 现有长上下文评估主要关注检索任务，允许模型忽略大部分上下文作为噪声，无法全面评估模型的长上下文推理能力。

**方法:** 开发Oolong基准测试，包含两个任务集：Oolong-synth（自然合成任务）和Oolong-real（真实对话数据推理），要求模型进行文本块原子分析、分类、计数以及时空和用户关系推理。

**结果:** 即使是GPT-5、Claude-Sonnet-4和Gemini-2.5-Pro等前沿模型，在128K上下文长度下对两个任务集的准确率都低于50%。

**结论:** 当前模型在需要深度分析和聚合推理的长上下文任务上表现不佳，Oolong基准的发布将促进能够处理大量文本推理的模型发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Oolong%3A+Evaluating+Long+Context+Reasoning+and+Aggregation+Capabilities，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2511.02817，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02817&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** As model context lengths continue to grow, concerns about whether models
effectively use the full context length have persisted. While several carefully
designed long-context evaluations have recently been released, these
evaluations tend to rely on retrieval from one or more sections of the context,
which allows nearly all of the context tokens to be disregarded as noise. This
represents only one type of task that might be performed with long context. We
introduce Oolong, a benchmark of long-context reasoning tasks that require
analyzing individual chunks of text on an atomic level, and then aggregating
these analyses to answer distributional questions. Oolong is separated into two
task sets: Oolong-synth, a set of naturalistic synthetic tasks, where we can
easily ablate components of the reasoning problem; and Oolong-real, a
downstream setting which requires reasoning over real-world conversational
data. Oolong requires models to reason over large quantities of examples, to
perform both classification and counting in-context, and to reason over
temporal and user relations. Even frontier models struggle on Oolong, with
GPT-5, Claude-Sonnet-4, and Gemini-2.5-Pro all achieving less than 50% accuracy
on both splits at 128K. We release the data and evaluation harness for Oolong
to enable further development of models that can reason over large quantities
of text.

</details>
