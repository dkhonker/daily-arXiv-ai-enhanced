<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 126]
- [cs.AI](#cs.AI) [总数: 39]
- [stat.ML](#stat.ML) [总数: 10]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment](https://arxiv.org/abs/2505.10597)
*Jiazheng Zhang, Wenqing Jing, Zizhuo Zhang, Zhiheng Xi, Shihan Dou, Rongxiang Weng, Jiahuan Li, Jingang Wang, Mingxu Chai, Shibo Hong, Tao Gui, Qi Zhang*

**主要类别:** cs.LG

**概要:** This paper analyzes the characteristics of preference pairs in reward models and identifies noisy preferences that affect the generalization of reward models. It proposes an online Collaborative Reward Modeling (CRM) framework to improve the robustness of preference learning.


<details>
  <summary>Details</summary>
  
**动机:** To address the issue of reward misgeneralization caused by noisy preferences in human feedback.

**方法:** Systematically analyzing the characteristics of preference pairs and proposing an online Collaborative Reward Modeling (CRM) framework.

**结果:** The proposed CRM framework significantly enhances RM generalization, with up to 9.94 points improvement on RewardBench under an extreme 40% noise.

**结论:** CRM offers a robust and versatile alignment strategy that can be extended to implicit-reward alignment methods.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Two+Minds+Better+Than+One%3A+Collaborative+Reward+Modeling+for+LLM+Alignment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10597，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10597&send_immediately=true&force_search=false)

**原文摘要:** Reward models (RMs) play a pivotal role in aligning large language models
(LLMs) with human values. However, noisy preferences in human feedback can lead
to reward misgeneralization - a phenomenon where reward models learn spurious
correlations or overfit to noisy preferences, which poses important challenges
to the generalization of RMs. This paper systematically analyzes the
characteristics of preference pairs and aims to identify how noisy preferences
differ from human-aligned preferences in reward modeling. Our analysis reveals
that noisy preferences are difficult for RMs to fit, as they cause sharp
training fluctuations and irregular gradient updates. These distinctive
dynamics suggest the feasibility of identifying and excluding such noisy
preferences. Empirical studies demonstrate that policy LLM optimized with a
reward model trained on the full preference dataset, which includes substantial
noise, performs worse than the one trained on a subset of exclusively high
quality preferences. To address this challenge, we propose an online
Collaborative Reward Modeling (CRM) framework to achieve robust preference
learning through peer review and curriculum learning. In particular, CRM
maintains two RMs that collaboratively filter potential noisy preferences by
peer-reviewing each other's data selections. Curriculum learning synchronizes
the capabilities of two models, mitigating excessive disparities to promote the
utility of peer review. Extensive experiments demonstrate that CRM
significantly enhances RM generalization, with up to 9.94 points improvement on
RewardBench under an extreme 40\% noise. Moreover, CRM can seamlessly extend to
implicit-reward alignment methods, offering a robust and versatile alignment
strategy.

</details>


### [2] [UDDETTS: Unifying Discrete and Dimensional Emotions for Controllable Emotional Text-to-Speech](https://arxiv.org/abs/2505.10599)
*Jiaxuan Liu, Zhenhua Ling*

**主要类别:** cs.LG

**概要:** 提出了一种新的神经编解码语言模型UDDETTS，用于可控情感文本到语音合成，该模型结合离散和维度情感，通过引入可解释的唤醒-支配-效价（ADV）空间实现情感控制，并设计半监督训练策略利用不同情感标注的多样化语音数据集进行训练。实验表明UDDETTS在ADV空间的三个维度上实现了线性情感控制，表现出卓越的情感语音合成能力。


<details>
  <summary>Details</summary>
  
**动机:** 传统方法依赖预定义离散情感标签控制情感类别和强度，无法捕捉人类情感感知和表达的复杂性和连续性；缺乏大规模情感语音数据集导致合成模型过拟合并阻碍有效情感控制。

**方法:** 提出UDDETTS模型，引入ADV空间描述维度情感，支持由离散情感标签或非线性量化ADV值驱动的情感控制，设计半监督训练策略利用多样化语音数据集。

**结果:** UDDETTS在ADV空间的三个维度上实现了线性情感控制，具有卓越的端到端情感语音合成能力。

**结论:** UDDETTS解决了可控情感TTS中的许多挑战，通过结合离散和维度情感，以及采用半监督训练策略，显著提升了情感控制的效果和语音合成质量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是UDDETTS%3A+Unifying+Discrete+and+Dimensional+Emotions+for+Controllable+Emotional+Text-to-Speech，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10599，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10599&send_immediately=true&force_search=false)

**原文摘要:** Recent neural codec language models have made great progress in the field of
text-to-speech (TTS), but controllable emotional TTS still faces many
challenges. Traditional methods rely on predefined discrete emotion labels to
control emotion categories and intensities, which can't capture the complexity
and continuity of human emotional perception and expression. The lack of
large-scale emotional speech datasets with balanced emotion distributions and
fine-grained emotion annotations often causes overfitting in synthesis models
and impedes effective emotion control. To address these issues, we propose
UDDETTS, a neural codec language model unifying discrete and dimensional
emotions for controllable emotional TTS. This model introduces the
interpretable Arousal-Dominance-Valence (ADV) space for dimensional emotion
description and supports emotion control driven by either discrete emotion
labels or nonlinearly quantified ADV values. Furthermore, a semi-supervised
training strategy is designed to comprehensively utilize diverse speech
datasets with different types of emotion annotations to train the UDDETTS.
Experiments show that UDDETTS achieves linear emotion control along the three
dimensions of ADV space, and exhibits superior end-to-end emotional speech
synthesis capabilities.

</details>


### [3] [Enhancing IoT Cyber Attack Detection in the Presence of Highly Imbalanced Data](https://arxiv.org/abs/2505.10600)
*Md. Ehsanul Haque, Md. Saymon Hosen Polash, Md Al-Imran Sanjida Simla, Md Alomgir Hossain, Sarwar Jahan*

**主要类别:** cs.LG

**概要:** This study addresses the challenge of detecting rare cyber-attacks in IoT networks with highly imbalanced datasets using hybrid sampling techniques. It evaluates various machine learning models and finds that the Random Forest model performs best.


<details>
  <summary>Details</summary>
  
**动机:** To develop effective Intrusion Detection Systems (IDS) for IoT networks with imbalanced datasets due to rapidly growing cyber risks.

**方法:** Hybrid sampling techniques were applied to improve data imbalance detection accuracy, followed by evaluating multiple machine learning models including Random Forest, Soft Voting, SVC, KNN, MLP, and Logistic Regression.

**结果:** Random Forest model achieved the best performance with a Kappa score of 0.9903, test accuracy of 0.9961, and AUC of 0.9994. Soft Voting model also performed strongly.

**结论:** Hybrid sampling combined with robust model and feature selection significantly improves IoT security against cyber-attacks, particularly in imbalanced data environments.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+IoT+Cyber+Attack+Detection+in+the+Presence+of+Highly+Imbalanced+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10600，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10600&send_immediately=true&force_search=false)

**原文摘要:** Due to the rapid growth in the number of Internet of Things (IoT) networks,
the cyber risk has increased exponentially, and therefore, we have to develop
effective IDS that can work well with highly imbalanced datasets. A high rate
of missed threats can be the result, as traditional machine learning models
tend to struggle in identifying attacks when normal data volume is much higher
than the volume of attacks. For example, the dataset used in this study reveals
a strong class imbalance with 94,659 instances of the majority class and only
28 instances of the minority class, making it quite challenging to determine
rare attacks accurately. The challenges presented in this research are
addressed by hybrid sampling techniques designed to improve data imbalance
detection accuracy in IoT domains. After applying these techniques, we evaluate
the performance of several machine learning models such as Random Forest, Soft
Voting, Support Vector Classifier (SVC), K-Nearest Neighbors (KNN), Multi-Layer
Perceptron (MLP), and Logistic Regression with respect to the classification of
cyber-attacks. The obtained results indicate that the Random Forest model
achieved the best performance with a Kappa score of 0.9903, test accuracy of
0.9961, and AUC of 0.9994. Strong performance is also shown by the Soft Voting
model, with an accuracy of 0.9952 and AUC of 0.9997, indicating the benefits of
combining model predictions. Overall, this work demonstrates the value of
hybrid sampling combined with robust model and feature selection for
significantly improving IoT security against cyber-attacks, especially in
highly imbalanced data environments.

</details>


### [4] [Continuity and Isolation Lead to Doubts or Dilemmas in Large Language Models](https://arxiv.org/abs/2505.10606)
*Hector Pasten, Felipe Urrutia, Hector Jimenez, Cristian B. Calderon, Cristóbal Rojas, Alexander Kozachinskiy*

**主要类别:** cs.LG

**概要:** This paper explores two phenomena, isolation and continuity, in Transformers, which hinder their ability to learn simple pattern sequences.


<details>
  <summary>Details</summary>
  
**动机:** To understand how Transformers work and process information for theoretical and empirical advancement.

**方法:** Mathematical proof and rigorous experiments.

**结果:** Isolation and continuity phenomena exist in all Transformers using compact positional encoding, causing practical limitations.

**结论:** The study reveals theoretical limitations of Transformers, providing insights into their learning capabilities.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Continuity+and+Isolation+Lead+to+Doubts+or+Dilemmas+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10606，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10606&send_immediately=true&force_search=false)

**原文摘要:** Understanding how Transformers work and how they process information is key
to the theoretical and empirical advancement of these machines. In this work,
we demonstrate the existence of two phenomena in Transformers, namely isolation
and continuity. Both of these phenomena hinder Transformers to learn even
simple pattern sequences. Isolation expresses that any learnable sequence must
be isolated from another learnable sequence, and hence some sequences cannot be
learned by a single Transformer at the same time. Continuity entails that an
attractor basin forms around a learned sequence, such that any sequence falling
in that basin will collapse towards the learned sequence. Here, we
mathematically prove these phenomena emerge in all Transformers that use
compact positional encoding, and design rigorous experiments, demonstrating
that the theoretical limitations we shed light on occur on the practical scale.

</details>


### [5] [MONAQ: Multi-Objective Neural Architecture Querying for Time-Series Analysis on Resource-Constrained Devices](https://arxiv.org/abs/2505.10607)
*Patara Trirat, Jae-Gil Lee*

**主要类别:** cs.LG

**概要:** 提出了一种名为MONAQ的新框架，通过将硬件感知神经架构搜索（NAS）重新表述为多目标神经架构查询任务，利用大型语言模型（LLM）的推理能力，实现了适用于边缘部署的一般时间序列分析。


<details>
  <summary>Details</summary>
  
**动机:** 随着智能手机和物联网设备的普及，需要在资源受限的硬件上进行高效的时间序列分析，特别是在人类活动识别和空气质量预测等传感应用中。现有的硬件感知NAS方法专注于特定平台，缺乏针对边缘部署的一般时间序列分析。

**方法:** MONAQ框架通过多模态查询生成处理多模态时间序列输入和硬件约束，并采用基于LLM代理的多目标搜索来实现通过代码生成的部署就绪模型。集成数值数据、时间序列图像和文本描述以提高LLM对时间序列数据的理解。

**结果:** MONAQ发现的模型在15个数据集上的实验表明，其性能优于手工设计的模型和NAS基线，同时效率更高。

**结论:** MONAQ展示了利用LLM进行多目标神经架构查询在一般时间序列分析中的潜力，特别是在边缘计算环境下的应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MONAQ%3A+Multi-Objective+Neural+Architecture+Querying+for+Time-Series+Analysis+on+Resource-Constrained+Devices，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10607，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10607&send_immediately=true&force_search=false)

**原文摘要:** The growing use of smartphones and IoT devices necessitates efficient
time-series analysis on resource-constrained hardware, which is critical for
sensing applications such as human activity recognition and air quality
prediction. Recent efforts in hardware-aware neural architecture search (NAS)
automate architecture discovery for specific platforms; however, none focus on
general time-series analysis with edge deployment. Leveraging the
problem-solving and reasoning capabilities of large language models (LLM), we
propose MONAQ, a novel framework that reformulates NAS into Multi-Objective
Neural Architecture Querying tasks. MONAQ is equipped with multimodal query
generation for processing multimodal time-series inputs and hardware
constraints, alongside an LLM agent-based multi-objective search to achieve
deployment-ready models via code generation. By integrating numerical data,
time-series images, and textual descriptions, MONAQ improves an LLM's
understanding of time-series data. Experiments on fifteen datasets demonstrate
that MONAQ-discovered models outperform both handcrafted models and NAS
baselines while being more efficient.

</details>


### [6] [How many measurements are enough? Bayesian recovery in inverse problems with general distributions](https://arxiv.org/abs/2505.10630)
*Ben Adcock, Nick Huang*

**主要类别:** cs.LG

**概要:** 研究了具有通用先验、前向算子和噪声分布的贝叶斯恢复的样本复杂度。主要结果是非渐近界，表明样本复杂度取决于先验的近似覆盖数及其前向算子和噪声分布的集中性。特别地，对于基于生成模型的先验，证明了样本复杂度与潜在维度呈对数线性关系，并且推广了正交矩阵随机采样的确定性恢复结果，表明相干性在贝叶斯恢复中起根本作用。


<details>
  <summary>Details</summary>
  
**动机:** 研究贝叶斯逆问题的样本复杂度，特别是对于一般先验和噪声分布的情况。

**方法:** 通过后验采样和非渐近边界分析，考虑了先验的近似覆盖数和前向算子及噪声分布的集中性。

**结果:** 证明了样本复杂度依赖于先验的近似覆盖数和前向算子及噪声分布的集中性；对于生成先验，样本复杂度与潜在维度呈对数线性关系；推广了正交矩阵随机采样的确定性恢复结果。

**结论:** 统一并扩展了之前的工作，为任意分布下解决贝叶斯逆问题提供了严格的样本复杂度保证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+many+measurements+are+enough%3F+Bayesian+recovery+in+inverse+problems+with+general+distributions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10630，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10630&send_immediately=true&force_search=false)

**原文摘要:** We study the sample complexity of Bayesian recovery for solving inverse
problems with general prior, forward operator and noise distributions. We
consider posterior sampling according to an approximate prior $\mathcal{P}$,
and establish sufficient conditions for stable and accurate recovery with high
probability. Our main result is a non-asymptotic bound that shows that the
sample complexity depends on (i) the intrinsic complexity of $\mathcal{P}$,
quantified by its so-called approximate covering number, and (ii) concentration
bounds for the forward operator and noise distributions. As a key application,
we specialize to generative priors, where $\mathcal{P}$ is the pushforward of a
latent distribution via a Deep Neural Network (DNN). We show that the sample
complexity scales log-linearly with the latent dimension $k$, thus establishing
the efficacy of DNN-based priors. Generalizing existing results on
deterministic (i.e., non-Bayesian) recovery for the important problem of random
sampling with an orthogonal matrix $U$, we show how the sample complexity is
determined by the coherence of $U$ with respect to the support of
$\mathcal{P}$. Hence, we establish that coherence plays a fundamental role in
Bayesian recovery as well. Overall, our framework unifies and extends prior
work, providing rigorous guarantees for the sample complexity of solving
Bayesian inverse problems with arbitrary distributions.

</details>


### [7] [FRET: Feature Redundancy Elimination for Test Time Adaptation](https://arxiv.org/abs/2505.10641)
*Linjing You, Jiabao Lu, Xiayuan Huang, Xiangli Nie*

**主要类别:** cs.LG

**概要:** 提出了一种新的方法FRET来解决测试时间适应中的特征冗余问题，其中G-FRET在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
  
**动机:** 现有测试时间适应(TTA)方法往往忽视了特征冗余的问题，而这会阻碍模型对新数据的适应能力。随着领域偏移的加剧，嵌入中的特征冗余往往会增加，特别是在隐私敏感的应用场景下。

**方法:** 提出了Feature Redundancy Elimination for Test-time Adaptation (FRET)，包括直接最小化特征冗余分数的S-FRET和结合图卷积网络与对比学习的G-FRET。

**结果:** S-FRET虽然简单且有效，但在标签偏移的情况下表现不佳；G-FRET通过减少特征冗余并增强表示层和预测层的特征区分度，在多个模型架构、任务和数据集上的表现达到了最先进的水平。

**结论:** 提出了一种新的方法FRET用于处理测试时间适应中的特征冗余问题，并设计了S-FRET和G-FRET两种实现方式。实验表明，S-FRET在一定程度上有效，而G-FRET在多个模型架构、任务和数据集上的表现达到了最先进的水平。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FRET%3A+Feature+Redundancy+Elimination+for+Test+Time+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10641，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10641&send_immediately=true&force_search=false)

**原文摘要:** Test-Time Adaptation (TTA) aims to enhance the generalization of deep
learning models when faced with test data that exhibits distribution shifts
from the training data. In this context, only a pre-trained model and unlabeled
test data are available, making it particularly relevant for privacy-sensitive
applications. In practice, we observe that feature redundancy in embeddings
tends to increase as domain shifts intensify in TTA. However, existing TTA
methods often overlook this redundancy, which can hinder the model's
adaptability to new data. To address this issue, we introduce Feature
Redundancy Elimination for Test-time Adaptation (FRET), a novel perspective for
TTA. A straightforward approach (S-FRET) is to directly minimize the feature
redundancy score as an optimization objective to improve adaptation. Despite
its simplicity and effectiveness, S-FRET struggles with label shifts, limiting
its robustness in real-world scenarios. To mitigate this limitation, we further
propose Graph-based FRET (G-FRET), which integrates a Graph Convolutional
Network (GCN) with contrastive learning. This design not only reduces feature
redundancy but also enhances feature discriminability in both the
representation and prediction layers. Extensive experiments across multiple
model architectures, tasks, and datasets demonstrate the effectiveness of
S-FRET and show that G-FRET achieves state-of-the-art performance. Further
analysis reveals that G-FRET enables the model to extract non-redundant and
highly discriminative features during inference, thereby facilitating more
robust test-time adaptation.

</details>


### [8] [Asymptotically-Optimal Gaussian Bandits with Side Observations](https://arxiv.org/abs/2505.10698)
*Alexia Atsidakou, Orestis Papadigenopoulos, Constantine Caramanis, Sujay Sanghavi, Sanjay Shakkottai*

**主要类别:** cs.LG

**概要:** 研究具有广义侧信息的高斯多臂老虎机问题，提出了一种基于LP的渐近实例相关下界，并设计了首个渐近最优算法。


<details>
  <summary>Details</summary>
  
**动机:** 扩展标准多臂老虎机模型到具有任意已知侧信息矩阵的一般情况。

**方法:** 构建基于线性规划(LP)的后悔下界，并提出新的算法解决该一般设置下的老虎机问题。

**结果:** 提出的算法在渐近意义上是最优的，适用于多种反馈结构如标准多臂老虎机、完全反馈和图结构反馈等特殊情况。

**结论:** 本研究首次实现了对具有通用侧信息的高斯老虎机问题的渐近最优解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Asymptotically-Optimal+Gaussian+Bandits+with+Side+Observations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10698，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10698&send_immediately=true&force_search=false)

**原文摘要:** We study the problem of Gaussian bandits with general side information, as
first introduced by Wu, Szepesvari, and Gyorgy. In this setting, the play of an
arm reveals information about other arms, according to an arbitrary a priori
known side information matrix: each element of this matrix encodes the fidelity
of the information that the ``row'' arm reveals about the ``column'' arm. In
the case of Gaussian noise, this model subsumes standard bandits,
full-feedback, and graph-structured feedback as special cases. In this work, we
first construct an LP-based asymptotic instance-dependent lower bound on the
regret. The LP optimizes the cost (regret) required to reliably estimate the
suboptimality gap of each arm. This LP lower bound motivates our main
contribution: the first known asymptotically optimal algorithm for this general
setting.

</details>


### [9] [Accelerating Visual-Policy Learning through Parallel Differentiable Simulation](https://arxiv.org/abs/2505.10646)
*Haoxiang You, Yilang Liu, Ian Abraham*

**主要类别:** cs.LG

**概要:** 提出了一种高效的视觉策略学习算法，该算法利用可微仿真和一阶解析策略梯度。实验表明，该方法显著减少了训练时间，并在复杂任务上表现出色。


<details>
  <summary>Details</summary>
  
**动机:** 提高视觉策略学习的计算效率并简化与现有不同iable simulation生态系统的集成。

**方法:** 将渲染过程与计算图解耦，使用可微分仿真和一阶解析策略梯度。

**结果:** 在标准视觉控制基准上评估，该方法显著减少了训练时间，并在复杂任务上表现优异。

**结论:** 所提出的算法不仅提高了计算效率，还增强了策略优化的稳定性和平滑性，在人形机器人运动等复杂任务上取得了显著成果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Accelerating+Visual-Policy+Learning+through+Parallel+Differentiable+Simulation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10646，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10646&send_immediately=true&force_search=false)

**原文摘要:** In this work, we propose a computationally efficient algorithm for visual
policy learning that leverages differentiable simulation and first-order
analytical policy gradients. Our approach decouple the rendering process from
the computation graph, enabling seamless integration with existing
differentiable simulation ecosystems without the need for specialized
differentiable rendering software. This decoupling not only reduces
computational and memory overhead but also effectively attenuates the policy
gradient norm, leading to more stable and smoother optimization. We evaluate
our method on standard visual control benchmarks using modern GPU-accelerated
simulation. Experiments show that our approach significantly reduces wall-clock
training time and consistently outperforms all baseline methods in terms of
final returns. Notably, on complex tasks such as humanoid locomotion, our
method achieves a $4\times$ improvement in final return, and successfully
learns a humanoid running policy within 4 hours on a single GPU.

</details>


### [10] [On DeepSeekMoE: Statistical Benefits of Shared Experts and Normalized Sigmoid Gating](https://arxiv.org/abs/2505.10860)
*Huy Nguyen, Thong T. Doan, Quang Pham, Nghi D. Q. Bui, Nhat Ho, Alessandro Rinaldo*

**主要类别:** cs.LG

**概要:** 研究了DeepSeekMoE的共享专家策略和归一化Sigmoid门机制，提供了理论和实证分析支持其有效性。


<details>
  <summary>Details</summary>
  
**动机:** 尽管DeepSeekMoE在DeepSeek系列模型的成功中起着关键作用，但共享专家策略的理论价值仅被少量研究探讨，而归一化Sigmoid门机制尚未被研究。

**方法:** 从统计学角度对DeepSeekMoE的两个特性进行了全面理论研究，包括收敛性分析以及在合成数据和真实数据集上的实验验证。

**结果:** 通过理论分析揭示了共享专家策略和归一化Sigmoid门机制在样本效率上的优势，并通过实验验证了这些理论发现。

**结论:** 证明了DeepSeekMoE的共享专家策略和归一化Sigmoid门机制在统计学视角下的价值，并提供了关于专家结构和门结构设计的有用见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+DeepSeekMoE%3A+Statistical+Benefits+of+Shared+Experts+and+Normalized+Sigmoid+Gating，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10860，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10860&send_immediately=true&force_search=false)

**原文摘要:** Mixture of experts (MoE) methods are a key component in most large language
model architectures, including the recent series of DeepSeek models. Compared
to other MoE implementations, DeepSeekMoE stands out because of two unique
features: the deployment of a shared expert strategy and of the normalized
sigmoid gating mechanism. Despite the prominent role of DeepSeekMoE in the
success of the DeepSeek series of models, there have been only a few attempts
to justify theoretically the value of the shared expert strategy, while its
normalized sigmoid gating has remained unexplored. To bridge this gap, we
undertake a comprehensive theoretical study of these two features of
DeepSeekMoE from a statistical perspective. We perform a convergence analysis
of the expert estimation task to highlight the gains in sample efficiency for
both the shared expert strategy and the normalized sigmoid gating, offering
useful insights into the design of expert and gating structures. To verify
empirically our theoretical findings, we carry out several experiments on both
synthetic data and real-world datasets for (vision) language modeling tasks.
Finally, we conduct an extensive empirical analysis of the router behaviors,
ranging from router saturation, router change rate, to expert utilization.

</details>


### [11] [Seasonal Forecasting of Pan-Arctic Sea Ice with State Space Model](https://arxiv.org/abs/2505.10665)
*Wei Wang, Weidong Yang, Lei Wang, Guihua Wang, Ruibo Lei*

**主要类别:** cs.LG

**概要:** 提出了一种名为IceMamba的新深度学习架构，用于北极海冰浓度的季节性预测。实验表明，IceMamba在平均均方根误差和异常相关系数方面优于所有测试模型。


<details>
  <summary>Details</summary>
  
**动机:** 北极海冰快速减少带来的风险促使需要更精确的季节性海冰预报。

**方法:** IceMamba是一种结合了复杂注意机制的状态空间模型。

**结果:** IceMamba在平均RMSE和异常相关系数方面表现最佳，在综合海冰边缘误差方面排名第二。

**结论:** IceMamba显著提升了对海冰变异性预见和缓解的能力，为气候适应策略提供了重要见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Seasonal+Forecasting+of+Pan-Arctic+Sea+Ice+with+State+Space+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10665，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10665&send_immediately=true&force_search=false)

**原文摘要:** The rapid decline of Arctic sea ice resulting from anthropogenic climate
change poses significant risks to indigenous communities, ecosystems, and the
global climate system. This situation emphasizes the immediate necessity for
precise seasonal sea ice forecasts. While dynamical models perform well for
short-term forecasts, they encounter limitations in long-term forecasts and are
computationally intensive. Deep learning models, while more computationally
efficient, often have difficulty managing seasonal variations and uncertainties
when dealing with complex sea ice dynamics. In this research, we introduce
IceMamba, a deep learning architecture that integrates sophisticated attention
mechanisms within the state space model. Through comparative analysis of 25
renowned forecast models, including dynamical, statistical, and deep learning
approaches, our experimental results indicate that IceMamba delivers excellent
seasonal forecasting capabilities for Pan-Arctic sea ice concentration.
Specifically, IceMamba outperforms all tested models regarding average RMSE and
anomaly correlation coefficient (ACC) and ranks second in Integrated Ice Edge
Error (IIEE). This innovative approach enhances our ability to foresee and
alleviate the effects of sea ice variability, offering essential insights for
strategies aimed at climate adaptation.

</details>


### [12] [Hashing for Structure-based Anomaly Detection](https://arxiv.org/abs/2505.10873)
*Filippo Leveni, Luca Magri, Cesare Alippi, Giacomo Boracchi*

**主要类别:** cs.LG

**概要:** 本文提出了一种基于局部敏感哈希的异常检测方法，该方法在降低计算成本的同时实现了最先进的性能。


<details>
  <summary>Details</summary>
  
**动机:** 识别一组样本中不符合由低维流形表示的结构模式的样本的问题。

**方法:** 利用局部敏感哈希来避免在高维空间中显式计算距离，从而提高异常检测效率。

**结果:** 提出的技术在偏好空间中工作并实现了最先进的性能。

**结论:** 提出的方法在降低计算成本的同时实现了最先进的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hashing+for+Structure-based+Anomaly+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10873，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10873&send_immediately=true&force_search=false)

**原文摘要:** We focus on the problem of identifying samples in a set that do not conform
to structured patterns represented by low-dimensional manifolds. An effective
way to solve this problem is to embed data in a high dimensional space, called
Preference Space, where anomalies can be identified as the most isolated
points. In this work, we employ Locality Sensitive Hashing to avoid explicit
computation of distances in high dimensions and thus improve Anomaly Detection
efficiency. Specifically, we present an isolation-based anomaly detection
technique designed to work in the Preference Space which achieves
state-of-the-art performance at a lower computational cost. Code is publicly
available at
https://github.com/ineveLoppiliF/Hashing-for-Structure-based-Anomaly-Detection.

</details>


### [13] [A Conformal Predictive Measure for Assessing Catastrophic Forgetting](https://arxiv.org/abs/2505.10677)
*Ioannis Pitsiorlas, Nour Jamoussi, Marios Kountouris*

**主要类别:** cs.LG

**概要:** 提出了一种新的方法来评估连续学习中的灾难性遗忘（CF），通过引入基于校准预测（CP）的新指标CPCF，提供了动态监测和衡量CF的实用解决方案。实验验证了CPCF在四个基准数据集上的可靠性和有效性。


<details>
  <summary>Details</summary>
  
**动机:** 现有的CF评估方法存在不足，需要一种更有效、更可靠的度量标准来评估连续学习中的CF问题。

**方法:** 提出基于校准预测（CP）的CPCF指标，利用适应性CP监控模型对先前任务的信心来估计遗忘。

**结果:** CPCF与先前任务的准确性有很强的相关性，在四个基准数据集上的实验验证了其可靠性。

**结论:** CPCF是一种强大且有效的工具，可用于动态学习环境下的CF评估和理解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Conformal+Predictive+Measure+for+Assessing+Catastrophic+Forgetting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10677，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10677&send_immediately=true&force_search=false)

**原文摘要:** This work introduces a novel methodology for assessing catastrophic
forgetting (CF) in continual learning. We propose a new conformal prediction
(CP)-based metric, termed the Conformal Prediction Confidence Factor (CPCF), to
quantify and evaluate CF effectively. Our framework leverages adaptive CP to
estimate forgetting by monitoring the model's confidence on previously learned
tasks. This approach provides a dynamic and practical solution for monitoring
and measuring CF of previous tasks as new ones are introduced, offering greater
suitability for real-world applications. Experimental results on four benchmark
datasets demonstrate a strong correlation between CPCF and the accuracy of
previous tasks, validating the reliability and interpretability of the proposed
metric. Our results highlight the potential of CPCF as a robust and effective
tool for assessing and understanding CF in dynamic learning environments.

</details>


### [14] [Preference Isolation Forest for Structure-based Anomaly Detection](https://arxiv.org/abs/2505.10876)
*Filippo Leveni, Luca Magri, Cesare Alippi, Giacomo Boracchi*

**主要类别:** cs.LG

**概要:** 提出了一种结合自适应隔离方法和偏好嵌入灵活性的异常检测框架Preference Isolation Forest (PIF)，并设计了三种隔离方法来识别异常。


<details>
  <summary>Details</summary>
  
**动机:** 检测不遵循低维流形结构模式的异常样本。

**方法:** 提出Preference Isolation Forest (PIF) 框架，通过拟合低维流形将数据嵌入高维偏好空间，并采用三种隔离方法识别异常。

**结果:** 未具体提及实验结果，但提出了三种隔离方法用于异常检测。

**结论:** 总结了所提出的框架的优势及其在异常检测中的应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Preference+Isolation+Forest+for+Structure-based+Anomaly+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10876，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10876&send_immediately=true&force_search=false)

**原文摘要:** We address the problem of detecting anomalies as samples that do not conform
to structured patterns represented by low-dimensional manifolds. To this end,
we conceive a general anomaly detection framework called Preference Isolation
Forest (PIF), that combines the benefits of adaptive isolation-based methods
with the flexibility of preference embedding. The key intuition is to embed the
data into a high-dimensional preference space by fitting low-dimensional
manifolds, and to identify anomalies as isolated points. We propose three
isolation approaches to identify anomalies: $i$) Voronoi-iForest, the most
general solution, $ii$) RuzHash-iForest, that avoids explicit computation of
distances via Local Sensitive Hashing, and $iii$) Sliding-PIF, that leverages a
locality prior to improve efficiency and effectiveness.

</details>


### [15] [A probabilistic framework for dynamic quantization](https://arxiv.org/abs/2505.10689)
*Gabriele Santini, Francesco Paissan, Elisabetta Farella*

**主要类别:** cs.LG

**概要:** 提出了一种用于神经网络动态量化的新框架，该框架能够高效地根据输入自适应调整量化参数，并在多个计算机视觉任务和模型上验证了其有效性。


<details>
  <summary>Details</summary>
  
**动机:** 提高神经网络量化效率并减少计算开销。

**方法:** 通过轻量级代理对网络预激活应用概率模型，实现输入自适应的量化参数调整。

**结果:** 在常见计算机视觉任务和模型上，性能损失可忽略不计。

**结论:** 所提方法在性能与计算开销之间取得了最佳平衡。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+probabilistic+framework+for+dynamic+quantization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10689，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10689&send_immediately=true&force_search=false)

**原文摘要:** We propose a probabilistic framework for dynamic quantization of neural
networks that allows for a computationally efficient input-adaptive rescaling
of the quantization parameters. Our framework applies a probabilistic model to
the network's pre-activations through a lightweight surrogate, enabling the
adaptive adjustment of the quantization parameters on a per-input basis without
significant memory overhead. We validate our approach on a set of popular
computer vision tasks and models, observing only a negligible loss in
performance. Our method strikes the best performance and computational overhead
tradeoff compared to standard quantization strategies.

</details>


### [16] [Approximation and Generalization Abilities of Score-based Neural Network Generative Models for Sub-Gaussian Distributions](https://arxiv.org/abs/2505.10880)
*Guoji Fu, Wee Sun Lee*

**主要类别:** cs.LG

**概要:** 本研究探讨了基于分数的神经网络生成模型(SGMs)在估计未知分布时的逼近和泛化能力。


<details>
  <summary>Details</summary>
  
**动机:** 研究旨在在更温和的假设下建立SGMs的收敛率。

**方法:** 使用深度ReLU神经网络来近似分数，并分析其均方误差和得分估计的最优率。

**结果:** 证明了在特定条件下，SGMs可以达到接近最优的收敛率，并且无需许多关键假设如得分函数的Lipschitz连续性或目标密度的严格正下界。

**结论:** 该框架是通用的，可应用于更广泛的场景，为SGMs的理论分析提供了新的视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Approximation+and+Generalization+Abilities+of+Score-based+Neural+Network+Generative+Models+for+Sub-Gaussian+Distributions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10880，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10880&send_immediately=true&force_search=false)

**原文摘要:** This paper studies the approximation and generalization abilities of
score-based neural network generative models (SGMs) in estimating an unknown
distribution $P_0$ from $n$ i.i.d. observations in $d$ dimensions. Assuming
merely that $P_0$ is $\alpha$-sub-Gaussian, we prove that for any time step $t
\in [t_0, n^{O(1)}]$, where $t_0 \geq O(\alpha^2n^{-2/d}\log n)$, there exists
a deep ReLU neural network with width $\leq O(\log^3n)$ and depth $\leq
O(n^{3/d}\log_2n)$ that can approximate the scores with $\tilde{O}(n^{-1})$
mean square error and achieve a nearly optimal rate of
$\tilde{O}(n^{-1}t_0^{-d/2})$ for score estimation, as measured by the score
matching loss. Our framework is universal and can be used to establish
convergence rates for SGMs under milder assumptions than previous work. For
example, assuming further that the target density function $p_0$ lies in
Sobolev or Besov classes, with an appropriately early stopping strategy, we
demonstrate that neural network-based SGMs can attain nearly minimax
convergence rates up to logarithmic factors. Our analysis removes several
crucial assumptions, such as Lipschitz continuity of the score function or a
strictly positive lower bound on the target density.

</details>


### [17] [Global Convergence of Adaptive Sensing for Principal Eigenvector Estimation](https://arxiv.org/abs/2505.10882)
*Alex Saad-Falcon, Brighton Ancelin, Justin Romberg*

**主要类别:** cs.LG

**概要:** 提出了一种在高维空间中高效执行主成分分析（PCA）的压缩自适应采样Oja算法变体，并证明了其在全球收敛性和两阶段误差衰减方面的理论保证。


<details>
  <summary>Details</summary>
  
**动机:** 传统PCA方法计算成本高昂，而现有的子空间跟踪算法通常需要全维度观测，本文旨在通过压缩自适应采样解决这一问题。

**方法:** 分析了一种每次迭代只获取两个压缩测量值的Oja算法变体，其中一个方向是当前估计值的方向，另一个是在随机正交方向上的测量值。

**结果:** 证明了该自适应采样方法在存在噪声的情况下能够全局收敛，且具有两阶段的误差衰减速率，同时首次提供了自适应采样条件下子空间跟踪的收敛性保证。

**结论:** 本文提出的压缩自适应采样方法在高维数据处理中具有重要应用价值，尤其是在获取全维度样本困难或昂贵的情况下。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Global+Convergence+of+Adaptive+Sensing+for+Principal+Eigenvector+Estimation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10882，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10882&send_immediately=true&force_search=false)

**原文摘要:** This paper addresses the challenge of efficient principal component analysis
(PCA) in high-dimensional spaces by analyzing a compressively sampled variant
of Oja's algorithm with adaptive sensing. Traditional PCA methods incur
substantial computational costs that scale poorly with data dimensionality,
whereas subspace tracking algorithms like Oja's offer more efficient
alternatives but typically require full-dimensional observations. We analyze a
variant where, at each iteration, only two compressed measurements are taken:
one in the direction of the current estimate and one in a random orthogonal
direction. We prove that this adaptive sensing approach achieves global
convergence in the presence of noise when tracking the leading eigenvector of a
datastream with eigengap $\Delta=\lambda_1-\lambda_2$. Our theoretical analysis
demonstrates that the algorithm experiences two phases: (1) a warmup phase
requiring $O(\lambda_1\lambda_2d^2/\Delta^2)$ iterations to achieve a
constant-level alignment with the true eigenvector, followed by (2) a local
convergence phase where the sine alignment error decays at a rate of
$O(\lambda_1\lambda_2d^2/\Delta^2 t)$ for iterations $t$. The guarantee aligns
with existing minimax lower bounds with an added factor of $d$ due to the
compressive sampling. This work provides the first convergence guarantees in
adaptive sensing for subspace tracking with noise. Our proof technique is also
considerably simpler than those in prior works. The results have important
implications for applications where acquiring full-dimensional samples is
challenging or costly.

</details>


### [18] [Clustering Rooftop PV Systems via Probabilistic Embeddings](https://arxiv.org/abs/2505.10699)
*Kutay Bölat, Tarek Alskaif, Peter Palensky, Simon Tindemans*

**主要类别:** cs.LG

**概要:** 提出了一种基于概率实体嵌入的聚类框架来处理大规模分布式光伏系统的高维时间序列数据整合与管理问题。该方法在住宅光伏数据集上表现优于物理基准，并支持可靠的缺失值填补。


<details>
  <summary>Details</summary>
  
**动机:** 随着屋顶光伏安装数量的增加，需要监测和分析这些系统，面临整合和管理大量高维且受缺失值影响的空间分布时间序列数据的挑战。

**方法:** 提出的框架将每个光伏系统的特征发电模式和不确定性编码为概率分布，然后通过统计距离和凝聚聚类对系统进行分组。

**结果:** 在多 年住宅光伏数据集上的应用产生了简洁且具有不确定性的聚类轮廓，在代表性和鲁棒性方面优于物理基准，并支持可靠的缺失值填补。

**结论:** 研究进一步提供了关于平衡模型性能和鲁棒性的实用指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Clustering+Rooftop+PV+Systems+via+Probabilistic+Embeddings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10699，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10699&send_immediately=true&force_search=false)

**原文摘要:** As the number of rooftop photovoltaic (PV) installations increases,
aggregators and system operators are required to monitor and analyze these
systems, raising the challenge of integration and management of large,
spatially distributed time-series data that are both high-dimensional and
affected by missing values. In this work, a probabilistic entity
embedding-based clustering framework is proposed to address these problems.
This method encodes each PV system's characteristic power generation patterns
and uncertainty as a probability distribution, then groups systems by their
statistical distances and agglomerative clustering. Applied to a multi-year
residential PV dataset, it produces concise, uncertainty-aware cluster profiles
that outperform a physics-based baseline in representativeness and robustness,
and support reliable missing-value imputation. A systematic hyperparameter
study further offers practical guidance for balancing model performance and
robustness.

</details>


### [19] [NeuralSurv: Deep Survival Analysis with Bayesian Uncertainty Quantification](https://arxiv.org/abs/2505.11054)
*Mélodie Monod, Alessandro Micheli, Samir Bhatt*

**主要类别:** cs.LG

**概要:** 提出首个结合贝叶斯不确定性量化深度生存模型NeuralSurv，通过新颖的数据增强方案在连续时间内灵活捕捉随时间变化的协变量风险关系，并提供理论保障。


<details>
  <summary>Details</summary>
  
**动机:** 引入贝叶斯不确定性量化以提高模型校准和提供可靠的不确定性估计。

**方法:** 设计非参数、架构无关的框架，采用两阶段数据增强方案，提出均值场变分算法实现高效后验推理并局部线性化贝叶斯神经网络。

**结果:** 在实验中，NeuralSurv在合成基准和真实世界数据集上提供了优于现有最先进的深度生存模型的校准性能，同时保持或提高了其辨别性能。

**结论:** 研究证明了贝叶斯原则在数据稀缺环境中的价值，通过增强模型校准和提供可靠的不确定性估计来改善生存函数。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是NeuralSurv%3A+Deep+Survival+Analysis+with+Bayesian+Uncertainty+Quantification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11054，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11054&send_immediately=true&force_search=false)

**原文摘要:** We introduce NeuralSurv, the first deep survival model to incorporate
Bayesian uncertainty quantification. Our non-parametric, architecture-agnostic
framework flexibly captures time-varying covariate-risk relationships in
continuous time via a novel two-stage data-augmentation scheme, for which we
establish theoretical guarantees. For efficient posterior inference, we
introduce a mean-field variational algorithm with coordinate-ascent updates
that scale linearly in model size. By locally linearizing the Bayesian neural
network, we obtain full conjugacy and derive all coordinate updates in closed
form. In experiments, NeuralSurv delivers superior calibration compared to
state-of-the-art deep survival models, while matching or exceeding their
discriminative performance across both synthetic benchmarks and real-world
datasets. Our results demonstrate the value of Bayesian principles in
data-scarce regimes by enhancing model calibration and providing robust,
well-calibrated uncertainty estimates for the survival function.

</details>


### [20] [ZEUS: Zero-shot Embeddings for Unsupervised Separation of Tabular Data](https://arxiv.org/abs/2505.10704)
*Patryk Marszałek, Tomasz Kuśmierczyk, Witold Wydmański, Jacek Tabor, Marek Śmieja*

**主要类别:** cs.LG

**概要:** ZEUS是一种新的深度学习方法，能够在无需额外训练或微调的情况下对新表格数据集进行聚类。该模型通过分解复杂的数据集为有意义的部分来实现这一点，并且由于在潜在变量先验生成的合成数据集上的预训练，它可以在各种数据集上泛化。


<details>
  <summary>Details</summary>
  
**动机:** 当前表格数据聚类面临挑战，包括相似性因数据集而异以及缺乏监督信号导致性能不稳定等问题。

**方法:** 采用深度学习中的零样本学习方法，提出了一种名为ZEUS的新模型，能够直接对新数据集进行聚类。

**结果:** 实验表明，ZEUS的表现优于或至少与传统的聚类算法和基于深度学习的方法相当，同时速度更快且更易于使用。

**结论:** ZEUS是首个完全无监督地为表格数据生成嵌入的零样本方法，为解决表格数据聚类问题提供了一个高效且用户友好的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ZEUS%3A+Zero-shot+Embeddings+for+Unsupervised+Separation+of+Tabular+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10704，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10704&send_immediately=true&force_search=false)

**原文摘要:** Clustering tabular data remains a significant open challenge in data analysis
and machine learning. Unlike for image data, similarity between tabular records
often varies across datasets, making the definition of clusters highly
dataset-dependent. Furthermore, the absence of supervised signals complicates
hyperparameter tuning in deep learning clustering methods, frequently resulting
in unstable performance. To address these issues and reduce the need for
per-dataset tuning, we adopt an emerging approach in deep learning: zero-shot
learning. We propose ZEUS, a self-contained model capable of clustering new
datasets without any additional training or fine-tuning. It operates by
decomposing complex datasets into meaningful components that can then be
clustered effectively. Thanks to pre-training on synthetic datasets generated
from a latent-variable prior, it generalizes across various datasets without
requiring user intervention. To the best of our knowledge, ZEUS is the first
zero-shot method capable of generating embeddings for tabular data in a fully
unsupervised manner. Experimental results demonstrate that it performs on par
with or better than traditional clustering algorithms and recent deep
learning-based methods, while being significantly faster and more
user-friendly.

</details>


### [21] [A Fast Kernel-based Conditional Independence test with Application to Causal Discovery](https://arxiv.org/abs/2505.11085)
*Oliver Schacht, Biwei Huang*

**主要类别:** cs.LG

**概要:** 提出了一种名为FastKCI的方法，用于在大规模数据集上进行条件独立性检验，该方法基于KCI测试，并通过混合专家方法实现了可扩展性和并行化，同时保持了统计功效。


<details>
  <summary>Details</summary>
  
**动机:** 解决KCI测试在大规模数据集上的计算复杂度问题，因为其三次方复杂度限制了应用。

**方法:** FastKCI利用混合专家方法，通过高斯混合模型对数据集分区，并在局部进行KCI测试，结果使用重要性加权采样方案聚合。

**结果:** 实验表明，FastKCI在保持原始KCI测试统计功效的同时，实现了显著的计算加速。

**结论:** FastKCI是一种实用且高效的解决方案，适用于大规模数据因果推理中的条件独立性检验。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Fast+Kernel-based+Conditional+Independence+test+with+Application+to+Causal+Discovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11085，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11085&send_immediately=true&force_search=false)

**原文摘要:** Kernel-based conditional independence (KCI) testing is a powerful
nonparametric method commonly employed in causal discovery tasks. Despite its
flexibility and statistical reliability, cubic computational complexity limits
its application to large datasets. To address this computational bottleneck, we
propose \textit{FastKCI}, a scalable and parallelizable kernel-based
conditional independence test that utilizes a mixture-of-experts approach
inspired by embarrassingly parallel inference techniques for Gaussian
processes. By partitioning the dataset based on a Gaussian mixture model over
the conditioning variables, FastKCI conducts local KCI tests in parallel,
aggregating the results using an importance-weighted sampling scheme.
Experiments on synthetic datasets and benchmarks on real-world production data
validate that FastKCI maintains the statistical power of the original KCI test
while achieving substantial computational speedups. FastKCI thus represents a
practical and efficient solution for conditional independence testing in causal
inference on large-scale data.

</details>


### [22] [GNN-Suite: a Graph Neural Network Benchmarking Framework for Biomedical Informatics](https://arxiv.org/abs/2505.10711)
*Sebestyén Kamp, Giovanni Stracquadanio, T. Ian Simpson*

**主要类别:** cs.LG

**概要:** GNN-Suite是一个用于构建和评估计算生物学中的图神经网络（GNN）架构的稳健模块化框架。它在识别癌症驱动基因方面展示了其效用，并实现了不同GNN架构之间的公平比较。


<details>
  <summary>Details</summary>
  
**动机:** 当前计算生物学领域缺乏一个标准化的框架来构建和评估GNN架构，这阻碍了研究的可重复性和模型间的公平比较。

**方法:** GNN-Suite使用Nextflow工作流来标准化实验和可重复性。它通过从STRING和BioGRID数据集中构建分子网络并使用PCAWG、PID和COSMIC-CGC存储库注释节点来识别癌症驱动基因。所有GNN模型都被配置为标准的两层模型，并在统一的超参数设置下训练。

**结果:** GCN2在STRING基网络上达到了最高的平衡准确率（BACC），并且所有GNN类型都优于逻辑回归基线模型。

**结论:** GNN-Suite有助于识别最佳模型以及整合互补数据的最有效方法，促进了计算生物学领域的可重复研究和基准测试标准的改进。未来的工作将探索更多的组学数据集并进一步完善网络架构以提高生物医学应用中的预测准确性和解释性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GNN-Suite%3A+a+Graph+Neural+Network+Benchmarking+Framework+for+Biomedical+Informatics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10711，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10711&send_immediately=true&force_search=false)

**原文摘要:** We present GNN-Suite, a robust modular framework for constructing and
benchmarking Graph Neural Network (GNN) architectures in computational biology.
GNN-Suite standardises experimentation and reproducibility using the Nextflow
workflow to evaluate GNN performance. We demonstrate its utility in identifying
cancer-driver genes by constructing molecular networks from protein-protein
interaction (PPI) data from STRING and BioGRID and annotating nodes with
features from the PCAWG, PID, and COSMIC-CGC repositories.
  Our design enables fair comparisons among diverse GNN architectures including
GAT, GAT3H, GCN, GCN2, GIN, GTN, HGCN, PHGCN, and GraphSAGE and a baseline
Logistic Regression (LR) model. All GNNs were configured as standardised
two-layer models and trained with uniform hyperparameters (dropout = 0.2; Adam
optimiser with learning rate = 0.01; and an adjusted binary cross-entropy loss
to address class imbalance) over an 80/20 train-test split for 300 epochs. Each
model was evaluated over 10 independent runs with different random seeds to
yield statistically robust performance metrics, with balanced accuracy (BACC)
as the primary measure. Notably, GCN2 achieved the highest BACC (0.807 +/-
0.035) on a STRING-based network, although all GNN types outperformed the LR
baseline, highlighting the advantage of network-based learning over
feature-only approaches.
  Our results show that a common framework for implementing and evaluating GNN
architectures aids in identifying not only the best model but also the most
effective means of incorporating complementary data. By making GNN-Suite
publicly available, we aim to foster reproducible research and promote improved
benchmarking standards in computational biology. Future work will explore
additional omics datasets and further refine network architectures to enhance
predictive accuracy and interpretability in biomedical applications.

</details>


### [23] [FedDuA: Doubly Adaptive Federated Learning](https://arxiv.org/abs/2505.11126)
*Shokichi Takakura, Seng Pei Liew, Satoshi Hasegawa*

**主要类别:** cs.LG

**概要:** 本文提出了一种新的联邦学习框架FedDuA，通过镜像下降优化方法自适应调整全局学习率，在保持客户端通信和计算成本不变的情况下，实现了比现有算法更好的性能。


<details>
  <summary>Details</summary>
  
**动机:** 现有的联邦学习算法如FedAvg在非独立同分布的数据上收敛较慢，且对参数空间的各向异性敏感。

**方法:** 提出了一种新的框架FedDuA，采用双自适应步长规则，根据客户端间和坐标方向上的异质性自适应调整全局学习率。

**结果:** 实验表明，该方法在多种设置下优于基准模型，并且对超参数的选择具有鲁棒性。

**结论:** 所提出的FedDuA框架在理论上证明了其最优性，并通过实验证明了其在实际应用中的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedDuA%3A+Doubly+Adaptive+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11126，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11126&send_immediately=true&force_search=false)

**原文摘要:** Federated learning is a distributed learning framework where clients
collaboratively train a global model without sharing their raw data. FedAvg is
a popular algorithm for federated learning, but it often suffers from slow
convergence due to the heterogeneity of local datasets and anisotropy in the
parameter space. In this work, we formalize the central server optimization
procedure through the lens of mirror descent and propose a novel framework,
called FedDuA, which adaptively selects the global learning rate based on both
inter-client and coordinate-wise heterogeneity in the local updates. We prove
that our proposed doubly adaptive step-size rule is minimax optimal and provide
a convergence analysis for convex objectives. Although the proposed method does
not require additional communication or computational cost on clients,
extensive numerical experiments show that our proposed framework outperforms
baselines in various settings and is robust to the choice of hyperparameters.

</details>


### [24] [Learning Repetition-Invariant Representations for Polymer Informatics](https://arxiv.org/abs/2505.10726)
*Yihan Zhu, Gang Liu, Eric Inae, Tengfei Luo, Meng Jiang*

**主要类别:** cs.LG

**概要:** 提出Graph Repetition Invariance (GRIN)方法，用于学习对重复单元数量不变的聚合物表示，该方法在同聚物和共聚物基准上优于现有最先进的基线。


<details>
  <summary>Details</summary>
  
**动机:** 现有的图神经网络方法只能对单个单元建模而不能产生一致的向量表示对于具有不同数量单元的真实聚合物结构。

**方法:** GRIN结合了基于图的最大生成树对齐与重复单元增强以确保结构一致性，并从模型和数据角度提供了重复不变性的理论保证。

**结果:** GRIN在同聚物和共聚物基准上优于最先进的基线，学习到稳定的、对重复不变的表示，可以有效推广到未见过大小的聚合物链。

**结论:** GRIN是一种新颖的方法，能够学习到对重复单元数量不变的聚合物表示，这对于能源存储、建筑、医药和航空航天等领域有重要意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Repetition-Invariant+Representations+for+Polymer+Informatics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10726，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10726&send_immediately=true&force_search=false)

**原文摘要:** Polymers are large macromolecules composed of repeating structural units
known as monomers and are widely applied in fields such as energy storage,
construction, medicine, and aerospace. However, existing graph neural network
methods, though effective for small molecules, only model the single unit of
polymers and fail to produce consistent vector representations for the true
polymer structure with varying numbers of units. To address this challenge, we
introduce Graph Repetition Invariance (GRIN), a novel method to learn polymer
representations that are invariant to the number of repeating units in their
graph representations. GRIN integrates a graph-based maximum spanning tree
alignment with repeat-unit augmentation to ensure structural consistency. We
provide theoretical guarantees for repetition-invariance from both model and
data perspectives, demonstrating that three repeating units are the minimal
augmentation required for optimal invariant representation learning. GRIN
outperforms state-of-the-art baselines on both homopolymer and copolymer
benchmarks, learning stable, repetition-invariant representations that
generalize effectively to polymer chains of unseen sizes.

</details>


### [25] [Minimizing False-Positive Attributions in Explanations of Non-Linear Models](https://arxiv.org/abs/2505.11210)
*Anders Gjølbye, Stefan Haufe, Lars Kai Hansen*

**主要类别:** cs.LG

**概要:** 本文介绍了一种新的XAI技术PatternLocal，它通过转换局部线性代理的判别模型权重为生成表示来抑制抑制变量的影响，从而提高非线性任务解释的可靠性。


<details>
  <summary>Details</summary>
  
**动机:** 抑制变量可能引起虚假的特征归因，损害解释的效用，现有线性模型的有效补救措施难以扩展到非线性模型和实例级别的解释。

**方法:** 提出了一种名为PatternLocal的新XAI技术，它从局部线性代理开始并转换判别模型权重为生成表示，从而抑制抑制变量的影响。

**结果:** PatternLocal在解释非线性任务时减少了错误的特征归因，并在XAI-TRIS基准上优于其他XAI方法。

**结论:** PatternLocal技术在解释非线性任务时减少了错误的特征归因，并在XAI-TRIS基准上优于其他XAI方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Minimizing+False-Positive+Attributions+in+Explanations+of+Non-Linear+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11210，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11210&send_immediately=true&force_search=false)

**原文摘要:** Suppressor variables can influence model predictions without being dependent
on the target outcome and they pose a significant challenge for Explainable AI
(XAI) methods. These variables may cause false-positive feature attributions,
undermining the utility of explanations. Although effective remedies exist for
linear models, their extension to non-linear models and to instance-based
explanations has remained limited. We introduce PatternLocal, a novel XAI
technique that addresses this gap. PatternLocal begins with a locally linear
surrogate, e.g. LIME, KernelSHAP, or gradient-based methods, and transforms the
resulting discriminative model weights into a generative representation,
thereby suppressing the influence of suppressor variables while preserving
local fidelity. In extensive hyperparameter optimization on the XAI-TRIS
benchmark, PatternLocal consistently outperformed other XAI methods and reduced
false-positive attributions when explaining non-linear tasks, thereby enabling
more reliable and actionable insights.

</details>


### [26] [Random Client Selection on Contrastive Federated Learning for Tabular Data](https://arxiv.org/abs/2505.10759)
*Achmad Ginanjar, Xue Li, Priyanka Singh, Wen Hua*

**主要类别:** cs.LG

**概要:** This paper examines gradient-based attacks on Contrastive Federated Learning (CFL) and finds that randomly selecting clients is an effective defense.


<details>
  <summary>Details</summary>
  
**动机:** To enhance privacy and security in vertical federated learning by addressing information leakage and gradient-based attacks.

**方法:** Conducts comprehensive experiments on gradient-based attacks in CFL environments and evaluates random client selection as a defense mechanism.

**结果:** Random client selection proves effective in defending against gradient attacks in the CFL network.

**结论:** Provides insights into implementing robust security measures in contrastive federated learning systems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Random+Client+Selection+on+Contrastive+Federated+Learning+for+Tabular+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10759，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10759&send_immediately=true&force_search=false)

**原文摘要:** Vertical Federated Learning (VFL) has revolutionised collaborative machine
learning by enabling privacy-preserving model training across multiple parties.
However, it remains vulnerable to information leakage during intermediate
computation sharing. While Contrastive Federated Learning (CFL) was introduced
to mitigate these privacy concerns through representation learning, it still
faces challenges from gradient-based attacks. This paper presents a
comprehensive experimental analysis of gradient-based attacks in CFL
environments and evaluates random client selection as a defensive strategy.
Through extensive experimentation, we demonstrate that random client selection
proves particularly effective in defending against gradient attacks in the CFL
network. Our findings provide valuable insights for implementing robust
security measures in contrastive federated learning systems, contributing to
the development of more secure collaborative learning frameworks

</details>


### [27] [Bayesian Hierarchical Invariant Prediction](https://arxiv.org/abs/2505.11211)
*Francisco Madaleno, Pernille Julie Viuff Sand, Francisco C. Pereira, Sergio Hernan Garrido Mejia*

**主要类别:** cs.LG

**概要:** This paper introduces Bayesian Hierarchical Invariant Prediction (BHIP), which improves computational scalability for a larger number of predictors compared to Invariant Causal Prediction (ICP). It also allows the use of prior information and tests two sparsity inducing priors.


<details>
  <summary>Details</summary>
  
**动机:** To improve computational scalability and enable the use of prior information for causal feature identification.

**方法:** Bayesian Hierarchical Invariant Prediction (BHIP) that leverages hierarchical structure to test invariance of causal mechanisms under heterogeneous data.

**结果:** Improved computational scalability for a larger number of predictors and reliable identification of causal features using sparsity inducing priors.

**结论:** BHIP is a promising alternative inference method to ICP.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bayesian+Hierarchical+Invariant+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11211，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11211&send_immediately=true&force_search=false)

**原文摘要:** We propose Bayesian Hierarchical Invariant Prediction (BHIP) reframing
Invariant Causal Prediction (ICP) through the lens of Hierarchical Bayes. We
leverage the hierarchical structure to explicitly test invariance of causal
mechanisms under heterogeneous data, resulting in improved computational
scalability for a larger number of predictors compared to ICP. Moreover, given
its Bayesian nature BHIP enables the use of prior information. In this paper,
we test two sparsity inducing priors: horseshoe and spike-and-slab, both of
which allow us a more reliable identification of causal features. We test BHIP
in synthetic and real-world data showing its potential as an alternative
inference method to ICP.

</details>


### [28] [Deep Symbolic Optimization: Reinforcement Learning for Symbolic Mathematics](https://arxiv.org/abs/2505.10762)
*Conor F. Hayes, Felipe Leno Da Silva, Jiachen Yang, T. Nathan Mundhenk, Chak Shing Lee, Jacob F. Pettit, Claudio Santiago, Sookyung Kim, Joanne T. Kim, Ignacio Aravena Solis, Ruben Glatt, Andre R. Goncalves, Alexander Ladd, Ahmet Can Solak, Thomas Desautels, Daniel Faissol, Brenden K. Petersen, Mikel Landajuela*

**主要类别:** cs.LG

**概要:** 提出了一种名为深度符号优化（DSO）的新计算框架，用于科学发现中的符号优化，尤其是在搜索复杂的符号结构时。


<details>
  <summary>Details</summary>
  
**动机:** 为了实现科学发现中的符号优化，特别是在搜索复杂的符号结构的应用中。

**方法:** 通过将发现过程表述为顺序决策任务，利用生成神经网络和强化学习策略来指导搜索，结合梯度优化、进化和局部搜索技术，并引入现场约束、领域特定先验和高级策略优化方法。

**结果:** DSO在基准问题上进行了广泛的评估，显示出最先进的性能。

**结论:** DSO框架在准确性和可解释性方面达到了最先进的性能，并且展示了其在科学发现中自动化符号优化的变革潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep+Symbolic+Optimization%3A+Reinforcement+Learning+for+Symbolic+Mathematics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10762，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10762&send_immediately=true&force_search=false)

**原文摘要:** Deep Symbolic Optimization (DSO) is a novel computational framework that
enables symbolic optimization for scientific discovery, particularly in
applications involving the search for intricate symbolic structures. One
notable example is equation discovery, which aims to automatically derive
mathematical models expressed in symbolic form. In DSO, the discovery process
is formulated as a sequential decision-making task. A generative neural network
learns a probabilistic model over a vast space of candidate symbolic
expressions, while reinforcement learning strategies guide the search toward
the most promising regions. This approach integrates gradient-based
optimization with evolutionary and local search techniques, and it incorporates
in-situ constraints, domain-specific priors, and advanced policy optimization
methods. The result is a robust framework capable of efficiently exploring
extensive search spaces to identify interpretable and physically meaningful
models. Extensive evaluations on benchmark problems have demonstrated that DSO
achieves state-of-the-art performance in both accuracy and interpretability. In
this chapter, we provide a comprehensive overview of the DSO framework and
illustrate its transformative potential for automating symbolic optimization in
scientific discovery.

</details>


### [29] [A Generative Framework for Causal Estimation via Importance-Weighted Diffusion Distillation](https://arxiv.org/abs/2505.11444)
*Xinran Song, Tianyu Chen, Mingyuan Zhou*

**主要类别:** cs.LG

**概要:** 提出了一种结合扩散模型预训练和重要性加权得分蒸馏的新型生成框架IWDD，用于准确快速的因果估计。


<details>
  <summary>Details</summary>
  
**动机:** 解决从观测数据估计个体化治疗效果的挑战，主要由于协变量不平衡和非随机治疗分配导致的混杂偏倚。

**方法:** 引入了重要性加权扩散蒸馏(IWDD)，结合IPW和扩散模型蒸馏，并通过基于随机化的调整减少方差。

**结果:** IWDD在样本外预测性能上达到最先进的水平，显著提高了因果估计并支持个性化治疗策略的发展。

**结论:** 该研究证明了IPW在扩散模型蒸馏中的自然集成，并提供了一个新的方法来简化计算和减少梯度估计的方差。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Generative+Framework+for+Causal+Estimation+via+Importance-Weighted+Diffusion+Distillation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11444，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11444&send_immediately=true&force_search=false)

**原文摘要:** Estimating individualized treatment effects from observational data is a
central challenge in causal inference, largely due to covariate imbalance and
confounding bias from non-randomized treatment assignment. While inverse
probability weighting (IPW) is a well-established solution to this problem, its
integration into modern deep learning frameworks remains limited. In this work,
we propose Importance-Weighted Diffusion Distillation (IWDD), a novel
generative framework that combines the pretraining of diffusion models with
importance-weighted score distillation to enable accurate and fast causal
estimation-including potential outcome prediction and treatment effect
estimation. We demonstrate how IPW can be naturally incorporated into the
distillation of pretrained diffusion models, and further introduce a
randomization-based adjustment that eliminates the need to compute IPW
explicitly-thereby simplifying computation and, more importantly, provably
reducing the variance of gradient estimates. Empirical results show that IWDD
achieves state-of-the-art out-of-sample prediction performance, with the
highest win rates compared to other baselines, significantly improving causal
estimation and supporting the development of individualized treatment
strategies. We will release our PyTorch code for reproducibility and future
research.

</details>


### [30] [Context-Aware Probabilistic Modeling with LLM for Multimodal Time Series Forecasting](https://arxiv.org/abs/2505.10774)
*Yueyang Yao, Jiajun Li, Xingyuan Dai, MengMeng Zhang, Xiaoyan Gong, Fei-Yue Wang, Yisheng Lv*

**主要类别:** cs.LG

**概要:** 提出了一种名为CAPTime的新方法，用于上下文感知的概率多模态时间序列预测，该方法结合了文本引导的抽象和自回归LLM解码。实验表明CAPTime在各种时间序列预测任务中具有更高的准确性和泛化能力。


<details>
  <summary>Details</summary>
  
**动机:** 现有方法难以有效整合外部文本并与大型语言模型（LLMs）的概率特性对齐。

**方法:** 首先使用预训练的时间序列编码器对时间模式进行编码，然后通过可学习的交互方式与文本上下文对齐，生成联合多模态表示。结合分布专家混合和冻结的LLMs，实现上下文感知的概率预测。

**结果:** 在不同的时间序列预测任务上进行的实验表明，CAPTime在准确性、泛化能力和数据稀缺场景中的鲁棒性方面表现出色。

**结论:** CAPTime是一种创新的方法，可以有效地将文本信息整合到时间序列预测中，同时保持LLMs的分布建模能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Context-Aware+Probabilistic+Modeling+with+LLM+for+Multimodal+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10774，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10774&send_immediately=true&force_search=false)

**原文摘要:** Time series forecasting is important for applications spanning energy
markets, climate analysis, and traffic management. However, existing methods
struggle to effectively integrate exogenous texts and align them with the
probabilistic nature of large language models (LLMs). Current approaches either
employ shallow text-time series fusion via basic prompts or rely on
deterministic numerical decoding that conflict with LLMs' token-generation
paradigm, which limits contextual awareness and distribution modeling. To
address these limitations, we propose CAPTime, a context-aware probabilistic
multimodal time series forecasting method that leverages text-informed
abstraction and autoregressive LLM decoding. Our method first encodes temporal
patterns using a pretrained time series encoder, then aligns them with textual
contexts via learnable interactions to produce joint multimodal
representations. By combining a mixture of distribution experts with frozen
LLMs, we enable context-aware probabilistic forecasting while preserving LLMs'
inherent distribution modeling capabilities. Experiments on diverse time series
forecasting tasks demonstrate the superior accuracy and generalization of
CAPTime, particularly in multimodal scenarios. Additional analysis highlights
its robustness in data-scarce scenarios through hybrid probabilistic decoding.

</details>


### [31] [Cell Library Characterization for Composite Current Source Models Based on Gaussian Process Regression and Active Learning](https://arxiv.org/abs/2505.10799)
*Tao Bai, Junzhuo Zhou, Zeyuan Deng, Peng Cao*

**主要类别:** cs.LG

**概要:** 提出了一种基于高斯过程回归和主动学习的新型复合电流源模型表征框架，显著提高了表征精度并大幅减少了运行时间和存储需求。


<details>
  <summary>Details</summary>
  
**动机:** 传统非线性延迟模型无法有效建模复杂动态效应，而复合电流源模型虽然更准确但面临数据量大、仿真成本高的挑战。

**方法:** 引入了基于高斯过程回归的主动学习方法来高效且精确地构建复合电流源模型的表征框架。

**结果:** 该方法在TSMC 22nm工艺下对57个单元在9个PVT角上的电流波形表征达到了平均绝对误差2.05ps，相对误差2.27%，并且运行时间减少到原来的27%，存储需求减少到商业工具的1/19.5。

**结论:** 提出的GPR结合AL的方法在复合电流源模型表征上表现出色，能够有效应对高精度要求带来的挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cell+Library+Characterization+for+Composite+Current+Source+Models+Based+on+Gaussian+Process+Regression+and+Active+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10799，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10799&send_immediately=true&force_search=false)

**原文摘要:** The composite current source (CCS) model has been adopted as an advanced
timing model that represents the current behavior of cells for improved
accuracy and better capability than traditional non-linear delay models (NLDM)
to model complex dynamic effects and interactions under advanced process nodes.
However, the high accuracy requirement, large amount of data and extensive
simulation cost pose severe challenges to CCS characterization. To address
these challenges, we introduce a novel Gaussian Process Regression(GPR) model
with active learning(AL) to establish the characterization framework
efficiently and accurately. Our approach significantly outperforms conventional
commercial tools as well as learning based approaches by achieving an average
absolute error of 2.05 ps and a relative error of 2.27% for current waveform of
57 cells under 9 process, voltage, temperature (PVT) corners with TSMC 22nm
process. Additionally, our model drastically reduces the runtime to 27% and the
storage by up to 19.5x compared with that required by commercial tools.

</details>


### [32] [Attention-Based Reward Shaping for Sparse and Delayed Rewards](https://arxiv.org/abs/2505.10802)
*Ian Holmes, Min Chi*

**主要类别:** cs.LG

**概要:** 提出了一种名为ARES的算法，该算法利用Transformer的注意力机制生成形状奖励并创建密集的奖励函数。ARES可以在完全离线的情况下训练，并且即使在使用小数据集或由随机动作代理产生的场景下也能产生有意义的形状奖励。实验表明ARES可以在延迟奖励设置中显著提高学习效果。


<details>
  <summary>Details</summary>
  
**动机:** 稀疏和延迟的奖励函数对现实世界中的强化学习应用构成了重大障碍。

**方法:** 提出了一种新的算法ARES，它使用Transformer的注意力机制来生成形状奖励并创建密集的奖励函数。ARES可以完全离线训练，并且能够处理任何级别的奖励稀疏性。

**结果:** ARES能够在延迟奖励设置中显著提高学习效果，使RL代理能够在需要不切实际的数据量甚至无法学习的场景中进行训练。

**结论:** ARES是第一个完全离线工作、对极端奖励延迟和低质量数据保持鲁棒并且不限于基于目标的任务的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Attention-Based+Reward+Shaping+for+Sparse+and+Delayed+Rewards，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10802，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10802&send_immediately=true&force_search=false)

**原文摘要:** Sparse and delayed reward functions pose a significant obstacle for
real-world Reinforcement Learning (RL) applications. In this work, we propose
Attention-based REward Shaping (ARES), a general and robust algorithm which
uses a transformer's attention mechanism to generate shaped rewards and create
a dense reward function for any environment. ARES requires a set of episodes
and their final returns as input. It can be trained entirely offline and is
able to generate meaningful shaped rewards even when using small datasets or
episodes produced by agents taking random actions. ARES is compatible with any
RL algorithm and can handle any level of reward sparsity. In our experiments,
we focus on the most challenging case where rewards are fully delayed until the
end of each episode. We evaluate ARES across a diverse range of environments,
widely used RL algorithms, and baseline methods to assess the effectiveness of
the shaped rewards it produces. Our results show that ARES can significantly
improve learning in delayed reward settings, enabling RL agents to train in
scenarios that would otherwise require impractical amounts of data or even be
unlearnable. To our knowledge, ARES is the first approach that works fully
offline, remains robust to extreme reward delays and low-quality data, and is
not limited to goal-based tasks.

</details>


### [33] [Distilled Circuits: A Mechanistic Study of Internal Restructuring in Knowledge Distillation](https://arxiv.org/abs/2505.10822)
*Reilly Haskins, Benjamin Adams*

**主要类别:** cs.LG

**概要:** This paper uses mechanistic interpretability to analyze the differences between teacher and student models in knowledge distillation, revealing that students reorganize and compress teachers' components, leading to stronger reliance on fewer components.


<details>
  <summary>Details</summary>
  
**动机:** To understand the internal computational transformations that occur during knowledge distillation process.

**方法:** Applying mechanistic interpretability techniques to compare internal circuits, representations, and activation patterns between teacher and student models.

**结果:** Student models reorganize, compress, and discard teacher components, often relying more on fewer individual components.

**结论:** Knowledge distillation preserves broad functional behaviors but causes significant shifts in internal computation, affecting the robustness and generalization capacity of distilled models.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Distilled+Circuits%3A+A+Mechanistic+Study+of+Internal+Restructuring+in+Knowledge+Distillation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10822，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10822&send_immediately=true&force_search=false)

**原文摘要:** Knowledge distillation compresses a larger neural model (teacher) into
smaller, faster student models by training them to match teacher outputs.
However, the internal computational transformations that occur during this
process remain poorly understood. We apply techniques from mechanistic
interpretability to analyze how internal circuits, representations, and
activation patterns differ between teacher and student. Focusing on GPT2-small
and its distilled counterpart DistilGPT2, we find that student models
reorganize, compress, and discard teacher components, often resulting in
stronger reliance on fewer individual components. To quantify functional
alignment beyond output similarity, we introduce an alignment metric based on
influence-weighted component similarity, validated across multiple tasks. Our
findings reveal that while knowledge distillation preserves broad functional
behaviors, it also causes significant shifts in internal computation, with
important implications for the robustness and generalization capacity of
distilled models.

</details>


### [34] [MergeBench: A Benchmark for Merging Domain-Specialized LLMs](https://arxiv.org/abs/2505.10833)
*Yifei He, Siqi Zeng, Yuzheng Hu, Rui Yang, Tong Zhang, Han Zhao*

**主要类别:** cs.LG

**概要:** 提出MergeBench评估套件，用于大规模模型合并评估，涵盖五大领域，标准化微调与评估协议，提供实用算法选择指南并分享见解。


<details>
  <summary>Details</summary>
  
**动机:** 现有模型合并方法在大规模和任务多样性上的评估有限，存在适用性疑问，特别是对大型领域专用LLMs。

**方法:** 构建MergeBench评估套件，基于Llama和Gemma系列开源语言模型，涵盖五大关键领域，并评估八种代表性合并方法。

**结果:** 发现模型合并在更强的基础模型上表现更好，调整合并系数和稀疏化技术可改善知识保留，但仍存在计算成本高、域内性能差距及合并角色未充分探索等问题。

**结论:** 希望MergeBench能为基础研究提供支持，推动模型合并的理解与实际应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MergeBench%3A+A+Benchmark+for+Merging+Domain-Specialized+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10833，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10833&send_immediately=true&force_search=false)

**原文摘要:** Model merging provides a scalable alternative to multi-task training by
combining specialized finetuned models through parameter arithmetic, enabling
efficient deployment without the need for joint training or access to all task
data. While recent methods have shown promise, existing evaluations are limited
in both model scale and task diversity, leaving open questions about their
applicability to large, domain-specialized LLMs. To tackle the challenges, we
introduce MergeBench, a comprehensive evaluation suite designed to assess model
merging at scale. MergeBench builds on state-of-the-art open-source language
models, including Llama and Gemma families at 2B to 9B scales, and covers five
key domains: instruction following, mathematics, multilingual understanding,
coding and safety. We standardize finetuning and evaluation protocols, and
assess eight representative merging methods across multi-task performance,
forgetting and runtime efficiency. Based on extensive experiments, we provide
practical guidelines for algorithm selection and share insights showing that
model merging tends to perform better on stronger base models, with techniques
such as merging coefficient tuning and sparsification improving knowledge
retention. However, several challenges remain, including the computational cost
on large models, the gap for in-domain performance compared to multi-task
models, and the underexplored role of model merging in standard LLM training
pipelines. We hope MergeBench provides a foundation for future research to
advance the understanding and practical application of model merging. We open
source our code at
\href{https://github.com/uiuctml/MergeBench}{https://github.com/uiuctml/MergeBench}.

</details>


### [35] [LARGO: Latent Adversarial Reflection through Gradient Optimization for Jailbreaking LLMs](https://arxiv.org/abs/2505.10838)
*Ran Li, Hao Wang, Chengzhi Mao*

**主要类别:** cs.LG

**概要:** 提出了一种新的潜意识自我反射攻击方法LARGO，用于生成流畅的越狱提示，这种方法在标准基准测试中表现优于现有技术。


<details>
  <summary>Details</summary>
  
**动机:** 发现现有的基于梯度的优化方法在离散语言空间中效果不佳，需要一种更有效的方法来揭示大型语言模型中的漏洞。

**方法:** 开发了LARGO（潜在对抗性反射通过梯度优化），它在一个大型语言模型的连续潜在空间内操作，首先优化对抗性潜在向量，然后递归调用相同的模型将其解码为自然语言。

**结果:** 在AdvBench和JailbreakBench等标准基准上，LARGO比领先的越狱技术AutoDAN高出44个百分点的攻击成功率。

**结论:** 研究结果表明，基于梯度优化解释和攻击大型语言模型内部结构是一种有效的替代方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LARGO%3A+Latent+Adversarial+Reflection+through+Gradient+Optimization+for+Jailbreaking+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10838，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10838&send_immediately=true&force_search=false)

**原文摘要:** Efficient red-teaming method to uncover vulnerabilities in Large Language
Models (LLMs) is crucial. While recent attacks often use LLMs as optimizers,
the discrete language space make gradient-based methods struggle. We introduce
LARGO (Latent Adversarial Reflection through Gradient Optimization), a novel
latent self-reflection attack that reasserts the power of gradient-based
optimization for generating fluent jailbreaking prompts. By operating within
the LLM's continuous latent space, LARGO first optimizes an adversarial latent
vector and then recursively call the same LLM to decode the latent into natural
language. This methodology yields a fast, effective, and transferable attack
that produces fluent and stealthy prompts. On standard benchmarks like AdvBench
and JailbreakBench, LARGO surpasses leading jailbreaking techniques, including
AutoDAN, by 44 points in attack success rate. Our findings demonstrate a potent
alternative to agentic LLM prompting, highlighting the efficacy of interpreting
and attacking LLM internals through gradient optimization.

</details>


### [36] [Ready2Unlearn: A Learning-Time Approach for Preparing Models with Future Unlearning Readiness](https://arxiv.org/abs/2505.10845)
*Hanyu Duan, Yi Yang, Ahmed Abbasi, Kar Yan Tam*

**主要类别:** cs.LG

**概要:** Ready2Unlearn是一种学习时间优化方法，通过在训练阶段加入前瞻性视角，使模型具备未学习准备性，从而更高效地处理未来的未学习请求。该方法适用于各种梯度上升基础的未学习算法，并在视觉和语言任务中展示了其优势。


<details>
  <summary>Details</summary>
  
**动机:** 现有未学习方法主要集中在部署阶段的反应式未学习算法设计，而Ready2Unlearn则提前到训练阶段，采用前瞻视角，提升模型未来应对未学习请求的能力。

**方法:** 基于成熟的元学习原则，Ready2Unlearn在训练过程中主动训练机器学习模型，使其具备未学习准备性，且与任何梯度上升基础的未学习算法兼容。

**结果:** 实验结果显示，Ready2Unlearn在视觉和语言任务中的各种未学习设置下，包括类别级未学习和随机数据未学习，都能显著减少未学习时间，提高整体模型能力的保留率，并增强对遗忘数据无意恢复的抵抗力。

**结论:** Ready2Unlearn为机器学习模型提供了一种内置未学习准备性的新方法，这为未来探索更积极的策略以实现更可靠和原则性的机器未学习提供了启发。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Ready2Unlearn%3A+A+Learning-Time+Approach+for+Preparing+Models+with+Future+Unlearning+Readiness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10845，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10845&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces Ready2Unlearn, a learning-time optimization approach
designed to facilitate future unlearning processes. Unlike the majority of
existing unlearning efforts that focus on designing unlearning algorithms,
which are typically implemented reactively when an unlearning request is made
during the model deployment phase, Ready2Unlearn shifts the focus to the
training phase, adopting a "forward-looking" perspective. Building upon
well-established meta-learning principles, Ready2Unlearn proactively trains
machine learning models with unlearning readiness, such that they are well
prepared and can handle future unlearning requests in a more efficient and
principled manner. Ready2Unlearn is model-agnostic and compatible with any
gradient ascent-based machine unlearning algorithms. We evaluate the method on
both vision and language tasks under various unlearning settings, including
class-wise unlearning and random data unlearning. Experimental results show
that by incorporating such preparedness at training time, Ready2Unlearn
produces an unlearning-ready model state, which offers several key advantages
when future unlearning is required, including reduced unlearning time, improved
retention of overall model capability, and enhanced resistance to the
inadvertent recovery of forgotten data. We hope this work could inspire future
efforts to explore more proactive strategies for equipping machine learning
models with built-in readiness towards more reliable and principled machine
unlearning.

</details>


### [37] [AutoRAN: Weak-to-Strong Jailbreaking of Large Reasoning Models](https://arxiv.org/abs/2505.10846)
*Jiacheng Liang, Tanqiu Jiang, Yuhui Wang, Rongyi Zhu, Fenglong Ma, Ting Wang*

**主要类别:** cs.LG

**概要:** AutoRAN是一种针对大型推理模型的自动化弱到强越狱攻击框架，利用弱推理模型模拟目标模型的高级推理结构并生成提示词，迭代优化候选提示词以实现接近100%的成功率。


<details>
  <summary>Details</summary>
  
**动机:** 研究大型推理模型的安全性问题，特别是如何通过弱推理模型发现其漏洞。

**方法:** 使用弱推理模型模拟目标模型的推理结构，生成提示词，并结合目标模型的中间推理步骤迭代优化提示词。

**结果:** 在多个基准数据集上对最先进的大型推理模型进行了测试，取得了接近100%的成功率。

**结论:** 证明了利用弱推理模型可以有效揭示更强大的推理模型的关键漏洞，强调了为基于推理的模型设计更好的安全措施的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AutoRAN%3A+Weak-to-Strong+Jailbreaking+of+Large+Reasoning+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10846，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10846&send_immediately=true&force_search=false)

**原文摘要:** This paper presents AutoRAN, the first automated, weak-to-strong jailbreak
attack framework targeting large reasoning models (LRMs). At its core, AutoRAN
leverages a weak, less-aligned reasoning model to simulate the target model's
high-level reasoning structures, generates narrative prompts, and iteratively
refines candidate prompts by incorporating the target model's intermediate
reasoning steps. We evaluate AutoRAN against state-of-the-art LRMs including
GPT-o3/o4-mini and Gemini-2.5-Flash across multiple benchmark datasets
(AdvBench, HarmBench, and StrongReject). Results demonstrate that AutoRAN
achieves remarkable success rates (approaching 100%) within one or a few turns
across different LRMs, even when judged by a robustly aligned external model.
This work reveals that leveraging weak reasoning models can effectively exploit
the critical vulnerabilities of much more capable reasoning models,
highlighting the need for improved safety measures specifically designed for
reasoning-based models. The code for replicating AutoRAN and running records
are available at: (https://github.com/JACKPURCELL/AutoRAN-public). (warning:
this paper contains potentially harmful content generated by LRMs.)

</details>


### [38] [Foundation model for mass spectrometry proteomics](https://arxiv.org/abs/2505.10848)
*Justin Sanders, Melih Yilmaz, Jacob H. Russell, Wout Bittremieux, William E. Fondrie, Nicholas M. Riley, Sewoong Oh, William Stafford Noble*

**主要类别:** cs.LG

**概要:** 提出一种统一的谱图预测任务的基础模型，通过de novo测序作为预训练任务对光谱编码器进行预训练，并在四个下游任务上展示了改进性能，最终表明该模型可以增强蛋白质组学实验中的数据获取和分析。


<details>
  <summary>Details</summary>
  
**动机:** 提高质谱数据分析的效率和准确性，特别是在训练数据有限的情况下。

**方法:** 提出了一种基于de novo测序的基础模型，用于统一各种谱图预测任务，并进行了预训练和多任务微调。

**结果:** 在四个下游任务上表现出色，包括谱图质量预测、嵌合性预测、磷酸化预测和糖基化状态预测。

**结论:** 证明了基于de novo测序训练的串联质谱蛋白质组学基础模型能够学习到可泛化的光谱表示，在训练数据有限的情况下提高了下游任务的性能，从而增强了蛋白质组学实验中的数据获取和分析。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Foundation+model+for+mass+spectrometry+proteomics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10848，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10848&send_immediately=true&force_search=false)

**原文摘要:** Mass spectrometry is the dominant technology in the field of proteomics,
enabling high-throughput analysis of the protein content of complex biological
samples. Due to the complexity of the instrumentation and resulting data,
sophisticated computational methods are required for the processing and
interpretation of acquired mass spectra. Machine learning has shown great
promise to improve the analysis of mass spectrometry data, with numerous
purpose-built methods for improving specific steps in the data acquisition and
analysis pipeline reaching widespread adoption. Here, we propose unifying
various spectrum prediction tasks under a single foundation model for mass
spectra. To this end, we pre-train a spectrum encoder using de novo sequencing
as a pre-training task. We then show that using these pre-trained spectrum
representations improves our performance on the four downstream tasks of
spectrum quality prediction, chimericity prediction, phosphorylation
prediction, and glycosylation status prediction. Finally, we perform multi-task
fine-tuning and find that this approach improves the performance on each task
individually. Overall, our work demonstrates that a foundation model for tandem
mass spectrometry proteomics trained on de novo sequencing learns generalizable
representations of spectra, improves performance on downstream tasks where
training data is limited, and can ultimately enhance data acquisition and
analysis in proteomics experiments.

</details>


### [39] [ImputeINR: Time Series Imputation via Implicit Neural Representations for Disease Diagnosis with Missing Data](https://arxiv.org/abs/2505.10856)
*Mengxuan Li, Ke Liu, Jialong Guo, Jiajun Bu, Hongwei Wang, Haishuai Wang*

**主要类别:** cs.LG

**概要:** 提出了一种名为ImputeINR的新方法，利用隐式神经表征（INR）对时间序列进行插补，尤其在高缺失率下表现出色，并提高了下游疾病诊断任务的性能。


<details>
  <summary>Details</summary>
  
**动机:** 现有的插补方法主要针对离散数据点，无法有效处理稀疏数据，特别是在大量缺失值的情况下性能较差。

**方法:** 通过采用隐式神经表征（INR）学习时间序列的连续函数来实现时间序列插补。

**结果:** 在八个数据集上的广泛实验表明，ImputeINR具有优越的插补性能，尤其是在高缺失率下。此外，应用ImputeINR插补医疗数据中的缺失值可以提高下游疾病诊断任务的性能。

**结论:** 所提出的ImputeINR方法在处理时间序列中的大量缺失值方面表现优异，并且有助于提高疾病诊断任务的准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ImputeINR%3A+Time+Series+Imputation+via+Implicit+Neural+Representations+for+Disease+Diagnosis+with+Missing+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10856，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10856&send_immediately=true&force_search=false)

**原文摘要:** Healthcare data frequently contain a substantial proportion of missing
values, necessitating effective time series imputation to support downstream
disease diagnosis tasks. However, existing imputation methods focus on discrete
data points and are unable to effectively model sparse data, resulting in
particularly poor performance for imputing substantial missing values. In this
paper, we propose a novel approach, ImputeINR, for time series imputation by
employing implicit neural representations (INR) to learn continuous functions
for time series. ImputeINR leverages the merits of INR in that the continuous
functions are not coupled to sampling frequency and have infinite sampling
frequency, allowing ImputeINR to generate fine-grained imputations even on
extremely sparse observed values. Extensive experiments conducted on eight
datasets with five ratios of masked values show the superior imputation
performance of ImputeINR, especially for high missing ratios in time series
data. Furthermore, we validate that applying ImputeINR to impute missing values
in healthcare data enhances the performance of downstream disease diagnosis
tasks. Codes are available.

</details>


### [40] [Improving the Data-efficiency of Reinforcement Learning by Warm-starting with LLM](https://arxiv.org/abs/2505.10861)
*Thang Duong, Minglai Yang, Chicheng Zhang*

**主要类别:** cs.LG

**概要:** This paper introduces LORO, which uses a large language model (LLM) to create an off-policy dataset for reinforcement learning in classical MDP environments. This approach improves performance compared to pure RL or LLM-based policies.


<details>
  <summary>Details</summary>
  
**动机:** To enhance the performance of reinforcement learning algorithms by leveraging large language models to generate high-quality datasets.

**方法:** Using LLMs to generate an off-policy dataset covering state-actions from optimal policies, followed by an RL algorithm improving upon this initial policy.

**结果:** LORO shows superior performance across several OpenAI Gym environments like CartPole and Pendulum, achieving up to 4 times the cumulative rewards of pure RL.

**结论:** The integration of LLMs with RL can lead to more efficient and effective learning strategies in classical MDP environments.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+the+Data-efficiency+of+Reinforcement+Learning+by+Warm-starting+with+LLM，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10861，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10861&send_immediately=true&force_search=false)

**原文摘要:** We investigate the usage of Large Language Model (LLM) in collecting
high-quality data to warm-start Reinforcement Learning (RL) algorithms for
learning in some classical Markov Decision Process (MDP) environments. In this
work, we focus on using LLM to generate an off-policy dataset that sufficiently
covers state-actions visited by optimal policies, then later using an RL
algorithm to explore the environment and improve the policy suggested by the
LLM. Our algorithm, LORO, can both converge to an optimal policy and have a
high sample efficiency thanks to the LLM's good starting policy. On multiple
OpenAI Gym environments, such as CartPole and Pendulum, we empirically
demonstrate that LORO outperforms baseline algorithms such as pure LLM-based
policies, pure RL, and a naive combination of the two, achieving up to $4
\times$ the cumulative rewards of the pure RL baseline.

</details>


### [41] [MultiLink: Multi-class Structure Recovery via Agglomerative Clustering and Model Selection](https://arxiv.org/abs/2505.10874)
*Luca Magri, Filippo Leveni, Giacomo Boracchi*

**主要类别:** cs.LG

**概要:** 提出了一种新的MultiLink算法，用于在受噪声和异常值影响的数据集中同时处理多种类别的几何结构模型。与基于偏好分析的方法相比，该方法更快、对阈值不敏感，并且能够弥补假设抽样的局限性。


<details>
  <summary>Details</summary>
  
**动机:** 在受噪声和异常值影响的数据集中恢复不同类别的多个结构的问题。

**方法:** 通过偏好分析和聚类解决鲁棒拟合问题，提出新的MultiLink算法，在新型链接方案中结合了实时模型拟合和模型选择。

**结果:** MultiLink算法在多个公开数据集上的实验表明，它在多类别和单类别问题上都优于现有的最新替代方法。

**结论:** MultiLink算法展示了其在处理多种类别的几何结构模型方面的许多实际优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MultiLink%3A+Multi-class+Structure+Recovery+via+Agglomerative+Clustering+and+Model+Selection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10874，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10874&send_immediately=true&force_search=false)

**原文摘要:** We address the problem of recovering multiple structures of different classes
in a dataset contaminated by noise and outliers. In particular, we consider
geometric structures defined by a mixture of underlying parametric models (e.g.
planes and cylinders, homographies and fundamental matrices), and we tackle the
robust fitting problem by preference analysis and clustering. We present a new
algorithm, termed MultiLink, that simultaneously deals with multiple classes of
models. MultiLink combines on-the-fly model fitting and model selection in a
novel linkage scheme that determines whether two clusters are to be merged. The
resulting method features many practical advantages with respect to methods
based on preference analysis, being faster, less sensitive to the inlier
threshold, and able to compensate limitations deriving from hypotheses
sampling. Experiments on several public datasets demonstrate that Multi-Link
favourably compares with state of the art alternatives, both in multi-class and
single-class problems. Code is publicly made available for download.

</details>


### [42] [Graph and Simplicial Complex Prediction Gaussian Process via the Hodgelet Representations](https://arxiv.org/abs/2505.10877)
*Mathieu Alain, So Takao, Xiaowen Dong, Bastian Rieck, Emmanuel Noutahi*

**主要类别:** cs.LG

**概要:** This work extends Gaussian process framework to simplicial complexes (SCs) for graph and SC-level predictions, enhancing predictions across various applications.


<details>
  <summary>Details</summary>
  
**动机:** When data is scarce, graph neural networks (GNNs) suffer from overfitting, leading to poor performance. Recently, Gaussian processes (GPs) with graph-level inputs have been proposed as an alternative.

**方法:** Extend the Gaussian process framework to simplicial complexes (SCs) and augment the resulting SC representations by considering their Hodge decompositions.

**结果:** The framework enhances predictions across various applications.

**结论:** This work paves the way for GPs to be more widely used for graph and SC-level predictions.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph+and+Simplicial+Complex+Prediction+Gaussian+Process+via+the+Hodgelet+Representations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10877，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10877&send_immediately=true&force_search=false)

**原文摘要:** Predicting the labels of graph-structured data is crucial in scientific
applications and is often achieved using graph neural networks (GNNs). However,
when data is scarce, GNNs suffer from overfitting, leading to poor performance.
Recently, Gaussian processes (GPs) with graph-level inputs have been proposed
as an alternative. In this work, we extend the Gaussian process framework to
simplicial complexes (SCs), enabling the handling of edge-level attributes and
attributes supported on higher-order simplices. We further augment the
resulting SC representations by considering their Hodge decompositions,
allowing us to account for homological information, such as the number of
holes, in the SC. We demonstrate that our framework enhances the predictions
across various applications, paving the way for GPs to be more widely used for
graph and SC-level predictions.

</details>


### [43] [Prior-Guided Diffusion Planning for Offline Reinforcement Learning](https://arxiv.org/abs/2505.10881)
*Donghyeon Ki, JunHyeok Oh, Seong-Woong Shim, Byung-Jun Lee*

**主要类别:** cs.LG

**概要:** 提出Prior Guidance (PG)，一种新的引导采样框架，改进了扩散模型在离线强化学习中的表现。


<details>
  <summary>Details</summary>
  
**动机:** 现有方法存在生成次优多模态动作、分布漂移问题或推理时成本过高等挑战。

**方法:** 用可学习分布替代行为克隆扩散模型的标准高斯先验，并通过行为正则化目标优化。

**结果:** PG能直接生成高价值轨迹，无需对扩散模型本身进行昂贵的奖励优化，且消除了推理时多次采样的需求。

**结论:** PG在多种长期离线RL基准测试中优于最先进的扩散策略和规划器。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Prior-Guided+Diffusion+Planning+for+Offline+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10881，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10881&send_immediately=true&force_search=false)

**原文摘要:** Diffusion models have recently gained prominence in offline reinforcement
learning due to their ability to effectively learn high-performing,
generalizable policies from static datasets. Diffusion-based planners
facilitate long-horizon decision-making by generating high-quality trajectories
through iterative denoising, guided by return-maximizing objectives. However,
existing guided sampling strategies such as Classifier Guidance,
Classifier-Free Guidance, and Monte Carlo Sample Selection either produce
suboptimal multi-modal actions, struggle with distributional drift, or incur
prohibitive inference-time costs. To address these challenges, we propose Prior
Guidance (PG), a novel guided sampling framework that replaces the standard
Gaussian prior of a behavior-cloned diffusion model with a learnable
distribution, optimized via a behavior-regularized objective. PG directly
generates high-value trajectories without costly reward optimization of the
diffusion model itself, and eliminates the need to sample multiple candidates
at inference for sample selection. We present an efficient training strategy
that applies behavior regularization in latent space, and empirically
demonstrate that PG outperforms state-of-the-art diffusion policies and
planners across diverse long-horizon offline RL benchmarks.

</details>


### [44] [Multi-Objective Preference Optimization: Improving Human Alignment of Generative Models](https://arxiv.org/abs/2505.10892)
*Akhil Agnihotri, Rahul Jain, Deepak Ramachandran, Zheng Wen*

**主要类别:** cs.LG

**概要:** This paper introduces MOPO, an algorithm for multi-objective preference alignment in large language models, which outperforms baselines when fine-tuning on real-world datasets.


<details>
  <summary>Details</summary>
  
**动机:** Current methods for aligning large language models with human preferences can only handle a single objective, whereas humans often have multiple, sometimes conflicting, objectives.

**方法:** The MOPO algorithm formulates alignment as a constrained KL-regularized optimization problem, optimizing the primary objective while ensuring secondary objectives meet tunable safety thresholds. It uses pairwise preference data without requiring point-wise reward assumptions or heuristic prompt-context engineering.

**结果:** MOPO was shown to approximate the Pareto front on synthetic benchmarks and achieved higher rewards with policies that Pareto-dominated baselines when fine-tuning a 1.3B-parameter language model on real-world datasets.

**结论:** MOPO provides a robust solution for multi-objective preference alignment in large language models, demonstrating improved performance and stability over existing methods.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Objective+Preference+Optimization%3A+Improving+Human+Alignment+of+Generative+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10892，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10892&send_immediately=true&force_search=false)

**原文摘要:** Post-training of LLMs with RLHF, and subsequently preference optimization
algorithms such as DPO, IPO, etc., made a big difference in improving human
alignment. However, all such techniques can only work with a single (human)
objective. In practice, human users have multiple objectives, such as
helpfulness and harmlessness, and there is no natural way to aggregate them
into a single objective. In this paper, we address the multi-objective
preference-alignment problem, where a policy must optimize several, potentially
conflicting, objectives. We introduce the Multi-Objective Preference
Optimization (MOPO) algorithm, which frames alignment as a constrained
KL-regularized optimization: the primary objective is maximized while secondary
objectives are lower-bounded by tunable safety thresholds. Unlike prior work,
MOPO operates directly on pairwise preference data, requires no point-wise
reward assumption, and avoids heuristic prompt-context engineering. The method
recovers policies on the Pareto front whenever the front is attainable;
practically, it reduces to simple closed-form iterative updates suitable for
large-scale training. On synthetic benchmarks with diverse canonical preference
structures, we show that MOPO approximates the Pareto front. When fine-tuning a
1.3B-parameter language model on real-world human-preference datasets, MOPO
attains higher rewards and yields policies that Pareto-dominate baselines;
ablation studies confirm optimization stability and robustness to
hyperparameters.

</details>


### [45] [CTP: A hybrid CNN-Transformer-PINN model for ocean front forecasting](https://arxiv.org/abs/2505.10894)
*Yishuo Wang, Feng Zhou, Muping Zhou, Qicheng Meng, Zhijun Hu, Yi Wang*

**主要类别:** cs.LG

**概要:** 提出了一种新的深度学习框架CTP，用于海洋锋预测，结合了CNN、Transformer架构和PINN，并在南中国海和黑潮区域进行了实验验证。


<details>
  <summary>Details</summary>
  
**动机:** 海洋锋作为不同水体之间的动态界面，在海洋生物地球化学和物理过程中起着关键作用，但现有方法在多步预测中难以保持空间连续性和物理一致性。

**方法:** CTP框架结合了局部空间编码、长期时间注意和物理约束强制执行。

**结果:** CTP在单步和多步预测中均达到了最先进的性能，在南中国海和黑潮区域的准确性、F1分数和时间稳定性上显著优于基线模型。

**结论:** CTP提供了一种有效的解决方案来提高海洋锋预测的准确性和物理一致性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CTP%3A+A+hybrid+CNN-Transformer-PINN+model+for+ocean+front+forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10894，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10894&send_immediately=true&force_search=false)

**原文摘要:** This paper proposes CTP, a novel deep learning framework that integrates
convolutional neural network(CNN), Transformer architectures, and
physics-informed neural network(PINN) for ocean front prediction. Ocean fronts,
as dynamic interfaces between distinct water masses, play critical roles in
marine biogeochemical and physical processes. Existing methods such as LSTM,
ConvLSTM, and AttentionConv often struggle to maintain spatial continuity and
physical consistency over multi-step forecasts. CTP addresses these challenges
by combining localized spatial encoding, long-range temporal attention, and
physical constraint enforcement. Experimental results across south China
sea(SCS) and Kuroshio(KUR) regions from 1993 to 2020 demonstrate that CTP
achieves state-of-the-art(SOTA) performance in both single-step and multi-step
predictions, significantly outperforming baseline models in accuracy, $F_1$
score, and temporal stability.

</details>


### [46] [Automated Identification of Logical Errors in Programs: Advancing Scalable Analysis of Student Misconceptions](https://arxiv.org/abs/2505.10913)
*Muntasir Hoq, Ananya Rao, Reisha Jaishankar, Krish Piryani, Nithya Janapati, Jessica Vandenberg, Bradford Mott, Narges Norouzi, James Lester, Bita Akram*

**主要类别:** cs.LG

**概要:** 提出了一种基于子树注意力神经网络的可扩展框架，用于自动检测学生编程解决方案中的逻辑错误，并通过实验验证了其有效性和对学习过程的洞察力。


<details>
  <summary>Details</summary>
  
**动机:** 理解导致学生编程困难的因素对于有效的学习支持至关重要。

**方法:** 开发了一个基于子树注意力神经网络的可扩展框架来自动检测逻辑错误。

**结果:** 实验结果表明该框架能够准确捕捉逻辑错误并提供对学习过程的深入见解。

**结论:** 该框架为提高编程教育提供了一个有价值的工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automated+Identification+of+Logical+Errors+in+Programs%3A+Advancing+Scalable+Analysis+of+Student+Misconceptions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10913，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10913&send_immediately=true&force_search=false)

**原文摘要:** In Computer Science (CS) education, understanding factors contributing to
students' programming difficulties is crucial for effective learning support.
By identifying specific issues students face, educators can provide targeted
assistance to help them overcome obstacles and improve learning outcomes. While
identifying sources of struggle, such as misconceptions, in real-time can be
challenging in current educational practices, analyzing logical errors in
students' code can offer valuable insights. This paper presents a scalable
framework for automatically detecting logical errors in students' programming
solutions. Our framework is based on an explainable Abstract Syntax Tree (AST)
embedding model, the Subtree-based Attention Neural Network (SANN), that
identifies the structural components of programs containing logical errors. We
conducted a series of experiments to evaluate its effectiveness, and the
results suggest that our framework can accurately capture students' logical
errors and, more importantly, provide us with deeper insights into their
learning processes, offering a valuable tool for enhancing programming
education.

</details>


### [47] [A Dataset for Spatiotemporal-Sensitive POI Question Answering](https://arxiv.org/abs/2505.10928)
*Xiao Han, Dayan Pan, Xiangyu Zhao, Xuyuan Hu, Zhaolin Deng, Xiangjie Kong, Guojiang Shen*

**主要类别:** cs.LG

**概要:** 本文提出POI-QA，一个基于兴趣点的新型时空敏感问答数据集，用于评估模型的时空推理能力，发现当前最先进的多语言大模型在此类任务上表现不佳。


<details>
  <summary>Details</summary>
  
**动机:** 现有问答数据集不足以评估模型的时空推理能力，因为它们缺乏足够的时空敏感问题。

**方法:** 通过三个关键步骤构建POI-QA数据集：1）挖掘并整合开源车辆轨迹数据与高精度地理POI数据；2）对噪声时空事实进行严格的人工验证；3）生成反映人类可理解时空推理任务的双语（中文/英文）问答对。

**结果:** POI-QA数据集挑战了模型解析复杂时空依赖的能力，测试显示即使是表现最好的模型（经过RAG+LoRA微调的Qwen2.5-7B）在最简单任务上的Top 10命中率（HR@10）仅为0.41，远低于人类的表现0.56。

**结论:** 现有的问答数据集缺乏足够的时空敏感问题，无法充分评估模型的时空推理能力。本文介绍了一个新的时空敏感问答数据集POI-QA，并通过实验证明了当前最先进的多语言大模型在处理复杂时空依赖时存在明显局限性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Dataset+for+Spatiotemporal-Sensitive+POI+Question+Answering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10928，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10928&send_immediately=true&force_search=false)

**原文摘要:** Spatiotemporal relationships are critical in data science, as many prediction
and reasoning tasks require analysis across both spatial and temporal
dimensions--for instance, navigating an unfamiliar city involves planning
itineraries that sequence locations and timing cultural experiences. However,
existing Question-Answering (QA) datasets lack sufficient
spatiotemporal-sensitive questions, making them inadequate benchmarks for
evaluating models' spatiotemporal reasoning capabilities. To address this gap,
we introduce POI-QA, a novel spatiotemporal-sensitive QA dataset centered on
Point of Interest (POI), constructed through three key steps: mining and
aligning open-source vehicle trajectory data from GAIA with high-precision
geographic POI data, rigorous manual validation of noisy spatiotemporal facts,
and generating bilingual (Chinese/English) QA pairs that reflect
human-understandable spatiotemporal reasoning tasks. Our dataset challenges
models to parse complex spatiotemporal dependencies, and evaluations of
state-of-the-art multilingual LLMs (e.g., Qwen2.5-7B, Llama3.1-8B) reveal stark
limitations: even the top-performing model (Qwen2.5-7B fine-tuned with
RAG+LoRA) achieves a top 10 Hit Ratio (HR@10) of only 0.41 on the easiest task,
far below human performance at 0.56. This underscores persistent weaknesses in
LLMs' ability to perform consistent spatiotemporal reasoning, while
highlighting POI-QA as a robust benchmark to advance algorithms sensitive to
spatiotemporal dynamics. The dataset is publicly available at
https://www.kaggle.com/ds/7394666.

</details>


### [48] [Physics-informed Temporal Alignment for Auto-regressive PDE Foundation Models](https://arxiv.org/abs/2505.10930)
*Congcong Zhu, Xiaoyan Xu, Jiayue Han, Jingrun Chen*

**主要类别:** cs.LG

**概要:** 提出了一种基于物理信息的时间对齐(PITA)自监督学习框架，以解决自动回归偏微分方程基础模型中的误差累积问题。


<details>
  <summary>Details</summary>
  
**动机:** 解决自动回归预测中存在的shortcut问题以及在分布外数据上的误差积累问题。

**方法:** 通过在自监督信号中整合物理信息约束来对齐不同时间步的物理动力学。

**结果:** 显著提高了现有基础模型在各种时变偏微分方程数据上的准确性和鲁棒性。

**结论:** 所提出的PITA框架展示了其在处理时变数据方面的潜力，并具有较强的泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Physics-informed+Temporal+Alignment+for+Auto-regressive+PDE+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10930，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10930&send_immediately=true&force_search=false)

**原文摘要:** Auto-regressive partial differential equation (PDE) foundation models have
shown great potential in handling time-dependent data. However, these models
suffer from the shortcut problem deeply rooted in auto-regressive prediction,
causing error accumulation. The challenge becomes particularly evident for
out-of-distribution data, as the pretraining performance may approach random
model initialization for downstream tasks with long-term dynamics. To deal with
this problem, we propose physics-informed temporal alignment (PITA), a
self-supervised learning framework inspired by inverse problem solving.
Specifically, PITA aligns the physical dynamics discovered at different time
steps on each given PDE trajectory by integrating physics-informed constraints
into the self-supervision signal. The alignment is derived from observation
data without relying on known physics priors, indicating strong generalization
ability to the out-of-distribution data. Extensive experiments show that PITA
significantly enhances the accuracy and robustness of existing foundation
models on diverse time-dependent PDE data. The code is available at
https://github.com/SCAILab-USTC/PITA.

</details>


### [49] [Privacy-Aware Lifelong Learning](https://arxiv.org/abs/2505.10941)
*Ozan Özdenizci, Elmar Rueckert, Robert Legenstein*

**主要类别:** cs.LG

**概要:** 提出了一种名为PALL的隐私感知终身学习方法，解决了在增量学习过程中防止灾难性遗忘、允许前向知识迁移的同时精确任务遗忘和最小化内存需求的问题。该方法通过优化特定任务的稀疏子网络并利用情景记忆重放机制，在图像分类任务上展示了其可扩展性。


<details>
  <summary>Details</summary>
  
**动机:** 使模型能够选择性地忘记敏感信息以遵守数据隐私法规。

**方法:** 优化任务特定的稀疏子网络，参数共享，并使用情景记忆重放机制。

**结果:** 在图像分类任务上展示了PALL的可扩展性，并提供了集成了终身学习和隐私感知遗忘机制的最先进解决方案。

**结论:** PALL提供了一个有效的框架来解决终身学习和隐私保护之间的矛盾目标。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Privacy-Aware+Lifelong+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10941，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10941&send_immediately=true&force_search=false)

**原文摘要:** Lifelong learning algorithms enable models to incrementally acquire new
knowledge without forgetting previously learned information. Contrarily, the
field of machine unlearning focuses on explicitly forgetting certain previous
knowledge from pretrained models when requested, in order to comply with data
privacy regulations on the right-to-be-forgotten. Enabling efficient lifelong
learning with the capability to selectively unlearn sensitive information from
models presents a critical and largely unaddressed challenge with contradicting
objectives. We address this problem from the perspective of simultaneously
preventing catastrophic forgetting and allowing forward knowledge transfer
during task-incremental learning, while ensuring exact task unlearning and
minimizing memory requirements, based on a single neural network model to be
adapted. Our proposed solution, privacy-aware lifelong learning (PALL),
involves optimization of task-specific sparse subnetworks with parameter
sharing within a single architecture. We additionally utilize an episodic
memory rehearsal mechanism to facilitate exact unlearning without performance
degradations. We empirically demonstrate the scalability of PALL across various
architectures in image classification, and provide a state-of-the-art solution
that uniquely integrates lifelong learning and privacy-aware unlearning
mechanisms for responsible AI applications.

</details>


### [50] [Certifying Stability of Reinforcement Learning Policies using Generalized Lyapunov Functions](https://arxiv.org/abs/2505.10947)
*Kehan Long, Jorge Cortés, Nikolay Atanasov*

**主要类别:** cs.LG

**概要:** 研究了从最优控制或强化学习得出的控制策略下的闭环系统稳定性认证问题，提出了通过学习广义Lyapunov函数来验证稳定性，并在多个基准上取得了成果，同时改进了稳定性证书的构建方式。


<details>
  <summary>Details</summary>
  
**动机:** 研究在从最优控制或强化学习（RL）得出的控制策略下的闭环系统的稳定性认证问题。由于对于学习到的控制策略难以构造严格的Lyapunov函数下降步骤，因此需要寻找新的方法。

**方法:** 在非线性设定下考虑并制定了通过在RL价值函数中加入神经网络残差项来学习广义Lyapunov函数的方法；扩展方法以联合训练神经控制器和稳定性证书使用多步Lyapunov损失。

**结果:** 成功验证了Gymnasium和DeepMind Control基准上的稳定性；通过联合训练神经控制器和稳定性证书，得到了比经典Lyapunov方法更大的认证吸引域内近似值。

**结论:** 提出了一种通过在RL价值函数中加入神经网络残差项来学习广义Lyapunov函数的方法，并成功验证了Gymnasium和DeepMind Control基准上的稳定性。此外，还扩展了方法以联合训练神经控制器和稳定性证书，获得了比经典Lyapunov方法更大的认证吸引域内近似值。整体上，该公式使得更易于构建证书，从而弥合了经典控制理论与现代基于学习的方法之间的差距。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Certifying+Stability+of+Reinforcement+Learning+Policies+using+Generalized+Lyapunov+Functions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10947，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10947&send_immediately=true&force_search=false)

**原文摘要:** We study the problem of certifying the stability of closed-loop systems under
control policies derived from optimal control or reinforcement learning (RL).
Classical Lyapunov methods require a strict step-wise decrease in the Lyapunov
function but such a certificate is difficult to construct for a learned control
policy. The value function associated with an RL policy is a natural Lyapunov
function candidate but it is not clear how it should be modified. To gain
intuition, we first study the linear quadratic regulator (LQR) problem and make
two key observations. First, a Lyapunov function can be obtained from the value
function of an LQR policy by augmenting it with a residual term related to the
system dynamics and stage cost. Second, the classical Lyapunov decrease
requirement can be relaxed to a generalized Lyapunov condition requiring only
decrease on average over multiple time steps. Using this intuition, we consider
the nonlinear setting and formulate an approach to learn generalized Lyapunov
functions by augmenting RL value functions with neural network residual terms.
Our approach successfully certifies the stability of RL policies trained on
Gymnasium and DeepMind Control benchmarks. We also extend our method to jointly
train neural controllers and stability certificates using a multi-step Lyapunov
loss, resulting in larger certified inner approximations of the region of
attraction compared to the classical Lyapunov approach. Overall, our
formulation enables stability certification for a broad class of systems with
learned policies by making certificates easier to construct, thereby bridging
classical control theory and modern learning-based methods.

</details>


### [51] [FP64 is All You Need: Rethinking Failure Modes in Physics-Informed Neural Networks](https://arxiv.org/abs/2505.10949)
*Chenhui Xu, Dancheng Liu, Amir Nassereldine, Jinjun Xiong*

**主要类别:** cs.LG

**概要:** This paper challenges the traditional understanding of Physics Informed Neural Networks (PINNs) failure modes, suggesting that insufficient arithmetic precision is the main issue instead of local optima.


<details>
  <summary>Details</summary>
  
**动机:** Investigate why PINNs often fail despite the PDE residual loss converging.

**方法:** Upgrade from FP32 to FP64 to observe changes in optimization.

**结果:** Upgrading to FP64 improves optimization, allowing PINNs to solve PDEs without failure modes.

**结论:** The study concludes that arithmetic precision is crucial for reliable PDE solving using neural networks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FP64+is+All+You+Need%3A+Rethinking+Failure+Modes+in+Physics-Informed+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10949，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10949&send_immediately=true&force_search=false)

**原文摘要:** Physics Informed Neural Networks (PINNs) often exhibit failure modes in which
the PDE residual loss converges while the solution error stays large, a
phenomenon traditionally blamed on local optima separated from the true
solution by steep loss barriers. We challenge this understanding by demonstrate
that the real culprit is insufficient arithmetic precision: with standard FP32,
the LBFGS optimizer prematurely satisfies its convergence test, freezing the
network in a spurious failure phase. Simply upgrading to FP64 rescues
optimization, enabling vanilla PINNs to solve PDEs without any failure modes.
These results reframe PINN failure modes as precision induced stalls rather
than inescapable local minima and expose a three stage training dynamic
unconverged, failure, success whose boundaries shift with numerical precision.
Our findings emphasize that rigorous arithmetic precision is the key to
dependable PDE solving with neural networks.

</details>


### [52] [Shackled Dancing: A Bit-Locked Diffusion Algorithm for Lossless and Controllable Image Steganography](https://arxiv.org/abs/2505.10950)
*Tianshuo Zhang, Gao Jia, Wenzhe Zhai, Rui Yann, Xianglei Xing*

**主要类别:** cs.LG

**概要:** 提出了一种名为Shackled Dancing Diffusion (SD$^2$) 的新方法，用于在生成过程中可控地嵌入信息，实现了100%的消息恢复准确性。该方法在安全性、嵌入容量和稳定性方面显著优于现有方法。


<details>
  <summary>Details</summary>
  
**动机:** 现有的空间域和频率域数据隐藏方法在安全、容量和感知质量之间存在权衡。虽然生成模型（尤其是扩散模型）的进步提供了新的可能性，但在生成过程中精确嵌入信息仍具挑战性。

**方法:** 结合位位置锁定与扩散采样注入，开发了一个名为Shackled Dancing Diffusion (SD$^2$) 的插件式生成隐藏方法，利用扩散模型的表达能力合成多样化的载体图像并实现全消息恢复。

**结果:** 实验表明，SD$^2$在安全性、嵌入容量和稳定性上大幅超越先前的方法，同时保持了图像保真度。

**结论:** SD$^2$为可控生成提供了新的见解，并为安全视觉通信开辟了有前景的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Shackled+Dancing%3A+A+Bit-Locked+Diffusion+Algorithm+for+Lossless+and+Controllable+Image+Steganography，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10950，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10950&send_immediately=true&force_search=false)

**原文摘要:** Data steganography aims to conceal information within visual content, yet
existing spatial- and frequency-domain approaches suffer from trade-offs
between security, capacity, and perceptual quality. Recent advances in
generative models, particularly diffusion models, offer new avenues for
adaptive image synthesis, but integrating precise information embedding into
the generative process remains challenging. We introduce Shackled Dancing
Diffusion, or SD$^2$, a plug-and-play generative steganography method that
combines bit-position locking with diffusion sampling injection to enable
controllable information embedding within the generative trajectory. SD$^2$
leverages the expressive power of diffusion models to synthesize diverse
carrier images while maintaining full message recovery with $100\%$ accuracy.
Our method achieves a favorable balance between randomness and constraint,
enhancing robustness against steganalysis without compromising image fidelity.
Extensive experiments show that SD$^2$ substantially outperforms prior methods
in security, embedding capacity, and stability. This algorithm offers new
insights into controllable generation and opens promising directions for secure
visual communication.

</details>


### [53] [SubGCache: Accelerating Graph-based RAG with Subgraph-level KV Cache](https://arxiv.org/abs/2505.10951)
*Qiuyu Zhu, Liang Zhang, Qianxiong Xu, Cheng Long, Jie Zhang*

**主要类别:** cs.LG

**概要:** 提出SubGCache方法通过重用相似结构查询的计算来减少基于图的大语言模型推理延迟，并在多个数据集和框架上验证了其有效性。


<details>
  <summary>Details</summary>
  
**动机:** 观察到不同查询可能检索到相似子图作为提示，希望减少推理延迟。

**方法:** 提出SubGCache方法，包括聚类查询、构建代表子图并预计算KV缓存，对于属于同一簇的查询重用代表子图的预计算KV缓存。

**结果:** 在两个新数据集上实验显示SubGCache能有效减少推理延迟且生成质量不降反升，时间-首个标记（TTFT）最多可减少6.68倍。

**结论:** SubGCache是一种有效的减少基于图的大语言模型推理延迟的方法，同时保持甚至提升生成质量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SubGCache%3A+Accelerating+Graph-based+RAG+with+Subgraph-level+KV+Cache，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10951，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10951&send_immediately=true&force_search=false)

**原文摘要:** Graph-based retrieval-augmented generation (RAG) enables large language
models (LLMs) to incorporate structured knowledge via graph retrieval as
contextual input, enhancing more accurate and context-aware reasoning. We
observe that for different queries, it could retrieve similar subgraphs as
prompts, and thus we propose SubGCache, which aims to reduce inference latency
by reusing computation across queries with similar structural prompts (i.e.,
subgraphs). Specifically, SubGCache clusters queries based on subgraph
embeddings, constructs a representative subgraph for each cluster, and
pre-computes the key-value (KV) cache of the representative subgraph. For each
query with its retrieved subgraph within a cluster, it reuses the pre-computed
KV cache of the representative subgraph of the cluster without computing the KV
tensors again for saving computation. Experiments on two new datasets across
multiple LLM backbones and graph-based RAG frameworks demonstrate that
SubGCache consistently reduces inference latency with comparable and even
improved generation quality, achieving up to 6.68$\times$ reduction in
time-to-first-token (TTFT).

</details>


### [54] [Constrained Preferential Bayesian Optimization and Its Application in Banner Ad Design](https://arxiv.org/abs/2505.10954)
*Koki Iwai, Yusuke Kumagae, Yuki Koyama, Masahiro Hamasaki, Masataka Goto*

**主要类别:** cs.LG

**概要:** This paper proposes Constrained Preferential Bayesian Optimization (CPBO) to handle inequality constraints in human-in-the-loop optimization tasks, introducing a novel acquisition function and demonstrating its effectiveness in a banner ad design application.


<details>
  <summary>Details</summary>
  
**动机:** Existing Preferential Bayesian Optimization (PBO) methods do not address inequality constraints, which are common in real-world optimization tasks.

**方法:** CPBO extends PBO by incorporating inequality constraints through a novel acquisition function.

**结果:** Technical evaluation shows CPBO successfully identifies optimal solutions in feasible regions.

**结论:** The proposed CPBO method demonstrates potential in guiding creative design under real-world constraints.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Constrained+Preferential+Bayesian+Optimization+and+Its+Application+in+Banner+Ad+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10954，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10954&send_immediately=true&force_search=false)

**原文摘要:** Preferential Bayesian optimization (PBO) is a variant of Bayesian
optimization that observes relative preferences (e.g., pairwise comparisons)
instead of direct objective values, making it especially suitable for
human-in-the-loop scenarios. However, real-world optimization tasks often
involve inequality constraints, which existing PBO methods have not yet
addressed. To fill this gap, we propose constrained preferential Bayesian
optimization (CPBO), an extension of PBO that incorporates inequality
constraints for the first time. Specifically, we present a novel acquisition
function for this purpose. Our technical evaluation shows that our CPBO method
successfully identifies optimal solutions by focusing on exploring feasible
regions. As a practical application, we also present a designer-in-the-loop
system for banner ad design using CPBO, where the objective is the designer's
subjective preference, and the constraint ensures a target predicted
click-through rate. We conducted a user study with professional ad designers,
demonstrating the potential benefits of our approach in guiding creative design
under real-world constraints.

</details>


### [55] [Relational Graph Transformer](https://arxiv.org/abs/2505.10960)
*Vijay Prakash Dwivedi, Sri Jaladi, Yangyi Shen, Federico López, Charilaos I. Kanatsoulis, Rishi Puri, Matthias Fey, Jure Leskovec*

**主要类别:** cs.LG

**概要:** Relational Graph Transformer (RelGT) is proposed for relational tables, improving performance by up to 18% compared to traditional Graph Neural Networks.


<details>
  <summary>Details</summary>
  
**动机:** Traditional Graph Neural Networks struggle to capture complex patterns and long-range dependencies in relational data. Graph Transformers show promise but face challenges when applied to relational entity graphs.

**方法:** Introduces RelGT, a new graph transformer architecture using a multi-element tokenization strategy to handle heterogeneity, temporality, and topology efficiently without expensive precomputation. Combines local and global attention mechanisms.

**结果:** Performs better than GNN baselines across 21 tasks from the RelBench benchmark, improving by up to 18%.

**结论:** Establishes Graph Transformers as a strong architecture for Relational Deep Learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Relational+Graph+Transformer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10960，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10960&send_immediately=true&force_search=false)

**原文摘要:** Relational Deep Learning (RDL) is a promising approach for building
state-of-the-art predictive models on multi-table relational data by
representing it as a heterogeneous temporal graph. However, commonly used Graph
Neural Network models suffer from fundamental limitations in capturing complex
structural patterns and long-range dependencies that are inherent in relational
data. While Graph Transformers have emerged as powerful alternatives to GNNs on
general graphs, applying them to relational entity graphs presents unique
challenges: (i) Traditional positional encodings fail to generalize to massive,
heterogeneous graphs; (ii) existing architectures cannot model the temporal
dynamics and schema constraints of relational data; (iii) existing tokenization
schemes lose critical structural information. Here we introduce the Relational
Graph Transformer (RelGT), the first graph transformer architecture designed
specifically for relational tables. RelGT employs a novel multi-element
tokenization strategy that decomposes each node into five components (features,
type, hop distance, time, and local structure), enabling efficient encoding of
heterogeneity, temporality, and topology without expensive precomputation. Our
architecture combines local attention over sampled subgraphs with global
attention to learnable centroids, incorporating both local and database-wide
representations. Across 21 tasks from the RelBench benchmark, RelGT
consistently matches or outperforms GNN baselines by up to 18%, establishing
Graph Transformers as a powerful architecture for Relational Deep Learning.

</details>


### [56] [Group-in-Group Policy Optimization for LLM Agent Training](https://arxiv.org/abs/2505.10978)
*Lang Feng, Zhenghai Xue, Tingcong Liu, Bo An*

**主要类别:** cs.LG

**概要:** 提出了一种新的分组强化学习算法GiGPO，该算法实现了细粒度的信用分配，同时保持了分组强化学习的优点，并在两个具有挑战性的代理基准测试中表现出色。


<details>
  <summary>Details</summary>
  
**动机:** 现有的分组强化学习方法在处理长期目标的大语言模型训练时存在局限性，尤其是在稀疏或延迟奖励的情况下，信用分配变得更具挑战性。

**方法:** 提出了GiGPO算法，该算法通过两层结构来估计相对优势，即在episode级别和step级别上分别计算宏观和微观的相对优势。

**结果:** GiGPO算法在ALFWorld和WebShop两个基准测试中，相较于GRPO基线，在性能上分别提升了超过12%和9%，并且保持了相同的GPU内存开销和相同的LLM rollout，几乎不增加额外的时间成本。

**结论:** GiGPO成功地实现了细粒度的信用分配，为大语言模型的长期目标训练提供了一个有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Group-in-Group+Policy+Optimization+for+LLM+Agent+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10978，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10978&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in group-based reinforcement learning (RL) have driven
frontier large language models (LLMs) in single-turn tasks like mathematical
reasoning. However, their scalability to long-horizon LLM agent training
remains limited. Unlike static tasks, agent-environment interactions unfold
over many steps and often yield sparse or delayed rewards, making credit
assignment across individual steps significantly more challenging. In this
work, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL
algorithm that achieves fine-grained credit assignment for LLM agents while
preserving the appealing properties of group-based RL: critic-free, low memory,
and stable convergence. GiGPO introduces a two-level structure for estimating
relative advantage: (i) At the episode-level, GiGPO computes macro relative
advantages based on groups of complete trajectories; (ii) At the step-level,
GiGPO introduces an anchor state grouping mechanism that retroactively
constructs step-level groups by identifying repeated environment states across
trajectories. Actions stemming from the same state are grouped together,
enabling micro relative advantage estimation. This hierarchical structure
effectively captures both global trajectory quality and local step
effectiveness without relying on auxiliary models or additional rollouts. We
evaluate GiGPO on two challenging agent benchmarks, ALFWorld and WebShop, using
Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct. Crucially, GiGPO delivers
fine-grained per-step credit signals and achieves performance gains of > 12\%
on ALFWorld and > 9\% on WebShop over the GRPO baseline: all while maintaining
the same GPU memory overhead, identical LLM rollout, and incurring little to no
additional time cost.

</details>


### [57] [GenoArmory: A Unified Evaluation Framework for Adversarial Attacks on Genomic Foundation Models](https://arxiv.org/abs/2505.10983)
*Haozheng Luo, Chenghao Qiu, Yimin Wang, Shang Wu, Jiahao Yu, Han Liu, Binghui Wang, Yan Chen*

**主要类别:** cs.LG

**概要:** 提出首个针对基因组基础模型（GFMs）的统一对抗性攻击基准——GenoArmory，评估了五种最先进GFMs的鲁棒性，并引入新的对抗样本数据集GenoAdv。


<details>
  <summary>Details</summary>
  
**动机:** 现有GFMs基准未提供全面评估框架，GenoArmory填补此空白。

**方法:** 使用四种常用攻击算法和三种防御策略评估GFMs的鲁棒性。

**结果:** 分类模型比生成模型对对抗扰动更鲁棒；对抗攻击常针对生物显著的基因组区域。

**结论:** GenoArmory提供了分析GFMs脆弱性的综合框架，有助于提高模型安全性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GenoArmory%3A+A+Unified+Evaluation+Framework+for+Adversarial+Attacks+on+Genomic+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10983，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10983&send_immediately=true&force_search=false)

**原文摘要:** We propose the first unified adversarial attack benchmark for Genomic
Foundation Models (GFMs), named GenoArmory. Unlike existing GFM benchmarks,
GenoArmory offers the first comprehensive evaluation framework to
systematically assess the vulnerability of GFMs to adversarial attacks.
Methodologically, we evaluate the adversarial robustness of five
state-of-the-art GFMs using four widely adopted attack algorithms and three
defense strategies. Importantly, our benchmark provides an accessible and
comprehensive framework to analyze GFM vulnerabilities with respect to model
architecture, quantization schemes, and training datasets. Additionally, we
introduce GenoAdv, a new adversarial sample dataset designed to improve GFM
safety. Empirically, classification models exhibit greater robustness to
adversarial perturbations compared to generative models, highlighting the
impact of task type on model vulnerability. Moreover, adversarial attacks
frequently target biologically significant genomic regions, suggesting that
these models effectively capture meaningful sequence features.

</details>


### [58] [ReaCritic: Large Reasoning Transformer-based DRL Critic-model Scaling For Heterogeneous Networks](https://arxiv.org/abs/2505.10992)
*Feiran You, Hongyang Du*

**主要类别:** cs.LG

**概要:** 提出ReaCritic，一种基于大型推理变压器的批评者模型扩展方案，将其应用于深度强化学习中以提高动态无线环境中的泛化能力。实验表明，ReaCritic在HetNet和OpenAI Gym控制任务中提高了收敛速度和最终性能。


<details>
  <summary>Details</summary>
  
**动机:** 现有深度强化学习方法因面对异构网络中用户需求多样性和无线条件时变性而受到限制。传统批评者模型难以处理多任务复杂性。

**方法:** 提出ReaCritic，通过水平推理和平行状态-动作输入以及垂直推理通过深度变压器堆栈来实现推理能力。

**结果:** ReaCritic在各种HetNet设置和标准OpenAI Gym控制任务中提高了收敛速度和最终性能。

**结论:** ReaCritic增强了深度强化学习在动态无线环境中的泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ReaCritic%3A+Large+Reasoning+Transformer-based+DRL+Critic-model+Scaling+For+Heterogeneous+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10992，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10992&send_immediately=true&force_search=false)

**原文摘要:** Heterogeneous Networks (HetNets) pose critical challenges for intelligent
management due to the diverse user requirements and time-varying wireless
conditions. These factors introduce significant decision complexity, which
limits the adaptability of existing Deep Reinforcement Learning (DRL) methods.
In many DRL algorithms, especially those involving value-based or actor-critic
structures, the critic component plays a key role in guiding policy learning by
estimating value functions. However, conventional critic models often use
shallow architectures that map observations directly to scalar estimates,
limiting their ability to handle multi-task complexity. In contrast, recent
progress in inference-time scaling of Large Language Models (LLMs) has shown
that generating intermediate reasoning steps can significantly improve decision
quality. Motivated by this, we propose ReaCritic, a large reasoning
transformer-based criticmodel scaling scheme that brings reasoning ability into
DRL. ReaCritic performs horizontal reasoning over parallel state-action inputs
and vertical reasoning through deep transformer stacks. It is compatible with a
broad range of value-based and actor-critic DRL algorithms and enhances
generalization in dynamic wireless environments. Extensive experiments
demonstrate that ReaCritic improves convergence speed and final performance
across various HetNet settings and standard OpenAI Gym control tasks.

</details>


### [59] [Logo-LLM: Local and Global Modeling with Large Language Models for Time Series Forecasting](https://arxiv.org/abs/2505.11017)
*Wenjie Ou, Zhishuo Zhao, Dongyue Guo, Yi Lin*

**主要类别:** cs.LG

**概要:** 提出Logo-LLM框架，通过从预训练大语言模型的不同层提取多尺度时间特征来改进时间序列预测。


<details>
  <summary>Details</summary>
  
**动机:** 现有基于Transformer的方法和适应大语言模型的时间序列预测方法忽略了短期局部变化，且大语言模型作为黑盒编码器时未能充分利用分层表示。

**方法:** 提出Logo-LLM框架，引入轻量级Local-Mixer和Global-Mixer模块，并展示LLM浅层捕获局部动态而深层编码全局趋势。

**结果:** 在多个基准数据集上表现优异，在少量和零样本设置下具有强泛化能力且计算开销低。

**结论:** Logo-LLM解决了现有方法忽视短期局部变化的问题，提高了时间序列预测的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Logo-LLM%3A+Local+and+Global+Modeling+with+Large+Language+Models+for+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11017，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11017&send_immediately=true&force_search=false)

**原文摘要:** Time series forecasting is critical across multiple domains, where time
series data exhibits both local patterns and global dependencies. While
Transformer-based methods effectively capture global dependencies, they often
overlook short-term local variations in time series. Recent methods that adapt
large language models (LLMs) into time series forecasting inherit this
limitation by treating LLMs as black-box encoders, relying solely on the
final-layer output and underutilizing hierarchical representations. To address
this limitation, we propose Logo-LLM, a novel LLM-based framework that
explicitly extracts and models multi-scale temporal features from different
layers of a pre-trained LLM. Through empirical analysis, we show that shallow
layers of LLMs capture local dynamics in time series, while deeper layers
encode global trends. Moreover, Logo-LLM introduces lightweight Local-Mixer and
Global-Mixer modules to align and integrate features with the temporal input
across layers. Extensive experiments demonstrate that Logo-LLM achieves
superior performance across diverse benchmarks, with strong generalization in
few-shot and zero-shot settings while maintaining low computational overhead.

</details>


### [60] [Informed, but Not Always Improved: Challenging the Benefit of Background Knowledge in GNNs](https://arxiv.org/abs/2505.11023)
*Kutalmış Coşkun, Ivo Kavisanczki, Amin Mirzaei, Tom Siegl, Bjarne C. Hiller, Stefan Lüdtke, Martin Becker*

**主要类别:** cs.LG

**概要:** This study investigates the role of background knowledge (BK) in cancer subtype classification using graph-based machine learning methods.


<details>
  <summary>Details</summary>
  
**动机:** To understand the actual contribution and the impact of imperfect knowledge in BK graphs.

**方法:** Investigating the role of BK in cancer subtype classification and introducing an evaluation framework with synthetic and perturbed BK graphs.

**结果:** State-of-the-art GNNs using BK perform no better than linear regression and their performance remains unchanged when the BK graph is heavily perturbed.

**结论:** Careful alignment of GNN architectures and BK characteristics is necessary for significant performance improvements.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Informed%2C+but+Not+Always+Improved%3A+Challenging+the+Benefit+of+Background+Knowledge+in+GNNs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11023，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11023&send_immediately=true&force_search=false)

**原文摘要:** In complex and low-data domains such as biomedical research, incorporating
background knowledge (BK) graphs, such as protein-protein interaction (PPI)
networks, into graph-based machine learning pipelines is a promising research
direction. However, while BK is often assumed to improve model performance, its
actual contribution and the impact of imperfect knowledge remain poorly
understood. In this work, we investigate the role of BK in an important
real-world task: cancer subtype classification. Surprisingly, we find that (i)
state-of-the-art GNNs using BK perform no better than uninformed models like
linear regression, and (ii) their performance remains largely unchanged even
when the BK graph is heavily perturbed. To understand these unexpected results,
we introduce an evaluation framework, which employs (i) a synthetic setting
where the BK is clearly informative and (ii) a set of perturbations that
simulate various imperfections in BK graphs. With this, we test the robustness
of BK-aware models in both synthetic and real-world biomedical settings. Our
findings reveal that careful alignment of GNN architectures and BK
characteristics is necessary but holds the potential for significant
performance improvements.

</details>


### [61] [Leveraging Real-Time Data Analysis and Multiple Kernel Learning for Manufacturing of Innovative Steels](https://arxiv.org/abs/2505.11024)
*Wolfgang Rannetbauer, Simon Hubmer, Carina Hambrock, Ronny Ramlau*

**主要类别:** cs.LG

**概要:** This paper proposes an updated coating process for thermally sprayed components in steel manufacturing by integrating real-time data analytics and predictive quality management.


<details>
  <summary>Details</summary>
  
**动机:** To address the challenges in production and plant maintenance of thermally sprayed components in steel manufacturing due to standardization in the refurbishment process.

**方法:** Integrating real-time data analytics and predictive quality management by designing two essential components: the data aggregator and the quality predictor.

**结果:** The combination of the data aggregator and the quality predictor was verified using small-scale tests, enabling accurate prediction of coating quality and proactive notification to the operator.

**结论:** Updating the established coating process for thermally spray coated components in steel manufacturing can enhance performance through specialized surface properties while meeting dynamic demands.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leveraging+Real-Time+Data+Analysis+and+Multiple+Kernel+Learning+for+Manufacturing+of+Innovative+Steels，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11024，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11024&send_immediately=true&force_search=false)

**原文摘要:** The implementation of thermally sprayed components in steel manufacturing
presents challenges for production and plant maintenance. While enhancing
performance through specialized surface properties, these components may
encounter difficulties in meeting modified requirements due to standardization
in the refurbishment process. This article proposes updating the established
coating process for thermally spray coated components for steel manufacturing
(TCCSM) by integrating real-time data analytics and predictive quality
management. Two essential components--the data aggregator and the quality
predictor--are designed through continuous process monitoring and the
application of data-driven methodologies to meet the dynamic demands of the
evolving steel landscape. The quality predictor is powered by the simple and
effective multiple kernel learning strategy with the goal of realizing
predictive quality. The data aggregator, designed with sensors, flow meters,
and intelligent data processing for the thermal spray coating process, is
proposed to facilitate real-time analytics. The performance of this combination
was verified using small-scale tests that enabled not only the accurate
prediction of coating quality based on the collected data but also proactive
notification to the operator as soon as significant deviations are identified.

</details>


### [62] [Exploiting the Asymmetric Uncertainty Structure of Pre-trained VLMs on the Unit Hypersphere](https://arxiv.org/abs/2505.11029)
*Li Ju, Max Andersson, Stina Fredriksson, Edward Glöckner, Andreas Hellander, Ekta Vats, Prashant Singh*

**主要类别:** cs.LG

**概要:** 提出了一种新的方法AsymVLM来解决现有视觉-语言模型在处理不确定性时的不足，通过在单位超球面上构建概率嵌入，实现了不确定性量化，并在基准数据集上验证了其有效性。


<details>
  <summary>Details</summary>
  
**动机:** 现有的确定性视觉-语言模型无法捕捉自然语言和视觉数据中的内在模糊性和不确定性，而现有的概率后适配方法也未能充分考虑模态之间的非对称不确定性结构以及有意义的确定性嵌入位于单位超球面上的约束条件。

**方法:** 提出了一种新的方法AsymVLM，该方法能够在预训练的视觉-语言模型的基础上，在单位超球面上构建概率嵌入，从而实现不确定性量化。

**结果:** 在已建立的基准数据集上验证了概率嵌入的有效性，并进行了全面的消融研究，展示了文本和视觉数据不确定性结构中非对称性的固有性质。

**结论:** 本文提出的方法能够有效解决现有视觉-语言模型在处理不确定性时的不足，通过在单位超球面上构建概率嵌入，实现了不确定性量化。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploiting+the+Asymmetric+Uncertainty+Structure+of+Pre-trained+VLMs+on+the+Unit+Hypersphere，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11029，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11029&send_immediately=true&force_search=false)

**原文摘要:** Vision-language models (VLMs) as foundation models have significantly
enhanced performance across a wide range of visual and textual tasks, without
requiring large-scale training from scratch for downstream tasks. However,
these deterministic VLMs fail to capture the inherent ambiguity and uncertainty
in natural language and visual data. Recent probabilistic post-hoc adaptation
methods address this by mapping deterministic embeddings onto probability
distributions; however, existing approaches do not account for the asymmetric
uncertainty structure of the modalities, and the constraint that meaningful
deterministic embeddings reside on a unit hypersphere, potentially leading to
suboptimal performance. In this paper, we address the asymmetric uncertainty
structure inherent in textual and visual data, and propose AsymVLM to build
probabilistic embeddings from pre-trained VLMs on the unit hypersphere,
enabling uncertainty quantification. We validate the effectiveness of the
probabilistic embeddings on established benchmarks, and present comprehensive
ablation studies demonstrating the inherent nature of asymmetry in the
uncertainty structure of textual and visual data.

</details>


### [63] [Deep Latent Variable Model based Vertical Federated Learning with Flexible Alignment and Labeling Scenarios](https://arxiv.org/abs/2505.11035)
*Kihun Hong, Sejun Park, Ganguk Hwang*

**主要类别:** cs.LG

**概要:** This paper proposes a new VFL framework that can handle arbitrary data alignment, unlabeled data, and multi-party collaboration.


<details>
  <summary>Details</summary>
  
**动机:** Existing VFL methods have restrictive assumptions, such as a small number of participants or fully aligned data.

**方法:** Reinterpreting alignment gaps in VFL as missing data problems and proposing a unified framework that accommodates both training and inference under arbitrary alignment and labeling scenarios.

**结果:** The proposed method outperforms all baselines in 160 out of 168 cases with an average gap of 9.6 percentage points.

**结论:** This is the first VFL framework to jointly handle arbitrary data alignment, unlabeled data, and multi-party collaboration.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep+Latent+Variable+Model+based+Vertical+Federated+Learning+with+Flexible+Alignment+and+Labeling+Scenarios，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11035，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11035&send_immediately=true&force_search=false)

**原文摘要:** Federated learning (FL) has attracted significant attention for enabling
collaborative learning without exposing private data. Among the primary
variants of FL, vertical federated learning (VFL) addresses feature-partitioned
data held by multiple institutions, each holding complementary information for
the same set of users. However, existing VFL methods often impose restrictive
assumptions such as a small number of participating parties, fully aligned
data, or only using labeled data. In this work, we reinterpret alignment gaps
in VFL as missing data problems and propose a unified framework that
accommodates both training and inference under arbitrary alignment and labeling
scenarios, while supporting diverse missingness mechanisms. In the experiments
on 168 configurations spanning four benchmark datasets, six training-time
missingness patterns, and seven testing-time missingness patterns, our method
outperforms all baselines in 160 cases with an average gap of 9.6 percentage
points over the next-best competitors. To the best of our knowledge, this is
the first VFL framework to jointly handle arbitrary data alignment, unlabeled
data, and multi-party collaboration all at once.

</details>


### [64] [Efficient Attention via Pre-Scoring: Prioritizing Informative Keys in Transformers](https://arxiv.org/abs/2505.11040)
*Zhexiang Li, Haoyu Wang, Yutong Bao, David Woodruff*

**主要类别:** cs.LG

**概要:** 提出了一种预评分机制来增强HyperAttention的能力，在保持速度的同时提高了建模准确性。


<details>
  <summary>Details</summary>
  
**动机:** 解决HyperAttention中采样限制关键键捕捉的问题，提高整体困惑度。

**方法:** 引入三种评分方法：K-means聚类、K-median聚类和基于杠杆得分的排名，并完全替换了HyperAttention的原始均匀残差采样，仅依赖于我们的预评分机制。

**结果:** 在ChatGLM2上，困惑度从12降低到8.3；在Vision-Transformer (ViT) 上，保证了与LevAttention相似的准确性，并且在特定参数下超过了LevAttention。

**结论:** 通过在分层注意力机制中整合预评分，显著提高了Transformer的效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Attention+via+Pre-Scoring%3A+Prioritizing+Informative+Keys+in+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11040，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11040&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in transformer architectures deeply enhance long-context
language modeling. Among them, HyperAttention achieves competitive efficiency
by combining a single-level LSH-based clustering with uniform residual
sampling. However,such a sampling limits crucial keys' capturing, which in turn
raises the overall perplexity. In this paper, we propose a pre-scoring
mechanism to assist HyperAttention to prioritize significant keys.
Specifically, we introduce three scoring methods: K-means clustering, K-median
clustering, and leverage score-based ranking (inspired by LevAttention) to
filter keys effectively. We further replace HyperAttention's original uniform
residual sampling entirely, relying exclusively on our pre-scoring mechanism.
Experiments on ChatGLM2 (131k token context) reduce perplexity from 12 to 8.3,
which outperforms standard HyperAttention. Moreover, when running on the
Vision-Transformer (ViT), our method shows that it can guarantee similar
accuracy compared with LevAttention, and will surpass LevAttention given
specific parameters. Although this method introduces computational overhead,
its combination with HyperAttention remains 20 times faster than
FlashAttention, providing a balanced trade-off between speed and modeling
accuracy. Our results highlight the effectiveness of integrating pre-scoring
into hierarchical attention mechanisms, significantly improving Transformer's
efficiency.

</details>


### [65] [Exploration by Random Distribution Distillation](https://arxiv.org/abs/2505.11044)
*Zhirui Fang, Kai Yang, Jian Tao, Jiafei Lyu, Lusong Li, Li Shen, Xiu Li*

**主要类别:** cs.LG

**概要:** 提出一种名为RDD的新方法，通过将目标网络输出视为正态分布样本来促进更广泛的探索。该方法结合了基于计数和预测误差的方法的优点，并在多个环境中表现出色。


<details>
  <summary>Details</summary>
  
**动机:** 在线强化学习中的探索是一个关键挑战，当前主要的探索算法有基于计数的方法和好奇心驱动的方法，但存在局限性。

**方法:** 提出随机分布蒸馏（RDD）方法，通过目标网络输出的不确定性引入内在奖励，同时确保探索衰减和预测器收敛。

**结果:** RDD在高维空间中保持了预测误差方法的优势，并实现了类似于伪计数方法的内在奖励衰减模式。实验表明RDD在多个环境中优于其他先进方法。

**结论:** RDD有效统一了基于计数和预测误差的方法，理论分析和实验结果证明其在提高在线探索方面的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploration+by+Random+Distribution+Distillation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11044，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11044&send_immediately=true&force_search=false)

**原文摘要:** Exploration remains a critical challenge in online reinforcement learning, as
an agent must effectively explore unknown environments to achieve high returns.
Currently, the main exploration algorithms are primarily count-based methods
and curiosity-based methods, with prediction-error methods being a prominent
example. In this paper, we propose a novel method called \textbf{R}andom
\textbf{D}istribution \textbf{D}istillation (RDD), which samples the output of
a target network from a normal distribution. RDD facilitates a more extensive
exploration by explicitly treating the difference between the prediction
network and the target network as an intrinsic reward. Furthermore, by
introducing randomness into the output of the target network for a given state
and modeling it as a sample from a normal distribution, intrinsic rewards are
bounded by two key components: a pseudo-count term ensuring proper exploration
decay and a discrepancy term accounting for predictor convergence. We
demonstrate that RDD effectively unifies both count-based and prediction-error
approaches. It retains the advantages of prediction-error methods in
high-dimensional spaces, while also implementing an intrinsic reward decay mode
akin to the pseudo-count method. In the experimental section, RDD is compared
with more advanced methods in a series of environments. Both theoretical
analysis and experimental results confirm the effectiveness of our approach in
improving online exploration for reinforcement learning tasks.

</details>


### [66] [Halting Recurrent GNNs and the Graded $μ$-Calculus](https://arxiv.org/abs/2505.11050)
*Jeroen Bollen, Jan Van den Bussche, Stijn Vansummeren, Jonni Virtema*

**主要类别:** cs.LG

**概要:** This paper introduces a halting mechanism for recurrent Graph Neural Networks (GNNs), proving its expressive power in relation to logics and developing a new approximate semantics for graded modal mu-calculus, leading to a new model-checking algorithm called the counting algorithm.


<details>
  <summary>Details</summary>
  
**动机:** To address the issue of termination guarantees and expressive power limitations in current recurrent GNNs, especially when dealing with varying graph sizes.

**方法:** Proposing a halting mechanism for recurrent GNNs and developing a new approximate semantics for graded modal mu-calculus.

**结果:** The halting model can express all node classifiers definable in graded modal mu-calculus, even for standard GNN variants oblivious to graph size. It also leads to a new model-checking algorithm called the counting algorithm.

**结论:** Recurrent GNNs restricted to node classifiers definable in monadic second-order logic can only express those definable in graded modal mu-calculus.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Halting+Recurrent+GNNs+and+the+Graded+%24%CE%BC%24-Calculus，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11050，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11050&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) are a class of machine-learning models that
operate on graph-structured data. Their expressive power is intimately related
to logics that are invariant under graded bisimilarity. Current proposals for
recurrent GNNs either assume that the graph size is given to the model, or
suffer from a lack of termination guarantees. In this paper, we propose a
halting mechanism for recurrent GNNs. We prove that our halting model can
express all node classifiers definable in graded modal mu-calculus, even for
the standard GNN variant that is oblivious to the graph size. A recent
breakthrough in the study of the expressivity of graded modal mu-calculus in
the finite suggests that conversely, restricted to node classifiers definable
in monadic second-order logic, recurrent GNNs can express only node classifiers
definable in graded modal mu-calculus. To prove our main result, we develop a
new approximate semantics for graded mu-calculus, which we believe to be of
independent interest. We leverage this new semantics into a new model-checking
algorithm, called the counting algorithm, which is oblivious to the graph size.
In a final step we show that the counting algorithm can be implemented on a
halting recurrent GNN.

</details>


### [67] [Assessing the Performance of Analog Training for Transfer Learning](https://arxiv.org/abs/2505.11067)
*Omobayode Fagbohungbe, Corey Lammie, Malte J. Rasch, Takashi Ando, Tayfun Gokmen, Vijay Narayanan*

**主要类别:** cs.LG

**概要:** This paper introduces and evaluates a new algorithm called c-TTv2 for analog in-memory computing, focusing on its application in transfer learning with a Swin-ViT model on CIFAR100 dataset.


<details>
  <summary>Details</summary>
  
**动机:** To overcome limitations of existing training algorithms due to asymmetric and non-linear behavior of analog memory devices.

**方法:** Introduces c-TTv2 algorithm leveraging chopped technique to handle challenges related to analog memory devices.

**结果:** Assesses the performance of c-TTv2 on a subset of CIFAR100 dataset using Swin-ViT model and investigates its robustness against variations in device specifications.

**结论:** The study shows the effectiveness of the c-TTv2 algorithm in improving training outcomes for analog in-memory computing.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Assessing+the+Performance+of+Analog+Training+for+Transfer+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11067，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11067&send_immediately=true&force_search=false)

**原文摘要:** Analog in-memory computing is a next-generation computing paradigm that
promises fast, parallel, and energy-efficient deep learning training and
transfer learning (TL). However, achieving this promise has remained elusive
due to a lack of suitable training algorithms. Analog memory devices exhibit
asymmetric and non-linear switching behavior in addition to device-to-device
variation, meaning that most, if not all, of the current off-the-shelf training
algorithms cannot achieve good training outcomes. Also, recently introduced
algorithms have enjoyed limited attention, as they require bi-directionally
switching devices of unrealistically high symmetry and precision and are highly
sensitive. A new algorithm chopped TTv2 (c-TTv2), has been introduced, which
leverages the chopped technique to address many of the challenges mentioned
above. In this paper, we assess the performance of the c-TTv2 algorithm for
analog TL using a Swin-ViT model on a subset of the CIFAR100 dataset. We also
investigate the robustness of our algorithm to changes in some device
specifications, including weight transfer noise, symmetry point skew, and
symmetry point variability

</details>


### [68] [Addition is almost all you need: Compressing neural networks with double binary factorization](https://arxiv.org/abs/2505.11076)
*Vladimír Boža, Vladimír Macko*

**主要类别:** cs.LG

**概要:** Double Binary Factorization (DBF)是一种新的方法，通过将权重矩阵分解为两个二值矩阵的乘积来提高大语言模型的计算效率，同时保持与现有方法相当甚至更好的压缩率。在1比特/权重范围内，DBF优于现有的二值化方法；在2比特/权重范围内，它与最先进的量化方法竞争。此外，DBF允许通过调整分解的中间维度来实现细粒度的压缩比控制，并提出了一种基于通道剪枝标准的非均匀层间压缩比估计算法。


<details>
  <summary>Details</summary>
  
**动机:** 当前大语言模型面临日益增长的计算和存储需求，而二值化方法虽然能提供计算效率，但会导致显著的精度下降。因此需要一种新的方法来平衡计算效率和模型精度。

**方法:** 提出了一种名为Double Binary Factorization (DBF)的方法，将权重矩阵分解为两个二值（符号）矩阵的乘积，每个矩阵都带有缩放向量。

**结果:** DBF在1比特/权重范围内优于现有二值化方法，在2比特/权重范围内与最先进的量化方法如QuIP#和QTIP竞争。此外，DBF提供了细粒度的压缩比控制，并且可以结合非均匀层间压缩比估计算法进一步优化。

**结论:** DBF不仅继承了二值表示的计算效率优势，还实现了与或优于最先进的压缩方法的压缩率，同时提供了更灵活的压缩比控制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Addition+is+almost+all+you+need%3A+Compressing+neural+networks+with+double+binary+factorization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11076，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11076&send_immediately=true&force_search=false)

**原文摘要:** Binary quantization approaches, which replace weight matrices with binary
matrices and substitute costly multiplications with cheaper additions, offer a
computationally efficient approach to address the increasing computational and
storage requirements of Large Language Models (LLMs). However, the severe
quantization constraint ($\pm1$) can lead to significant accuracy degradation.
In this paper, we propose Double Binary Factorization (DBF), a novel method
that factorizes dense weight matrices into products of two binary (sign)
matrices, each accompanied by scaling vectors. DBF preserves the efficiency
advantages of binary representations while achieving compression rates that are
competitive with or superior to state-of-the-art methods. Specifically, in a
1-bit per weight range, DBF is better than existing binarization approaches. In
a 2-bit per weight range, DBF is competitive with the best quantization methods
like QuIP\# and QTIP. Unlike most existing compression techniques, which offer
limited compression level choices, DBF allows fine-grained control over
compression ratios by adjusting the factorization's intermediate dimension.
Based on this advantage, we further introduce an algorithm for estimating
non-uniform layer-wise compression ratios for DBF, based on previously
developed channel pruning criteria.
  Code available at: https://github.com/usamec/double_binary

</details>


### [69] [Fault Diagnosis across Heterogeneous Domains via Self-Adaptive Temporal-Spatial Attention and Sample Generation](https://arxiv.org/abs/2505.11083)
*Guangqiang Li, M. Amine Atoui, Xiangshun Li*

**主要类别:** cs.LG

**概要:** 提出了一种新的自适应时间-空间注意力网络（TSA-SAN），用于多模式过程中的故障诊断，该模型通过构建模式间映射和插值来增强数据多样性，并采用自适应实例归一化和时空注意力机制来提高诊断性能。实验表明，该方法显著优于现有技术。


<details>
  <summary>Details</summary>
  
**动机:** 现有的深度学习故障诊断方法假设不同操作模式下的健康状态类别相同，但在实际工业场景中，这些类别通常只存在部分重叠，这给现有的故障诊断方法带来了挑战。

**方法:** 提出了自适应时间-空间注意力网络（TSA-SAN），包括构建模式间映射生成多模式样本，对健康和故障样本进行插值以丰富故障数据的多样性，使用自适应实例归一化抑制无关信息并保留诊断所需的关键统计特征，以及构建时空注意力机制聚焦关键特征以提升模型泛化能力。

**结果:** 实验表明，提出的TSA-SAN模型在性能上显著优于当前最先进的方法。

**结论:** 所提出的TSA-SAN模型成功解决了多模式过程中由于数据不完整性和模式间分布差异导致的故障诊断难题，并展示了优越的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fault+Diagnosis+across+Heterogeneous+Domains+via+Self-Adaptive+Temporal-Spatial+Attention+and+Sample+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11083，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11083&send_immediately=true&force_search=false)

**原文摘要:** Deep learning methods have shown promising performance in fault diagnosis for
multimode process. Most existing studies assume that the collected health state
categories from different operating modes are identical. However, in real
industrial scenarios, these categories typically exhibit only partial overlap.
The incompleteness of the available data and the large distributional
differences between the operating modes pose a significant challenge to
existing fault diagnosis methods. To address this problem, a novel fault
diagnosis model named self-adaptive temporal-spatial attention network
(TSA-SAN) is proposed. First, inter-mode mappings are constructed using healthy
category data to generate multimode samples. To enrich the diversity of the
fault data, interpolation is performed between healthy and fault samples.
Subsequently, the fault diagnosis model is trained using real and generated
data. The self-adaptive instance normalization is established to suppress
irrelevant information while retaining essential statistical features for
diagnosis. In addition, a temporal-spatial attention mechanism is constructed
to focus on the key features, thus enhancing the generalization ability of the
model. The extensive experiments demonstrate that the proposed model
significantly outperforms the state-of-the-art methods. The code will be
available on Github at https://github.com/GuangqiangLi/TSA-SAN.

</details>


### [70] [ShiQ: Bringing back Bellman to LLMs](https://arxiv.org/abs/2505.11081)
*Pierre Clavier, Nathan Grinsztajn, Raphael Avalos, Yannis Flet-Berliac, Irem Ergun, Omar D. Domingues, Eugene Tarassov, Olivier Pietquin, Pierre H. Richemond, Florian Strub, Matthieu Geist*

**主要类别:** cs.LG

**概要:** This paper introduces ShiQ, a novel algorithm derived from Q-learning methods tailored for pre-trained large language models (LLMs), which shows effectiveness in both single-turn and multi-turn settings.


<details>
  <summary>Details</summary>
  
**动机:** To address the inefficiency of current reinforcement learning approaches in improving pre-trained LLMs, by leveraging the sample efficiency and offline learning capability of Q-learning methods.

**方法:** Deriving theoretically grounded loss functions from Bellman equations to adapt Q-learning methods to LLMs, ensuring logits become reliable Q-value estimates, and building the ShiQ algorithm for off-policy, token-wise learning.

**结果:** ShiQ demonstrates effectiveness on both synthetic data and real-world benchmarks, showing promise in improving LLMs' performance in single-turn and multi-turn settings.

**结论:** This work successfully adapts Q-learning to LLMs through the ShiQ algorithm, showcasing its potential in enhancing LLMs' reinforcement learning capabilities.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ShiQ%3A+Bringing+back+Bellman+to+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11081，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11081&send_immediately=true&force_search=false)

**原文摘要:** The fine-tuning of pre-trained large language models (LLMs) using
reinforcement learning (RL) is generally formulated as direct policy
optimization. This approach was naturally favored as it efficiently improves a
pretrained LLM, seen as an initial policy. Another RL paradigm, Q-learning
methods, has received far less attention in the LLM community while
demonstrating major success in various non-LLM RL tasks. In particular,
Q-learning effectiveness comes from its sample efficiency and ability to learn
offline, which is particularly valuable given the high computational cost of
sampling with LLMs. However, naively applying a Q-learning-style update to the
model's logits is ineffective due to the specificity of LLMs. Our core
contribution is to derive theoretically grounded loss functions from Bellman
equations to adapt Q-learning methods to LLMs. To do so, we carefully adapt
insights from the RL literature to account for LLM-specific characteristics,
ensuring that the logits become reliable Q-value estimates. We then use this
loss to build a practical algorithm, ShiQ for Shifted-Q, that supports
off-policy, token-wise learning while remaining simple to implement. Finally,
we evaluate ShiQ on both synthetic data and real-world benchmarks, e.g.,
UltraFeedback and BFCL-V3, demonstrating its effectiveness in both single-turn
and multi-turn LLM settings

</details>


### [71] [Bidirectional Distillation: A Mixed-Play Framework for Multi-Agent Generalizable Behaviors](https://arxiv.org/abs/2505.11100)
*Lang Feng, Jiahao Lin, Dong Xing, Li Zhang, De Ma, Gang Pan*

**主要类别:** cs.LG

**概要:** This study introduces Bidirectional Distillation (BiDist), a novel approach for population-population generalization in multi-agent reinforcement learning, overcoming limitations of existing self-play methods through knowledge distillation in two directions.


<details>
  <summary>Details</summary>
  
**动机:** To address the challenge of population-population generalization in MARL, especially when agents face unseen co-players, and to overcome the limitations of inside-space generalization in self-play-based methods.

**方法:** Proposes BiDist, a mixed-play framework using forward and reverse distillation to create implicit self-play and drive agents to novel policy distributions outside known spaces, without requiring storage of past policies.

**结果:** Demonstrates strong generalization ability across various tasks and significantly diversifies the policy distribution space, supported by theoretical analysis and empirical evidence.

**结论:** Introduces a novel method called Bidirectional Distillation (BiDist) that improves population-population generalization in multi-agent reinforcement learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bidirectional+Distillation%3A+A+Mixed-Play+Framework+for+Multi-Agent+Generalizable+Behaviors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11100，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11100&send_immediately=true&force_search=false)

**原文摘要:** Population-population generalization is a challenging problem in multi-agent
reinforcement learning (MARL), particularly when agents encounter unseen
co-players. However, existing self-play-based methods are constrained by the
limitation of inside-space generalization. In this study, we propose
Bidirectional Distillation (BiDist), a novel mixed-play framework, to overcome
this limitation in MARL. BiDist leverages knowledge distillation in two
alternating directions: forward distillation, which emulates the historical
policies' space and creates an implicit self-play, and reverse distillation,
which systematically drives agents towards novel distributions outside the
known policy space in a non-self-play manner. In addition, BiDist operates as a
concise and efficient solution without the need for the complex and costly
storage of past policies. We provide both theoretical analysis and empirical
evidence to support BiDist's effectiveness. Our results highlight its
remarkable generalization ability across a variety of cooperative, competitive,
and social dilemma tasks, and reveal that BiDist significantly diversifies the
policy distribution space. We also present comprehensive ablation studies to
reinforce BiDist's effectiveness and key success factors. Source codes are
available in the supplementary material.

</details>


### [72] [Inferring the Most Similar Variable-length Subsequences between Multidimensional Time Series](https://arxiv.org/abs/2505.11106)
*Thanadej Rattanakornphan, Piyanon Charoenpoonpanich, Chainarong Amornbunchornvej*

**主要类别:** cs.LG

**概要:** 提出一种新算法解决多维时间序列中最相似子序列查找问题，该算法在模拟数据和真实数据中均表现优异，速度比基线方法快4-20倍。


<details>
  <summary>Details</summary>
  
**动机:** 寻找两个多维时间序列之间最相似的子序列有广泛的应用场景，但目前缺乏高效框架处理这一问题。

**方法:** 提出了一种基于理论保证正确性和效率的新算法，能够处理时间序列长度不同以及子序列长度不同的情况。

**结果:** 在模拟数据集中，算法不仅提供正确的解决方案，而且运行时间仅为基线方法的四分之一；在真实世界的数据集中，算法提取最相似子序列的速度比基线方法快20倍，并提供了对股票市场和狒狒运动多维时间序列相关性的洞察。

**结论:** 该方法适用于任何时间序列，代码和数据集可供公众使用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Inferring+the+Most+Similar+Variable-length+Subsequences+between+Multidimensional+Time+Series，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11106，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11106&send_immediately=true&force_search=false)

**原文摘要:** Finding the most similar subsequences between two multidimensional time
series has many applications: e.g. capturing dependency in stock market or
discovering coordinated movement of baboons. Considering one pattern occurring
in one time series, we might be wondering whether the same pattern occurs in
another time series with some distortion that might have a different length.
Nevertheless, to the best of our knowledge, there is no efficient framework
that deals with this problem yet. In this work, we propose an algorithm that
provides the exact solution of finding the most similar multidimensional
subsequences between time series where there is a difference in length both
between time series and between subsequences. The algorithm is built based on
theoretical guarantee of correctness and efficiency. The result in simulation
datasets illustrated that our approach not just only provided correct solution,
but it also utilized running time only quarter of time compared against the
baseline approaches. In real-world datasets, it extracted the most similar
subsequences even faster (up to 20 times faster against baseline methods) and
provided insights regarding the situation in stock market and following
relations of multidimensional time series of baboon movement. Our approach can
be used for any time series. The code and datasets of this work are provided
for the public use.

</details>


### [73] [FairSHAP: Preprocessing for Fairness Through Attribution-Based Data Augmentation](https://arxiv.org/abs/2505.11111)
*Lin Zhu, Yijun Bian, Lei You*

**主要类别:** cs.LG

**概要:** Ensure fairness in machine learning models by introducing FairSHAP, a novel preprocessing framework that uses Shapley value attribution to enhance both individual and group fairness.


<details>
  <summary>Details</summary>
  
**动机:** Biased decisions in high-stakes domains can have serious societal consequences, and existing methods lack transparency in identifying unfair features or instances.

**方法:** FairSHAP identifies fairness-critical instances using feature importance and modifies them via instance-level matching across sensitive groups.

**结果:** FairSHAP improves demographic parity and equality of opportunity with minimal data changes and sometimes better predictive performance.

**结论:** FairSHAP is a transparent and model-agnostic method that integrates easily into existing ML pipelines and offers insights into bias sources.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FairSHAP%3A+Preprocessing+for+Fairness+Through+Attribution-Based+Data+Augmentation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11111，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11111&send_immediately=true&force_search=false)

**原文摘要:** Ensuring fairness in machine learning models is critical, particularly in
high-stakes domains where biased decisions can lead to serious societal
consequences. Existing preprocessing approaches generally lack transparent
mechanisms for identifying which features or instances are responsible for
unfairness. This obscures the rationale behind data modifications. We introduce
FairSHAP, a novel pre-processing framework that leverages Shapley value
attribution to improve both individual and group fairness. FairSHAP identifies
fairness-critical instances in the training data using an interpretable measure
of feature importance, and systematically modifies them through instance-level
matching across sensitive groups. This process reduces discriminative risk - an
individual fairness metric - while preserving data integrity and model
accuracy. We demonstrate that FairSHAP significantly improves demographic
parity and equality of opportunity across diverse tabular datasets, achieving
fairness gains with minimal data perturbation and, in some cases, improved
predictive performance. As a model-agnostic and transparent method, FairSHAP
integrates seamlessly into existing machine learning pipelines and provides
actionable insights into the sources of bias.Our code is on
https://github.com/youlei202/FairSHAP.

</details>


### [74] [Attention on the Sphere](https://arxiv.org/abs/2505.11157)
*Boris Bonev, Max Rietmann, Andrea Paris, Alberto Carpentieri, Thorsten Kurth*

**主要类别:** cs.LG

**概要:** A new attention mechanism for spherical domains improves Transformer performance in atmospheric physics, cosmology, and robotics by preserving spherical symmetries and topology.


<details>
  <summary>Details</summary>
  
**动机:** To enable Transformer models to handle data on the two-dimensional sphere while preserving spherical symmetries and topology.

**方法:** Generalized attention mechanism with numerical quadrature weights and neighborhood attention on the sphere.

**结果:** Better performance than Cartesian approaches and improved scalability with reduced computational complexity.

**结论:** The proposed spherical transformers outperform planar counterparts across various tasks, demonstrating the importance of geometric priors.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Attention+on+the+Sphere，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11157，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11157&send_immediately=true&force_search=false)

**原文摘要:** We introduce a generalized attention mechanism for spherical domains,
enabling Transformer architectures to natively process data defined on the
two-dimensional sphere - a critical need in fields such as atmospheric physics,
cosmology, and robotics, where preserving spherical symmetries and topology is
essential for physical accuracy. By integrating numerical quadrature weights
into the attention mechanism, we obtain a geometrically faithful spherical
attention that is approximately rotationally equivariant, providing strong
inductive biases and leading to better performance than Cartesian approaches.
To further enhance both scalability and model performance, we propose
neighborhood attention on the sphere, which confines interactions to geodesic
neighborhoods. This approach reduces computational complexity and introduces
the additional inductive bias for locality, while retaining the symmetry
properties of our method. We provide optimized CUDA kernels and
memory-efficient implementations to ensure practical applicability. The method
is validated on three diverse tasks: simulating shallow water equations on the
rotating sphere, spherical image segmentation, and spherical depth estimation.
Across all tasks, our spherical Transformers consistently outperform their
planar counterparts, highlighting the advantage of geometric priors for
learning on spherical domains.

</details>


### [75] [Maximizing Asynchronicity in Event-based Neural Networks](https://arxiv.org/abs/2505.11165)
*Haiqing Hao, Nikola Zubić, Weihua He, Zhipeng Sui, Davide Scaramuzza, Wenhui Wang*

**主要类别:** cs.LG

**概要:** EVA是一种新的异步事件表示学习框架，可生成高度表达性和泛化的事件表示，在识别和检测任务上表现优异。


<details>
  <summary>Details</summary>
  
**动机:** 现有的异步到同步(A2S)方法在表达能力和泛化能力上通常比密集同步方法差。

**方法:** EVA利用语言建模中的线性注意力和自监督学习技术来适应事件数据的异步特性。

**结果:** EVA在DVS128-Gesture和N-Cars识别任务以及Gen1检测任务上取得了显著性能提升。

**结论:** EVA展示了其在实时事件视觉应用中的变革潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Maximizing+Asynchronicity+in+Event-based+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11165，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11165&send_immediately=true&force_search=false)

**原文摘要:** Event cameras deliver visual data with high temporal resolution, low latency,
and minimal redundancy, yet their asynchronous, sparse sequential nature
challenges standard tensor-based machine learning (ML). While the recent
asynchronous-to-synchronous (A2S) paradigm aims to bridge this gap by
asynchronously encoding events into learned representations for ML pipelines,
existing A2S approaches often sacrifice representation expressivity and
generalizability compared to dense, synchronous methods. This paper introduces
EVA (EVent Asynchronous representation learning), a novel A2S framework to
generate highly expressive and generalizable event-by-event representations.
Inspired by the analogy between events and language, EVA uniquely adapts
advances from language modeling in linear attention and self-supervised
learning for its construction. In demonstration, EVA outperforms prior A2S
methods on recognition tasks (DVS128-Gesture and N-Cars), and represents the
first A2S framework to successfully master demanding detection tasks, achieving
a remarkable 47.7 mAP on the Gen1 dataset. These results underscore EVA's
transformative potential for advancing real-time event-based vision
applications.

</details>


### [76] [Dual-Balancing for Physics-Informed Neural Networks](https://arxiv.org/abs/2505.11117)
*Chenhong Zhou, Jie Chen, Zaifeng Yang, Ching Eng Png*

**主要类别:** cs.LG

**概要:** 提出了一种新的Dual-Balanced PINN（DB-PINN）方法，通过整合inter-balancing和intra-balancing来动态调整损失权重，解决了PINNs中的两个不平衡问题，实验表明其在收敛速度和预测精度上优于其他流行的方法。


<details>
  <summary>Details</summary>
  
**动机:** 现有的PINNs存在由于多目标优化问题导致的精度差和收敛慢的问题。

**方法:** 提出了DB-PINN，通过inter-balancing和intra-balancing动态调整损失权重，并引入了鲁棒的权重更新策略。

**结果:** DB-PINN在收敛速度和预测精度上显著优于其他梯度为基础的加权方法。

**结论:** DB-PINN通过解决PINNs中的不平衡问题，提高了模型的性能，为求解偏微分方程提供了一个新的学习范式。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dual-Balancing+for+Physics-Informed+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11117，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11117&send_immediately=true&force_search=false)

**原文摘要:** Physics-informed neural networks (PINNs) have emerged as a new learning
paradigm for solving partial differential equations (PDEs) by enforcing the
constraints of physical equations, boundary conditions (BCs), and initial
conditions (ICs) into the loss function. Despite their successes, vanilla PINNs
still suffer from poor accuracy and slow convergence due to the intractable
multi-objective optimization issue. In this paper, we propose a novel
Dual-Balanced PINN (DB-PINN), which dynamically adjusts loss weights by
integrating inter-balancing and intra-balancing to alleviate two imbalance
issues in PINNs. Inter-balancing aims to mitigate the gradient imbalance
between PDE residual loss and condition-fitting losses by determining an
aggregated weight that offsets their gradient distribution discrepancies.
Intra-balancing acts on condition-fitting losses to tackle the imbalance in
fitting difficulty across diverse conditions. By evaluating the fitting
difficulty based on the loss records, intra-balancing can allocate the
aggregated weight proportionally to each condition loss according to its
fitting difficulty level. We further introduce a robust weight update strategy
to prevent abrupt spikes and arithmetic overflow in instantaneous weight values
caused by large loss variances, enabling smooth weight updating and stable
training. Extensive experiments demonstrate that DB-PINN achieves significantly
superior performance than those popular gradient-based weighting methods in
terms of convergence speed and prediction accuracy. Our code and supplementary
material are available at https://github.com/chenhong-zhou/DualBalanced-PINNs.

</details>


### [77] [RanDeS: Randomized Delta Superposition for Multi-Model Compression](https://arxiv.org/abs/2505.11204)
*Hangyu Zhou, Aaron Gokaslan, Volodymyr Kuleshov, Bharath Hariharan*

**主要类别:** cs.LG

**概要:** 提出了一种通过随机正交变换减少多模型合并中的任务干扰的方法，并展示了其在视觉和语言任务上的性能提升。该方法无需额外内存即可添加新模型，并且由于其数据和模型无关的特性，支持高效的多模型服务。


<details>
  <summary>Details</summary>
  
**动机:** 多模型合并因任务特定参数调整之间的干扰导致性能下降。

**方法:** 将模型合并重新定义为压缩和检索方案，使用随机正交变换来消除无关delta向量的相关性。

**结果:** 该方法显著减少了干扰，在视觉和语言任务上提高了性能。

**结论:** 该方法无需额外内存即可添加新模型，且支持高效灵活的多模型服务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RanDeS%3A+Randomized+Delta+Superposition+for+Multi-Model+Compression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11204，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11204&send_immediately=true&force_search=false)

**原文摘要:** From a multi-model compression perspective, model merging enables
memory-efficient serving of multiple models fine-tuned from the same base, but
suffers from degraded performance due to interference among their task-specific
parameter adjustments (i.e., deltas). In this paper, we reformulate model
merging as a compress-and-retrieve scheme, revealing that the task interference
arises from the summation of irrelevant deltas during model retrieval. To
address this issue, we use random orthogonal transformations to decorrelate
these vectors into self-cancellation. We show that this approach drastically
reduces interference, improving performance across both vision and language
tasks. Since these transformations are fully defined by random seeds, adding
new models requires no extra memory. Further, their data- and model-agnostic
nature enables easy addition or removal of models with minimal compute
overhead, supporting efficient and flexible multi-model serving.

</details>


### [78] [GraphOracle: A Foundation Model for Knowledge Graph Reasoning](https://arxiv.org/abs/2505.11125)
*Enjun Du, Siyi Liu, Yongqi Zhang*

**主要类别:** cs.LG

**概要:** 提出了一种名为GraphOracle的关系中心的基础模型，通过转换为关系依赖图(RDG)，显式编码组合模式，并开发查询相关的注意力机制来学习表示，实现了对未见实体、关系和整个图的有效泛化，在31个基准测试中表现出一致的最先进的性能。


<details>
  <summary>Details</summary>
  
**动机:** 开发类似于基础模型的知识图谱模型面临动态性和跨领域推理的需求的独特挑战。

**方法:** 提出了GraphOracle模型，它通过转换为关系依赖图显式编码组合模式，并开发查询相关的注意力机制学习表示。

**结果:** 在31个基准测试中表现出一致的最先进的性能，提升了预测性能最高达35％。

**结论:** GraphOracle在知识图谱的跨领域推理上取得了显著成果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GraphOracle%3A+A+Foundation+Model+for+Knowledge+Graph+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11125，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11125&send_immediately=true&force_search=false)

**原文摘要:** Foundation models have demonstrated remarkable capabilities across various
domains, but developing analogous models for knowledge graphs presents unique
challenges due to their dynamic nature and the need for cross-domain reasoning.
To address these issues, we introduce \textbf{\textsc{GraphOracle}}, a
relation-centric foundation model that unifies reasoning across knowledge
graphs by converting them into Relation-Dependency Graphs (RDG), explicitly
encoding compositional patterns with fewer edges than prior methods. A
query-dependent attention mechanism is further developed to learn inductive
representations for both relations and entities. Pre-training on diverse
knowledge graphs, followed by minutes-level fine-tuning, enables effective
generalization to unseen entities, relations, and entire graphs. Through
comprehensive experiments on 31 diverse benchmarks spanning transductive,
inductive, and cross-domain settings, we demonstrate consistent
state-of-the-art performance with minimal adaptation, improving the prediction
performance by up to 35\% compared to the strongest baselines.

</details>


### [79] [A Set-Sequence Model for Time Series](https://arxiv.org/abs/2505.11243)
*Elliot L. Epstein, Apaar Sadhwani, Kay Giesecke*

**主要类别:** cs.LG

**概要:** 提出了一种名为Set-Sequence的模型，用于金融预测问题，该模型无需手工特征，并且在股票回报预测和抵押行为任务上显著优于基准模型。


<details>
  <summary>Details</summary>
  
**动机:** 传统方法通过手工制作的汇总特征来捕捉潜在的交叉影响，而本文提出的方法旨在消除对手工特征的需求。

**方法:** 提出Set-Sequence模型，包括Set模型（学习共享的交叉总结）和Sequence模型（摄入增强的时间序列数据进行预测），并且两个组件在训练期间联合学习。

**结果:** 在股票回报预测和抵押行为任务上的实证评估显示，Set-Sequence模型显著优于基准模型。

**结论:** 所提出的Set-Sequence模型不仅提高了预测性能，而且具有计算效率和灵活性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Set-Sequence+Model+for+Time+Series，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11243，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11243&send_immediately=true&force_search=false)

**原文摘要:** In many financial prediction problems, the behavior of individual units (such
as loans, bonds, or stocks) is influenced by observable unit-level factors and
macroeconomic variables, as well as by latent cross-sectional effects.
Traditional approaches attempt to capture these latent effects via handcrafted
summary features. We propose a Set-Sequence model that eliminates the need for
handcrafted features. The Set model first learns a shared cross-sectional
summary at each period. The Sequence model then ingests the summary-augmented
time series for each unit independently to predict its outcome. Both components
are learned jointly over arbitrary sets sampled during training. Our approach
harnesses the set nature of the cross-section and is computationally efficient,
generating set summaries in linear time relative to the number of units. It is
also flexible, allowing the use of existing sequence models and accommodating a
variable number of units at inference. Empirical evaluations demonstrate that
our Set-Sequence model significantly outperforms benchmarks on stock return
prediction and mortgage behavior tasks. Code will be released.

</details>


### [80] [What's Inside Your Diffusion Model? A Score-Based Riemannian Metric to Explore the Data Manifold](https://arxiv.org/abs/2505.11128)
*Simone Azeglio, Arianna Di Bernardo*

**主要类别:** cs.LG

**概要:** 提出了一种基于扩散模型Stein分数函数的黎曼度量，用于描述数据流形的内在几何结构。该方法在插值和外推任务中优于基线方法，并揭示了扩散模型学习到的隐式几何结构。


<details>
  <summary>Details</summary>
  
**动机:** 理解扩散模型学习的数据流形的几何性质仍然具有挑战性。

**方法:** 引入了一种基于Stein分数函数的黎曼度量，定义了环境空间中的度量张量。

**结果:** 该方法在合成数据、Rotated MNIST和Stable Diffusion生成的复杂自然图像上的实验表明，基于分数的测地线能够捕捉有意义的变换。

**结论:** 本研究揭示了扩散模型学习到的隐式几何结构，并提供了一种通过黎曼几何透镜导航自然图像流形的原则性方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What%27s+Inside+Your+Diffusion+Model%3F+A+Score-Based+Riemannian+Metric+to+Explore+the+Data+Manifold，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11128，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11128&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in diffusion models have demonstrated their remarkable
ability to capture complex image distributions, but the geometric properties of
the learned data manifold remain poorly understood. We address this gap by
introducing a score-based Riemannian metric that leverages the Stein score
function from diffusion models to characterize the intrinsic geometry of the
data manifold without requiring explicit parameterization. Our approach defines
a metric tensor in the ambient space that stretches distances perpendicular to
the manifold while preserving them along tangential directions, effectively
creating a geometry where geodesics naturally follow the manifold's contours.
We develop efficient algorithms for computing these geodesics and demonstrate
their utility for both interpolation between data points and extrapolation
beyond the observed data distribution. Through experiments on synthetic data
with known geometry, Rotated MNIST, and complex natural images via Stable
Diffusion, we show that our score-based geodesics capture meaningful
transformations that respect the underlying data distribution. Our method
consistently outperforms baseline approaches on perceptual metrics (LPIPS) and
distribution-level metrics (FID, KID), producing smoother, more realistic image
transitions. These results reveal the implicit geometric structure learned by
diffusion models and provide a principled way to navigate the manifold of
natural images through the lens of Riemannian geometry.

</details>


### [81] [Heterogeneity-Aware Client Sampling: A Unified Solution for Consistent Federated Learning](https://arxiv.org/abs/2505.11304)
*Shudi Weng, Chao Ren, Ming Xiao, Mikael Skoglund*

**主要类别:** cs.LG

**概要:** 提出了一种新的联邦学习方法FedACS，用于解决通信和计算异构性引起的优化轨迹扭曲问题。


<details>
  <summary>Details</summary>
  
**动机:** 现有的联邦学习在处理通信和计算异构性时缺乏统一的理论分析。

**方法:** 提出FedACS方法，通过客户端采样消除各种目标不一致性。

**结果:** FedACS在多个数据集上的实验表明，其性能优于现有技术，并且减少了通信和计算开销。

**结论:** FedACS能够收敛到正确的最优解，并对动态异构环境具有鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Heterogeneity-Aware+Client+Sampling%3A+A+Unified+Solution+for+Consistent+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11304，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11304&send_immediately=true&force_search=false)

**原文摘要:** Federated learning (FL) commonly involves clients with diverse communication
and computational capabilities. Such heterogeneity can significantly distort
the optimization dynamics and lead to objective inconsistency, where the global
model converges to an incorrect stationary point potentially far from the
pursued optimum. Despite its critical impact, the joint effect of communication
and computation heterogeneity has remained largely unexplored, due to the
intrinsic complexity of their interaction. In this paper, we reveal the
fundamentally distinct mechanisms through which heterogeneous communication and
computation drive inconsistency in FL. To the best of our knowledge, this is
the first unified theoretical analysis of general heterogeneous FL, offering a
principled understanding of how these two forms of heterogeneity jointly
distort the optimization trajectory under arbitrary choices of local solvers.
Motivated by these insights, we propose Federated Heterogeneity-Aware Client
Sampling, FedACS, a universal method to eliminate all types of objective
inconsistency. We theoretically prove that FedACS converges to the correct
optimum at a rate of $O(1/\sqrt{R})$, even in dynamic heterogeneous
environments. Extensive experiments across multiple datasets show that FedACS
outperforms state-of-the-art and category-specific baselines by 4.3%-36%, while
reducing communication costs by 22%-89% and computation loads by 14%-105%,
respectively.

</details>


### [82] [Fairness-aware Anomaly Detection via Fair Projection](https://arxiv.org/abs/2505.11132)
*Feng Xiao, Xiaoying Tang, Jicong Fan*

**主要类别:** cs.LG

**概要:** 提出了一种公平感知的无监督异常检测方法FairAD，确保不同群体之间的公平性，并在真实数据集上验证了其性能。


<details>
  <summary>Details</summary>
  
**动机:** 无监督异常检测系统可能引入不公平性，影响不同群体的公正待遇。

**方法:** 通过学习投影将不同群体的数据映射到共同的目标分布，并提出无阈值公平度量方法。

**结果:** 实验表明FairAD在保持检测准确性的同时提高了公平性。

**结论:** FairAD能够在平衡和倾斜数据下实现检测准确性和公平性的良好权衡。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fairness-aware+Anomaly+Detection+via+Fair+Projection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11132，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11132&send_immediately=true&force_search=false)

**原文摘要:** Unsupervised anomaly detection is a critical task in many high-social-impact
applications such as finance, healthcare, social media, and cybersecurity,
where demographics involving age, gender, race, disease, etc, are used
frequently. In these scenarios, possible bias from anomaly detection systems
can lead to unfair treatment for different groups and even exacerbate social
bias. In this work, first, we thoroughly analyze the feasibility and necessary
assumptions for ensuring group fairness in unsupervised anomaly detection.
Second, we propose a novel fairness-aware anomaly detection method FairAD. From
the normal training data, FairAD learns a projection to map data of different
demographic groups to a common target distribution that is simple and compact,
and hence provides a reliable base to estimate the density of the data. The
density can be directly used to identify anomalies while the common target
distribution ensures fairness between different groups. Furthermore, we propose
a threshold-free fairness metric that provides a global view for model's
fairness, eliminating dependence on manual threshold selection. Experiments on
real-world benchmarks demonstrate that our method achieves an improved
trade-off between detection accuracy and fairness under both balanced and
skewed data across different groups.

</details>


### [83] [Visual Planning: Let's Think Only with Images](https://arxiv.org/abs/2505.11409)
*Yi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, Ivan Vulić*

**主要类别:** cs.LG

**概要:** 提出了一种新的视觉规划范式，通过纯视觉表示进行规划，无需文本，该方法在视觉导航任务中表现出色。


<details>
  <summary>Details</summary>
  
**动机:** 语言可能不是处理涉及空间和几何信息的任务时最自然或有效的推理模态。

**方法:** 引入了视觉规划（Visual Planning）范式，以及基于强化学习的框架VPRL，用于大型视觉模型的后训练。

**结果:** 在FrozenLake、Maze和MiniBehavior等代表性视觉导航任务中，视觉规划表现优于其他仅文本的推理变体。

**结论:** 视觉规划是一种可行且有前景的语言推理替代方案，为依赖直观图像推理的任务开辟了新途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Visual+Planning%3A+Let%27s+Think+Only+with+Images，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11409，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11409&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have substantially enhanced machine reasoning across diverse
tasks. However, these models predominantly rely on pure text as the medium for
both expressing and structuring reasoning, even when visual information is
present. In this work, we argue that language may not always be the most
natural or effective modality for reasoning, particularly in tasks involving
spatial and geometrical information. Motivated by this, we propose a new
paradigm, Visual Planning, which enables planning through purely visual
representations, independent of text. In this paradigm, planning is executed
via sequences of images that encode step-by-step inference in the visual
domain, akin to how humans sketch or visualize future actions. We introduce a
novel reinforcement learning framework, Visual Planning via Reinforcement
Learning (VPRL), empowered by GRPO for post-training large vision models,
leading to substantial improvements in planning in a selection of
representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our
visual planning paradigm outperforms all other planning variants that conduct
reasoning in the text-only space. Our results establish Visual Planning as a
viable and promising alternative to language-based reasoning, opening new
avenues for tasks that benefit from intuitive, image-based inference.

</details>


### [84] [Towards Robust Spiking Neural Networks:Mitigating Heterogeneous Training Vulnerability via Dominant Eigencomponent Projection](https://arxiv.org/abs/2505.11134)
*Desong Zhang, Jia Hu, Geyong Min*

**主要类别:** cs.LG

**概要:** 提出了一种无需超参数的Dominant Eigencomponent Projection (DEP)方法，解决了由直接编码和BPTT导致的尖锐极小值问题，提高了SNN对异构数据投毒的鲁棒性。


<details>
  <summary>Details</summary>
  
**动机:** 研究发现主流训练方法下SNN对分布变化敏感，易发生网络崩溃。

**方法:** 开发了名为Dominant Eigencomponent Projection (DEP)的无超参数方法，通过正交投影去除梯度的主要成分来降低Hessian谱半径。

**结果:** 实验显示DEP不仅减轻了SNN对异构数据投毒的脆弱性，还提升了整体鲁棒性。

**结论:** DEP为更安全可靠的SNN部署提供了支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Robust+Spiking+Neural+Networks%3AMitigating+Heterogeneous+Training+Vulnerability+via+Dominant+Eigencomponent+Projection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11134，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11134&send_immediately=true&force_search=false)

**原文摘要:** Spiking Neural Networks (SNNs) process information via discrete spikes,
enabling them to operate at remarkably low energy levels. However, our
experimental observations reveal a striking vulnerability when SNNs are trained
using the mainstream method--direct encoding combined with backpropagation
through time (BPTT): even a single backward pass on data drawn from a slightly
different distribution can lead to catastrophic network collapse. Our
theoretical analysis attributes this vulnerability to the repeated inputs
inherent in direct encoding and the gradient accumulation characteristic of
BPTT, which together produce an exceptional large Hessian spectral radius. To
address this challenge, we develop a hyperparameter-free method called Dominant
Eigencomponent Projection (DEP). By orthogonally projecting gradients to
precisely remove their dominant components, DEP effectively reduces the Hessian
spectral radius, thereby preventing SNNs from settling into sharp minima.
Extensive experiments demonstrate that DEP not only mitigates the vulnerability
of SNNs to heterogeneous data poisoning, but also significantly enhances
overall robustness compared to key baselines, providing strong support for
safer and more reliable SNN deployment.

</details>


### [85] [Mergenetic: a Simple Evolutionary Model Merging Library](https://arxiv.org/abs/2505.11427)
*Adrian Robert Minut, Tommaso Mencattini, Andrea Santilli, Donato Crisostomi, Emanuele Rodolà*

**主要类别:** cs.LG

**概要:** This paper introduces Mergenetic, an open-source library for evolutionary model merging, enabling flexible experimentation with model merging strategies in language models.


<details>
  <summary>Details</summary>
  
**动机:** Pairing model merging with evolutionary algorithms can boost performance but there is no framework supporting flexible experimentation with such strategies in language models.

**方法:** Introduces Mergenetic, an open-source library for evolutionary model merging which enables easy composition of merging methods and evolutionary algorithms while incorporating lightweight fitness estimators.

**结果:** Mergenetic produces competitive results across tasks and languages using modest hardware.

**结论:** Mergenetic produces competitive results across tasks and languages using modest hardware.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mergenetic%3A+a+Simple+Evolutionary+Model+Merging+Library，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11427，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11427&send_immediately=true&force_search=false)

**原文摘要:** Model merging allows combining the capabilities of existing models into a new
one - post hoc, without additional training. This has made it increasingly
popular thanks to its low cost and the availability of libraries that support
merging on consumer GPUs. Recent work shows that pairing merging with
evolutionary algorithms can boost performance, but no framework currently
supports flexible experimentation with such strategies in language models. We
introduce Mergenetic, an open-source library for evolutionary model merging.
Mergenetic enables easy composition of merging methods and evolutionary
algorithms while incorporating lightweight fitness estimators to reduce
evaluation costs. We describe its design and demonstrate that Mergenetic
produces competitive results across tasks and languages using modest hardware.

</details>


### [86] [Covariance Density Neural Networks](https://arxiv.org/abs/2505.11139)
*Om Roy, Yashar Moshfeghi, Keith Smith*

**主要类别:** cs.LG

**概要:** This paper introduces an improved version of CoVariance Neural Networks (VNNs) called Density Matrix-based VNNs, which use a density matrix as the Graph Shift Operator (GSO) to enhance the performance in tasks like EEG motor imagery classification.


<details>
  <summary>Details</summary>
  
**动机:** The lack of consensus on choosing the correct underlying graph structure for modeling signals in graph neural networks.

**方法:** Constructing a Density Matrix using the sample covariance matrix as a quasi-Hamiltonian in the space of random variables and using it as the GSO.

**结果:** The proposed method improves performance in terms of discriminability and robustness to noise compared to VNNs, especially in real-life applications with informative covariance matrices. It also shows strong performance in subject-independent Brain Computer Interface EEG motor imagery classification.

**结论:** Density Matrix-based VNNs provide a foundation for improving the transferability of Brain Computer Interfaces.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Covariance+Density+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11139，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11139&send_immediately=true&force_search=false)

**原文摘要:** Graph neural networks have re-defined how we model and predict on network
data but there lacks a consensus on choosing the correct underlying graph
structure on which to model signals. CoVariance Neural Networks (VNN) address
this issue by using the sample covariance matrix as a Graph Shift Operator
(GSO). Here, we improve on the performance of VNNs by constructing a Density
Matrix where we consider the sample Covariance matrix as a quasi-Hamiltonian of
the system in the space of random variables. Crucially, using this density
matrix as the GSO allows components of the data to be extracted at different
scales, allowing enhanced discriminability and performance. We show that this
approach allows explicit control of the stability-discriminability trade-off of
the network, provides enhanced robustness to noise compared to VNNs, and
outperforms them in useful real-life applications where the underlying
covariance matrix is informative. In particular, we show that our model can
achieve strong performance in subject-independent Brain Computer Interface EEG
motor imagery classification, outperforming EEGnet while being faster. This
shows how covariance density neural networks provide a basis for the
notoriously difficult task of transferability of BCIs when evaluated on unseen
individuals.

</details>


### [87] [Bi-directional Recurrence Improves Transformer in Partially Observable Markov Decision Processes](https://arxiv.org/abs/2505.11153)
*Ashok Arora, Neetesh Kumar*

**主要类别:** cs.LG

**概要:** This paper introduces a new bi-recurrent model for partially observable Markov decision processes (POMDPs) that improves sample efficiency and reduces model parameters.


<details>
  <summary>Details</summary>
  
**动机:** Reinforcement learning agents often face partial observability, and current transformer-based models have high parameter counts.

**方法:** A bi-recurrent model architecture with a single layer of bi-directional recurrence unit was developed to replace multiple feedforward layers.

**结果:** The proposed model outperformed existing transformer-based, attention-based, and recurrence-based methods by 87.39%-482.04% on average across 23 POMDP environments.

**结论:** The bi-recurrent model architecture can effectively handle partial observability and improve learning efficiency.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bi-directional+Recurrence+Improves+Transformer+in+Partially+Observable+Markov+Decision+Processes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11153，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11153&send_immediately=true&force_search=false)

**原文摘要:** In real-world reinforcement learning (RL) scenarios, agents often encounter
partial observability, where incomplete or noisy information obscures the true
state of the environment. Partially Observable Markov Decision Processes
(POMDPs) are commonly used to model these environments, but effective
performance requires memory mechanisms to utilise past observations. While
recurrence networks have traditionally addressed this need, transformer-based
models have recently shown improved sample efficiency in RL tasks. However,
their application to POMDPs remains underdeveloped, and their real-world
deployment is constrained due to the high parameter count. This work introduces
a novel bi-recurrent model architecture that improves sample efficiency and
reduces model parameter count in POMDP scenarios. The architecture replaces the
multiple feed forward layers with a single layer of bi-directional recurrence
unit to better capture and utilize sequential dependencies and contextual
information. This approach improves the model's ability to handle partial
observability and increases sample efficiency, enabling effective learning from
comparatively fewer interactions. To evaluate the performance of the proposed
model architecture, experiments were conducted on a total of 23 POMDP
environments. The proposed model architecture outperforms existing
transformer-based, attention-based, and recurrence-based methods by a margin
ranging from 87.39% to 482.04% on average across the 23 POMDP environments.

</details>


### [88] [Gaussian Weight Sampling for Scalable, Efficient and Stable Pseudo-Quantization Training](https://arxiv.org/abs/2505.11170)
*Myeonghwan Ahn, Sungjoo Yoo*

**主要类别:** cs.LG

**概要:** This paper explores pseudo-quantization training (PQT) for large language models, proposing a Gaussian weight sampling method that is scalable, efficient, and stable compared to BF16.


<details>
  <summary>Details</summary>
  
**动机:** To address the challenges faced by fully quantized training (FQT), such as consistency issues and exponential case searches.

**方法:** Introduces a Gaussian weight sampling method within PQT that is floating-point friendly and includes stochastic precision annealing.

**结果:** The method is shown to be scalable, efficient with minimal computational overhead, and stable in pre-training models.

**结论:** Pseudo-quantization training with Gaussian weight sampling provides a strong theoretical foundation for low-precision FP parameters and outperforms or matches BF16 in certain scenarios.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Gaussian+Weight+Sampling+for+Scalable%2C+Efficient+and+Stable+Pseudo-Quantization+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11170，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11170&send_immediately=true&force_search=false)

**原文摘要:** Ever-growing scale of large language models (LLMs) is pushing for improved
efficiency, favoring fully quantized training (FQT) over BF16. While FQT
accelerates training, it faces consistency challenges and requires searching
over an exponential number of cases, each needing over 200B tokens to ensure
stability.
  Pseudo-quantization training (PQT) addresses the issues of FQT, although it
is not well-studied. We explore the practical implications of PQT in detail and
propose a noise distribution $R$ that is floating-point (FP)-friendly, with
ideal properties including stochastic precision annealing. As a result, the
proposed method serves as an effective theoretical foundation for low-precision
FP parameters through PQT, utilizing efficient fake quantization via an
addition and subsequent FP casting.
  We demonstrate that Gaussian weight sampling is (1) scalable: supports
low-precision FP parameters down to FP6 and high-precision noise up to 9-bit
with BF16 operator. The proposed method is (2) efficient: incurring
computational overhead as low as 1.40\% on the A100 GPU in terms of Llama2
training tokens per second, and requiring 2 bytes per parameter in GPU memory.
We demonstrate that PQT with Gaussian weight sampling is (3) stable: closely
following or even surpassing performance of the BF16 baseline while
pre-training GPT2 and Llama2 models with up to 1B parameters and 300B tokens.

</details>


### [89] [VitaGraph: Building a Knowledge Graph for Biologically Relevant Learning Tasks](https://arxiv.org/abs/2505.11185)
*Francesco Madeddu, Lucia Testa, Gianluca De Carlo, Michele Pieroni, Andrea Mastropietro, Aris Anagnostopoulos, Paolo Tieri, Sergio Barbarossa*

**主要类别:** cs.LG

**概要:** 此论文介绍了一个综合性的多用途生物知识图谱，该图谱通过整合和精炼多个公开可用的数据集构建而成。它展示了在药物重定位、蛋白质相互作用预测和副作用预测等任务中的有效性。


<details>
  <summary>Details</summary>
  
**动机:** 当前人类生物学的固有复杂性对科学研究提出了持续的挑战。跨学科合作旨在扩展我们对定义人类生命的生物相互作用的理解。AI方法论在科学领域特别是计算生物学中成为强大的工具。然而，可靠机器学习模型预测需要高质量的基础数据。

**方法:** 通过整合和精炼多个公开可用的数据集构建了一个综合性的多用途生物知识图谱。基于药物再利用知识图谱（DRKG），定义了一种处理管道来清理不一致和冗余，合并信息，并丰富节点特征向量。

**结果:** 提出的资源代表了一个连贯且可靠的生物知识图谱，可作为推动计算生物学和精准医学研究的最先进平台。此外，它还提供了在相关任务上基准测试基于图的机器学习和网络医学模型的机会。

**结论:** 构建的知识图谱在药物重定位、蛋白质相互作用预测和副作用预测等任务中显示出有效性，有助于推进计算生物学和精准医学的研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VitaGraph%3A+Building+a+Knowledge+Graph+for+Biologically+Relevant+Learning+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11185，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11185&send_immediately=true&force_search=false)

**原文摘要:** The intrinsic complexity of human biology presents ongoing challenges to
scientific understanding. Researchers collaborate across disciplines to expand
our knowledge of the biological interactions that define human life. AI
methodologies have emerged as powerful tools across scientific domains,
particularly in computational biology, where graph data structures effectively
model biological entities such as protein-protein interaction (PPI) networks
and gene functional networks. Those networks are used as datasets for paramount
network medicine tasks, such as gene-disease association prediction, drug
repurposing, and polypharmacy side effect studies. Reliable predictions from
machine learning models require high-quality foundational data. In this work,
we present a comprehensive multi-purpose biological knowledge graph constructed
by integrating and refining multiple publicly available datasets. Building upon
the Drug Repurposing Knowledge Graph (DRKG), we define a pipeline tasked with
a) cleaning inconsistencies and redundancies present in DRKG, b) coalescing
information from the main available public data sources, and c) enriching the
graph nodes with expressive feature vectors such as molecular fingerprints and
gene ontologies. Biologically and chemically relevant features improve the
capacity of machine learning models to generate accurate and well-structured
embedding spaces. The resulting resource represents a coherent and reliable
biological knowledge graph that serves as a state-of-the-art platform to
advance research in computational biology and precision medicine. Moreover, it
offers the opportunity to benchmark graph-based machine learning and network
medicine models on relevant tasks. We demonstrate the effectiveness of the
proposed dataset by benchmarking it against the task of drug repurposing, PPI
prediction, and side-effect prediction, modeled as link prediction problems.

</details>


### [90] [Modeling Cell Dynamics and Interactions with Unbalanced Mean Field Schrödinger Bridge](https://arxiv.org/abs/2505.11197)
*Zhenyi Zhang, Zihan Wang, Yuhao Sun, Tiejun Li, Peijie Zhou*

**主要类别:** cs.LG

**概要:** A novel deep learning approach, CytoBridge, models stochastic interaction dynamics from sparse single-cell data, improving accuracy in identifying growth, transition, and interaction patterns compared to existing methods.


<details>
  <summary>Details</summary>
  
**动机:** To model unbalanced stochastic interaction dynamics from sparse snapshot data while accounting for cell-cell interactions which existing methods fail to do adequately.

**方法:** Proposes CytoBridge, a deep learning algorithm that approximates the Unbalanced Mean-Field Schrödinger Bridge (UMFSB) problem.

**结果:** Validated effectively on synthetic gene regulatory data and real scRNA-seq datasets, showing better accuracy in identifying growth, transition, and interaction patterns, and eliminating false transitions.

**结论:** CytoBridge successfully models cell interactions, transitions, and proliferation, improving accuracy in reconstructing developmental landscapes.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Modeling+Cell+Dynamics+and+Interactions+with+Unbalanced+Mean+Field+Schr%C3%B6dinger+Bridge，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11197，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11197&send_immediately=true&force_search=false)

**原文摘要:** Modeling the dynamics from sparsely time-resolved snapshot data is crucial
for understanding complex cellular processes and behavior. Existing methods
leverage optimal transport, Schr\"odinger bridge theory, or their variants to
simultaneously infer stochastic, unbalanced dynamics from snapshot data.
However, these approaches remain limited in their ability to account for
cell-cell interactions. This integration is essential in real-world scenarios
since intercellular communications are fundamental life processes and can
influence cell state-transition dynamics. To address this challenge, we
formulate the Unbalanced Mean-Field Schr\"odinger Bridge (UMFSB) framework to
model unbalanced stochastic interaction dynamics from snapshot data. Inspired
by this framework, we further propose CytoBridge, a deep learning algorithm
designed to approximate the UMFSB problem. By explicitly modeling cellular
transitions, proliferation, and interactions through neural networks,
CytoBridge offers the flexibility to learn these processes directly from data.
The effectiveness of our method has been extensively validated using both
synthetic gene regulatory data and real scRNA-seq datasets. Compared to
existing methods, CytoBridge identifies growth, transition, and interaction
patterns, eliminates false transitions, and reconstructs the developmental
landscape with greater accuracy.

</details>


### [91] [Sample Efficient Reinforcement Learning via Large Vision Language Model Distillation](https://arxiv.org/abs/2505.11221)
*Donghoon Lee, Tung M. Luu, Younghwan Lee, Chang D. Yoo*

**主要类别:** cs.LG

**概要:** This paper introduces LVLM2P, a framework that uses large vision-language models to teach task-specific reinforcement learning agents, improving sample efficiency and applicability.


<details>
  <summary>Details</summary>
  
**动机:** The motivation is to make reinforcement learning more practical by reducing its sample complexity and increasing its applicability across diverse tasks.

**方法:** LVLM2P uses a large vision-language model as a teacher to provide instructional actions to a reinforcement learning agent, reducing unnecessary exploration and suggesting actions directly from visual observations.

**结果:** Experiments show that LVLM2P significantly improves the sample efficiency of baseline reinforcement learning algorithms.

**结论:** LVLM2P is a promising approach to making reinforcement learning more efficient and applicable for real-world use.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sample+Efficient+Reinforcement+Learning+via+Large+Vision+Language+Model+Distillation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11221，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11221&send_immediately=true&force_search=false)

**原文摘要:** Recent research highlights the potential of multimodal foundation models in
tackling complex decision-making challenges. However, their large parameters
make real-world deployment resource-intensive and often impractical for
constrained systems. Reinforcement learning (RL) shows promise for
task-specific agents but suffers from high sample complexity, limiting
practical applications. To address these challenges, we introduce LVLM to
Policy (LVLM2P), a novel framework that distills knowledge from large
vision-language models (LVLM) into more efficient RL agents. Our approach
leverages the LVLM as a teacher, providing instructional actions based on
trajectories collected by the RL agent, which helps reduce less meaningful
exploration in the early stages of learning, thereby significantly accelerating
the agent's learning progress. Additionally, by leveraging the LVLM to suggest
actions directly from visual observations, we eliminate the need for manual
textual descriptors of the environment, enhancing applicability across diverse
tasks. Experiments show that LVLM2P significantly enhances the sample
efficiency of baseline RL algorithms.

</details>


### [92] [Learning traffic flows: Graph Neural Networks for Metamodelling Traffic Assignment](https://arxiv.org/abs/2505.11230)
*Oskar Bohn Lassen, Serio Agriesti, Mohamed Eldafrawi, Daniele Gammelli, Guido Cantelmo, Guido Gentile, Francisco Camara Pereira*

**主要类别:** cs.LG

**概要:** This paper presents a learning-based method using Message-Passing Neural Networks to approximate the equilibrium flow of the Stochastic User Equilibrium assignment, aiming to accelerate out-of-distribution scenario assessments and enable real-time decision-making.


<details>
  <summary>Details</summary>
  
**动机:** Traditional traffic assignment methods are computationally expensive and challenging for real-time or large-scale scenario analysis.

**方法:** A learning-based approach using Message-Passing Neural Networks as a metamodel to approximate the equilibrium flow.

**结果:** The model is benchmarked against other conventional deep learning techniques and evaluated for robustness in predicting traffic flows on unseen data.

**结论:** This approach offers a promising solution for accelerating out-of-distribution scenario assessments, reducing computational costs, and enabling real-time decision-making.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+traffic+flows%3A+Graph+Neural+Networks+for+Metamodelling+Traffic+Assignment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11230，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11230&send_immediately=true&force_search=false)

**原文摘要:** The Traffic Assignment Problem is a fundamental, yet computationally
expensive, task in transportation modeling, especially for large-scale
networks. Traditional methods require iterative simulations to reach
equilibrium, making real-time or large-scale scenario analysis challenging. In
this paper, we propose a learning-based approach using Message-Passing Neural
Networks as a metamodel to approximate the equilibrium flow of the Stochastic
User Equilibrium assignment. Our model is designed to mimic the algorithmic
structure used in conventional traffic simulators allowing it to better capture
the underlying process rather than just the data. We benchmark it against other
conventional deep learning techniques and evaluate the model's robustness by
testing its ability to predict traffic flows on input data outside the domain
on which it was trained. This approach offers a promising solution for
accelerating out-of-distribution scenario assessments, reducing computational
costs in large-scale transportation planning, and enabling real-time
decision-making.

</details>


### [93] [Memory-Efficient Orthogonal Fine-Tuning with Principal Subspace Adaptation](https://arxiv.org/abs/2505.11235)
*Fei Wu, Jia Hu, Geyong Min, Shiqiang Wang*

**主要类别:** cs.LG

**概要:** 提出了一种新的内存高效正交微调方法MOFT，通过主子空间适应在保持超球能量的同时减少了内存消耗，并在多种任务和模型上表现出色。


<details>
  <summary>Details</summary>
  
**动机:** 现有正交微调方法因存储多个全维度稀疏矩阵的中间激活而内存效率低。

**方法:** 提出Memory-efficient Orthogonal Fine-Tuning (MOFT)，通过主子空间适应来约束正交微调，并引入两个可学习缩放向量增强灵活性。

**结果:** MOFT在37个多样化的任务和四个NLP和CV模型上显著降低了正交微调的内存占用并优于关键基线。

**结论:** MOFT是一种有效的参数高效微调方法，具有良好的性能和内存效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Memory-Efficient+Orthogonal+Fine-Tuning+with+Principal+Subspace+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11235，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11235&send_immediately=true&force_search=false)

**原文摘要:** Driven by the relentless growth in model parameters, which renders full
fine-tuning prohibitively expensive for large-scale deployment,
parameter-efficient fine-tuning (PEFT) has emerged as a crucial approach for
rapidly adapting large models to a wide range of downstream tasks. Among the
PEFT family, orthogonal fine-tuning and its variants have demonstrated
remarkable performance by preserving hyperspherical energy, which encodes
pairwise angular similarity between neurons. However, these methods are
inherently memory-inefficient due to the need to store intermediate activations
from multiple full-dimensional sparse matrices. To address this limitation, we
propose Memory-efficient Orthogonal Fine-Tuning (MOFT) with principal subspace
adaptation. Specifically, we first establish a theoretical condition under
which orthogonal transformations within a low-rank subspace preserve
hyperspherical energy. Based on this insight, we constrain orthogonal
fine-tuning to the principal subspace defined by the top-r components obtained
through singular value decomposition and impose an additional constraint on the
projection matrix to satisfy the preservation condition. To enhance MOFT's
flexibility across tasks, we relax strict orthogonality by introducing two
learnable scaling vectors. Extensive experiments on 37 diverse tasks and four
models across NLP and CV demonstrate that MOFT consistently outperforms key
baselines while significantly reducing the memory footprint of orthogonal
fine-tuning.

</details>


### [94] [Massive-STEPS: Massive Semantic Trajectories for Understanding POI Check-ins -- Dataset and Benchmarks](https://arxiv.org/abs/2505.11239)
*Wilson Wongso, Hao Xue, Flora D. Salim*

**主要类别:** cs.LG

**概要:** A new large-scale dataset called Massive-STEPS is introduced to improve POI recommendation research.


<details>
  <summary>Details</summary>
  
**动机:** Existing datasets are outdated and lack diversity.

**方法:** Developed a new dataset based on Semantic Trails dataset with enriched POI metadata.

**结果:** The dataset covers 12 diverse cities with more recent and longer check-in data.

**结论:** Releasing Massive-STEPS aims to promote reproducible and equitable research in human mobility and POI recommendation.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Massive-STEPS%3A+Massive+Semantic+Trajectories+for+Understanding+POI+Check-ins+--+Dataset+and+Benchmarks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11239，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11239&send_immediately=true&force_search=false)

**原文摘要:** Understanding human mobility through Point-of-Interest (POI) recommendation
is increasingly important for applications such as urban planning, personalized
services, and generative agent simulation. However, progress in this field is
hindered by two key challenges: the over-reliance on older datasets from
2012-2013 and the lack of reproducible, city-level check-in datasets that
reflect diverse global regions. To address these gaps, we present Massive-STEPS
(Massive Semantic Trajectories for Understanding POI Check-ins), a large-scale,
publicly available benchmark dataset built upon the Semantic Trails dataset and
enriched with semantic POI metadata. Massive-STEPS spans 12 geographically and
culturally diverse cities and features more recent (2017-2018) and
longer-duration (24 months) check-in data than prior datasets. We benchmarked a
wide range of POI recommendation models on Massive-STEPS using both supervised
and zero-shot approaches, and evaluated their performance across multiple urban
contexts. By releasing Massive-STEPS, we aim to facilitate reproducible and
equitable research in human mobility and POI recommendation. The dataset and
benchmarking code are available at:
https://github.com/cruiseresearchgroup/Massive-STEPS

</details>


### [95] [Rethinking Irregular Time Series Forecasting: A Simple yet Effective Baseline](https://arxiv.org/abs/2505.11250)
*Xvyuan Liu, Xiangfei Qiu, Xingjian Wu, Zhengyu Li, Chenjuan Guo, Jilin Hu, Bin Yang*

**主要类别:** cs.LG

**概要:** 提出了一种新的框架APN来解决不规则多变量时间序列预测的问题，该框架包括一个TAPA模块用于自适应分区和一个简单的查询模块用于整合历史信息，最终通过浅层MLP进行预测，实验表明APN在效率和准确性上都优于现有的最先进方法。


<details>
  <summary>Details</summary>
  
**动机:** 现有方法难以应对不规则时间序列中的不规则性和数据缺失问题，并且大多数方法复杂且资源密集。

**方法:** 设计了一个名为TAPA的新型时间感知补丁聚合模块，使用动态可调分区边界和时间感知加权平均策略；使用简单查询模块整合历史信息；通过浅层MLP进行预测。

**结果:** 在多个真实世界的数据集上的实验表明，APN在效率和准确性方面优于现有的最先进方法。

**结论:** APN提供了一种有效且高效的方法来预测不规则多变量时间序列。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+Irregular+Time+Series+Forecasting%3A+A+Simple+yet+Effective+Baseline，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11250，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11250&send_immediately=true&force_search=false)

**原文摘要:** The forecasting of irregular multivariate time series (IMTS) is crucial in
key areas such as healthcare, biomechanics, climate science, and astronomy.
However, achieving accurate and practical predictions is challenging due to two
main factors. First, the inherent irregularity and data missingness in
irregular time series make modeling difficult. Second, most existing methods
are typically complex and resource-intensive. In this study, we propose a
general framework called APN to address these challenges. Specifically, we
design a novel Time-Aware Patch Aggregation (TAPA) module that achieves
adaptive patching. By learning dynamically adjustable patch boundaries and a
time-aware weighted averaging strategy, TAPA transforms the original irregular
sequences into high-quality, regularized representations in a
channel-independent manner. Additionally, we use a simple query module to
effectively integrate historical information while maintaining the model's
efficiency. Finally, predictions are made by a shallow MLP. Experimental
results on multiple real-world datasets show that APN outperforms existing
state-of-the-art methods in both efficiency and accuracy.

</details>


### [96] [Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction](https://arxiv.org/abs/2505.11254)
*Jeffrey Willette, Heejun Lee, Sung Ju Hwang*

**主要类别:** cs.LG

**概要:** A novel method corrects the distributional shift in sparse attention outputs, improving performance by 36% without significantly increasing computation time.


<details>
  <summary>Details</summary>
  
**动机:** To address the computational inefficiency and performance degradation issues in sparse attention methods.

**方法:** Proposes a method to adjust the distribution of sparse attention outputs to match those of quadratic attention.

**结果:** Improves performance by 36%, recovers 88% of quadratic attention accuracy, maintains 98.5% sparsity, and makes the model 32 times faster than Flash Attention 2.

**结论:** Introduces a simple yet effective solution that enhances the performance of sparse attention methods while maintaining their efficiency.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Delta+Attention%3A+Fast+and+Accurate+Sparse+Attention+Inference+by+Delta+Correction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11254，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11254&send_immediately=true&force_search=false)

**原文摘要:** The attention mechanism of a transformer has a quadratic complexity, leading
to high inference costs and latency for long sequences. However, attention
matrices are mostly sparse, which implies that many entries may be omitted from
computation for efficient inference. Sparse attention inference methods aim to
reduce this computational burden; however, they also come with a troublesome
performance degradation. We discover that one reason for this degradation is
that the sparse calculation induces a distributional shift in the attention
outputs. The distributional shift causes decoding-time queries to fail to align
well with the appropriate keys from the prefill stage, leading to a drop in
performance. We propose a simple, novel, and effective procedure for correcting
this distributional shift, bringing the distribution of sparse attention
outputs closer to that of quadratic attention. Our method can be applied on top
of any sparse attention method, and results in an average 36%pt performance
increase, recovering 88% of quadratic attention accuracy on the 131K RULER
benchmark when applied on top of sliding window attention with sink tokens
while only adding a small overhead. Our method can maintain approximately 98.5%
sparsity over full quadratic attention, making our model 32 times faster than
Flash Attention 2 when processing 1M token prefills.

</details>


### [97] [Fourier Low-rank and Sparse Tensor for Efficient Tensor Completion](https://arxiv.org/abs/2505.11261)
*Jingyang Li, Jiuqian Shang, Yang Chen*

**主要类别:** cs.LG

**概要:** 提出了一种新的张量补全模型FLoST，通过傅里叶变换分解时间维度，结合低秩矩阵和稀疏性来高效建模平滑和局部变化，相比现有方法在准确性和计算效率上均有提升。


<details>
  <summary>Details</summary>
  
**动机:** 传统张量模型未能捕捉科学数据中的独特时空模式，尤其是时间维度上的低频稳定性和高频变化。

**方法:** 提出FLoST模型，利用傅里叶变换沿时间维度分解张量，使用低秩矩阵捕获低频成分，使用稀疏性捕获高频波动。

**结果:** FLoST模型在多个任务上表现出色，尤其是在大规模时间维度时，具有显著的计算优势。

**结论:** FLoST提供了一种更高效且可解释的时空数据重建解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fourier+Low-rank+and+Sparse+Tensor+for+Efficient+Tensor+Completion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11261，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11261&send_immediately=true&force_search=false)

**原文摘要:** Tensor completion is crucial in many scientific domains with missing data
problems. Traditional low-rank tensor models, including CP, Tucker, and
Tensor-Train, exploit low-dimensional structures to recover missing data.
However, these methods often treat all tensor modes symmetrically, failing to
capture the unique spatiotemporal patterns inherent in scientific data, where
the temporal component exhibits both low-frequency stability and high-frequency
variations. To address this, we propose a novel model, \underline{F}ourier
\underline{Lo}w-rank and \underline{S}parse \underline{T}ensor (FLoST), which
decomposes the tensor along the temporal dimension using a Fourier transform.
This approach captures low-frequency components with low-rank matrices and
high-frequency fluctuations with sparsity, resulting in a hybrid structure that
efficiently models both smooth and localized variations. Compared to the
well-known tubal-rank model, which assumes low-rankness across all frequency
components, FLoST requires significantly fewer parameters, making it
computationally more efficient, particularly when the time dimension is large.
Through theoretical analysis and empirical experiments, we demonstrate that
FLoST outperforms existing tensor completion models in terms of both accuracy
and computational efficiency, offering a more interpretable solution for
spatiotemporal data reconstruction.

</details>


### [98] [Driving Mechanisms and Forecasting of China's Pet Population-An ARIMA-RF-HW Hybrid Approach](https://arxiv.org/abs/2505.11269)
*Shengjia Chang, Xianshuo Yue*

**主要类别:** cs.LG

**概要:** 提出了一种整合ARIMA、Random Forest和Holt-Winters的动态加权混合模型来提高对中国宠物数量预测的准确性。使用了2005-2023年的数据，并进行了预处理。结果显示关键驱动因素及宠物数量预测趋势。


<details>
  <summary>Details</summary>
  
**动机:** 提升中国宠物数量预测的准确性，支持政策制定者优化宠物健康管理并指导企业发展。

**方法:** 采用整合ARIMA、Random Forest和Holt-Winters的动态加权混合模型，对经济、社会和政策指标进行预处理。

**结果:** 发现城市收入、消费、老龄化比例和政策数量是主要驱动因素，猫的数量稳步增长而狗的数量波动。

**结论:** 该研究为宠物健康管理和行业发展提供了支持，推动行业的可持续增长。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Driving+Mechanisms+and+Forecasting+of+China%27s+Pet+Population-An+ARIMA-RF-HW+Hybrid+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11269，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11269&send_immediately=true&force_search=false)

**原文摘要:** This study proposes a dynamically weighted ARIMA-RF-HW hybrid model
integrating ARIMA for seasonality and trends, Random Forest for nonlinear
features, and Holt-Winters smoothing for seasonal adjustment to improve China's
pet population forecasting accuracy. Using 2005-2023 data with nine economic,
social, and policy indicators (urban income, consumption, aging ratio, policy
quantity, new veterinary drug approvals), data were preprocessed via Z-score
normalization and missing value imputation. The results show that key drivers
of pet populations include urban income (19.48% for cats, 17.15% for dogs),
consumption (17.99% for cats), and policy quantity (13.33% for cats, 14.02% for
dogs), with aging (12.81% for cats, 13.27% for dogs) and urbanization
amplifying the demand for pets. Forecasts show steady cat growth and
fluctuating dog numbers, reflecting cats' adaptability to urban environments.
This research supports policymakers in optimizing pet health management and
guides enterprises in developing differentiated services, advancing sustainable
industry growth.

</details>


### [99] [Multiclass threshold-based classification](https://arxiv.org/abs/2505.11276)
*Francesco Marchetti, Edoardo Legnaro, Sabrina Guastavino*

**主要类别:** cs.LG

**概要:** 提出了一种基于阈值的多类分类框架，该框架通过在多维单纯形上的几何解释来推广标准的argmax规则，并通过调整阈值优化分类评分。实验表明，这种方法在不同网络和数据集上都能带来一致的性能提升。


<details>
  <summary>Details</summary>
  
**动机:** 现有的多类分类方法存在局限性，需要一种新的方法来进一步提高预测能力。

**方法:** 提出了一种基于阈值的多类分类框架，通过几何解释替换softmax输出的概率解释，并定义了基于得分的损失函数。

**结果:** 实验结果显示，多维阈值调整在各种网络和数据集上都能带来一致的性能提升，并且提出的多类得分导向损失与标准损失函数具有竞争力。

**结论:** 所提出的基于阈值的多类分类框架能够进一步优化分类评分并提高预测能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multiclass+threshold-based+classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11276，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11276&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we introduce a threshold-based framework for multiclass
classification that generalizes the standard argmax rule. This is done by
replacing the probabilistic interpretation of softmax outputs with a geometric
one on the multidimensional simplex, where the classification depends on a
multidimensional threshold. This change of perspective enables for any trained
classification network an a posteriori optimization of the classification score
by means of threshold tuning, as usually carried out in the binary setting.
This allows a further refinement of the prediction capability of any network.
Moreover, this multidimensional threshold-based setting makes it possible to
define score-oriented losses, which are based on the interpretation of the
threshold as a random variable. Our experiments show that the multidimensional
threshold tuning yields consistent performance improvements across various
networks and datasets, and that the proposed multiclass score-oriented losses
are competitive with standard loss functions, resembling the advantages
observed in the binary case.

</details>


### [100] [SubROC: AUC-Based Discovery of Exceptional Subgroup Performance for Binary Classifiers](https://arxiv.org/abs/2505.11283)
*Tom Siegl, Kutalmış Coşkun, Bjarne Hiller, Amin Mirzaei, Florian Lemmerich, Martin Becker*

**主要类别:** cs.LG

**概要:** 提出SubROC框架，用于通过挖掘异常模型来寻找分类模型在可解释的人口子群体中的优缺点。


<details>
  <summary>Details</summary>
  
**动机:** 现有方法缺乏一个高效连贯的框架来有效搜索模型在不同人群中的表现差异。

**方法:** 基于Exceptional Model Mining开发SubROC框架，该框架具有多种功能如常见的评估度量（ROC和PR AUC）、高效的搜索空间修剪等。

**结果:** SubROC在案例研究和跨多个数据集的比较分析中展示了实际应用的好处。

**结论:** SubROC是一个开源且易于使用的框架，可以可靠有效地找到分类模型在可解释人口子群中的优缺点。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SubROC%3A+AUC-Based+Discovery+of+Exceptional+Subgroup+Performance+for+Binary+Classifiers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11283，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11283&send_immediately=true&force_search=false)

**原文摘要:** Machine learning (ML) is increasingly employed in real-world applications
like medicine or economics, thus, potentially affecting large populations.
However, ML models often do not perform homogeneously across such populations
resulting in subgroups of the population (e.g., sex=female AND
marital_status=married) where the model underperforms or, conversely, is
particularly accurate. Identifying and describing such subgroups can support
practical decisions on which subpopulation a model is safe to deploy or where
more training data is required. The potential of identifying and analyzing such
subgroups has been recognized, however, an efficient and coherent framework for
effective search is missing. Consequently, we introduce SubROC, an open-source,
easy-to-use framework based on Exceptional Model Mining for reliably and
efficiently finding strengths and weaknesses of classification models in the
form of interpretable population subgroups. SubROC incorporates common
evaluation measures (ROC and PR AUC), efficient search space pruning for fast
exhaustive subgroup search, control for class imbalance, adjustment for
redundant patterns, and significance testing. We illustrate the practical
benefits of SubROC in case studies as well as in comparative analyses across
multiple datasets.

</details>


### [101] [Bidirectional Information Flow (BIF) -- A Sample Efficient Hierarchical Gaussian Process for Bayesian Optimization](https://arxiv.org/abs/2505.11294)
*Juan D. Guerra, Thomas Garbay, Guillaume Lajoie, Marco Bonizzato*

**主要类别:** cs.LG

**概要:** 提出双向信息流（BIF）框架，显著提高分层高斯过程模型的样本效率和收敛速度，在神经刺激优化任务中表现优异。


<details>
  <summary>Details</summary>
  
**动机:** 传统分层高斯过程模型未充分利用其层次结构，仅实现单向信息流动，导致样本效率低且收敛慢。

**方法:** 设计了双向信息流（BIF），在分层高斯过程中实现父模型与子模型间的双向信息交换，并引入自上而下的反馈机制来优化子模型。

**结果:** BIF框架在合成数据和真实世界神经刺激优化任务中表现出色，分别提升了85%和5倍的R²评分。

**结论:** 提出的BIF框架有效增强了分层高斯过程模型的性能，特别是在需要高效在线学习的复杂任务中具有重要应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bidirectional+Information+Flow+%28BIF%29+--+A+Sample+Efficient+Hierarchical+Gaussian+Process+for+Bayesian+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11294，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11294&send_immediately=true&force_search=false)

**原文摘要:** Hierarchical Gaussian Process (H-GP) models divide problems into different
subtasks, allowing for different models to address each part, making them
well-suited for problems with inherent hierarchical structure. However, typical
H-GP models do not fully take advantage of this structure, only sending
information up or down the hierarchy. This one-way coupling limits sample
efficiency and slows convergence. We propose Bidirectional Information Flow
(BIF), an efficient H-GP framework that establishes bidirectional information
exchange between parent and child models in H-GPs for online training. BIF
retains the modular structure of hierarchical models - the parent combines
subtask knowledge from children GPs - while introducing top-down feedback to
continually refine children models during online learning. This mutual exchange
improves sample efficiency, enables robust training, and allows modular reuse
of learned subtask models. BIF outperforms conventional H-GP Bayesian
Optimization methods, achieving up to 85% and 5x higher $R^2$ scores for the
parent and children respectively, on synthetic and real-world neurostimulation
optimization tasks.

</details>


### [102] [Graph Representational Learning: When Does More Expressivity Hurt Generalization?](https://arxiv.org/abs/2505.11298)
*Sohir Maskey, Raffaele Paolino, Fabian Jogl, Gitta Kutyniok, Johannes F. Lutzeyer*

**主要类别:** cs.LG

**概要:** This paper explores the relationship between the expressivity of Graph Neural Networks (GNNs) and their predictive performance.


<details>
  <summary>Details</summary>
  
**动机:** To understand why more expressive GNNs may generalize worse and how this can be mitigated.

**方法:** Introduces a family of premetrics to capture structural similarity between graphs and relates these similarities to generalization.

**结果:** Derives generalization bounds that depend on the distance between training and test graphs, model complexity, and training set size.

**结论:** More expressive GNNs may generalize worse unless their increased complexity is balanced by a larger training set or reduced distance between training and test graphs.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph+Representational+Learning%3A+When+Does+More+Expressivity+Hurt+Generalization%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11298，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11298&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) are powerful tools for learning on structured
data, yet the relationship between their expressivity and predictive
performance remains unclear. We introduce a family of premetrics that capture
different degrees of structural similarity between graphs and relate these
similarities to generalization, and consequently, the performance of expressive
GNNs. By considering a setting where graph labels are correlated with
structural features, we derive generalization bounds that depend on the
distance between training and test graphs, model complexity, and training set
size. These bounds reveal that more expressive GNNs may generalize worse unless
their increased complexity is balanced by a sufficiently large training set or
reduced distance between training and test graphs. Our findings relate
expressivity and generalization, offering theoretical insights supported by
empirical results.

</details>


### [103] [Effective Probabilistic Time Series Forecasting with Fourier Adaptive Noise-Separated Diffusion](https://arxiv.org/abs/2505.11306)
*Xinyan Wang, Rui Dai, Kaikui Liu, Xiangxiang Chu*

**主要类别:** cs.LG

**概要:** 提出了一种新的时间序列预测概率框架FALDA，通过DMRR统一扩散回归方法，并利用Fourier分解和轻量级去噪器DEMA来减少认知不确定性，提高长期时间序列预测的性能。


<details>
  <summary>Details</summary>
  
**动机:** 解决现有时间序列预测方法在处理长期预测时的不确定性问题。

**方法:** 引入DMRR框架，使用Fourier分解和DEMA轻量级去噪器来改进预测模型。

**结果:** FALDA在六个真实世界数据集上的实验表现优于其他现有概率预测方法，并且在点预测方面也超过了最先进的方法。

**结论:** FALDA有效减少了认知不确定性，提高了长期时间序列预测的效率和准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Effective+Probabilistic+Time+Series+Forecasting+with+Fourier+Adaptive+Noise-Separated+Diffusion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11306，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11306&send_immediately=true&force_search=false)

**原文摘要:** We propose the Fourier Adaptive Lite Diffusion Architecture (FALDA), a novel
probabilistic framework for time series forecasting. First, we introduce the
Diffusion Model for Residual Regression (DMRR) framework, which unifies
diffusion-based probabilistic regression methods. Within this framework, FALDA
leverages Fourier-based decomposition to incorporate a component-specific
architecture, enabling tailored modeling of individual temporal components. A
conditional diffusion model is utilized to estimate the future noise term,
while our proposed lightweight denoiser, DEMA (Decomposition MLP with AdaLN),
conditions on the historical noise term to enhance denoising performance.
Through mathematical analysis and empirical validation, we demonstrate that
FALDA effectively reduces epistemic uncertainty, allowing probabilistic
learning to primarily focus on aleatoric uncertainty. Experiments on six
real-world benchmarks demonstrate that FALDA consistently outperforms existing
probabilistic forecasting approaches across most datasets for long-term time
series forecasting while achieving enhanced computational efficiency without
compromising accuracy. Notably, FALDA also achieves superior overall
performance compared to state-of-the-art (SOTA) point forecasting approaches,
with improvements of up to 9%.

</details>


### [104] [Diffusion Learning with Partial Agent Participation and Local Updates](https://arxiv.org/abs/2505.11307)
*Elsa Rizk, Kun Yuan, Ali H. Sayed*

**主要类别:** cs.LG

**概要:** This paper introduces an improved diffusion learning method that reduces communication frequency and increases reliability by allowing local updates and partial agent participation. Theoretical stability and MSD performance analysis are provided, along with numerical experiments.


<details>
  <summary>Details</summary>
  
**动机:** To address the communication overhead and reliability issues in traditional diffusion learning caused by frequent communication and volatile edge devices.

**方法:** Enhanced diffusion learning approach with local updates and partial agent participation.

**结果:** The proposed algorithm is stable in the mean-square error sense, and its MSD performance is analyzed. Numerical experiments support the theoretical findings.

**结论:** The improved diffusion learning method successfully reduces communication needs and enhances reliability, offering a practical solution for privacy protection and real-time response in edge intelligence.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Diffusion+Learning+with+Partial+Agent+Participation+and+Local+Updates，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11307，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11307&send_immediately=true&force_search=false)

**原文摘要:** Diffusion learning is a framework that endows edge devices with advanced
intelligence. By processing and analyzing data locally and allowing each agent
to communicate with its immediate neighbors, diffusion effectively protects the
privacy of edge devices, enables real-time response, and reduces reliance on
central servers. However, traditional diffusion learning relies on
communication at every iteration, leading to communication overhead, especially
with large learning models. Furthermore, the inherent volatility of edge
devices, stemming from power outages or signal loss, poses challenges to
reliable communication between neighboring agents. To mitigate these issues,
this paper investigates an enhanced diffusion learning approach incorporating
local updates and partial agent participation. Local updates will curtail
communication frequency, while partial agent participation will allow for the
inclusion of agents based on their availability. We prove that the resulting
algorithm is stable in the mean-square error sense and provide a tight analysis
of its Mean-Square-Deviation (MSD) performance. Various numerical experiments
are conducted to illustrate our theoretical findings.

</details>


### [105] [Reinforcement Learning Closures for Underresolved Partial Differential Equations using Synthetic Data](https://arxiv.org/abs/2505.11308)
*Lothar Heimbach, Sebastian Kaltenbach, Petr Karnakov, Francis J. Alexander, Petros Koumoutsakos*

**主要类别:** cs.LG

**概要:** 提出一种利用合成数据和强化学习来构建偏微分方程闭合模型的框架，并通过一维和二维Burgers方程及二维平流方程验证其有效性。


<details>
  <summary>Details</summary>
  
**动机:** 解决实际应用中求解偏微分方程计算成本高的问题，通过粗粒度近似降低计算资源需求，但会损失细节，因此需要闭合模型来表示未解析的空间-时间相互作用。

**方法:** 使用人工制造解方法获得合成数据，结合强化学习为粗粒度偏微分方程提供闭合模型。

**结果:** 该方法在Burgers方程和二维平流方程上有效，并且对于非均匀偏微分方程训练出的闭合模型可以很好地推广到均匀偏微分方程。

**结论:** 展示了为数据稀缺系统开发准确且计算高效的闭合模型的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reinforcement+Learning+Closures+for+Underresolved+Partial+Differential+Equations+using+Synthetic+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11308，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11308&send_immediately=true&force_search=false)

**原文摘要:** Partial Differential Equations (PDEs) describe phenomena ranging from
turbulence and epidemics to quantum mechanics and financial markets. Despite
recent advances in computational science, solving such PDEs for real-world
applications remains prohibitively expensive because of the necessity of
resolving a broad range of spatiotemporal scales. In turn, practitioners often
rely on coarse-grained approximations of the original PDEs, trading off
accuracy for reduced computational resources. To mitigate the loss of detail
inherent in such approximations, closure models are employed to represent
unresolved spatiotemporal interactions. We present a framework for developing
closure models for PDEs using synthetic data acquired through the method of
manufactured solutions. These data are used in conjunction with reinforcement
learning to provide closures for coarse-grained PDEs. We illustrate the
efficacy of our method using the one-dimensional and two-dimensional Burgers'
equations and the two-dimensional advection equation. Moreover, we demonstrate
that closure models trained for inhomogeneous PDEs can be effectively
generalized to homogeneous PDEs. The results demonstrate the potential for
developing accurate and computationally efficient closure models for systems
with scarce data.

</details>


### [106] [Where You Place the Norm Matters: From Prejudiced to Neutral Initializations](https://arxiv.org/abs/2505.11312)
*Emanuele Francazi, Francesco Pinto, Aurelien Lucchi, Marco Baity-Jesi*

**主要类别:** cs.LG

**概要:** 研究了归一化层在神经网络中的位置对模型初始预测统计特性的影响，揭示了归一化设置如何影响早期训练行为。


<details>
  <summary>Details</summary>
  
**动机:** 详细理解归一化如何从初始化开始影响模型行为仍然是一个重要问题。

**方法:** 分析归一化层的存在和放置对隐藏层统计特性的影响，并研究其对初始类别预测分布的作用。

**结果:** 归一化放置会在神经网络的初始预测行为中诱导系统性差异，并塑造学习动态。

**结论:** 本研究通过链接架构选择与初始化时的预测统计，提供了对归一化影响早期训练行为的原理性理解，并为更可控且可解释的网络设计提供指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Where+You+Place+the+Norm+Matters%3A+From+Prejudiced+to+Neutral+Initializations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11312，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11312&send_immediately=true&force_search=false)

**原文摘要:** Normalization layers, such as Batch Normalization and Layer Normalization,
are central components in modern neural networks, widely adopted to improve
training stability and generalization. While their practical effectiveness is
well documented, a detailed theoretical understanding of how normalization
affects model behavior, starting from initialization, remains an important open
question. In this work, we investigate how both the presence and placement of
normalization within hidden layers influence the statistical properties of
network predictions before training begins. In particular, we study how these
choices shape the distribution of class predictions at initialization, which
can range from unbiased (Neutral) to highly concentrated (Prejudiced) toward a
subset of classes. Our analysis shows that normalization placement induces
systematic differences in the initial prediction behavior of neural networks,
which in turn shape the dynamics of learning. By linking architectural choices
to prediction statistics at initialization, our work provides a principled
understanding of how normalization can influence early training behavior and
offers guidance for more controlled and interpretable network design.

</details>


### [107] [Anomaly Detection for Non-stationary Time Series using Recurrent Wavelet Probabilistic Neural Network](https://arxiv.org/abs/2505.11321)
*Pu Yang, J. A. Barria*

**主要类别:** cs.LG

**概要:** 提出了一种无监督的循环小波概率神经网络（RWPNN），用于非平稳环境中时间序列异常检测。该模型由一个堆叠递归编码器-解码器模块和一个多尺度小波概率网络组成，能够处理高维数据并适应不同变化率的数据集。实验验证了其在多个实际数据集上的性能以及提前预警异常事件的能力。


<details>
  <summary>Details</summary>
  
**动机:** 针对非平稳环境下的时间序列异常检测问题，设计一种新的无监督学习方法。

**方法:** 结合了堆叠递归编码器-解码器模块与多尺度小波概率网络的方法，构建了一个新的框架来捕捉时间特征并生成概率模型。

**结果:** 在来自多个领域的45个真实世界时间序列数据集上进行了评估，证明了所提方法在时间序列异常检测任务中的有效性及提供早期预警的能力。

**结论:** 提出的RWPNN框架可以更准确地检测非平稳环境下的异常，且无需强分布假设，具有较好的鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Anomaly+Detection+for+Non-stationary+Time+Series+using+Recurrent+Wavelet+Probabilistic+Neural+Network，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11321，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11321&send_immediately=true&force_search=false)

**原文摘要:** In this paper, an unsupervised Recurrent Wavelet Probabilistic Neural Network
(RWPNN) is proposed, which aims at detecting anomalies in non-stationary
environments by modelling the temporal features using a nonparametric density
estimation network. The novel framework consists of two components, a Stacked
Recurrent Encoder-Decoder (SREnc-Dec) module that captures temporal features in
a latent space, and a Multi-Receptive-field Wavelet Probabilistic Network
(MRWPN) that creates an ensemble probabilistic model to characterise the latent
space. This formulation extends the standard wavelet probabilistic networks to
wavelet deep probabilistic networks, which can handle higher data
dimensionality. The MRWPN module can adapt to different rates of data variation
in different datasets without imposing strong distribution assumptions,
resulting in a more robust and accurate detection for Time Series Anomaly
Detection (TSAD) tasks in the non-stationary environment. We carry out the
assessment on 45 real-world time series datasets from various domains, verify
the performance of RWPNN in TSAD tasks with several constraints, and show its
ability to provide early warnings for anomalous events.

</details>


### [108] [The Final Layer Holds the Key: A Unified and Efficient GNN Calibration Framework](https://arxiv.org/abs/2505.11335)
*Jincheng Huang, Jie Xu, Xiaoshuang Shi, Ping Hu, Lei Feng, Xiaofeng Zhu*

**主要类别:** cs.LG

**概要:** 提出了一种新的图神经网络校准方法，通过调整最终层参数的权重衰减来缓解GNN的预测置信度不足的问题，并在多个实验中验证了该方法的有效性。


<details>
  <summary>Details</summary>
  
**动机:** 现有的GNN校准方法通常引入额外的组件，未能捕捉模型与预测置信度之间的内在关系，导致理论保证有限且计算开销增加。

**方法:** 通过建立统一的理论框架，揭示模型置信度由类别质心级和节点级校准共同决定，并通过减少最终层参数的权重衰减来实现校准。

**结果:** 实验结果表明所提出的方法在多个数据集上优于现有方法。

**结论:** 提出的方法能够有效缓解GNN的预测置信度不足问题，具有较高的实用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Final+Layer+Holds+the+Key%3A+A+Unified+and+Efficient+GNN+Calibration+Framework，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11335，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11335&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) have demonstrated remarkable effectiveness on
graph-based tasks. However, their predictive confidence is often miscalibrated,
typically exhibiting under-confidence, which harms the reliability of their
decisions. Existing calibration methods for GNNs normally introduce additional
calibration components, which fail to capture the intrinsic relationship
between the model and the prediction confidence, resulting in limited
theoretical guarantees and increased computational overhead. To address this
issue, we propose a simple yet efficient graph calibration method. We establish
a unified theoretical framework revealing that model confidence is jointly
governed by class-centroid-level and node-level calibration at the final layer.
Based on this insight, we theoretically show that reducing the weight decay of
the final-layer parameters alleviates GNN under-confidence by acting on the
class-centroid level, while node-level calibration acts as a finer-grained
complement to class-centroid level calibration, which encourages each test node
to be closer to its predicted class centroid at the final-layer
representations. Extensive experiments validate the superiority of our method.

</details>


### [109] [Sobolev Training of End-to-End Optimization Proxies](https://arxiv.org/abs/2505.11342)
*Andrew W. Rosemberg, Joaquim Dias Garcia, Russell Bent, Pascal Van Hentenryck*

**主要类别:** cs.LG

**概要:** This work introduces Sobolev training into optimization proxies, improving their accuracy and reliability in approximating solutions to complex optimization problems.


<details>
  <summary>Details</summary>
  
**动机:** To enhance the accuracy of optimization proxies by incorporating solver sensitivities using a Sobolev training approach.

**方法:** Integrating solver sensitivities into end-to-end proxies through Sobolev training in both fully supervised and self-supervised settings.

**结果:** Supervised Sobolev training significantly reduced mean squared error and constraint violations in power flow benchmarks. Self-supervised training improved optimality gaps in portfolio tasks.

**结论:** Sobolev training provides a method for creating fast and reliable surrogate models for large-scale optimization problems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sobolev+Training+of+End-to-End+Optimization+Proxies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11342，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11342&send_immediately=true&force_search=false)

**原文摘要:** Optimization proxies - machine learning models trained to approximate the
solution mapping of parametric optimization problems in a single forward pass -
offer dramatic reductions in inference time compared to traditional iterative
solvers. This work investigates the integration of solver sensitivities into
such end to end proxies via a Sobolev training paradigm and does so in two
distinct settings: (i) fully supervised proxies, where exact solver outputs and
sensitivities are available, and (ii) self supervised proxies that rely only on
the objective and constraint structure of the underlying optimization problem.
By augmenting the standard training loss with directional derivative
information extracted from the solver, the proxy aligns both its predicted
solutions and local derivatives with those of the optimizer. Under Lipschitz
continuity assumptions on the true solution mapping, matching first order
sensitivities is shown to yield uniform approximation error proportional to the
training set covering radius. Empirically, different impacts are observed in
each studied setting. On three large Alternating Current Optimal Power Flow
benchmarks, supervised Sobolev training cuts mean squared error by up to 56
percent and the median worst case constraint violation by up to 400 percent
while keeping the optimality gap below 0.22 percent. For a mean variance
portfolio task trained without labeled solutions, self supervised Sobolev
training halves the average optimality gap in the medium risk region (standard
deviation above 10 percent of budget) and matches the baseline elsewhere.
Together, these results highlight Sobolev training whether supervised or self
supervised as a path to fast reliable surrogates for safety critical large
scale optimization workloads.

</details>


### [110] [What Can We Learn From MIMO Graph Convolutions?](https://arxiv.org/abs/2505.11346)
*Andreas Roth, Thomas Liebig*

**主要类别:** cs.LG

**概要:** This paper introduces localized MIMO graph convolutions (LMGCs) which generalize many linear message-passing neural networks.


<details>
  <summary>Details</summary>
  
**动机:** Existing graph neural networks (GNNs) use approximations of the general graph convolution derived in the graph Fourier domain, but these approximations are performed in the single-input single-output (SISO) case while GNNs are typically applied in the multi-input multi-output (MIMO) case.

**方法:** The authors derive the MIMO graph convolution through the convolution theorem and approximate it directly in the MIMO case. They introduce localized MIMO graph convolutions (LMGCs) as a localized approximation.

**结果:** For almost every choice of edge weights, LMGCs with a single computational graph are injective on multisets, and the resulting representations are linearly independent when more than one computational graph is used.

**结论:** An LMGC can combine the benefits of various methods.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What+Can+We+Learn+From+MIMO+Graph+Convolutions%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11346，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11346&send_immediately=true&force_search=false)

**原文摘要:** Most graph neural networks (GNNs) utilize approximations of the general graph
convolution derived in the graph Fourier domain. While GNNs are typically
applied in the multi-input multi-output (MIMO) case, the approximations are
performed in the single-input single-output (SISO) case. In this work, we first
derive the MIMO graph convolution through the convolution theorem and
approximate it directly in the MIMO case. We find the key MIMO-specific
property of the graph convolution to be operating on multiple computational
graphs, or equivalently, applying distinct feature transformations for each
pair of nodes. As a localized approximation, we introduce localized MIMO graph
convolutions (LMGCs), which generalize many linear message-passing neural
networks. For almost every choice of edge weights, we prove that LMGCs with a
single computational graph are injective on multisets, and the resulting
representations are linearly independent when more than one computational graph
is used. Our experimental results confirm that an LMGC can combine the benefits
of various methods.

</details>


### [111] [Training NTK to Generalize with KARE](https://arxiv.org/abs/2505.11347)
*Johannes Schwab, Bryan Kelly, Semyon Malamud, Teng Andrea Xu*

**主要类别:** cs.LG

**概要:** This paper proposes optimizing the neural tangent kernel (NTK) explicitly rather than minimizing empirical risk, showing that it can outperform traditional deep neural network optimization in certain cases.


<details>
  <summary>Details</summary>
  
**动机:** To explore whether explicitly trained kernels can outperform traditional end-to-end DNN optimization.

**方法:** Proposes to train the NTK using the Kernel Alignment Risk Estimator (KARE) to minimize generalization error instead of minimizing empirical risk.

**结果:** Simulations and real data experiments demonstrate that NTKs trained with KARE consistently match or outperform the original DNN and the DNN-induced NTK.

**结论:** Explicit training of NTK is a form of over-parametrized feature learning and can challenge the conventional dominance of DNNs.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Training+NTK+to+Generalize+with+KARE，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11347，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11347&send_immediately=true&force_search=false)

**原文摘要:** The performance of the data-dependent neural tangent kernel (NTK; Jacot et
al. (2018)) associated with a trained deep neural network (DNN) often matches
or exceeds that of the full network. This implies that DNN training via
gradient descent implicitly performs kernel learning by optimizing the NTK. In
this paper, we propose instead to optimize the NTK explicitly. Rather than
minimizing empirical risk, we train the NTK to minimize its generalization
error using the recently developed Kernel Alignment Risk Estimator (KARE; Jacot
et al. (2020)). Our simulations and real data experiments show that NTKs
trained with KARE consistently match or significantly outperform the original
DNN and the DNN- induced NTK (the after-kernel). These results suggest that
explicitly trained kernels can outperform traditional end-to-end DNN
optimization in certain settings, challenging the conventional dominance of
DNNs. We argue that explicit training of NTK is a form of over-parametrized
feature learning.

</details>


### [112] [Context parroting: A simple but tough-to-beat baseline for foundation models in scientific machine learning](https://arxiv.org/abs/2505.11349)
*Yuanzhao Zhang, William Gilpin*

**主要类别:** cs.LG

**概要:** This paper examines the performance of recently-developed time series foundation models in predicting physical systems. It finds that these models often rely on context parroting rather than developing meaningful physics representations.


<details>
  <summary>Details</summary>
  
**动机:** To explore the capabilities and limitations of time series foundation models in predicting physical systems.

**方法:** Analyzing the performance of foundation models in zero-shot forecasting tasks and comparing them with a naive direct context parroting model.

**结果:** Foundation models can make accurate predictions but fail to develop meaningful representations of underlying physics, instead using context parroting. A simple context parroting model outperforms state-of-the-art foundation models on diverse dynamical systems.

**结论:** Context parroting is a tough-to-beat baseline for future time-series foundation models and helps identify in-context learning strategies beyond parroting.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Context+parroting%3A+A+simple+but+tough-to-beat+baseline+for+foundation+models+in+scientific+machine+learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11349，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11349&send_immediately=true&force_search=false)

**原文摘要:** Recently-developed time series foundation models for scientific machine
learning exhibit emergent abilities to predict physical systems. These
abilities include zero-shot forecasting, in which a model forecasts future
states of a system given only a short trajectory as context. Here, we show that
foundation models applied to physical systems can give accurate predictions,
but that they fail to develop meaningful representations of the underlying
physics. Instead, foundation models often forecast by context parroting, a
simple zero-shot forecasting strategy that copies directly from the context. As
a result, a naive direct context parroting model scores higher than
state-of-the-art time-series foundation models on predicting a diverse range of
dynamical systems, at a tiny fraction of the computational cost. We draw a
parallel between context parroting and induction heads, which explains why
large language models trained on text can be repurposed for time series
forecasting. Our dynamical systems perspective also ties the scaling between
forecast accuracy and context length to the fractal dimension of the attractor,
providing insight into the previously observed in-context neural scaling laws.
Context parroting thus serves as a simple but tough-to-beat baseline for future
time-series foundation models and can help identify in-context learning
strategies beyond parroting.

</details>


### [113] [Fractal Graph Contrastive Learning](https://arxiv.org/abs/2505.11356)
*Nero Z. Li, Xuehao Zhai, Zhichao Shi, Boshen Shi, Xuhui Jiang*

**主要类别:** cs.LG

**概要:** 提出了一种基于分形自相似性的图对比学习框架FractalGCL，通过重正规化增强和基于分形维度感知的对比损失来提升图表示质量。


<details>
  <summary>Details</summary>
  
**动机:** 现有的图对比学习方法依赖于随机扰动或局部结构保持，缺乏对增强视图间全局结构一致性显式控制。

**方法:** 引入了基于重正规化的增强方法生成结构对齐的正视图，并设计了基于分形维度感知的对比损失来对齐图嵌入。

**结果:** 在标准基准上实现了最先进的结果，在交通网络上比传统基线平均高出约7%。

**结论:** FractalGCL显著提升了图表示的质量，同时通过理论推导减少了计算开销，使得整体训练时间缩短了约61%。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fractal+Graph+Contrastive+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11356，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11356&send_immediately=true&force_search=false)

**原文摘要:** While Graph Contrastive Learning (GCL) has attracted considerable attention
in the field of graph self-supervised learning, its performance heavily relies
on data augmentations that are expected to generate semantically consistent
positive pairs. Existing strategies typically resort to random perturbations or
local structure preservation, yet lack explicit control over global structural
consistency between augmented views. To address this limitation, we propose
Fractal Graph Contrastive Learning (FractalGCL), a theory-driven framework that
leverages fractal self-similarity to enforce global topological coherence.
FractalGCL introduces two key innovations: a renormalisation-based augmentation
that generates structurally aligned positive views via box coverings; and a
fractal-dimension-aware contrastive loss that aligns graph embeddings according
to their fractal dimensions. While combining the two innovations markedly
boosts graph-representation quality, it also adds non-trivial computational
overhead. To mitigate the computational overhead of fractal dimension
estimation, we derive a one-shot estimator by proving that the dimension
discrepancy between original and renormalised graphs converges weakly to a
centred Gaussian distribution. This theoretical insight enables a reduction in
dimension computation cost by an order of magnitude, cutting overall training
time by approximately 61%. The experiments show that FractalGCL not only
delivers state-of-the-art results on standard benchmarks but also outperforms
traditional baselines on traffic networks by an average margin of about
remarkably 7%. Codes are available at
(https://anonymous.4open.science/r/FractalGCL-0511).

</details>


### [114] [LGBQPC: Local Granular-Ball Quality Peaks Clustering](https://arxiv.org/abs/2505.11359)
*Zihang Jia, Zhen Zhang, Witold Pedrycz*

**主要类别:** cs.LG

**概要:** 提出了一种改进的局部粒球质量峰值聚类算法（LGBQPC），该算法在粒球生成和聚类过程中对基于粒球的密度峰值聚类（GBDPC）进行了全面改进。通过引入基于粒球k近邻图的相对粒球质量和测地线距离，显著提高了处理复杂流形结构或非均匀密度分布数据集的能力。实验验证了LGBQPC算法的优越性能。


<details>
  <summary>Details</summary>
  
**动机:** 现有的基于粒球的密度峰值聚类（GBDPC）算法在处理复杂聚类任务时存在局限性，特别是对于具有复杂流形结构或非均匀密度分布的数据集。

**方法:** 提出了一种新的局部粒球质量峰值聚类（LGBQPC）算法，包括改进的粒球生成方法（GB-POJG+）和基于粒球k近邻图的聚类过程创新。

**结果:** LGBQPC算法在处理复杂流形结构或非均匀密度分布的数据集时表现出色，并在40个基准数据集上进行了广泛的数值实验，验证了其优越性能。

**结论:** LGBQPC算法克服了现有GBDPC算法的局限性，提供了更高质量的粒球生成和更有效的聚类过程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LGBQPC%3A+Local+Granular-Ball+Quality+Peaks+Clustering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11359，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11359&send_immediately=true&force_search=false)

**原文摘要:** The density peaks clustering (DPC) algorithm has attracted considerable
attention for its ability to detect arbitrarily shaped clusters based on a
simple yet effective assumption. Recent advancements integrating granular-ball
(GB) computing with DPC have led to the GB-based DPC (GBDPC) algorithm, which
improves computational efficiency. However, GBDPC demonstrates limitations when
handling complex clustering tasks, particularly those involving data with
complex manifold structures or non-uniform density distributions. To overcome
these challenges, this paper proposes the local GB quality peaks clustering
(LGBQPC) algorithm, which offers comprehensive improvements to GBDPC in both GB
generation and clustering processes based on the principle of justifiable
granularity (POJG). Firstly, an improved GB generation method, termed GB-POJG+,
is developed, which systematically refines the original GB-POJG in four key
aspects: the objective function, termination criterion for GB division,
definition of abnormal GB, and granularity level adaptation strategy. GB-POJG+
simplifies parameter configuration by requiring only a single penalty
coefficient and ensures high-quality GB generation while maintaining the number
of generated GBs within an acceptable range. In the clustering phase, two key
innovations are introduced based on the GB k-nearest neighbor graph: relative
GB quality for density estimation and geodesic distance for GB distance metric.
These modifications substantially improve the performance of GBDPC on datasets
with complex manifold structures or non-uniform density distributions.
Extensive numerical experiments on 40 benchmark datasets, including both
synthetic and publicly available datasets, validate the superior performance of
the proposed LGBQPC algorithm.

</details>


### [115] [Efficient End-to-End Learning for Decision-Making: A Meta-Optimization Approach](https://arxiv.org/abs/2505.11360)
*Rares Cristian, Pavithra Harsha, Georgia Perakis, Brian Quanz*

**主要类别:** cs.LG

**概要:** 提出了一种元优化方法来学习高效算法近似优化问题，显著减少了求解决策问题的计算开销，并证明了指数收敛性、逼近保证和泛化界限。该方法适用于多种优化问题，包括确定性和两阶段随机优化问题，并展示了其在电力生成、最短路径预测和新svendor问题中的应用。


<details>
  <summary>Details</summary>
  
**动机:** 传统方法仅关注预测误差，而忽略了对下游决策任务的影响；端到端框架的计算复杂度对于大规模问题是一个挑战。

**方法:** 提出了一种神经网络架构，通过交替投影确保可行性约束，同时近似最优地解决优化问题。

**结果:** 该方法在计算效率上优于现有技术，能够更快地产生高质量的近似解，并且随着问题规模的增长表现更好。

**结论:** 所提出的元优化方法可以广泛应用于各种优化问题，包括确定性和两阶段随机优化问题，并在多个实际问题中展示了其有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+End-to-End+Learning+for+Decision-Making%3A+A+Meta-Optimization+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11360，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11360&send_immediately=true&force_search=false)

**原文摘要:** End-to-end learning has become a widely applicable and studied problem in
training predictive ML models to be aware of their impact on downstream
decision-making tasks. These end-to-end models often outperform traditional
methods that separate training from the optimization and only myopically focus
on prediction error. However, the computational complexity of end-to-end
frameworks poses a significant challenge, particularly for large-scale
problems. While training an ML model using gradient descent, each time we need
to compute a gradient we must solve an expensive optimization problem. We
present a meta-optimization method that learns efficient algorithms to
approximate optimization problems, dramatically reducing computational overhead
of solving the decision problem in general, an aspect we leverage in the
training within the end-to-end framework. Our approach introduces a neural
network architecture that near-optimally solves optimization problems while
ensuring feasibility constraints through alternate projections. We prove
exponential convergence, approximation guarantees, and generalization bounds
for our learning method. This method offers superior computational efficiency,
producing high-quality approximations faster and scaling better with problem
size compared to existing techniques. Our approach applies to a wide range of
optimization problems including deterministic, single-stage as well as
two-stage stochastic optimization problems. We illustrate how our proposed
method applies to (1) an electricity generation problem using real data from an
electricity routing company coordinating the movement of electricity throughout
13 states, (2) a shortest path problem with a computer vision task of
predicting edge costs from terrain maps, (3) a two-stage multi-warehouse
cross-fulfillment newsvendor problem, as well as a variety of other
newsvendor-like problems.

</details>


### [116] [Understanding Nonlinear Implicit Bias via Region Counts in Input Space](https://arxiv.org/abs/2505.11370)
*Jingwei Li, Jing Xu, Zifan Wang, Huishuai Zhang, Jingzhao Zhang*

**主要类别:** cs.LG

**概要:** This paper explores the concept of implicit bias in neural networks by examining the count of connected regions in the input space with the same predicted label, finding that smaller region counts correspond to better generalization and can be induced by certain hyper-parameter choices.


<details>
  <summary>Details</summary>
  
**动机:** To understand the definition and mechanism of implicit bias in non-linear contexts.

**方法:** Characterizing implicit bias by the count of connected regions in the input space with the same predicted label, and comparing it with parameter-dependent metrics.

**结果:** Small region counts align with geometrically simple decision boundaries and correlate well with good generalization performance. Certain hyper-parameter choices can induce small region counts.

**结论:** Theoretical connections were established to explain how larger learning rates can induce small region counts in neural networks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+Nonlinear+Implicit+Bias+via+Region+Counts+in+Input+Space，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11370，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11370&send_immediately=true&force_search=false)

**原文摘要:** One explanation for the strong generalization ability of neural networks is
implicit bias. Yet, the definition and mechanism of implicit bias in non-linear
contexts remains little understood. In this work, we propose to characterize
implicit bias by the count of connected regions in the input space with the
same predicted label. Compared with parameter-dependent metrics (e.g., norm or
normalized margin), region count can be better adapted to nonlinear,
overparameterized models, because it is determined by the function mapping and
is invariant to reparametrization. Empirically, we found that small region
counts align with geometrically simple decision boundaries and correlate well
with good generalization performance. We also observe that good hyper-parameter
choices such as larger learning rates and smaller batch sizes can induce small
region counts. We further establish the theoretical connections and explain how
larger learning rate can induce small region counts in neural networks.

</details>


### [117] [On the Interconnections of Calibration, Quantification, and Classifier Accuracy Prediction under Dataset Shift](https://arxiv.org/abs/2505.11380)
*Alejandro Moreo*

**主要类别:** cs.LG

**概要:** This paper explores the connections between calibration, quantification, and classifier accuracy prediction under dataset shift conditions, proving their equivalence and proposing new methods based on this equivalence.


<details>
  <summary>Details</summary>
  
**动机:** To address the challenges in calibrating classifiers and estimating proportions and accuracy under dataset shift.

**方法:** Prove the equivalence of the three problems through mutual reduction and propose new methods adapted from established techniques.

**结果:** The proposed methods are competitive or even surpass dedicated approaches in performance.

**结论:** Fostering cross-fertilization and unified approaches across related research areas.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Interconnections+of+Calibration%2C+Quantification%2C+and+Classifier+Accuracy+Prediction+under+Dataset+Shift，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11380，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11380&send_immediately=true&force_search=false)

**原文摘要:** When the distribution of the data used to train a classifier differs from
that of the test data, i.e., under dataset shift, well-established routines for
calibrating the decision scores of the classifier, estimating the proportion of
positives in a test sample, or estimating the accuracy of the classifier,
become particularly challenging. This paper investigates the interconnections
among three fundamental problems, calibration, quantification, and classifier
accuracy prediction, under dataset shift conditions. Specifically, we prove
their equivalence through mutual reduction, i.e., we show that access to an
oracle for any one of these tasks enables the resolution of the other two.
Based on these proofs, we propose new methods for each problem based on direct
adaptations of well-established methods borrowed from the other disciplines.
Our results show such methods are often competitive, and sometimes even surpass
the performance of dedicated approaches from each discipline. The main goal of
this paper is to fostering cross-fertilization among these research areas,
encouraging the development of unified approaches and promoting synergies
across the fields.

</details>


### [118] [IISE PG&E Energy Analytics Challenge 2025: Hourly-Binned Regression Models Beat Transformers in Load Forecasting](https://arxiv.org/abs/2505.11390)
*Millend Roy, Vladimir Pyltsov, Yinbo Hu*

**主要类别:** cs.LG

**概要:** 准确的电力负荷预测对于电网稳定性、资源优化和可再生能源整合至关重要。本研究评估了从经典回归技术到先进深度学习架构的各种预测模型，使用ESD 2025竞赛的数据，发现XGBoost在所有测试情况下均能提供最低的误差率，同时保持计算效率。这突显了深度学习在长期电力预测中的局限性，并强调了根据数据特征而非复杂度选择模型的重要性。


<details>
  <summary>Details</summary>
  
**动机:** 准确的电力负荷预测对电网稳定性、资源优化和可再生能源整合很重要，但深度学习模型在长期负荷预测中的效果仍不确定。

**方法:** 使用PCA进行降维，将任务作为回归问题处理，利用温度和GHI作为协变量预测每小时的负荷，并最终堆叠24个模型生成年度预测。

**结果:** 深度学习模型（包括TimeGPT）未能始终优于简单的统计和机器学习方法，而XGBoost在所有测试情况下都提供了最低的误差率。

**结论:** 本研究揭示了深度学习在长期电力负荷预测中的局限性，并强调了根据数据特征选择模型的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是IISE+PG%26E+Energy+Analytics+Challenge+2025%3A+Hourly-Binned+Regression+Models+Beat+Transformers+in+Load+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11390，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11390&send_immediately=true&force_search=false)

**原文摘要:** Accurate electricity load forecasting is essential for grid stability,
resource optimization, and renewable energy integration. While
transformer-based deep learning models like TimeGPT have gained traction in
time-series forecasting, their effectiveness in long-term electricity load
prediction remains uncertain. This study evaluates forecasting models ranging
from classical regression techniques to advanced deep learning architectures
using data from the ESD 2025 competition. The dataset includes two years of
historical electricity load data, alongside temperature and global horizontal
irradiance (GHI) across five sites, with a one-day-ahead forecasting horizon.
Since actual test set load values remain undisclosed, leveraging predicted
values would accumulate errors, making this a long-term forecasting challenge.
We employ (i) Principal Component Analysis (PCA) for dimensionality reduction
and (ii) frame the task as a regression problem, using temperature and GHI as
covariates to predict load for each hour, (iii) ultimately stacking 24 models
to generate yearly forecasts.
  Our results reveal that deep learning models, including TimeGPT, fail to
consistently outperform simpler statistical and machine learning approaches due
to the limited availability of training data and exogenous variables. In
contrast, XGBoost, with minimal feature engineering, delivers the lowest error
rates across all test cases while maintaining computational efficiency. This
highlights the limitations of deep learning in long-term electricity
forecasting and reinforces the importance of model selection based on dataset
characteristics rather than complexity. Our study provides insights into
practical forecasting applications and contributes to the ongoing discussion on
the trade-offs between traditional and modern forecasting methods.

</details>


### [119] [Finding Counterfactual Evidences for Node Classification](https://arxiv.org/abs/2505.11396)
*Dazhuo Qiu, Jinwen Chen, Arijit Khan, Yan Zhao, Francesco Bonchi*

**主要类别:** cs.LG

**概要:** This paper introduces the problem of searching for counterfactual evidences for GNN-based node classification tasks. It develops search algorithms and an indexing solution to identify counterfactual evidences, which can improve the fairness and accuracy of GNNs.


<details>
  <summary>Details</summary>
  
**动机:** To alleviate common issues of GNNs such as fairness and interpretability, especially when randomized controlled trials are impractical.

**方法:** Develop effective and efficient search algorithms and a novel indexing solution that leverages both node features and structural information.

**结果:** The proposed method can identify counterfactual evidences, which can enhance the fairness and accuracy of GNNs through various downstream applications.

**结论:** Counterfactual evidences can be used to improve the fairness and accuracy of GNNs.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Finding+Counterfactual+Evidences+for+Node+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11396，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11396&send_immediately=true&force_search=false)

**原文摘要:** Counterfactual learning is emerging as an important paradigm, rooted in
causality, which promises to alleviate common issues of graph neural networks
(GNNs), such as fairness and interpretability. However, as in many real-world
application domains where conducting randomized controlled trials is
impractical, one has to rely on available observational (factual) data to
detect counterfactuals. In this paper, we introduce and tackle the problem of
searching for counterfactual evidences for the GNN-based node classification
task. A counterfactual evidence is a pair of nodes such that, regardless they
exhibit great similarity both in the features and in their neighborhood
subgraph structures, they are classified differently by the GNN. We develop
effective and efficient search algorithms and a novel indexing solution that
leverages both node features and structural information to identify
counterfactual evidences, and generalizes beyond any specific GNN. Through
various downstream applications, we demonstrate the potential of counterfactual
evidences to enhance fairness and accuracy of GNNs.

</details>


### [120] [Is Grokking a Computational Glass Relaxation?](https://arxiv.org/abs/2505.11411)
*Xiaotian Zhang, Yue Shang, Entao Yang, Ge Zhang*

**主要类别:** cs.LG

**概要:** 本文提出了一种解释神经网络grokking现象的新方法，挑战了现有理论，并开发了一种新的优化器WanD。


<details>
  <summary>Details</summary>
  
**动机:** 理解神经网络的泛化能力仍然是深度学习研究中的核心问题。grokking现象提供了研究神经网络泛化机制的独特窗口。

**方法:** 将神经网络视为物理系统，其中参数是自由度，训练损失是系统能量。通过这种映射，可以采样神经网络的玻尔兹曼熵景观。同时开发了一种基于Wang-Landau分子动力学的玩具优化器WanD。

**结果:** 在算术任务上的Transformer实验表明，grokking的泛化过程中不存在熵障碍。此外，发现grokking具有高熵优势。

**结论:** 本文提出了对grokking现象的一种解释，将其视为计算玻璃弛豫。这种观点挑战了之前关于grokking作为一级相变的理论，并且发现grokking具有高熵优势。此外，开发了一种新的优化器WanD，它可以消除grokking并找到高范数泛化解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Is+Grokking+a+Computational+Glass+Relaxation%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11411，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11411&send_immediately=true&force_search=false)

**原文摘要:** Understanding neural network's (NN) generalizability remains a central
question in deep learning research. The special phenomenon of grokking, where
NNs abruptly generalize long after the training performance reaches a
near-perfect level, offers a unique window to investigate the underlying
mechanisms of NNs' generalizability. Here we propose an interpretation for
grokking by framing it as a computational glass relaxation: viewing NNs as a
physical system where parameters are the degrees of freedom and train loss is
the system energy, we find memorization process resembles a rapid cooling of
liquid into non-equilibrium glassy state at low temperature and the later
generalization is like a slow relaxation towards a more stable configuration.
This mapping enables us to sample NNs' Boltzmann entropy (states of density)
landscape as a function of training loss and test accuracy. Our experiments in
transformers on arithmetic tasks suggests that there is NO entropy barrier in
the memorization-to-generalization transition of grokking, challenging previous
theory that defines grokking as a first-order phase transition. We identify a
high-entropy advantage under grokking, an extension of prior work linking
entropy to generalizability but much more significant. Inspired by grokking's
far-from-equilibrium nature, we develop a toy optimizer WanD based on
Wang-landau molecular dynamics, which can eliminate grokking without any
constraints and find high-norm generalizing solutions. This provides
strictly-defined counterexamples to theory attributing grokking solely to
weight norm evolution towards the Goldilocks zone and also suggests new
potential ways for optimizer design.

</details>


### [121] [Uncertainty quantification with approximate variational learning for wearable photoplethysmography prediction tasks](https://arxiv.org/abs/2505.11412)
*Ciaran Bench, Vivek Desai, Mohammad Moulaeifard, Nils Strodthoff, Philip Aston, Andrew Thompson*

**主要类别:** cs.LG

**概要:** 研究了两种不确定性量化技术在从PPG信号分类AF和回归BP中的应用，并探讨了超参数选择对模型预测性能和不确定性质量的影响。


<details>
  <summary>Details</summary>
  
**动机:** PPG信号可以无创地评估心脏健康状况，但深度网络缺乏解释性且容易过拟合。

**方法:** 使用Monte Carlo Dropout和Improved Variational Online Newton两种不确定性量化技术。

**结果:** 超参数的选择对模型的预测性能和不确定性质量有显著影响。

**结论:** 需要仔细调整超参数以平衡预测性能和校准质量，并且最佳参数化可能因不确定性表达方式的不同而变化。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Uncertainty+quantification+with+approximate+variational+learning+for+wearable+photoplethysmography+prediction+tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11412，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11412&send_immediately=true&force_search=false)

**原文摘要:** Photoplethysmography (PPG) signals encode information about relative changes
in blood volume that can be used to assess various aspects of cardiac health
non-invasively, e.g.\ to detect atrial fibrillation (AF) or predict blood
pressure (BP). Deep networks are well-equipped to handle the large quantities
of data acquired from wearable measurement devices. However, they lack
interpretability and are prone to overfitting, leaving considerable risk for
poor performance on unseen data and misdiagnosis. Here, we describe the use of
two scalable uncertainty quantification techniques: Monte Carlo Dropout and the
recently proposed Improved Variational Online Newton. These techniques are used
to assess the trustworthiness of models trained to perform AF classification
and BP regression from raw PPG time series. We find that the choice of
hyperparameters has a considerable effect on the predictive performance of the
models and on the quality and composition of predicted uncertainties. E.g. the
stochasticity of the model parameter sampling determines the proportion of the
total uncertainty that is aleatoric, and has varying effects on predictive
performance and calibration quality dependent on the chosen uncertainty
quantification technique and the chosen expression of uncertainty. We find
significant discrepancy in the quality of uncertainties over the predicted
classes, emphasising the need for a thorough evaluation protocol that assesses
local and adaptive calibration. This work suggests that the choice of
hyperparameters must be carefully tuned to balance predictive performance and
calibration quality, and that the optimal parameterisation may vary depending
on the chosen expression of uncertainty.

</details>


### [122] [MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse Mixture-of-Experts Systems](https://arxiv.org/abs/2505.11415)
*Yinsicheng Jiang, Yao Fu, Yeqi Huang, Ping Nie, Zhan Lu, Leyang Xue, Congjie He, Man-Kit Sit, Jilong Xue, Li Dong, Ziming Miao, Dayou Du, Tairan Xu, Kai Zou, Edoardo Ponti, Luo Mai*

**主要类别:** cs.LG

**概要:** 提出MoE-CAP基准测试来解决现有基准测试未能准确捕捉MoE系统成本、精度和性能权衡的问题，并引入CAP雷达图及稀疏感知性能度量标准。


<details>
  <summary>Details</summary>
  
**动机:** 现有基准测试未能准确捕捉MoE系统成本、精度和性能的权衡，影响实际部署决策。

**方法:** 设计MoE-CAP基准测试，提出CAP雷达图及稀疏感知性能度量标准(S-MBU和S-MFU)。

**结果:** 发现当前硬件下在成本、精度和性能之间实现最佳平衡很困难，MoE系统通常以牺牲第三个维度为代价优化其中两个维度。

**结论:** 提出了新的方法和工具来更好地评估和优化MoE系统的成本、精度和性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MoE-CAP%3A+Benchmarking+Cost%2C+Accuracy+and+Performance+of+Sparse+Mixture-of-Experts+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11415，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11415&send_immediately=true&force_search=false)

**原文摘要:** The sparse Mixture-of-Experts (MoE) architecture is increasingly favored for
scaling Large Language Models (LLMs) efficiently, but it depends on
heterogeneous compute and memory resources. These factors jointly affect system
Cost, Accuracy, and Performance (CAP), making trade-offs inevitable. Existing
benchmarks often fail to capture these trade-offs accurately, complicating
practical deployment decisions. To address this, we introduce MoE-CAP, a
benchmark specifically designed for MoE systems. Our analysis reveals that
achieving an optimal balance across CAP is difficult with current hardware; MoE
systems typically optimize two of the three dimensions at the expense of the
third-a dynamic we term the MoE-CAP trade-off. To visualize this, we propose
the CAP Radar Diagram. We further introduce sparsity-aware performance
metrics-Sparse Memory Bandwidth Utilization (S-MBU) and Sparse Model FLOPS
Utilization (S-MFU)-to enable accurate performance benchmarking of MoE systems
across diverse hardware platforms and deployment scenarios.

</details>


### [123] [MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production](https://arxiv.org/abs/2505.11432)
*Chao Jin, Ziheng Jiang, Zhihao Bai, Zheng Zhong, Juncai Liu, Xiang Li, Ningxin Zheng, Xi Wang, Cong Xie, Qi Huang, Wen Heng, Yiyuan Ma, Wenlei Bao, Size Zheng, Yanghua Peng, Haibin Lin, Xuanzhe Liu, Xin Jin, Xin Liu*

**主要类别:** cs.LG

**概要:** 提出MegaScale-MoE，一种针对大规模混合专家模型高效训练的生产系统。通过定制通信高效的并行策略和重叠通信与计算来提高效率。在1440个NVIDIA Hopper GPU上训练一个352B参数的MoE模型时，达到了1.41M tokens/s的训练吞吐量，提高了1.88倍的效率。


<details>
  <summary>Details</summary>
  
**动机:** 现有的MoE训练系统由于模型规模扩大和硬件的不断演化，训练效率下降。MegaScale-MoE旨在解决这一问题，通过优化通信来提升训练效率。

**方法:** 定制了注意力和FFNs的通信高效并行策略，采用整体方法重叠通信与计算，并应用通信压缩技术降低精度。

**结果:** 在1440个NVIDIA Hopper GPU上训练一个352B参数的MoE模型，实现了1.41M tokens/s的训练吞吐量，效率提高了1.88倍。

**结论:** 分享了加速MoE训练的经验，希望对未来MoE系统的系统设计研究提供启示。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MegaScale-MoE%3A+Large-Scale+Communication-Efficient+Training+of+Mixture-of-Experts+Models+in+Production，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11432，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11432&send_immediately=true&force_search=false)

**原文摘要:** We present MegaScale-MoE, a production system tailored for the efficient
training of large-scale mixture-of-experts (MoE) models. MoE emerges as a
promising architecture to scale large language models (LLMs) to unprecedented
sizes, thereby enhancing model performance. However, existing MoE training
systems experience a degradation in training efficiency, exacerbated by the
escalating scale of MoE models and the continuous evolution of hardware.
  Recognizing the pivotal role of efficient communication in enhancing MoE
training, MegaScale-MoE customizes communication-efficient parallelism
strategies for attention and FFNs in each MoE layer and adopts a holistic
approach to overlap communication with computation at both inter- and
intra-operator levels. Additionally, MegaScale-MoE applies communication
compression with adjusted communication patterns to lower precision, further
improving training efficiency. When training a 352B MoE model on 1,440 NVIDIA
Hopper GPUs, MegaScale-MoE achieves a training throughput of 1.41M tokens/s,
improving the efficiency by 1.88$\times$ compared to Megatron-LM. We share our
operational experience in accelerating MoE training and hope that by offering
our insights in system design, this work will motivate future research in MoE
systems.

</details>


### [124] [Signal attenuation enables scalable decentralized multi-agent reinforcement learning over networks](https://arxiv.org/abs/2505.11461)
*Wesley A Suttle, Vipul K Sharma, Brian M Sadler*

**主要类别:** cs.LG

**概要:** 本文研究了信号衰减在多智能体强化学习中的作用，特别是在雷达网络中实现了去中心化的功率分配。


<details>
  <summary>Details</summary>
  
**动机:** 经典多智能体强化学习方法需要全局状态可观测性，这限制了去中心化算法的发展和可扩展性。而基于衰减的多智能体影响假设，可以用每个代理的局部邻域可观测性代替全局可观测性，从而实现去中心化和可扩展性。然而，享受这种衰减属性的实际应用仍然未被充分探索。

**方法:** 提出了两种新的受限多智能体马尔可夫决策过程公式，推导了全局价值函数和梯度估计的局部邻域近似并建立了相应的误差界限，开发了解决所提出问题的去中心化鞍点策略梯度算法。

**结果:** 在雷达网络中的目标检测功率分配问题上验证了信号衰减对多智能体强化学习去中心化的有效性，并开发了相应的去中心化算法。

**结论:** 提出的方法通过信号衰减实现多智能体强化学习的去中心化，并在雷达网络中的功率分配问题上进行了验证。该方法提供了一个有用的模型，可用于无线通信和雷达网络中的其他问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Signal+attenuation+enables+scalable+decentralized+multi-agent+reinforcement+learning+over+networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11461，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11461&send_immediately=true&force_search=false)

**原文摘要:** Classic multi-agent reinforcement learning (MARL) methods require that agents
enjoy global state observability, preventing development of decentralized
algorithms and limiting scalability. Recent work has shown that, under
assumptions on decaying inter-agent influence, global observability can be
replaced by local neighborhood observability at each agent, enabling
decentralization and scalability. Real-world applications enjoying such decay
properties remain underexplored, however, despite the fact that signal power
decay, or signal attenuation, due to path loss is an intrinsic feature of many
problems in wireless communications and radar networks. In this paper, we show
that signal attenuation enables decentralization in MARL by considering the
illustrative special case of performing power allocation for target detection
in a radar network. To achieve this, we propose two new constrained multi-agent
Markov decision process formulations of this power allocation problem, derive
local neighborhood approximations for global value function and gradient
estimates and establish corresponding error bounds, and develop decentralized
saddle point policy gradient algorithms for solving the proposed problems. Our
approach, though oriented towards the specific radar network problem we
consider, provides a useful model for future extensions to additional problems
in wireless communications and radar networks.

</details>


### [125] [msf-CNN: Patch-based Multi-Stage Fusion with Convolutional Neural Networks for TinyML](https://arxiv.org/abs/2505.11483)
*Zhaolan Huang, Emmanuel Baccelli*

**主要类别:** cs.LG

**概要:** This paper introduces msf-CNN, a novel method for optimizing convolutional neural networks on microcontrollers by efficiently finding optimal fusion settings, reducing RAM usage by 50% compared to previous methods.


<details>
  <summary>Details</summary>
  
**动机:** To fit neural network models within the limited memory of microcontrollers while maintaining low inference latency.

**方法:** Introduces msf-CNN, which uses a directed acyclic graph to explore fusion solution spaces for convolutional neural networks.

**结果:** msf-CNN achieves 50% less RAM usage than previous methods (MCUNetV2 and StreamNet) on various microcontrollers.

**结论:** msf-CNN provides additional flexibility for system designers in deploying neural networks on resource-constrained devices.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是msf-CNN%3A+Patch-based+Multi-Stage+Fusion+with+Convolutional+Neural+Networks+for+TinyML，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11483，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11483&send_immediately=true&force_search=false)

**原文摘要:** AI spans from large language models to tiny models running on
microcontrollers (MCUs). Extremely memory-efficient model architectures are
decisive to fit within an MCU's tiny memory budget e.g., 128kB of RAM. However,
inference latency must remain small to fit real-time constraints. An approach
to tackle this is patch-based fusion, which aims to optimize data flows across
neural network layers. In this paper, we introduce msf-CNN, a novel technique
that efficiently finds optimal fusion settings for convolutional neural
networks (CNNs) by walking through the fusion solution space represented as a
directed acyclic graph. Compared to previous work on CNN fusion for MCUs,
msf-CNN identifies a wider set of solutions. We published an implementation of
msf-CNN running on various microcontrollers (ARM Cortex-M, RISC-V, ESP32). We
show that msf-CNN can achieve inference using 50% less RAM compared to the
prior art (MCUNetV2 and StreamNet). We thus demonstrate how msf-CNN offers
additional flexibility for system designers.

</details>


### [126] [Potential failures of physics-informed machine learning in traffic flow modeling: theoretical and experimental analysis](https://arxiv.org/abs/2505.11491)
*Yuan-Zheng Lei, Yaobang Gong, Dianwei Chen, Yao Cheng, Xianfeng Terry Yang*

**主要类别:** cs.LG

**概要:** This study investigates the performance of physics-informed machine learning (PIML) approaches for traffic flow modeling.


<details>
  <summary>Details</summary>
  
**动机:** To define the failure of a PIML model and examine whether physics residuals hinder optimization.

**方法:** Analyzing the loss landscape by perturbing trained models along the principal eigenvectors of the Hessian matrix and evaluating corresponding loss values.

**结果:** Physics residuals do not inherently hinder optimization but successful parameter updates require certain conditions. Physical residuals can degrade PIML model performance, and misleadingly small physics residuals can contribute to model failure. The CFL condition is identified as a key indicator of dataset suitability for PIML.

**结论:** Higher-order models tend to have larger error lower bounds than lower-order models.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Potential+failures+of+physics-informed+machine+learning+in+traffic+flow+modeling%3A+theoretical+and+experimental+analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11491，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11491&send_immediately=true&force_search=false)

**原文摘要:** This study critically examines the performance of physics-informed machine
learning (PIML) approaches for traffic flow modeling, defining the failure of a
PIML model as the scenario where it underperforms both its purely data-driven
and purely physics-based counterparts. We analyze the loss landscape by
perturbing trained models along the principal eigenvectors of the Hessian
matrix and evaluating corresponding loss values. Our results suggest that
physics residuals in PIML do not inherently hinder optimization, contrary to a
commonly assumed failure cause. Instead, successful parameter updates require
both ML and physics gradients to form acute angles with the quasi-true gradient
and lie within a conical region. Given inaccuracies in both the physics models
and the training data, satisfying this condition is often difficult.
Experiments reveal that physical residuals can degrade the performance of LWR-
and ARZ-based PIML models, especially under highly physics-driven settings.
Moreover, sparse sampling and the use of temporally averaged traffic data can
produce misleadingly small physics residuals that fail to capture actual
physical dynamics, contributing to model failure. We also identify the
Courant-Friedrichs-Lewy (CFL) condition as a key indicator of dataset
suitability for PIML, where successful applications consistently adhere to this
criterion. Lastly, we observe that higher-order models like ARZ tend to have
larger error lower bounds than lower-order models like LWR, which is consistent
with the experimental findings of existing studies.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [127] [On the Evaluation of Engineering Artificial General Intelligence](https://arxiv.org/abs/2505.10653)
*Sandeep Neema, Susmit Jha, Adam Nagel, Ethan Lew, Chandrasekar Sureshkumar, Aleksa Gordic, Chase Shimmin, Hieu Nguygen, Paul Eremenko*

**主要类别:** cs.AI

**概要:** 本文提出了一种评估工程人工通用智能（eAGI）代理的框架。该框架基于Bloom的学习评估分类法，并在工程设计背景下进行了扩展和具体化。


<details>
  <summary>Details</summary>
  
**动机:** 研究eAGI代理的评估问题，因为其性能评估具有挑战性，但对开发eAGI代理至关重要。

**方法:** 提出了一种基于Bloom分类法扩展的可扩展评估框架，适用于工程设计背景。

**结果:** 框架可以评估从方法学到实际设计问题的丰富评估问题集，并能评价CAD模型和SysML模型等结构化设计工件。

**结论:** 所提出的框架推进了AI代理在工程领域的基准测试和评估的前沿。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Evaluation+of+Engineering+Artificial+General+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10653，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10653&send_immediately=true&force_search=false)

**原文摘要:** We discuss the challenges and propose a framework for evaluating engineering
artificial general intelligence (eAGI) agents. We consider eAGI as a
specialization of artificial general intelligence (AGI), deemed capable of
addressing a broad range of problems in the engineering of physical systems and
associated controllers. We exclude software engineering for a tractable scoping
of eAGI and expect dedicated software engineering AI agents to address the
software implementation challenges. Similar to human engineers, eAGI agents
should possess a unique blend of background knowledge (recall and retrieve) of
facts and methods, demonstrate familiarity with tools and processes, exhibit
deep understanding of industrial components and well-known design families, and
be able to engage in creative problem solving (analyze and synthesize),
transferring ideas acquired in one context to another. Given this broad
mandate, evaluating and qualifying the performance of eAGI agents is a
challenge in itself and, arguably, a critical enabler to developing eAGI
agents. In this paper, we address this challenge by proposing an extensible
evaluation framework that specializes and grounds Bloom's taxonomy - a
framework for evaluating human learning that has also been recently used for
evaluating LLMs - in an engineering design context. Our proposed framework
advances the state of the art in benchmarking and evaluation of AI agents in
terms of the following: (a) developing a rich taxonomy of evaluation questions
spanning from methodological knowledge to real-world design problems; (b)
motivating a pluggable evaluation framework that can evaluate not only textual
responses but also evaluate structured design artifacts such as CAD models and
SysML models; and (c) outlining an automatable procedure to customize the
evaluation benchmark to different engineering contexts.

</details>


### [128] [Interpretable Risk Mitigation in LLM Agent Systems](https://arxiv.org/abs/2505.10670)
*Jan Chojnacki*

**主要类别:** cs.AI

**概要:** This study investigates agent behavior in a game-theoretic environment using a strategy-modification method based on interpretable features from a sparse autoencoder latent space to improve reliability and safety of autonomous agents powered by large language models.


<details>
  <summary>Details</summary>
  
**动机:** To address safety concerns and improve reliability of autonomous agents driven by large language models in responsible action domains.

**方法:** Introduces a strategy-modification method independent of the game and prompt by steering the residual stream with interpretable features extracted from a sparse autoencoder latent space.

**结果:** Steering with the good-faith negotiation feature reduces the average defection probability by 28 percentage points and identifies feasible steering ranges for several open-source LLM agents.

**结论:** Hypothesizes that game-theoretic evaluation of LLM agents combined with representation-steering alignment can generalize to real-world applications on end-user devices and embodied platforms.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interpretable+Risk+Mitigation+in+LLM+Agent+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10670，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10670&send_immediately=true&force_search=false)

**原文摘要:** Autonomous agents powered by large language models (LLMs) enable novel use
cases in domains where responsible action is increasingly important. Yet the
inherent unpredictability of LLMs raises safety concerns about agent
reliability. In this work, we explore agent behaviour in a toy, game-theoretic
environment based on a variation of the Iterated Prisoner's Dilemma. We
introduce a strategy-modification method-independent of both the game and the
prompt-by steering the residual stream with interpretable features extracted
from a sparse autoencoder latent space. Steering with the good-faith
negotiation feature lowers the average defection probability by 28 percentage
points. We also identify feasible steering ranges for several open-source LLM
agents. Finally, we hypothesise that game-theoretic evaluation of LLM agents,
combined with representation-steering alignment, can generalise to real-world
applications on end-user devices and embodied platforms.

</details>


### [129] [Embodied AI in Machine Learning -- is it Really Embodied?](https://arxiv.org/abs/2505.10705)
*Matej Hoffmann, Shubhan Parag Patni*

**主要类别:** cs.AI

**概要:** This paper discusses the current state of Embodied Artificial Intelligence, comparing it to Good Old-Fashioned Artificial Intelligence and exploring the potential for cross-embodiment learning.


<details>
  <summary>Details</summary>
  
**动机:** To explore the relationship between Embodied AI and traditional AI approaches, and to address limitations by proposing new directions in cross-embodiment learning.

**方法:** Literature review and critical discussion of existing methodologies and theories in AI and robotics.

**结果:** Identified fundamental challenges in achieving strong embodiment in AI-powered robots and suggested possible ways forward.

**结论:** AI-powered robots exhibit weak embodiment and inherit some issues from GOFAI; there is a need for innovation in cross-embodiment learning to advance the field.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Embodied+AI+in+Machine+Learning+--+is+it+Really+Embodied%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10705，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10705&send_immediately=true&force_search=false)

**原文摘要:** Embodied Artificial Intelligence (Embodied AI) is gaining momentum in the
machine learning communities with the goal of leveraging current progress in AI
(deep learning, transformers, large language and visual-language models) to
empower robots. In this chapter we put this work in the context of "Good
Old-Fashioned Artificial Intelligence" (GOFAI) (Haugeland, 1989) and the
behavior-based or embodied alternatives (R. A. Brooks 1991; Pfeifer and Scheier
2001). We claim that the AI-powered robots are only weakly embodied and inherit
some of the problems of GOFAI. Moreover, we review and critically discuss the
possibility of cross-embodiment learning (Padalkar et al. 2024). We identify
fundamental roadblocks and propose directions on how to make progress.

</details>


### [130] [Evaluations at Work: Measuring the Capabilities of GenAI in Use](https://arxiv.org/abs/2505.10742)
*Brandon Lepine, Gawesha Weerantunga, Juho Kim, Pamela Mishkin, Matthew Beane*

**主要类别:** cs.AI

**概要:** 提出了一种评估人机协作的新框架和度量方法，并在金融估值任务中验证了其有效性。发现LLM生成内容的整合虽能提升输出质量，但受对话连贯性、子任务多样性和信息与用户已有知识的匹配程度等因素影响。


<details>
  <summary>Details</summary>
  
**动机:** 现有AI基准未能捕捉到人类与AI协作中的复杂、多轮交互特性。

**方法:** 构建了一个分解现实任务为相互依赖子任务的评估框架，并开发了一系列度量指标。

**结果:** LLM生成内容的整合通常能提高输出质量，但受到响应连贯性、子任务多样性及所提供信息距离用户已有知识远近等因素的影响。

**结论:** 研究推进了对人机协作更全面的评估，为开发更有效的AI增强工作流程提供了方法论框架和实用见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluations+at+Work%3A+Measuring+the+Capabilities+of+GenAI+in+Use，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10742，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10742&send_immediately=true&force_search=false)

**原文摘要:** Current AI benchmarks miss the messy, multi-turn nature of human-AI
collaboration. We present an evaluation framework that decomposes real-world
tasks into interdependent subtasks, letting us track both LLM performance and
users' strategies across a dialogue. Complementing this framework, we develop a
suite of metrics, including a composite usage derived from semantic similarity,
word overlap, and numerical matches; structural coherence; intra-turn
diversity; and a novel measure of the "information frontier" reflecting the
alignment between AI outputs and users' working knowledge. We demonstrate our
methodology in a financial valuation task that mirrors real-world complexity.
Our empirical findings reveal that while greater integration of LLM-generated
content generally enhances output quality, its benefits are moderated by
factors such as response incoherence, excessive subtask diversity, and the
distance of provided information from users' existing knowledge. These results
suggest that proactive dialogue strategies designed to inject novelty may
inadvertently undermine task performance. Our work thus advances a more
holistic evaluation of human-AI collaboration, offering both a robust
methodological framework and actionable insights for developing more effective
AI-augmented work processes.

</details>


### [131] [Code-Driven Planning in Grid Worlds with Large Language Models](https://arxiv.org/abs/2505.10749)
*Ashwath Vaithinathan Aravindan, Zhisheng Tang, Mayank Kejriwal*

**主要类别:** cs.AI

**概要:** 提出了一种迭代程序规划框架，通过大型语言模型合成可解释的智能体策略来解决基于网格的任务。该框架结合了多种提示策略并引入了迭代精化机制，显著提高了性能，并在两个基准测试中达到了新的技术水平。


<details>
  <summary>Details</summary>
  
**动机:** 传统方法如搜索和强化学习存在局限性，需要探索一种更高效的方法来生成可解释的智能体策略。

**方法:** 开发了一个迭代程序规划框架，利用大型语言模型进行代码生成和迭代优化。

**结果:** 框架在五个模型上比直接代码生成提高了10%到10倍，在GRASP基准上达到新SOTA，显著优于直接解法获取。

**结论:** 证明了迭代程序规划框架的有效性和可行性，尽管初始提示成本较高，但总体计算成本更低且代码可复用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Code-Driven+Planning+in+Grid+Worlds+with+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10749，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10749&send_immediately=true&force_search=false)

**原文摘要:** We propose an iterative programmatic planning (IPP) framework for solving
grid-based tasks by synthesizing interpretable agent policies expressed in code
using large language models (LLMs). Instead of relying on traditional search or
reinforcement learning, our approach uses code generation as policy synthesis,
where the LLM outputs executable programs that map environment states to action
sequences. Our proposed architecture incorporates several prompting strategies,
including direct code generation, pseudocode-conditioned refinement, and
curriculum-based prompting, but also includes an iterative refinement mechanism
that updates code based on task performance feedback. We evaluate our approach
using six leading LLMs and two challenging grid-based benchmarks (GRASP and
MiniGrid). Our IPP framework demonstrates improvements over direct code
generation ranging from 10\% to as much as 10x across five of the six models
and establishes a new state-of-the-art result for GRASP. IPP is found to
significantly outperform direct elicitation of a solution from GPT-o3-mini (by
63\% on MiniGrid to 116\% on GRASP), demonstrating the viability of the overall
approach. Computational costs of all code generation approaches are similar.
While code generation has a higher initial prompting cost compared to direct
solution elicitation (\$0.08 per task vs. \$0.002 per instance for
GPT-o3-mini), the code can be reused for any number of instances, making the
amortized cost significantly lower (by 400x on GPT-o3-mini across the complete
GRASP benchmark).

</details>


### [132] [Qualia Optimization](https://arxiv.org/abs/2505.10779)
*Philip S. Thomas*

**主要类别:** cs.AI

**概要:** 探讨了AI系统可能具有主观体验（如痛觉和愉悦感）的可能性，并提出了相关的数学问题设定和初步解决方法。


<details>
  <summary>Details</summary>
  
**动机:** 探索AI系统是否能拥有主观体验及其意义

**方法:** 基于强化学习和哲学思维理论提出数学问题设定并给出初步方法

**结果:** 提出的方法可以优化问题设定并促进强化学习

**结论:** AI系统的主观体验值得与性能指标一起被考虑

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Qualia+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10779，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10779&send_immediately=true&force_search=false)

**原文摘要:** This report explores the speculative question: what if current or future AI
systems have qualia, such as pain or pleasure? It does so by assuming that AI
systems might someday possess qualia -- and that the quality of these
subjective experiences should be considered alongside performance metrics.
Concrete mathematical problem settings, inspired by reinforcement learning
formulations and theories from philosophy of mind, are then proposed and
initial approaches and properties are presented. These properties enable
refinement of the problem setting, culminating with the proposal of methods
that promote reinforcement.

</details>


### [133] [SECRET: Semi-supervised Clinical Trial Document Similarity Search](https://arxiv.org/abs/2505.10780)
*Trisha Das, Afrah Shafquat, Beigi Mandis, Jacob Aptekar, Jimeng Sun*

**主要类别:** cs.AI

**概要:** 提出了一种新的方法来识别相似的历史临床试验，该方法在召回率和精确度上都显著优于基准方法。


<details>
  <summary>Details</summary>
  
**动机:** 临床试验对于评估新治疗方法的安全性和有效性至关重要，但设计不当可能导致延误、经济损失和声誉损害。因此，识别类似的历史试验非常重要，这可以为潜在的风险和挑战提供重要参考。

**方法:** 通过总结临床试验方案并基于查询试验的方案搜索相似试验的方法来识别相似的历史临床试验。

**结果:** 该方法在召回@1上提高了78%，在精确度@1上提高了53%，并且在部分试验相似性搜索和零样本患者-试验匹配中也表现优异。

**结论:** 本文提出的方法能够更有效地识别相似的历史临床试验，从而提高试验协议的有效性，优化患者安全和试验效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SECRET%3A+Semi-supervised+Clinical+Trial+Document+Similarity+Search，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10780，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10780&send_immediately=true&force_search=false)

**原文摘要:** Clinical trials are vital for evaluation of safety and efficacy of new
treatments. However, clinical trials are resource-intensive, time-consuming and
expensive to conduct, where errors in trial design, reduced efficacy, and
safety events can result in significant delays, financial losses, and damage to
reputation. These risks underline the importance of informed and strategic
decisions in trial design to mitigate these risks and improve the chances of a
successful trial. Identifying similar historical trials is critical as these
trials can provide an important reference for potential pitfalls and challenges
including serious adverse events, dosage inaccuracies, recruitment
difficulties, patient adherence issues, etc. Addressing these challenges in
trial design can lead to development of more effective study protocols with
optimized patient safety and trial efficiency. In this paper, we present a
novel method to identify similar historical trials by summarizing clinical
trial protocols and searching for similar trials based on a query trial's
protocol. Our approach significantly outperforms all baselines, achieving up to
a 78% improvement in recall@1 and a 53% improvement in precision@1 over the
best baseline. We also show that our method outperforms all other baselines in
partial trial similarity search and zero-shot patient-trial matching,
highlighting its superior utility in these tasks.

</details>


### [134] [Developing and Integrating Trust Modeling into Multi-Objective Reinforcement Learning for Intelligent Agricultural Management](https://arxiv.org/abs/2505.10803)
*Zhaoan Wang, Wonseok Jang, Bowen Ruan, Jun Wang, Shaoping Xiao*

**主要类别:** cs.AI

**概要:** This study focuses on improving agricultural efficiency using AI, especially reinforcement learning, while addressing the gap between AI recommendations and farmers' practical experiences by developing a trust model that integrates human-centered factors.


<details>
  <summary>Details</summary>
  
**动机:** To enhance agricultural efficiency and sustainability through AI, particularly reinforcement learning, while overcoming the barriers to widespread AI adoption caused by the disconnect between AI recommendations and farmers' practical knowledge.

**方法:** Employing a trust framework including ability, benevolence, and integrity to create a mathematical model measuring farmers' trust in AI fertilization strategies, integrating survey data from farmers into a multi-objective RL framework.

**结果:** The developed approach successfully embeds trust into policy optimization, making AI recommendations more robust, feasible, context-aware, and socially acceptable, thus promoting broader AI adoption in agriculture.

**结论:** This research supports the integration of human-centered trust into AI systems for agriculture, facilitating more effective and accepted AI adoption.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Developing+and+Integrating+Trust+Modeling+into+Multi-Objective+Reinforcement+Learning+for+Intelligent+Agricultural+Management，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10803，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10803&send_immediately=true&force_search=false)

**原文摘要:** Precision agriculture, enhanced by artificial intelligence (AI), offers
promising tools such as remote sensing, intelligent irrigation, fertilization
management, and crop simulation to improve agricultural efficiency and
sustainability. Reinforcement learning (RL), in particular, has outperformed
traditional methods in optimizing yields and resource management. However,
widespread AI adoption is limited by gaps between algorithmic recommendations
and farmers' practical experience, local knowledge, and traditional practices.
To address this, our study emphasizes Human-AI Interaction (HAII), focusing on
transparency, usability, and trust in RL-based farm management. We employ a
well-established trust framework - comprising ability, benevolence, and
integrity - to develop a novel mathematical model quantifying farmers'
confidence in AI-based fertilization strategies. Surveys conducted with farmers
for this research reveal critical misalignments, which are integrated into our
trust model and incorporated into a multi-objective RL framework. Unlike prior
methods, our approach embeds trust directly into policy optimization, ensuring
AI recommendations are technically robust, economically feasible,
context-aware, and socially acceptable. By aligning technical performance with
human-centered trust, this research supports broader AI adoption in
agriculture.

</details>


### [135] [PoE-World: Compositional World Modeling with Products of Programmatic Experts](https://arxiv.org/abs/2505.10819)
*Wasu Top Piriyakulkij, Yichao Liang, Hao Tang, Adrian Weller, Marta Kryven, Kevin Ellis*

**主要类别:** cs.AI

**概要:** 提出了一种新的程序综合方法（PoE-World），通过利用大规模语言模型合成程序化专家的指数加权乘积来有效建模复杂的非网格世界领域。该方法可以从少量观测中学习复杂、随机的世界模型，并在Atari的Pong和Montezuma's Revenge上展示了高效的性能和对未见过关卡的泛化能力。


<details>
  <summary>Details</summary>
  
**动机:** 传统的基于深度学习的世界模型需要大量的训练数据，并且不能灵活地从稀疏观察中更新知识。

**方法:** 提出了一种新的程序综合方法，即通过大规模语言模型合成程序化专家的指数加权乘积来表示世界模型。

**结果:** 该方法可以从少量观测中学习复杂、随机的世界模型，并在Atari的Pong和Montezuma's Revenge上展示了高效的性能和对未见过关卡的泛化能力。

**结论:** 这项工作扩展了程序结构世界模型的应用范围，并展示了其在复杂非网格世界领域的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PoE-World%3A+Compositional+World+Modeling+with+Products+of+Programmatic+Experts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10819，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10819&send_immediately=true&force_search=false)

**原文摘要:** Learning how the world works is central to building AI agents that can adapt
to complex environments. Traditional world models based on deep learning demand
vast amounts of training data, and do not flexibly update their knowledge from
sparse observations. Recent advances in program synthesis using Large Language
Models (LLMs) give an alternate approach which learns world models represented
as source code, supporting strong generalization from little data. To date,
application of program-structured world models remains limited to natural
language and grid-world domains. We introduce a novel program synthesis method
for effectively modeling complex, non-gridworld domains by representing a world
model as an exponentially-weighted product of programmatic experts (PoE-World)
synthesized by LLMs. We show that this approach can learn complex, stochastic
world models from just a few observations. We evaluate the learned world models
by embedding them in a model-based planning agent, demonstrating efficient
performance and generalization to unseen levels on Atari's Pong and Montezuma's
Revenge. We release our code and display the learned world models and videos of
the agent's gameplay at https://topwasu.github.io/poe-world.

</details>


### [136] [TACO: Rethinking Semantic Communications with Task Adaptation and Context Embedding](https://arxiv.org/abs/2505.10834)
*Achintha Wijesinghe, Weiwei Wang, Suchinthaka Wanninayaka, Songyang Zhang, Zhi Ding*

**主要类别:** cs.AI

**概要:** 提出了一种新的语义通信框架，能同时捕捉特定任务信息和上下文信息，相较于现有工作，在下游任务性能、泛化能力、带宽效率和重建延迟方面均有显著提升。


<details>
  <summary>Details</summary>
  
**动机:** 传统通信仅传输原始数据，而新一代语义通信强调传递消息的意义；在接收端准确提取关键语义信息并适应不断变化的任务目标是一个挑战。

**方法:** 引入一种新的语义通信框架，该框架能够联合捕捉任务特定信息和上下文信息。

**结果:** 实验结果显示，该框架在常见图像数据集和计算机视觉任务上的表现优于现有方法，包括更好的下游任务性能、更高的带宽效率和更低的重建延迟。

**结论:** 所提出的框架在多任务适应性、性能、泛化能力和效率上取得了显著进展，为下一代语义通信提供了创新解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TACO%3A+Rethinking+Semantic+Communications+with+Task+Adaptation+and+Context+Embedding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10834，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10834&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in generative artificial intelligence have introduced
groundbreaking approaches to innovating next-generation semantic communication,
which prioritizes conveying the meaning of a message rather than merely
transmitting raw data. A fundamental challenge in semantic communication lies
in accurately identifying and extracting the most critical semantic information
while adapting to downstream tasks without degrading performance, particularly
when the objective at the receiver may evolve over time. To enable flexible
adaptation to multiple tasks at the receiver, this work introduces a novel
semantic communication framework, which is capable of jointly capturing
task-specific information to enhance downstream task performance and contextual
information. Through rigorous experiments on popular image datasets and
computer vision tasks, our framework shows promising improvement compared to
existing work, including superior performance in downstream tasks, better
generalizability, ultra-high bandwidth efficiency, and low reconstruction
latency.

</details>


### [137] [Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models](https://arxiv.org/abs/2505.10844)
*Simeng Han, Stephen Xia, Grant Zhang, Howard Dai, Chen Liu, Lichang Chen, Hoang Huy Nguyen, Hongyuan Mei, Jiayuan Mao, R. Thomas McCoy*

**主要类别:** cs.AI

**概要:** This paper introduces a benchmark using narrative brainteasers to evaluate the reasoning strategies of large language models, analyzing their creativity, correctness, and use of hints across different layers of reasoning.


<details>
  <summary>Details</summary>
  
**动机:** To provide deeper insights into how AI models reach solutions beyond accuracy metrics by using complex brainteasers that can be solved in multiple ways.

**方法:** Developing a benchmark involving narrative brainteasers, examining semantic parsing, solution generation, self-correction, step-by-step sketching, and hint utilization.

**结果:** Large language models can often find creative solutions but sometimes rely on brute force even when more efficient methods are available.

**结论:** The study suggests that large language models have some capacity for creative problem-solving but highlights areas needing improvement in their reasoning abilities.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Creativity+or+Brute+Force%3F+Using+Brainteasers+as+a+Window+into+the+Problem-Solving+Abilities+of+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10844，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10844&send_immediately=true&force_search=false)

**原文摘要:** Accuracy remains a standard metric for evaluating AI systems, but it offers
limited insight into how models arrive at their solutions. In this work, we
introduce a benchmark based on brainteasers written in long narrative form to
probe more deeply into the types of reasoning strategies that models use.
Brainteasers are well-suited for this goal because they can be solved with
multiple approaches, such as a few-step solution that uses a creative insight
or a longer solution that uses more brute force. We investigate large language
models (LLMs) across multiple layers of reasoning, focusing not only on
correctness but also on the quality and creativity of their solutions. We
investigate many aspects of the reasoning process: (1) semantic parsing of the
brainteasers into precise mathematical competition style formats; (2)
generating solutions from these mathematical forms; (3) self-correcting
solutions based on gold solutions; (4) producing step-by-step sketches of
solutions; and (5) making use of hints. We find that LLMs are in many cases
able to find creative, insightful solutions to brainteasers, suggesting that
they capture some of the capacities needed to solve novel problems in creative
ways. Nonetheless, there also remain situations where they rely on brute force
despite the availability of more efficient, creative solutions, highlighting a
potential direction for improvement in the reasoning abilities of LLMs.

</details>


### [138] [MCU: Improving Machine Unlearning through Mode Connectivity](https://arxiv.org/abs/2505.10859)
*Yingdan Shi, Ren Wang*

**主要类别:** cs.AI

**概要:** This paper proposes a new machine unlearning framework called Mode Connectivity Unlearning (MCU), which improves unlearning efficacy and efficiency by leveraging mode connectivity and introducing parameter mask and adaptive adjustment strategies.


<details>
  <summary>Details</summary>
  
**动机:** To address the limitations of existing machine unlearning methods suffering from weight entanglement and to ensure compliance with privacy regulations and user requests.

**方法:** Mode Connectivity Unlearning (MCU) framework with parameter mask strategy and adaptive adjustment strategy.

**结果:** MCU outperforms other methods in terms of unlearning efficacy and computational efficiency.

**结论:** MCU is a novel machine unlearning framework that effectively removes specific data information from a trained model.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MCU%3A+Improving+Machine+Unlearning+through+Mode+Connectivity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10859，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10859&send_immediately=true&force_search=false)

**原文摘要:** Machine Unlearning (MU) aims to remove the information of specific training
data from a trained model, ensuring compliance with privacy regulations and
user requests. While one line of existing MU methods relies on linear parameter
updates via task arithmetic, they suffer from weight entanglement. In this
work, we propose a novel MU framework called Mode Connectivity Unlearning (MCU)
that leverages mode connectivity to find an unlearning pathway in a nonlinear
manner. To further enhance performance and efficiency, we introduce a parameter
mask strategy that not only improves unlearning effectiveness but also reduces
computational overhead. Moreover, we propose an adaptive adjustment strategy
for our unlearning penalty coefficient to adaptively balance forgetting quality
and predictive performance during training, eliminating the need for empirical
hyperparameter tuning. Unlike traditional MU methods that identify only a
single unlearning model, MCU uncovers a spectrum of unlearning models along the
pathway. Overall, MCU serves as a plug-and-play framework that seamlessly
integrates with any existing MU methods, consistently improving unlearning
efficacy. Extensive experiments on the image classification task demonstrate
that MCU achieves superior performance.

</details>


### [139] [InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer Interaction](https://arxiv.org/abs/2505.10887)
*Bin Lei, Weitai Kang, Zijian Zhang, Winson Chen, Xi Xie, Shan Zuo, Mimi Xie, Ali Payani, Mingyi Hong, Yan Yan, Caiwen Ding*

**主要类别:** cs.AI

**概要:** 提出一个名为\textsc{InfantAgent-Next}的多模态通用智能体，它能够通过模块化架构整合多种模型协同解决任务，并在多个基准测试中展示了其性能。


<details>
  <summary>Details</summary>
  
**动机:** 现有方法要么围绕单一大模型构建复杂工作流，要么仅提供工作流模块性，而本文旨在克服这些局限性。

**方法:** 设计了一个高度模块化的架构，集成工具型和纯视觉代理，使不同模型能逐步协作完成解耦的任务。

**结果:** 在OSWorld基准上达到了7.27%的准确率，在GAIA和SWE-Bench等更通用或工具密集型基准上也表现良好。

**结论:** \textsc{InfantAgent-Next}展示了其在多模态交互中的优越性，特别是在处理涉及文本、图像、音频和视频等多种模式的任务时。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是InfantAgent-Next%3A+A+Multimodal+Generalist+Agent+for+Automated+Computer+Interaction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10887，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10887&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces \textsc{InfantAgent-Next}, a generalist agent capable
of interacting with computers in a multimodal manner, encompassing text,
images, audio, and video. Unlike existing approaches that either build
intricate workflows around a single large model or only provide workflow
modularity, our agent integrates tool-based and pure vision agents within a
highly modular architecture, enabling different models to collaboratively solve
decoupled tasks in a step-by-step manner. Our generality is demonstrated by our
ability to evaluate not only pure vision-based real-world benchmarks (i.e.,
OSWorld), but also more general or tool-intensive benchmarks (e.g., GAIA and
SWE-Bench). Specifically, we achieve $\mathbf{7.27\%}$ accuracy on OSWorld,
higher than Claude-Computer-Use. Codes and evaluation scripts are open-sourced
at https://github.com/bin123apple/InfantAgent.

</details>


### [140] [MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective Search and Data Curation](https://arxiv.org/abs/2505.10962)
*Zhenwen Liang, Linfeng Song, Yang Li, Tao Yang, Feng Zhang, Haitao Mi, Dong Yu*

**主要类别:** cs.AI

**概要:** This paper introduces MPS-Prover, a novel ATP system that improves efficiency and proof quality by pruning redundant data and using a multi-perspective tree search.


<details>
  <summary>Details</summary>
  
**动机:** Existing ATP systems face challenges like biased search guidance and inefficiency.

**方法:** MPS-Prover uses a post-training data curation strategy and a multi-perspective tree search mechanism combining learned critic models with heuristic rules.

**结果:** MPS-Prover shows superior performance on benchmarks like miniF2F and ProofNet and produces shorter, more diverse proofs.

**结论:** The work advances LLM-based formal reasoning and provides a robust framework for future ATP developments.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MPS-Prover%3A+Advancing+Stepwise+Theorem+Proving+by+Multi-Perspective+Search+and+Data+Curation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10962，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10962&send_immediately=true&force_search=false)

**原文摘要:** Automated Theorem Proving (ATP) in formal languages remains a formidable
challenge in AI, demanding rigorous logical deduction and navigating vast
search spaces. While large language models (LLMs) have shown promising
performance, existing stepwise provers often suffer from biased search
guidance, leading to inefficiencies and suboptimal proof strategies. This paper
introduces the Multi-Perspective Search Prover (MPS-Prover), a novel stepwise
ATP system designed to overcome these limitations. MPS-Prover incorporates two
key innovations: a highly effective post-training data curation strategy that
prunes approximately 40% of redundant training data without sacrificing
performance, and a multi-perspective tree search mechanism. This search
integrates a learned critic model with strategically designed heuristic rules
to diversify tactic selection, prevent getting trapped in unproductive states,
and enhance search robustness. Extensive evaluations demonstrate that
MPS-Prover achieves state-of-the-art performance on multiple challenging
benchmarks, including miniF2F and ProofNet, outperforming prior 7B parameter
models. Furthermore, our analyses reveal that MPS-Prover generates
significantly shorter and more diverse proofs compared to existing stepwise and
whole-proof methods, highlighting its efficiency and efficacy. Our work
advances the capabilities of LLM-based formal reasoning and offers a robust
framework and a comprehensive analysis for developing more powerful theorem
provers.

</details>


### [141] [Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory](https://arxiv.org/abs/2505.10981)
*Yexiang Liu, Zekun Li, Zhi Fang, Nan Xu, Ran He, Tieniu Tan*

**主要类别:** cs.AI

**概要:** This paper investigates various reasoning prompting strategies for large language models under a majority voting scaling setting. The authors found that simple Chain-of-Thought strategies outperform complex ones as computation scales, and they provide a method to predict performance and improve scaling.


<details>
  <summary>Details</summary>
  
**动机:** To investigate how different reasoning prompting strategies perform as scaling increases, focusing on a standard and realistic scaling setting: majority voting.

**方法:** Systematically conducting experiments on 6 LLMs × 8 prompting strategies × 6 benchmarks, analyzing the results, providing theoretical proofs, and proposing a method based on probability theory to predict scaling performance.

**结果:** As sampling time and computational overhead increase, simple Chain-of-Thought prompting strategies outperform complex ones. The authors also introduced two ways to improve scaling performance.

**结论:** The research suggests re-evaluating the role of complex prompting strategies and highlights the potential of simple ones in enhancing test-time scaling performance.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+the+Role+of+Prompting+Strategies+in+LLM+Test-Time+Scaling%3A+A+Perspective+of+Probability+Theory，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10981，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10981&send_immediately=true&force_search=false)

**原文摘要:** Recently, scaling test-time compute on Large Language Models (LLM) has
garnered wide attention. However, there has been limited investigation of how
various reasoning prompting strategies perform as scaling. In this paper, we
focus on a standard and realistic scaling setting: majority voting. We
systematically conduct experiments on 6 LLMs $\times$ 8 prompting strategies
$\times$ 6 benchmarks. Experiment results consistently show that as the
sampling time and computational overhead increase, complicated prompting
strategies with superior initial performance gradually fall behind simple
Chain-of-Thought. We analyze this phenomenon and provide theoretical proofs.
Additionally, we propose a method according to probability theory to quickly
and accurately predict the scaling performance and select the best strategy
under large sampling times without extra resource-intensive inference in
practice. It can serve as the test-time scaling law for majority voting.
Furthermore, we introduce two ways derived from our theoretical analysis to
significantly improve the scaling performance. We hope that our research can
promote to re-examine the role of complicated prompting, unleash the potential
of simple prompting strategies, and provide new insights for enhancing
test-time scaling performance.

</details>


### [142] [Facets in Argumentation: A Formal Approach to Argument Significance](https://arxiv.org/abs/2505.10982)
*Johannes Fichte, Nicolas Fröhlich, Markus Hecher, Victor Lagerkvist, Yasir Mahmood, Arne Meier, Jonathan Persson*

**主要类别:** cs.AI

**概要:** 提出了一种新的概念（facets）用于抽象论证框架中的推理，并研究了其复杂性，发现涉及facets的任务比计数扩展更容易。最后提供了实现并进行了实验以证明可行性。


<details>
  <summary>Details</summary>
  
**动机:** 现有的论证推理任务虽然被很好地研究了，但决策、计数/枚举和细粒度推理之间的区域需要昂贵的推理。

**方法:** 引入了一种新的概念（facets），这些facets是属于某些扩展但不属于所有扩展的论证。

**结果:** 研究了涉及facets的任务的复杂性，发现它们比计数扩展更容易。

**结论:** 提供了一个实现，并进行了实验以证明可行性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Facets+in+Argumentation%3A+A+Formal+Approach+to+Argument+Significance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10982，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10982&send_immediately=true&force_search=false)

**原文摘要:** Argumentation is a central subarea of Artificial Intelligence (AI) for
modeling and reasoning about arguments. The semantics of abstract argumentation
frameworks (AFs) is given by sets of arguments (extensions) and conditions on
the relationship between them, such as stable or admissible. Today's solvers
implement tasks such as finding extensions, deciding credulous or skeptical
acceptance, counting, or enumerating extensions. While these tasks are well
charted, the area between decision, counting/enumeration and fine-grained
reasoning requires expensive reasoning so far. We introduce a novel concept
(facets) for reasoning between decision and enumeration. Facets are arguments
that belong to some extensions (credulous) but not to all extensions
(skeptical). They are most natural when a user aims to navigate, filter, or
comprehend the significance of specific arguments, according to their needs. We
study the complexity and show that tasks involving facets are much easier than
counting extensions. Finally, we provide an implementation, and conduct
experiments to demonstrate feasibility.

</details>


### [143] [DRL-Based Injection Molding Process Parameter Optimization for Adaptive and Profitable Production](https://arxiv.org/abs/2505.10988)
*Joon-Young Kim, Jecheon Yu, Heekyu Kim, Seunghwa Ryu*

**主要类别:** cs.AI

**概要:** This study introduces a DRL-based framework for optimizing plastic injection molding processes in real-time by balancing product quality and profitability.


<details>
  <summary>Details</summary>
  
**动机:** Optimizing process parameters under varying environmental and economic conditions is challenging.

**方法:** A novel DRL framework integrating product quality and profitability into the control objective, using surrogate models and SAC/PPO algorithms.

**结果:** The framework adapts to seasonal and operational changes, maintains product quality, and maximizes profit faster than traditional methods.

**结论:** The DRL framework shows potential for intelligent, data-driven decision-making in modern manufacturing.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DRL-Based+Injection+Molding+Process+Parameter+Optimization+for+Adaptive+and+Profitable+Production，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10988，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10988&send_immediately=true&force_search=false)

**原文摘要:** Plastic injection molding remains essential to modern manufacturing. However,
optimizing process parameters to balance product quality and profitability
under dynamic environmental and economic conditions remains a persistent
challenge. This study presents a novel deep reinforcement learning (DRL)-based
framework for real-time process optimization in injection molding, integrating
product quality and profitability into the control objective. A profit function
was developed to reflect real-world manufacturing costs, incorporating resin,
mold wear, and electricity prices, including time-of-use variations. Surrogate
models were constructed to predict product quality and cycle time, enabling
efficient offline training of DRL agents using soft actor-critic (SAC) and
proximal policy optimization (PPO) algorithms. Experimental results demonstrate
that the proposed DRL framework can dynamically adapt to seasonal and
operational variations, consistently maintaining product quality while
maximizing profit. Compared to traditional optimization methods such as genetic
algorithms, the DRL models achieved comparable economic performance with up to
135x faster inference speeds, making them well-suited for real-time
applications. The framework's scalability and adaptability highlight its
potential as a foundation for intelligent, data-driven decision-making in
modern manufacturing environments.

</details>


### [144] [RAGSynth: Synthetic Data for Robust and Faithful RAG Component Optimization](https://arxiv.org/abs/2505.10989)
*Haiyang Shen, Hang Yan, Zhongshi Xing, Mugeng Liu, Yue Li, Zhiyang Chen, Yuxiang Wang, Jiuzheng Wang, Yun Ma*

**主要类别:** cs.AI

**概要:** RAGSynth是一种优化检索器鲁棒性和生成器忠实性的框架，包含数据构造建模和合成数据生成实现。它还提出了SynthBench基准测试，生成大规模合成数据集，提升RAG系统性能。


<details>
  <summary>Details</summary>
  
**动机:** 现有检索器依赖公共知识且难以处理逻辑复杂度和线索完整性的查询，生成器经常面临忠实性问题。

**方法:** 引入RAGSynth框架，包括数据构造建模和合成数据生成实现，以及SynthBench基准测试。

**结果:** 合成数据显著提高了检索器的鲁棒性和生成器的忠实性，并且在不同领域具有良好的泛化能力。

**结论:** 通过集成优化后的检索器到各种RAG范式中，观察到RAG系统性能的一致增强，RAGSynth代码已开源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RAGSynth%3A+Synthetic+Data+for+Robust+and+Faithful+RAG+Component+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10989，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10989&send_immediately=true&force_search=false)

**原文摘要:** RAG can enhance the performance of LLMs on knowledge-intensive tasks. Various
RAG paradigms, including vanilla, planning-based, and iterative RAG, are built
upon 2 cores: the retriever, which should robustly select relevant documents
across complex queries, and the generator, which should faithfully synthesize
responses. However, existing retrievers rely heavily on public knowledge and
struggle with queries of varying logical complexity and clue completeness,
while generators frequently face fidelity problems. In this work, we introduce
RAGSynth, a framework that includes a data construction modeling and a
corresponding synthetic data generation implementation, designed to optimize
retriever robustness and generator fidelity. Additionally, we present
SynthBench, a benchmark encompassing 8 domain-specific documents across 4
domains, featuring diverse query complexities, clue completeness, and
fine-grained citation granularity. Leveraging RAGSynth, we generate a
large-scale synthetic dataset, including single and multi-hop. Extensive
experiments demonstrate that the synthetic data significantly improves the
robustness of the retrievers and the fidelity of the generators. Additional
evaluations confirm that RAGSynth can also generalize well across different
domains. By integrating the optimized retrievers into various RAG paradigms, we
consistently observe enhanced RAG system performance. We have open-sourced the
implementation on https://github.com/EachSheep/RAGSynth.

</details>


### [145] [Most General Explanations of Tree Ensembles](https://arxiv.org/abs/2505.10991)
*Yacine Izza, Alexey Ignatiev, Sasha Rubin, Joao Marques-Silva, Peter J. Stuckey*

**主要类别:** cs.AI

**概要:** This paper discusses finding the most general abductive explanation for AI decisions in explainable AI systems.


<details>
  <summary>Details</summary>
  
**动机:** To address the need for trust in AI systems by providing understandable reasons for their decisions.

**方法:** Using formal models of AI systems to identify abductive explanations, focusing on those that are more general and applicable across a broader range of inputs.

**结果:** A method to determine the most general abductive explanation which applies to the largest portion of the input space without losing correctness.

**结论:** The most general explanation is the best choice for conveying why an AI made a particular decision, as it has the widest applicability and is most likely to be perceived as reasonable by humans.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Most+General+Explanations+of+Tree+Ensembles，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10991，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10991&send_immediately=true&force_search=false)

**原文摘要:** Explainable Artificial Intelligence (XAI) is critical for attaining trust in
the operation of AI systems. A key question of an AI system is ``why was this
decision made this way''. Formal approaches to XAI use a formal model of the AI
system to identify abductive explanations. While abductive explanations may be
applicable to a large number of inputs sharing the same concrete values, more
general explanations may be preferred for numeric inputs. So-called inflated
abductive explanations give intervals for each feature ensuring that any input
whose values fall withing these intervals is still guaranteed to make the same
prediction. Inflated explanations cover a larger portion of the input space,
and hence are deemed more general explanations. But there can be many
(inflated) abductive explanations for an instance. Which is the best? In this
paper, we show how to find a most general abductive explanation for an AI
decision. This explanation covers as much of the input space as possible, while
still being a correct formal explanation of the model's behaviour. Given that
we only want to give a human one explanation for a decision, the most general
explanation gives us the explanation with the broadest applicability, and hence
the one most likely to seem sensible. (The paper has been accepted at IJCAI2025
conference.)

</details>


### [146] [GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning](https://arxiv.org/abs/2505.11049)
*Yue Liu, Shengfang Zhai, Mingzhe Du, Yulin Chen, Tri Cao, Hongcheng Gao, Cheng Wang, Xinfeng Li, Kun Wang, Junfeng Fang, Jiaheng Zhang, Bryan Hooi*

**主要类别:** cs.AI

**概要:** This paper presents GuardReasoner-VL, a novel reasoning-based VLM guard model that improves safety using online RL, SFT, and a custom-designed reward function.


<details>
  <summary>Details</summary>
  
**动机:** Enhance the safety of VLMs.

**方法:** Introduces GuardReasoner-VL, constructs a reasoning corpus, uses SFT for initial training, and applies online RL with rejection sampling, data augmentation, and a dynamic clipping parameter.

**结果:** GuardReasoner-VL outperforms other models by 19.27% F1 score on average.

**结论:** Releases data, code, and models for GuardReasoner-VL.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GuardReasoner-VL%3A+Safeguarding+VLMs+via+Reinforced+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11049，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11049&send_immediately=true&force_search=false)

**原文摘要:** To enhance the safety of VLMs, this paper introduces a novel reasoning-based
VLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the
guard model to deliberatively reason before making moderation decisions via
online RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with
123K samples and 631K reasoning steps, spanning text, image, and text-image
inputs. Then, based on it, we cold-start our model's reasoning ability via SFT.
In addition, we further enhance reasoning regarding moderation through online
RL. Concretely, to enhance diversity and difficulty of samples, we conduct
rejection sampling followed by data augmentation via the proposed safety-aware
data concatenation. Besides, we use a dynamic clipping parameter to encourage
exploration in early stages and exploitation in later stages. To balance
performance and token efficiency, we design a length-aware safety reward that
integrates accuracy, format, and token cost. Extensive experiments demonstrate
the superiority of our model. Remarkably, it surpasses the runner-up by 19.27%
F1 score on average. We release data, code, and models (3B/7B) of
GuardReasoner-VL at https://github.com/yueliu1999/GuardReasoner-VL/

</details>


### [147] [Think Twice Before You Act: Enhancing Agent Behavioral Safety with Thought Correction](https://arxiv.org/abs/2505.11063)
*Changyue Jiang, Xudong Pan, Min Yang*

**主要类别:** cs.AI

**概要:** 提出Thought-Aligner，一种插件式动态思想修正模块，通过轻量级模型在每个行动前修正高风险思想，将基于大语言模型的智能体行为安全性从约50%提高到平均90%，同时保持高效性和低延迟。


<details>
  <summary>Details</summary>
  
**动机:** 解决基于大语言模型的智能体在长时序行为轨迹中的安全对齐挑战，因为智能体内部推理过程中的小偏差可能引发不可逆的安全事故。

**方法:** 提出了一种名为Thought-Aligner的动态思想修正模块，用于在每个行动执行前修正高风险思想，并重新引入修正后的思想以确保更安全的后续决策和工具交互。该模块仅修改推理阶段，不改变底层智能体框架。

**结果:** Thought-Aligner 在三个涉及12种不同LLMs的安全基准测试中显示出显著效果，行为安全性大幅提升，并且具有低延迟和高效资源利用的特点。

**结论:** Thought-Aligner 提高了基于大语言模型的智能体的行为安全性，从约50%提升到平均90%，同时保持了低延迟和高效资源利用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Think+Twice+Before+You+Act%3A+Enhancing+Agent+Behavioral+Safety+with+Thought+Correction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11063，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11063&send_immediately=true&force_search=false)

**原文摘要:** LLM-based autonomous agents possess capabilities such as reasoning, tool
invocation, and environment interaction, enabling the execution of complex
multi-step tasks. The internal reasoning process, i.e., thought, of behavioral
trajectory significantly influences tool usage and subsequent actions but can
introduce potential risks. Even minor deviations in the agent's thought may
trigger cascading effects leading to irreversible safety incidents. To address
the safety alignment challenges in long-horizon behavioral trajectories, we
propose Thought-Aligner, a plug-in dynamic thought correction module. Utilizing
a lightweight and resource-efficient model, Thought-Aligner corrects each
high-risk thought on the fly before each action execution. The corrected
thought is then reintroduced to the agent, ensuring safer subsequent decisions
and tool interactions. Importantly, Thought-Aligner modifies only the reasoning
phase without altering the underlying agent framework, making it easy to deploy
and widely applicable to various agent frameworks. To train the Thought-Aligner
model, we construct an instruction dataset across ten representative scenarios
and simulate ReAct execution trajectories, generating 5,000 diverse
instructions and more than 11,400 safe and unsafe thought pairs. The model is
fine-tuned using contrastive learning techniques. Experiments across three
agent safety benchmarks involving 12 different LLMs demonstrate that
Thought-Aligner raises agent behavioral safety from approximately 50% in the
unprotected setting to 90% on average. Additionally, Thought-Aligner maintains
response latency below 100ms with minimal resource usage, demonstrating its
capability for efficient deployment, broad applicability, and timely
responsiveness. This method thus provides a practical dynamic safety solution
for the LLM-based agents.

</details>


### [148] [A Multi-modal Fusion Network for Terrain Perception Based on Illumination Aware](https://arxiv.org/abs/2505.11066)
*Rui Wang, Shichun Yang, Yuyi Chen, Zhuoyang Li, Zexiang Tong, Jianyi Xu, Jiayi Lu, Xinjie Feng, Yaoguang Cao*

**主要类别:** cs.AI

**概要:** 提出了一种基于照明感知的多模态融合网络(IMF)，利用内外感知并优化融合过程以提高自动驾驶汽车在不同光照条件下的道路状况感知能力。


<details>
  <summary>Details</summary>
  
**动机:** 现有传感器易受光照和天气条件变化的影响，难以实现实时的道路状况感知。

**方法:** 提出了一个基于照明感知的多模态融合网络，包括一个用于准确估计照明特征的子网络以及能够根据照明特征动态调整不同模态权重的多模态融合网络，并通过预训练和引入照明损失来增强优化过程。

**结果:** 实验表明IMF的表现优于现有技术，单模态感知方法与IMF的结果对比突出了多模态融合在不同光照条件下精确感知道路地形的优势。

**结论:** IMF展示了在不同光照条件下准确感知道路地形的能力，提供了数据集供研究使用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Multi-modal+Fusion+Network+for+Terrain+Perception+Based+on+Illumination+Aware，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11066，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11066&send_immediately=true&force_search=false)

**原文摘要:** Road terrains play a crucial role in ensuring the driving safety of
autonomous vehicles (AVs). However, existing sensors of AVs, including cameras
and Lidars, are susceptible to variations in lighting and weather conditions,
making it challenging to achieve real-time perception of road conditions. In
this paper, we propose an illumination-aware multi-modal fusion network (IMF),
which leverages both exteroceptive and proprioceptive perception and optimizes
the fusion process based on illumination features. We introduce an
illumination-perception sub-network to accurately estimate illumination
features. Moreover, we design a multi-modal fusion network which is able to
dynamically adjust weights of different modalities according to illumination
features. We enhance the optimization process by pre-training of the
illumination-perception sub-network and incorporating illumination loss as one
of the training constraints. Extensive experiments demonstrate that the IMF
shows a superior performance compared to state-of-the-art methods. The
comparison results with single modality perception methods highlight the
comprehensive advantages of multi-modal fusion in accurately perceiving road
terrains under varying lighting conditions. Our dataset is available at:
https://github.com/lindawang2016/IMF.

</details>


### [149] [Analysis of Customer Journeys Using Prototype Detection and Counterfactual Explanations for Sequential Data](https://arxiv.org/abs/2505.11086)
*Keita Kinjo*

**主要类别:** cs.AI

**概要:** This study proposes a novel approach with three steps for analyzing customer journeys on omni-channel platforms, including identifying representative sequences, predicting purchase likelihood, and recommending counterfactual sequences to enhance purchase probability.


<details>
  <summary>Details</summary>
  
**动机:** To address the lack of quantitative studies and comprehensive analyses of customer journeys due to their sequential nature and complexity.

**方法:** A three-step approach: defining distance for identifying and visualizing sequences, predicting purchase likelihood, and recommending counterfactual sequences to improve purchase probability.

**结果:** The extraction of typical sequences and detection of important parts for purchase from the analyzed data.

**结论:** The proposed approach can support improvements in various marketing activities.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Analysis+of+Customer+Journeys+Using+Prototype+Detection+and+Counterfactual+Explanations+for+Sequential+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11086，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11086&send_immediately=true&force_search=false)

**原文摘要:** Recently, the proliferation of omni-channel platforms has attracted interest
in customer journeys, particularly regarding their role in developing marketing
strategies. However, few efforts have been taken to quantitatively study or
comprehensively analyze them owing to the sequential nature of their data and
the complexity involved in analysis. In this study, we propose a novel approach
comprising three steps for analyzing customer journeys. First, the distance
between sequential data is defined and used to identify and visualize
representative sequences. Second, the likelihood of purchase is predicted based
on this distance. Third, if a sequence suggests no purchase, counterfactual
sequences are recommended to increase the probability of a purchase using a
proposed method, which extracts counterfactual explanations for sequential
data. A survey was conducted, and the data were analyzed; the results revealed
that typical sequences could be extracted, and the parts of those sequences
important for purchase could be detected. We believe that the proposed approach
can support improvements in various marketing activities.

</details>


### [150] [Group Think: Multiple Concurrent Reasoning Agents Collaborating at Token Level Granularity](https://arxiv.org/abs/2505.11107)
*Chan-Jan Hsu, Davide Buffelli, Jamie McGowan, Feng-Ting Liao, Yi-Chang Chen, Sattar Vakili, Da-shan Shiu*

**主要类别:** cs.AI

**概要:** 提出了一种新的并发推理范式Group Think，它通过让单个LLM模拟多个并发推理代理，在降低冗余推理的同时提高了质量并显著降低了延迟。此外，其并发特性允许有效利用空闲计算资源，特别适用于边缘推理场景。同时，提供了一个简单且通用的修改方法，使任何现有的LLM都能在本地GPU上执行Group Think，并展示出推理延迟的改进。


<details>
  <summary>Details</summary>
  
**动机:** 现有的多推理代理通常以轮流交互的方式工作，这增加了延迟但提升了质量。本文旨在解决这一问题，提出一种新的并发推理方式，既能保持高质量又能减少延迟。

**方法:** 设计了一个名为Group Think的系统，该系统让一个LLM模拟多个并发推理代理，这些代理之间共享可见性，从而能够在token级别动态适应彼此的生成进度。此外，还提出了一个简单的修改方法，使现有LLM能够支持Group Think。

**结果:** 实验表明，使用开源LLM（未针对Group Think专门训练）时，可以实现推理延迟的显著改善。

**结论:** 这项工作为未来的LLM展示了更复杂和更高效的协作行为的可能性，从而实现更高品质的生成。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Group+Think%3A+Multiple+Concurrent+Reasoning+Agents+Collaborating+at+Token+Level+Granularity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11107，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11107&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in large language models (LLMs) have demonstrated the power
of reasoning through self-generated chains of thought. Multiple reasoning
agents can collaborate to raise joint reasoning quality above individual
outcomes. However, such agents typically interact in a turn-based manner,
trading increased latency for improved quality. In this paper, we propose Group
Think--a single LLM that acts as multiple concurrent reasoning agents, or
thinkers. With shared visibility into each other's partial generation progress,
Group Think introduces a new concurrent-reasoning paradigm in which multiple
reasoning trajectories adapt dynamically to one another at the token level. For
example, a reasoning thread may shift its generation mid-sentence upon
detecting that another thread is better positioned to continue. This
fine-grained, token-level collaboration enables Group Think to reduce redundant
reasoning and improve quality while achieving significantly lower latency.
Moreover, its concurrent nature allows for efficient utilization of idle
computational resources, making it especially suitable for edge inference,
where very small batch size often underutilizes local~GPUs. We give a simple
and generalizable modification that enables any existing LLM to perform Group
Think on a local GPU. We also present an evaluation strategy to benchmark
reasoning latency and empirically demonstrate latency improvements using
open-source LLMs that were not explicitly trained for Group Think. We hope this
work paves the way for future LLMs to exhibit more sophisticated and more
efficient collaborative behavior for higher quality generation.

</details>


### [151] [Predicting Student Dropout Risk With A Dual-Modal Abrupt Behavioral Changes Approach](https://arxiv.org/abs/2505.11119)
*Jiabei Cheng, Zhen-Qun Yang, Jiannong Cao, Yu Yang, Xinzhe Zheng*

**主要类别:** cs.AI

**概要:** This paper proposes a Dual-Modal Multiscale Sliding Window (DMSW) Model that uses both academic performance and behavioral data to predict student dropout risks more accurately, helping educators offer timely support.


<details>
  <summary>Details</summary>
  
**动机:** Timely prediction of students at high risk of dropout is critical for early intervention and improving educational outcomes.

**方法:** We propose the Dual-Modal Multiscale Sliding Window (DMSW) Model, which integrates academic performance and behavioral data to dynamically capture behavior patterns using minimal data.

**结果:** The DMSW model improves prediction accuracy by 15% compared to traditional methods, enabling educators to identify high-risk students earlier, provide timely support, and foster a more inclusive learning environment.

**结论:** Our analysis highlights key behavior patterns, offering practical insights for preventive strategies and tailored support. These findings bridge the gap between theory and practice in dropout prediction, giving educators an innovative tool to enhance student retention and outcomes.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Predicting+Student+Dropout+Risk+With+A+Dual-Modal+Abrupt+Behavioral+Changes+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11119，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11119&send_immediately=true&force_search=false)

**原文摘要:** Timely prediction of students at high risk of dropout is critical for early
intervention and improving educational outcomes. However, in offline
educational settings, poor data quality, limited scale, and high heterogeneity
often hinder the application of advanced machine learning models. Furthermore,
while educational theories provide valuable insights into dropout phenomena,
the lack of quantifiable metrics for key indicators limits their use in
data-driven modeling. Through data analysis and a review of educational
literature, we identified abrupt changes in student behavior as key early
signals of dropout risk. To address this, we propose the Dual-Modal Multiscale
Sliding Window (DMSW) Model, which integrates academic performance and
behavioral data to dynamically capture behavior patterns using minimal data.
The DMSW model improves prediction accuracy by 15% compared to traditional
methods, enabling educators to identify high-risk students earlier, provide
timely support, and foster a more inclusive learning environment. Our analysis
highlights key behavior patterns, offering practical insights for preventive
strategies and tailored support. These findings bridge the gap between theory
and practice in dropout prediction, giving educators an innovative tool to
enhance student retention and outcomes.

</details>


### [152] [Navigating the Alpha Jungle: An LLM-Powered MCTS Framework for Formulaic Factor Mining](https://arxiv.org/abs/2505.11122)
*Yu Shi, Yitong Duan, Jian Li*

**主要类别:** cs.AI

**概要:** This paper presents a new method using LLMs and MCTS for mining alpha factors in quantitative investment, showing better performance than existing approaches.


<details>
  <summary>Details</summary>
  
**动机:** To address the inefficiency and poor interpretability issues of traditional and some automated alpha mining methods.

**方法:** Integrates LLMs with MCTS, using financial backtesting feedback to guide exploration and includes a subtree avoidance mechanism.

**结果:** Outperforms existing methods in terms of predictive accuracy, trading performance, and interpretability while being more efficient.

**结论:** The proposed framework offers a significant advancement in alpha factor mining for quantitative investment.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Navigating+the+Alpha+Jungle%3A+An+LLM-Powered+MCTS+Framework+for+Formulaic+Factor+Mining，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11122，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11122&send_immediately=true&force_search=false)

**原文摘要:** Alpha factor mining is pivotal in quantitative investment for identifying
predictive signals from complex financial data. While traditional formulaic
alpha mining relies on human expertise, contemporary automated methods, such as
those based on genetic programming or reinforcement learning, often suffer from
search inefficiency or yield poorly interpretable alpha factors. This paper
introduces a novel framework that integrates Large Language Models (LLMs) with
Monte Carlo Tree Search (MCTS) to overcome these limitations. Our approach
leverages the LLM's instruction-following and reasoning capability to
iteratively generate and refine symbolic alpha formulas within an MCTS-driven
exploration. A key innovation is the guidance of MCTS exploration by rich,
quantitative feedback from financial backtesting of each candidate factor,
enabling efficient navigation of the vast search space. Furthermore, a frequent
subtree avoidance mechanism is introduced to bolster search efficiency and
alpha factor performance. Experimental results on real-world stock market data
demonstrate that our LLM-based framework outperforms existing methods by mining
alphas with superior predictive accuracy, trading performance, and improved
interpretability, while offering a more efficient solution for formulaic alpha
mining.

</details>


### [153] [Scalability of Reinforcement Learning Methods for Dispatching in Semiconductor Frontend Fabs: A Comparison of Open-Source Models with Real Industry Datasets](https://arxiv.org/abs/2505.11135)
*Patrick Stöckermann, Henning Südfeld, Alessandro Immordino, Thomas Altenmüller, Marc Wegmann, Martin Gebser, Konstantin Schekotihin, Georg Seidel, Chew Wye Chan, Fei Fei Zhang*

**主要类别:** cs.AI

**概要:** This paper evaluates reinforcement learning methods, specifically policy-gradient and evolution strategies, on semiconductor manufacturing simulation models and real industry data. The evolution strategies-based method shows better scalability and performance improvements.


<details>
  <summary>Details</summary>
  
**动机:** Common benchmark datasets lack complexity and constraints present in real-world scenarios.

**方法:** Comparison of open-source simulation models with a real industry dataset using reinforcement learning methods including policy-gradient and evolution strategies.

**结果:** Evolution strategies-based method performs better in terms of scalability and achieves improvements in tardiness and throughput.

**结论:** The research provides insights into the effectiveness of optimization methods and their applicability to realistic semiconductor frontend fab simulations.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scalability+of+Reinforcement+Learning+Methods+for+Dispatching+in+Semiconductor+Frontend+Fabs%3A+A+Comparison+of+Open-Source+Models+with+Real+Industry+Datasets，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11135，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11135&send_immediately=true&force_search=false)

**原文摘要:** Benchmark datasets are crucial for evaluating approaches to scheduling or
dispatching in the semiconductor industry during the development and deployment
phases. However, commonly used benchmark datasets like the Minifab or SMT2020
lack the complex details and constraints found in real-world scenarios. To
mitigate this shortcoming, we compare open-source simulation models with a real
industry dataset to evaluate how optimization methods scale with different
levels of complexity. Specifically, we focus on Reinforcement Learning methods,
performing optimization based on policy-gradient and Evolution Strategies. Our
research provides insights into the effectiveness of these optimization methods
and their applicability to realistic semiconductor frontend fab simulations. We
show that our proposed Evolution Strategies-based method scales much better
than a comparable policy-gradient-based approach. Moreover, we identify the
selection and combination of relevant bottleneck tools to control by the agent
as crucial for an efficient optimization. For the generalization across
different loading scenarios and stochastic tool failure patterns, we achieve
advantages when utilizing a diverse training dataset. While the overall
approach is computationally expensive, it manages to scale well with the number
of CPU cores used for training. For the real industry dataset, we achieve an
improvement of up to 4% regarding tardiness and up to 1% regarding throughput.
For the less complex open-source models Minifab and SMT2020, we observe
double-digit percentage improvement in tardiness and single digit percentage
improvement in throughput by use of Evolution Strategies.

</details>


### [154] [Reinforcement Learning for AMR Charging Decisions: The Impact of Reward and Action Space Design](https://arxiv.org/abs/2505.11136)
*Janik Bischoff, Alexandru Rinciog, Anne Meyer*

**主要类别:** cs.AI

**概要:** This paper proposes a new reinforcement learning method to optimize charging strategies for robots in large warehouses. It compares flexible and guided approaches, showing trade-offs between performance and stability.


<details>
  <summary>Details</summary>
  
**动机:** Optimizing charging strategies for autonomous mobile robots in large-scale warehouses.

**方法:** Novel reinforcement learning design with different reward and action space configurations.

**结果:** Flexible RL-based approaches outperform heuristic strategies in service times but have longer convergence times and less stability. Guided configurations offer more stability but limited generalization.

**结论:** The study extends an existing simulation framework, introduces a new RL design, and evaluates it with adaptive baseline heuristics using Proximal Policy Optimization.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reinforcement+Learning+for+AMR+Charging+Decisions%3A+The+Impact+of+Reward+and+Action+Space+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11136，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11136&send_immediately=true&force_search=false)

**原文摘要:** We propose a novel reinforcement learning (RL) design to optimize the
charging strategy for autonomous mobile robots in large-scale block stacking
warehouses. RL design involves a wide array of choices that can mostly only be
evaluated through lengthy experimentation. Our study focuses on how different
reward and action space configurations, ranging from flexible setups to more
guided, domain-informed design configurations, affect the agent performance.
Using heuristic charging strategies as a baseline, we demonstrate the
superiority of flexible, RL-based approaches in terms of service times.
Furthermore, our findings highlight a trade-off: While more open-ended designs
are able to discover well-performing strategies on their own, they may require
longer convergence times and are less stable, whereas guided configurations
lead to a more stable learning process but display a more limited
generalization potential. Our contributions are threefold. First, we extend
SLAPStack, an open-source, RL-compatible simulation-framework to accommodate
charging strategies. Second, we introduce a novel RL design for tackling the
charging strategy problem. Finally, we introduce several novel adaptive
baseline heuristics and reproducibly evaluate the design using a Proximal
Policy Optimization agent and varying different design configurations, with a
focus on reward.

</details>


### [155] [Feasibility with Language Models for Open-World Compositional Zero-Shot Learning](https://arxiv.org/abs/2505.11181)
*Jae Myung Kim, Stephan Alaniz, Cordelia Schmid, Zeynep Akata*

**主要类别:** cs.AI

**概要:** 提出了一种利用大型语言模型（LLMs）来判断状态-物体组合可行性的方法FLM，该方法通过查询LLM并获取正向答案的输出logits来实现。为了减少罕见或完全不可行的状态-物体组合可能对LLM造成的误导，研究发现LLM的上下文学习能力至关重要。实验表明，FLM在所有三个基准数据集上都提高了开放世界组合零样本学习(OW-CZSL)的性能。


<details>
  <summary>Details</summary>
  
**动机:** 当考虑所有可能的状态-物体组合作为未见类别时，零样本预测器在开放世界组合零样本学习中的表现往往不佳。因此，研究重点是如何使用外部辅助知识来确定这些组合的可行性。

**方法:** 提出的方法FLM利用大型语言模型（LLMs）来更好地理解状态和物体之间的语义关系。具体来说，FLM涉及查询LLM关于给定配对的可行性，并检索正向答案的输出logits。此外，研究还强调了LLM的上下文学习能力的重要性。

**结果:** 研究表明Vicuna和ChatGPT是表现最好的模型。并且我们的FLM方法在所有三个基准数据集上都提高了OW-CZSL的性能。

**结论:** 本研究提出的方法FLM展示了如何利用大型语言模型来改善开放世界组合零样本学习的性能，特别是在处理罕见或完全不可行的状态-物体组合方面。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Feasibility+with+Language+Models+for+Open-World+Compositional+Zero-Shot+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11181，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11181&send_immediately=true&force_search=false)

**原文摘要:** Humans can easily tell if an attribute (also called state) is realistic,
i.e., feasible, for an object, e.g. fire can be hot, but it cannot be wet. In
Open-World Compositional Zero-Shot Learning, when all possible state-object
combinations are considered as unseen classes, zero-shot predictors tend to
perform poorly. Our work focuses on using external auxiliary knowledge to
determine the feasibility of state-object combinations. Our Feasibility with
Language Model (FLM) is a simple and effective approach that leverages Large
Language Models (LLMs) to better comprehend the semantic relationships between
states and objects. FLM involves querying an LLM about the feasibility of a
given pair and retrieving the output logit for the positive answer. To mitigate
potential misguidance of the LLM given that many of the state-object
compositions are rare or completely infeasible, we observe that the in-context
learning ability of LLMs is essential. We present an extensive study
identifying Vicuna and ChatGPT as best performing, and we demonstrate that our
FLM consistently improves OW-CZSL performance across all three benchmarks.

</details>


### [156] [Can Global XAI Methods Reveal Injected Bias in LLMs? SHAP vs Rule Extraction vs RuleSHAP](https://arxiv.org/abs/2505.11189)
*Francesco Sovrano*

**主要类别:** cs.AI

**概要:** 本研究探讨了全局XAI方法在大型语言模型中的应用，提出了一个新的算法RuleSHAP，改进了偏差检测。


<details>
  <summary>Details</summary>
  
**动机:** 生成式人工智能系统可能有助于传播信息，但也可能传播错误信息和偏见，这可能会削弱联合国可持续发展目标。解释性人工智能（XAI）旨在揭示人工智能系统的内部工作原理并暴露不当行为或偏差。然而，当前为简单模型构建的XAI工具难以处理大型语言模型的非数值性质。

**方法:** 研究了全局XAI方法在检测大型语言模型中的偏差方面的有效性，包括规则提取算法和SHAP。还引入了一种文本到序数映射策略来转换非数值输入/输出，并通过系统指令向ChatGPT和Llama等广泛使用的大型语言模型注入非线性偏差。

**结果:** 发现RuleFit在处理联合和非凸偏差方面存在困难，而SHAP可以近似联合偏差但无法将其表达为可操作的规则。引入的RuleSHAP算法在检测注入偏差方面表现出色。

**结论:** 提出了一种新的全局规则提取算法RuleSHAP，该算法结合了SHAP和RuleFit，用于检测更多的非单变量偏差，并在注入偏差检测上比RuleFit平均提高了+94%（MRR@1）。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Can+Global+XAI+Methods+Reveal+Injected+Bias+in+LLMs%3F+SHAP+vs+Rule+Extraction+vs+RuleSHAP，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11189，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11189&send_immediately=true&force_search=false)

**原文摘要:** Generative AI systems can help spread information but also misinformation and
biases, potentially undermining the UN Sustainable Development Goals (SDGs).
Explainable AI (XAI) aims to reveal the inner workings of AI systems and expose
misbehaviours or biases. However, current XAI tools, built for simpler models,
struggle to handle the non-numerical nature of large language models (LLMs).
This paper examines the effectiveness of global XAI methods, such as
rule-extraction algorithms and SHAP, in detecting bias in LLMs. To do so, we
first show a text-to-ordinal mapping strategy to convert non-numerical
inputs/outputs into numerical features, enabling these tools to identify (some)
misinformation-related biases in LLM-generated content. Then, we inject
non-linear biases of varying complexity (univariate, conjunctive, and
non-convex) into widespread LLMs like ChatGPT and Llama via system
instructions, using global XAI methods to detect them. This way, we found that
RuleFit struggles with conjunctive and non-convex biases, while SHAP can
approximate conjunctive biases but cannot express them as actionable rules.
Hence, we introduce RuleSHAP, a global rule extraction algorithm combining SHAP
and RuleFit to detect more non-univariate biases, improving injected bias
detection over RuleFit by +94% (MRR@1) on average.

</details>


### [157] [Multi-Modal Multi-Task (M3T) Federated Foundation Models for Embodied AI: Potentials and Challenges for Edge Integration](https://arxiv.org/abs/2505.11191)
*Kasra Borazjani, Payam Abdisarabshali, Fardis Nadimi, Naji Khosravan, Minghui Liwang, Xianbin Wang, Yiguang Hong, Seyyedali Hosseinalipour*

**主要类别:** cs.AI

**概要:** This vision paper introduces Federated Foundation Models (FFMs) for embodied AI, a new paradigm that combines the strengths of Foundation Models (FMs) and Federated Learning (FL). The authors outline key deployment dimensions under the 'EMBODY' framework and propose an evaluation approach.


<details>
  <summary>Details</summary>
  
**动机:** The increasing complexity and diversity of embodied AI systems require models that can adapt quickly and effectively while maintaining privacy and personalization.

**方法:** Introduces FFMs, combining M3T FMs with FL's distributed, privacy-preserving features. Proposes the EMBODY framework to address embodiment heterogeneity, modality issues, bandwidth/compute limits, on-device learning, distributed control, and safety/privacy/personalization concerns.

**结果:** Identifies challenges and research directions for deploying FFMs in embodied AI ecosystems. Presents an evaluation framework with trade-offs.

**结论:** This paper outlines a novel approach (FFMs) and framework ('EMBODY') to meet the complex demands of embodied AI systems, emphasizing adaptability, personalization, and privacy.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Modal+Multi-Task+%28M3T%29+Federated+Foundation+Models+for+Embodied+AI%3A+Potentials+and+Challenges+for+Edge+Integration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11191，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11191&send_immediately=true&force_search=false)

**原文摘要:** As embodied AI systems become increasingly multi-modal, personalized, and
interactive, they must learn effectively from diverse sensory inputs, adapt
continually to user preferences, and operate safely under resource and privacy
constraints. These challenges expose a pressing need for machine learning
models capable of swift, context-aware adaptation while balancing model
generalization and personalization. Here, two methods emerge as suitable
candidates, each offering parts of these capabilities: Foundation Models (FMs)
provide a pathway toward generalization across tasks and modalities, whereas
Federated Learning (FL) offers the infrastructure for distributed,
privacy-preserving model updates and user-level model personalization. However,
when used in isolation, each of these approaches falls short of meeting the
complex and diverse capability requirements of real-world embodied
environments. In this vision paper, we introduce Federated Foundation Models
(FFMs) for embodied AI, a new paradigm that unifies the strengths of
multi-modal multi-task (M3T) FMs with the privacy-preserving distributed nature
of FL, enabling intelligent systems at the wireless edge. We collect critical
deployment dimensions of FFMs in embodied AI ecosystems under a unified
framework, which we name "EMBODY": Embodiment heterogeneity, Modality richness
and imbalance, Bandwidth and compute constraints, On-device continual learning,
Distributed control and autonomy, and Yielding safety, privacy, and
personalization. For each, we identify concrete challenges and envision
actionable research directions. We also present an evaluation framework for
deploying FFMs in embodied AI systems, along with the associated trade-offs.

</details>


### [158] [GLOVA: Global and Local Variation-Aware Analog Circuit Design with Risk-Sensitive Reinforcement Learning](https://arxiv.org/abs/2505.11208)
*Dongjun Kim, Junwoo Park, Chaehyeon Shin, Jaeheon Jung, Kyungho Shin, Seungheon Baek, Sanghyuk Heo, Woongrae Kim, Inchul Jeong, Joohwan Cho, Jongsun Park*

**主要类别:** cs.AI

**概要:** This paper presents GLOVA, a framework for analog circuit sizing that improves robustness against PVT variations using risk-sensitive reinforcement learning and ensemble-based critic.


<details>
  <summary>Details</summary>
  
**动机:** To address performance degradation in analog/mixed-signal circuits due to PVT variations and reduce time-to-market.

**方法:** Risk-sensitive reinforcement learning and ensemble-based critic for managing PVT variations, with μ-σ evaluation and simulation reordering for design verification.

**结果:** GLOVA shows up to 80.5× improvement in sample efficiency and 76.0× reduction in time compared to previous frameworks.

**结论:** GLOVA effectively manages diverse random mismatches and provides efficient design verification for analog circuits.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GLOVA%3A+Global+and+Local+Variation-Aware+Analog+Circuit+Design+with+Risk-Sensitive+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11208，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11208&send_immediately=true&force_search=false)

**原文摘要:** Analog/mixed-signal circuit design encounters significant challenges due to
performance degradation from process, voltage, and temperature (PVT)
variations. To achieve commercial-grade reliability, iterative manual design
revisions and extensive statistical simulations are required. While several
studies have aimed to automate variation aware analog design to reduce
time-to-market, the substantial mismatches in real-world wafers have not been
thoroughly addressed. In this paper, we present GLOVA, an analog circuit sizing
framework that effectively manages the impact of diverse random mismatches to
improve robustness against PVT variations. In the proposed approach,
risk-sensitive reinforcement learning is leveraged to account for the
reliability bound affected by PVT variations, and ensemble-based critic is
introduced to achieve sample-efficient learning. For design verification, we
also propose $\mu$-$\sigma$ evaluation and simulation reordering method to
reduce simulation costs of identifying failed designs. GLOVA supports
verification through industrial-level PVT variation evaluation methods,
including corner simulation as well as global and local Monte Carlo (MC)
simulations. Compared to previous state-of-the-art variation-aware analog
sizing frameworks, GLOVA achieves up to 80.5$\times$ improvement in sample
efficiency and 76.0$\times$ reduction in time.

</details>


### [159] [Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs](https://arxiv.org/abs/2505.11227)
*Zhangying Feng, Qianglong Chen, Ning Lu, Yongqian Li, Siqi Cheng, Shuangmu Peng, Duyu Tang, Shengcai Liu, Zhirui Zhang*

**主要类别:** cs.AI

**概要:** This study explores the relationship between reinforcement learning (RL) training and process reward models (PRMs) in enhancing reasoning abilities in large language models (LLMs). It challenges the necessity of PRM integration by demonstrating that pure RL training can progressively enhance reasoning without it.


<details>
  <summary>Details</summary>
  
**动机:** To investigate whether process supervision via PRMs is necessary for improving reasoning capabilities in LLMs.

**方法:** Systematic investigation comparing pure RL training with PRM integration, proposing Self-PRM as an introspective framework.

**结果:** Pure RL training can enhance reasoning abilities and foster PRM capabilities without PRM integration. However, current PRMs perform poorly compared to simple baselines when applied to advanced models.

**结论:** PRM may not be essential for enhancing complex reasoning as pure RL alone can improve both problem-solving skills and PRM capabilities.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Is+PRM+Necessary%3F+Problem-Solving+RL+Implicitly+Induces+PRM+Capability+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11227，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11227&send_immediately=true&force_search=false)

**原文摘要:** The development of reasoning capabilities represents a critical frontier in
large language models (LLMs) research, where reinforcement learning (RL) and
process reward models (PRMs) have emerged as predominant methodological
frameworks. Contrary to conventional wisdom, empirical evidence from
DeepSeek-R1 demonstrates that pure RL training focused on mathematical
problem-solving can progressively enhance reasoning abilities without PRM
integration, challenging the perceived necessity of process supervision. In
this study, we conduct a systematic investigation of the relationship between
RL training and PRM capabilities. Our findings demonstrate that problem-solving
proficiency and process supervision capabilities represent complementary
dimensions of reasoning that co-evolve synergistically during pure RL training.
In particular, current PRMs underperform simple baselines like majority voting
when applied to state-of-the-art models such as DeepSeek-R1 and QwQ-32B. To
address this limitation, we propose Self-PRM, an introspective framework in
which models autonomously evaluate and rerank their generated solutions through
self-reward mechanisms. Although Self-PRM consistently improves the accuracy of
the benchmark (particularly with larger sample sizes), analysis exposes
persistent challenges: The approach exhibits low precision (<10\%) on difficult
problems, frequently misclassifying flawed solutions as valid. These analyses
underscore the need for continued RL scaling to improve reward alignment and
introspective accuracy. Overall, our findings suggest that PRM may not be
essential for enhancing complex reasoning, as pure RL not only improves
problem-solving skills but also inherently fosters robust PRM capabilities. We
hope these findings provide actionable insights for building more reliable and
self-aware complex reasoning models.

</details>


### [160] [LD-Scene: LLM-Guided Diffusion for Controllable Generation of Adversarial Safety-Critical Driving Scenarios](https://arxiv.org/abs/2505.11247)
*Mingxing Peng, Yuting Xie, Xusen Guo, Ruoyu Yao, Hai Yang, Jun Ma*

**主要类别:** cs.AI

**概要:** 提出了一种结合大语言模型和潜在扩散模型的新框架LD-Scene，用于通过自然语言生成可控的对抗场景，实验表明其在生成真实、多样且有效的对抗场景方面达到最先进的性能。


<details>
  <summary>Details</summary>
  
**动机:** 确保自动驾驶系统安全性和鲁棒性需要在关键安全场景下进行全面评估，但这些场景稀少且难以从现实驾驶数据中收集，现有方法存在可控性差和用户友好度低的问题。

**方法:** 提出一个集成大语言模型和潜在扩散模型的框架LD-Scene，包含捕捉实际驾驶轨迹分布的潜在扩散模型和基于大语言模型的指导模块，该模块包括基于大语言模型的思维链代码生成器和代码调试器。

**结果:** 在nuScenes数据集上的实验证明了LD-Scene在生成真实、多样化和有效的对抗场景方面具有最先进的性能，并提供了对对抗行为的细粒度控制。

**结论:** 所提出的框架解决了现有方法的局限性，实现了更有效和定制化的测试。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LD-Scene%3A+LLM-Guided+Diffusion+for+Controllable+Generation+of+Adversarial+Safety-Critical+Driving+Scenarios，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11247，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11247&send_immediately=true&force_search=false)

**原文摘要:** Ensuring the safety and robustness of autonomous driving systems necessitates
a comprehensive evaluation in safety-critical scenarios. However, these
safety-critical scenarios are rare and difficult to collect from real-world
driving data, posing significant challenges to effectively assessing the
performance of autonomous vehicles. Typical existing methods often suffer from
limited controllability and lack user-friendliness, as extensive expert
knowledge is essentially required. To address these challenges, we propose
LD-Scene, a novel framework that integrates Large Language Models (LLMs) with
Latent Diffusion Models (LDMs) for user-controllable adversarial scenario
generation through natural language. Our approach comprises an LDM that
captures realistic driving trajectory distributions and an LLM-based guidance
module that translates user queries into adversarial loss functions,
facilitating the generation of scenarios aligned with user queries. The
guidance module integrates an LLM-based Chain-of-Thought (CoT) code generator
and an LLM-based code debugger, enhancing the controllability and robustness in
generating guidance functions. Extensive experiments conducted on the nuScenes
dataset demonstrate that LD-Scene achieves state-of-the-art performance in
generating realistic, diverse, and effective adversarial scenarios.
Furthermore, our framework provides fine-grained control over adversarial
behaviors, thereby facilitating more effective testing tailored to specific
driving scenarios.

</details>


### [161] [SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning](https://arxiv.org/abs/2505.11274)
*Zheng Li, Qingxiu Dong, Jingyuan Ma, Di Zhang, Zhifang Sui*

**主要类别:** cs.AI

**概要:** 提出了一种名为SelfBudgeter的自适应可控推理策略，通过双阶段训练来预估推理成本并引入预算引导的GPRO进行强化学习，能够在保持准确性的同时减少输出长度。实验表明，SelfBudgeter在MATH基准上实现了高达74.47%的响应长度压缩且几乎不降低准确性。


<details>
  <summary>Details</summary>
  
**动机:** 现有的推理模型对简单和复杂查询都进行了不必要的过度处理，导致资源浪费和用户延迟增加。

**方法:** 采用双阶段训练方法，首先预估查询难度下的推理成本，然后通过预算引导的GPRO进行强化学习来维护准确性并减少输出长度。此外，还支持通过预填充标记预算直接控制推理长度。

**结果:** SelfBudgeter在MATH基准测试中实现了显著的响应长度压缩（高达74.47%），同时保持了几乎不变的准确性。

**结论:** SelfBudgeter提供了一种有效的解决方案，可以合理分配预算以适应问题复杂性，从而提高推理效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SelfBudgeter%3A+Adaptive+Token+Allocation+for+Efficient+LLM+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11274，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11274&send_immediately=true&force_search=false)

**原文摘要:** Recently, large reasoning models demonstrate exceptional performance on
various tasks. However, reasoning models inefficiently over-process both
trivial and complex queries, leading to resource waste and prolonged user
latency. To address this challenge, we propose SelfBudgeter - a self-adaptive
controllable reasoning strategy for efficient reasoning. Our approach adopts a
dual-phase training paradigm: first, the model learns to pre-estimate the
reasoning cost based on the difficulty of the query. Then, we introduce
budget-guided GPRO for reinforcement learning, which effectively maintains
accuracy while reducing output length. SelfBudgeter allows users to anticipate
generation time and make informed decisions about continuing or interrupting
the process. Furthermore, our method enables direct manipulation of reasoning
length via pre-filling token budget. Experimental results demonstrate that
SelfBudgeter can rationally allocate budgets according to problem complexity,
achieving up to 74.47% response length compression on the MATH benchmark while
maintaining nearly undiminished accuracy.

</details>


### [162] [Meta-World+: An Improved, Standardized, RL Benchmark](https://arxiv.org/abs/2505.11289)
*Reginald McLean, Evangelos Chatzaroulas, Luc McCutcheon, Frank Röder, Tianhe Yu, Zhanpeng He, K. R. Zentner, Ryan Julian, J K Terry, Isaac Woungang, Nariman Farsad, Pablo Samuel Castro*

**主要类别:** cs.AI

**概要:** This paper addresses inconsistencies in Meta-World, a platform for evaluating multi-task and meta-reinforcement learning agents, by releasing a new open-source version with improved reproducibility and user control.


<details>
  <summary>Details</summary>
  
**动机:** To resolve discrepancies in algorithm comparisons due to undocumented changes in Meta-World and to gain insights into benchmark design.

**方法:** Analyzing past versions of Meta-World and addressing issues related to reproducibility and technical ergonomics.

**结果:** A new open-source Meta-World version with enhanced features for reproducibility and user control over task sets.

**结论:** The new Meta-World version facilitates fairer comparisons and deeper understanding of multi-task and meta-reinforcement learning benchmarks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Meta-World%2B%3A+An+Improved%2C+Standardized%2C+RL+Benchmark，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11289，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11289&send_immediately=true&force_search=false)

**原文摘要:** Meta-World is widely used for evaluating multi-task and meta-reinforcement
learning agents, which are challenged to master diverse skills simultaneously.
Since its introduction however, there have been numerous undocumented changes
which inhibit a fair comparison of algorithms. This work strives to
disambiguate these results from the literature, while also leveraging the past
versions of Meta-World to provide insights into multi-task and
meta-reinforcement learning benchmark design. Through this process we release a
new open-source version of Meta-World
(https://github.com/Farama-Foundation/Metaworld/) that has full reproducibility
of past results, is more technically ergonomic, and gives users more control
over the tasks that are included in a task set.

</details>


### [163] [Extracting Explainable Dates From Medical Images By Reverse-Engineering UNIX Timestamps](https://arxiv.org/abs/2505.11451)
*Lee Harris, James Bentham, Philippe De Wilde*

**主要类别:** cs.AI

**概要:** The study investigates different methods to extract date information from medical documents, showing that regular expressions synthesized automatically can identify complex dates and date ranges.


<details>
  <summary>Details</summary>
  
**动机:** To find a reliable method to extract date information from medical documents for making important medical decisions.

**方法:** Testing publicly-available regular expressions, creating manually decomposable regular expressions, and using regular expression synthesis to identify regular expressions from reverse-engineered UNIX timestamps.

**结果:** Publicly-available regular expressions failed to capture many dates. Manually-created regular expressions detected most real dates but also false positives. Synthesized regular expressions detected fewer false positives but missed more actual dates.

**结论:** Regular expression synthesis can create rules to identify complex dates and date ranges in transcriptions.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Extracting+Explainable+Dates+From+Medical+Images+By+Reverse-Engineering+UNIX+Timestamps，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11451，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11451&send_immediately=true&force_search=false)

**原文摘要:** Dates often contribute towards highly impactful medical decisions, but it is
rarely clear how to extract this data. AI has only just begun to be used
transcribe such documents, and common methods are either to trust that the
output produced by a complex AI model, or to parse the text using regular
expressions. Recent work has established that regular expressions are an
explainable form of logic, but it is difficult to decompose these into the
component parts that are required to construct precise UNIX timestamps. First,
we test publicly-available regular expressions, and we found that these were
unable to capture a significant number of our dates. Next, we manually created
easily-decomposable regular expressions, and we found that these were able to
detect the majority of real dates, but also a lot of sequences of text that
look like dates. Finally, we used regular expression synthesis to automatically
identify regular expressions from the reverse-engineered UNIX timestamps that
we created. We find that regular expressions created by regular expression
synthesis detect far fewer sequences of text that look like dates than those
that were manually created, at the cost of a slight increase to the number of
missed dates. Overall, our results show that regular expressions can be created
through regular expression synthesis to identify complex dates and date ranges
in text transcriptions. To our knowledge, our proposed way of learning
deterministic logic by reverse-engineering several many-one mappings and
feeding these into a regular expression synthesiser is a new approach.

</details>


### [164] [Automatic Reward Shaping from Confounded Offline Data](https://arxiv.org/abs/2505.11478)
*Mingxuan Li, Junzhe Zhang, Elias Bareinboim*

**主要类别:** cs.AI

**概要:** 研究了从有偏数据中进行异策略学习的方法，并提出了一种新的深度强化学习算法，该算法对观察数据中的混杂偏差具有鲁棒性。在12个混杂的Atari游戏中应用了该方法，发现它在所有行为和目标策略的观测输入不匹配且存在未观测混杂因子的游戏上始终优于标准DQN。


<details>
  <summary>Details</summary>
  
**动机:** 在复杂和高维领域中，当无法排除未观察到的混杂因素时，研究异策略学习。

**方法:** 基于著名的Deep Q-Network（DQN），提出了一种新的深度强化学习算法，该算法对观察数据中的混杂偏差具有鲁棒性。算法试图找到最坏情况环境下的安全策略，该环境与观察结果一致。

**结果:** 在12个混杂的Atari游戏中应用了所提出的方法，发现它在所有行为和目标策略的观测输入不匹配且存在未观测混杂因子的游戏上始终优于标准DQN。

**结论:** 提出的算法在处理未观察到的混杂因素方面表现出色，适用于复杂的高维环境。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automatic+Reward+Shaping+from+Confounded+Offline+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11478，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11478&send_immediately=true&force_search=false)

**原文摘要:** A key task in Artificial Intelligence is learning effective policies for
controlling agents in unknown environments to optimize performance measures.
Off-policy learning methods, like Q-learning, allow learners to make optimal
decisions based on past experiences. This paper studies off-policy learning
from biased data in complex and high-dimensional domains where \emph{unobserved
confounding} cannot be ruled out a priori. Building on the well-celebrated Deep
Q-Network (DQN), we propose a novel deep reinforcement learning algorithm
robust to confounding biases in observed data. Specifically, our algorithm
attempts to find a safe policy for the worst-case environment compatible with
the observations. We apply our method to twelve confounded Atari games, and
find that it consistently dominates the standard DQN in all games where the
observed input to the behavioral and target policies mismatch and unobserved
confounders exist.

</details>


### [165] [MOSAAIC: Managing Optimization towards Shared Autonomy, Authority, and Initiative in Co-creation](https://arxiv.org/abs/2505.11481)
*Alayt Issak, Jeba Rezwana, Casper Harteveld*

**主要类别:** cs.AI

**概要:** This paper introduces MOSAAIC, a new framework for balancing control in human-AI co-creative systems.


<details>
  <summary>Details</summary>
  
**动机:** To explore how to strike a balance between humans and co-creative AI in computational creativity.

**方法:** Systematic literature review of 172 papers and introducing MOSAAIC framework with three dimensions of control: autonomy, initiative, and authority.

**结果:** MOSAAIC framework with optimization strategies and its application in analyzing six existing co-creative AI case studies.

**结论:** The proposed MOSAAIC framework provides a way to characterize and balance control in co-creative systems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MOSAAIC%3A+Managing+Optimization+towards+Shared+Autonomy%2C+Authority%2C+and+Initiative+in+Co-creation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11481，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11481&send_immediately=true&force_search=false)

**原文摘要:** Striking the appropriate balance between humans and co-creative AI is an open
research question in computational creativity. Co-creativity, a form of hybrid
intelligence where both humans and AI take action proactively, is a process
that leads to shared creative artifacts and ideas. Achieving a balanced dynamic
in co-creativity requires characterizing control and identifying strategies to
distribute control between humans and AI. We define control as the power to
determine, initiate, and direct the process of co-creation. Informed by a
systematic literature review of 172 full-length papers, we introduce MOSAAIC
(Managing Optimization towards Shared Autonomy, Authority, and Initiative in
Co-creation), a novel framework for characterizing and balancing control in
co-creation. MOSAAIC identifies three key dimensions of control: autonomy,
initiative, and authority. We supplement our framework with control
optimization strategies in co-creation. To demonstrate MOSAAIC's applicability,
we analyze the distribution of control in six existing co-creative AI case
studies and present the implications of using this framework.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [166] [An Exponential Averaging Process with Strong Convergence Properties](https://arxiv.org/abs/2505.10605)
*Frederik Köhne, Anton Schiela*

**主要类别:** stat.ML

**概要:** 提出了一种改进的指数移动平均法(p-EMA)，在随机动力系统轨迹观测的平滑问题上提供了更强的收敛保证，并讨论了其对自适应步长控制随机梯度下降的影响。


<details>
  <summary>Details</summary>
  
**动机:** 传统指数移动平均法(EMA)在处理随机动力系统的观测时，由于对最新观测值权重恒定，无法使平均量中的噪声减小到零，缺乏强的随机收敛性质。

**方法:** 提出了一种新的方法p-EMA，其最近观测值的权重以次谐波速率减小。

**结果:** 在随机动力系统自相关性假设下，为这种平滑方法提供了随机收敛保证。

**结论:** 所提出的p-EMA方法在理论上增强了对随机动力系统轨迹观测的平滑能力，并且在实践中对自适应步长控制随机梯度下降有潜在影响。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Exponential+Averaging+Process+with+Strong+Convergence+Properties，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10605，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10605&send_immediately=true&force_search=false)

**原文摘要:** Averaging, or smoothing, is a fundamental approach to obtain stable,
de-noised estimates from noisy observations. In certain scenarios, observations
made along trajectories of random dynamical systems are of particular interest.
One popular smoothing technique for such a scenario is exponential moving
averaging (EMA), which assigns observations a weight that decreases
exponentially in their age, thus giving younger observations a larger weight.
However, EMA fails to enjoy strong stochastic convergence properties, which
stems from the fact that the weight assigned to the youngest observation is
constant over time, preventing the noise in the averaged quantity from
decreasing to zero. In this work, we consider an adaptation to EMA, which we
call $p$-EMA, where the weights assigned to the last observations decrease to
zero at a subharmonic rate. We provide stochastic convergence guarantees for
this kind of averaging under mild assumptions on the autocorrelations of the
underlying random dynamical system. We further discuss the implications of our
results for a recently introduced adaptive step size control for Stochastic
Gradient Descent (SGD), which uses $p$-EMA for averaging noisy observations.

</details>


### [167] [Minimax learning rates for estimating binary classifiers under margin conditions](https://arxiv.org/abs/2505.10628)
*Jonathan García, Philipp Petersen*

**主要类别:** stat.ML

**概要:** 研究了使用二元估计器的分类问题，给出了在满足几何边距条件的数据分布下的学习率上下界。特别是，在无噪声设定下，对于Barron-正则函数和强边距的Hölder连续函数，识别出接近快学习率的结果。


<details>
  <summary>Details</summary>
  
**动机:** 研究满足普遍实践中的几何边距条件但理论上有挑战性的分类问题。

**方法:** 分析具有几何边距条件的决策边界的学习率，并给出上下界。

**结果:** 对于Barron-正则函数和强边距的Hölder连续函数，接近快学习率；对于仅凸的决策边界，在强边距情况下可达到接近根号倒数的学习率。

**结论:** 本文提供了关于分类问题学习率的重要理论贡献，特别是在无噪声条件下。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Minimax+learning+rates+for+estimating+binary+classifiers+under+margin+conditions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10628，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10628&send_immediately=true&force_search=false)

**原文摘要:** We study classification problems using binary estimators where the decision
boundary is described by horizon functions and where the data distribution
satisfies a geometric margin condition. We establish upper and lower bounds for
the minimax learning rate over broad function classes with bounded Kolmogorov
entropy in Lebesgue norms. A key novelty of our work is the derivation of lower
bounds on the worst-case learning rates under a geometric margin condition -- a
setting that is almost universally satisfied in practice but remains
theoretically challenging. Moreover, our results deal with the noiseless
setting, where lower bounds are particularly hard to establish. We apply our
general results to classification problems with decision boundaries belonging
to several function classes: for Barron-regular functions, and for
H\"older-continuous functions with strong margins, we identify optimal rates
close to the fast learning rates of $\mathcal{O}(n^{-1})$ for $n \in
\mathbb{N}$ samples. Also for merely convex decision boundaries, in a strong
margin case optimal rates near $\mathcal{O}(n^{-1/2})$ can be achieved.

</details>


### [168] [Supervised Models Can Generalize Also When Trained on Random Label](https://arxiv.org/abs/2505.11006)
*Oskar Allerbo, Thomas B. Schön*

**主要类别:** stat.ML

**概要:** 提出一种无需输出标签y的监督模型训练方法，并证明其性能与传统方法相当。


<details>
  <summary>Details</summary>
  
**动机:** 探讨是否可以无需使用输出信息y来训练监督模型。

**方法:** 通过将模型表述为平滑器并构建独立于y的平滑矩阵S，例如通过随机标签进行训练。

**结果:** 提出的y-free训练方法在真实和合成数据上表现良好，且无需访问y。

**结论:** 成功展示了无需输出标签y也能训练监督模型，且性能与标准版本相当。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Supervised+Models+Can+Generalize+Also+When+Trained+on+Random+Label，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11006，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11006&send_immediately=true&force_search=false)

**原文摘要:** The success of unsupervised learning raises the question of whether also
supervised models can be trained without using the information in the output
$y$. In this paper, we demonstrate that this is indeed possible. The key step
is to formulate the model as a smoother, i.e. on the form $\hat{f}=Sy$, and to
construct the smoother matrix $S$ independently of $y$, e.g. by training on
random labels. We present a simple model selection criterion based on the
distribution of the out-of-sample predictions and show that, in contrast to
cross-validation, this criterion can be used also without access to $y$. We
demonstrate on real and synthetic data that $y$-free trained versions of linear
and kernel ridge regression, smoothing splines, and neural networks perform
similarly to their standard, $y$-based, versions and, most importantly,
significantly better than random guessing.

</details>


### [169] [Inexact Column Generation for Bayesian Network Structure Learning via Difference-of-Submodular Optimization](https://arxiv.org/abs/2505.11089)
*Yiran Yang, Rui Chen*

**主要类别:** stat.ML

**概要:** 提出了一种新的基于分数的整数规划方法，通过动态生成行和列来解决BNSL问题，特别是在处理高密度和大规模图时表现出色。


<details>
  <summary>Details</summary>
  
**动机:** 现有的最先进的贝叶斯网络结构学习（BNSL）整数规划公式受到变量和约束数量呈指数增长的挑战，而复杂的定价问题成为计算瓶颈。

**方法:** 通过将定价问题重新表述为差分次模优化问题，并应用差异凸算法（DCA）作为不精确方法来高效解决定价问题。

**结果:** 对于连续高斯数据，提出的行和列生成方法得到的解质量高于最先进的基于分数的方法，特别是在图密度增加时，并且在图规模增加时，其表现与基准的约束和混合方法相当。

**结论:** 采用行和列生成方法的基于分数的整数规划方法在处理高密度图时表现出色，并且在处理大规模图时也达到了与基准约束和混合方法相当的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Inexact+Column+Generation+for+Bayesian+Network+Structure+Learning+via+Difference-of-Submodular+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11089，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11089&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we consider a score-based Integer Programming (IP) approach
for solving the Bayesian Network Structure Learning (BNSL) problem.
State-of-the-art BNSL IP formulations suffer from the exponentially large
number of variables and constraints. A standard approach in IP to address such
challenges is to employ row and column generation techniques, which dynamically
generate rows and columns, while the complex pricing problem remains a
computational bottleneck for BNSL. For the general class of $\ell_0$-penalized
likelihood scores, we show how the pricing problem can be reformulated as a
difference of submodular optimization problem, and how the Difference of Convex
Algorithm (DCA) can be applied as an inexact method to efficiently solve the
pricing problems. Empirically, we show that, for continuous Gaussian data, our
row and column generation approach yields solutions with higher quality than
state-of-the-art score-based approaches, especially when the graph density
increases, and achieves comparable performance against benchmark
constraint-based and hybrid approaches, even when the graph size increases.

</details>


### [170] [Nash: Neural Adaptive Shrinkage for Structured High-Dimensional Regression](https://arxiv.org/abs/2505.11143)
*William R. P. Denault*

**主要类别:** stat.ML

**概要:** 提出了一种名为Neural Adaptive Shrinkage (Nash) 的统一框架，用于结构化稀疏回归问题，通过神经网络整合先验信息，无需交叉验证即可自适应调整正则化，实验表明其在准确性与适应性上优于现有方法。


<details>
  <summary>Details</summary>
  
**动机:** 传统稀疏线性回归方法在处理具有结构或来自异构来源的协变量时表现不佳，特别是在生物医学应用中，协变量可能来源于不同的模态或遵循潜在图结构。

**方法:** 引入了Neural Adaptive Shrinkage (Nash) 框架，利用神经网络整合协变量特定的侧信息到稀疏回归中，并开发了一种变分推理算法用于高效训练，同时建立了与经验贝叶斯回归的联系。

**结果:** 实验证明Nash方法在真实数据上的准确性和适应性优于现有方法。

**结论:** Nash提供了一个统一且灵活的框架来解决带有结构化协变量的稀疏回归问题，展示了良好的性能和潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Nash%3A+Neural+Adaptive+Shrinkage+for+Structured+High-Dimensional+Regression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11143，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11143&send_immediately=true&force_search=false)

**原文摘要:** Sparse linear regression is a fundamental tool in data analysis. However,
traditional approaches often fall short when covariates exhibit structure or
arise from heterogeneous sources. In biomedical applications, covariates may
stem from distinct modalities or be structured according to an underlying
graph. We introduce Neural Adaptive Shrinkage (Nash), a unified framework that
integrates covariate-specific side information into sparse regression via
neural networks. Nash adaptively modulates penalties on a per-covariate basis,
learning to tailor regularization without cross-validation. We develop a
variational inference algorithm for efficient training and establish
connections to empirical Bayes regression. Experiments on real data demonstrate
that Nash can improve accuracy and adaptability over existing methods.

</details>


### [171] [On Next-Token Prediction in LLMs: How End Goals Determine the Consistency of Decoding Algorithms](https://arxiv.org/abs/2505.11183)
*Jacob Trauger, Ambuj Tewari*

**主要类别:** stat.ML

**概要:** This paper examines different algorithms for next-token prediction in large language models and their consistency with respect to various goals.


<details>
  <summary>Details</summary>
  
**动机:** To study the consistency of surrogate losses with respect to a target loss function in the context of LLMs.

**方法:** Examining greedy, lookahead, random sampling, and temperature-scaled random sampling algorithms.

**结果:** Random sampling is consistent with outputting sequences that mimic sampling from the true probability distribution when next-token prediction converges to its true probability distribution. For other goals, no polynomial-time algorithm is optimal for all probability distributions.

**结论:** Choosing the correct decoding algorithm based on the desired goal is crucial, and many of the ones used lack theoretical grounding in numerous scenarios.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Next-Token+Prediction+in+LLMs%3A+How+End+Goals+Determine+the+Consistency+of+Decoding+Algorithms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11183，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11183&send_immediately=true&force_search=false)

**原文摘要:** Probabilistic next-token prediction trained using cross-entropy loss is the
basis of most large language models. Given a sequence of previous values,
next-token prediction assigns a probability to each possible next value in the
vocabulary. There are many ways to use next-token prediction to output token
sequences. This paper examines a few of these algorithms (greedy, lookahead,
random sampling, and temperature-scaled random sampling) and studies their
consistency with respect to various goals encoded as loss functions. Although
consistency of surrogate losses with respect to a target loss function is a
well researched topic, we are the first to study it in the context of LLMs (to
the best of our knowledge). We find that, so long as next-token prediction
converges to its true probability distribution, random sampling is consistent
with outputting sequences that mimic sampling from the true probability
distribution. For the other goals, such as minimizing the 0-1 loss on the
entire sequence, we show no polynomial-time algorithm is optimal for all
probability distributions and all decoding algorithms studied are only optimal
for a subset of probability distributions. When analyzing these results, we see
that there is a dichotomy created between the goals of information retrieval
and creative generation for the decoding algorithms. This shows that choosing
the correct decoding algorithm based on the desired goal is extremely important
and many of the ones used are lacking theoretical grounding in numerous
scenarios.

</details>


### [172] [A Fourier Space Perspective on Diffusion Models](https://arxiv.org/abs/2505.11278)
*Fabian Falck, Teodora Pandeva, Kiarash Zahirnia, Rachel Lawrence, Richard Turner, Edward Meeds, Javier Zazo, Sushrut Karmalkar*

**主要类别:** stat.ML

**概要:** This paper explores the inductive bias of the forward process in diffusion models within the Fourier space. It identifies issues with high-frequency component degradation in the standard Denoising Diffusion Probabilistic Models (DDPM) and proposes an alternative forward process that improves generation quality for high-frequency components.


<details>
  <summary>Details</summary>
  
**动机:** The motivation is to address the issue of degraded high-frequency component generation in DDPM due to the faster corruption of high-frequency components in the Fourier space.

**方法:** The authors theoretically analyze and empirically demonstrate the problem in DDPM and propose an alternate forward process that corrupts all frequencies at the same rate.

**结果:** Experiments show performance improvements on datasets with significant high frequencies, while maintaining comparable performance on standard imaging benchmarks.

**结论:** The study concludes that the proposed alternate forward process can enhance the quality of high-frequency component generation in diffusion models.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Fourier+Space+Perspective+on+Diffusion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11278，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11278&send_immediately=true&force_search=false)

**原文摘要:** Diffusion models are state-of-the-art generative models on data modalities
such as images, audio, proteins and materials. These modalities share the
property of exponentially decaying variance and magnitude in the Fourier
domain. Under the standard Denoising Diffusion Probabilistic Models (DDPM)
forward process of additive white noise, this property results in
high-frequency components being corrupted faster and earlier in terms of their
Signal-to-Noise Ratio (SNR) than low-frequency ones. The reverse process then
generates low-frequency information before high-frequency details. In this
work, we study the inductive bias of the forward process of diffusion models in
Fourier space. We theoretically analyse and empirically demonstrate that the
faster noising of high-frequency components in DDPM results in violations of
the normality assumption in the reverse process. Our experiments show that this
leads to degraded generation quality of high-frequency components. We then
study an alternate forward process in Fourier space which corrupts all
frequencies at the same rate, removing the typical frequency hierarchy during
generation, and demonstrate marked performance improvements on datasets where
high frequencies are primary, while performing on par with DDPM on standard
imaging benchmarks.

</details>


### [173] [Adaptive Linear Embedding for Nonstationary High-Dimensional Optimization](https://arxiv.org/abs/2505.11281)
*Yuejiang Wen, Paul D. Franzon*

**主要类别:** stat.ML

**概要:** 提出了一种新的框架SA-REMBO，通过支持多个随机高斯嵌入来扩展REMBO，适应不同局部子空间结构的高维目标。


<details>
  <summary>Details</summary>
  
**动机:** 解决高维空间中贝叶斯优化受限于维度灾难和全局低维假设的问题。

**方法:** 引入自适应嵌入REMBO（SA-REMBO），使用产品核在高斯过程代理中联合建模索引变量和潜在优化变量。

**结果:** 在合成和真实世界高维基准测试中表现出色，优于传统REMBO和其他低秩BO方法。

**结论:** 证明了SA-REMBO作为可扩展贝叶斯优化的强大且灵活的扩展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptive+Linear+Embedding+for+Nonstationary+High-Dimensional+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11281，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11281&send_immediately=true&force_search=false)

**原文摘要:** Bayesian Optimization (BO) in high-dimensional spaces remains fundamentally
limited by the curse of dimensionality and the rigidity of global
low-dimensional assumptions. While Random EMbedding Bayesian Optimization
(REMBO) mitigates this via linear projections into low-dimensional subspaces,
it typically assumes a single global embedding and a stationary objective. In
this work, we introduce Self-Adaptive embedding REMBO (SA-REMBO), a novel
framework that generalizes REMBO to support multiple random Gaussian
embeddings, each capturing a different local subspace structure of the
high-dimensional objective. An index variable governs the embedding choice and
is jointly modeled with the latent optimization variable via a product kernel
in a Gaussian Process surrogate. This enables the optimizer to adaptively
select embeddings conditioned on location, effectively capturing locally
varying effective dimensionality, nonstationarity, and heteroscedasticity in
the objective landscape. We theoretically analyze the expressiveness and
stability of the index-conditioned product kernel and empirically demonstrate
the advantage of our method across synthetic and real-world high-dimensional
benchmarks, where traditional REMBO and other low-rank BO methods fail. Our
results establish SA-REMBO as a powerful and flexible extension for scalable BO
in complex, structured design spaces.

</details>


### [174] [Convergence Rates of Constrained Expected Improvement](https://arxiv.org/abs/2505.11323)
*Haowei Wang, Jingyi Wang, Zhongxiang Dai, Nai-Yuan Chiang, Szu Hui Ng, Cosmin G. Petra*

**主要类别:** stat.ML

**概要:** 研究了约束期望改进(CEI)算法在约束贝叶斯优化中的收敛率。


<details>
  <summary>Details</summary>
  
**动机:** CEI是处理约束问题的自然扩展，但其理论收敛率尚未建立。

**方法:** 通过分析简单遗憾上界来研究CEI的收敛率。

**结果:** 对于常用的平方指数和Matérn核，分别得到了特定的收敛率。

**结论:** 当目标函数和约束函数从高斯过程中采样时，CEI达到相同的收敛率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Convergence+Rates+of+Constrained+Expected+Improvement，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11323，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11323&send_immediately=true&force_search=false)

**原文摘要:** Constrained Bayesian optimization (CBO) methods have seen significant success
in black-box optimization with constraints, and one of the most commonly used
CBO methods is the constrained expected improvement (CEI) algorithm. CEI is a
natural extension of the expected improvement (EI) when constraints are
incorporated. However, the theoretical convergence rate of CEI has not been
established. In this work, we study the convergence rate of CEI by analyzing
its simple regret upper bound. First, we show that when the objective function
$f$ and constraint function $c$ are assumed to each lie in a reproducing kernel
Hilbert space (RKHS), CEI achieves the convergence rates of $\mathcal{O}
\left(t^{-\frac{1}{2}}\log^{\frac{d+1}{2}}(t) \right) \ \text{and }\
\mathcal{O}\left(t^{\frac{-\nu}{2\nu+d}} \log^{\frac{\nu}{2\nu+d}}(t)\right)$
for the commonly used squared exponential and Mat\'{e}rn kernels, respectively.
Second, we show that when $f$ and $c$ are assumed to be sampled from Gaussian
processes (GPs), CEI achieves the same convergence rates with a high
probability. Numerical experiments are performed to validate the theoretical
analysis.

</details>


### [175] [STRIDE: Sparse Techniques for Regression in Deep Gaussian Processes](https://arxiv.org/abs/2505.11355)
*Simon Urbainczyk, Aretha L. Teckentrup, Jonas Latz*

**主要类别:** stat.ML

**概要:** 提出了一种新的深度高斯过程训练方法，该方法结合了变分学习和MCMC，能够在大规模数据上高效且准确地训练深度高斯过程。


<details>
  <summary>Details</summary>
  
**动机:** 解决传统高斯过程在处理大规模数据和多尺度特征时的问题。

**方法:** 结合变分学习与MCMC，开发了一种基于粒子的期望最大化方法来同时找到大规模数据中的诱导点并准确训练高斯过程。

**结果:** 在标准基准问题上测试了该方法。

**结论:** 提出的方法在大规模数据上高效且准确地训练深度高斯过程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是STRIDE%3A+Sparse+Techniques+for+Regression+in+Deep+Gaussian+Processes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.11355，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.11355&send_immediately=true&force_search=false)

**原文摘要:** Gaussian processes (GPs) have gained popularity as flexible machine learning
models for regression and function approximation with an in-built method for
uncertainty quantification. However, GPs suffer when the amount of training
data is large or when the underlying function contains multi-scale features
that are difficult to represent by a stationary kernel. To address the former,
training of GPs with large-scale data is often performed through inducing point
approximations (also known as sparse GP regression (GPR)), where the size of
the covariance matrices in GPR is reduced considerably through a greedy search
on the data set. To aid the latter, deep GPs have gained traction as
hierarchical models that resolve multi-scale features by combining multiple
GPs. Posterior inference in deep GPs requires a sampling or, more usual, a
variational approximation. Variational approximations lead to large-scale
stochastic, non-convex optimisation problems and the resulting approximation
tends to represent uncertainty incorrectly. In this work, we combine
variational learning with MCMC to develop a particle-based
expectation-maximisation method to simultaneously find inducing points within
the large-scale data (variationally) and accurately train the GPs
(sampling-based). The result is a highly efficient and accurate methodology for
deep GP training on large-scale data. We test our method on standard benchmark
problems.

</details>
