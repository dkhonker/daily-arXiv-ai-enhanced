<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 112]
- [cs.AI](#cs.AI) [总数: 45]
- [stat.ML](#stat.ML) [总数: 8]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Latent Factorization of Tensors with Threshold Distance Weighted Loss for Traffic Data Estimation](https://arxiv.org/abs/2506.22441)
*Lei Yang*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的TDWLFT模型，通过引入阈值距离加权损失函数提高对异常值的鲁棒性，在交通速度数据集上的实验表明该模型在预测准确性和计算效率上优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 智能交通系统依赖于完整和高质量的时空交通数据，但在实际数据收集过程中，通信故障和传感器故障等问题会导致数据不完整或损坏，这为ITS的发展带来了挑战。现有的LFT模型容易受到异常值的影响，因此需要改进。

**方法:** 提出了TDWLFT模型，通过引入阈值距离加权（TDW）损失函数，为每个样本分配不同的权重，从而降低模型对异常值的敏感性。

**结果:** 在两个来自不同城市环境的交通速度数据集上的广泛实验证明，TDWLFT模型在预测准确性和计算效率方面均优于现有最先进方法。

**结论:** TDWLFT模型通过使用TDW损失函数提高了对异常值的鲁棒性，并且在实验中表现出更优的性能，可作为处理缺失时空交通数据的有效工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Latent+Factorization+of+Tensors+with+Threshold+Distance+Weighted+Loss+for+Traffic+Data+Estimation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22441，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22441&send_immediately=true&force_search=false)

**原文摘要:** Intelligent transportation systems (ITS) rely heavily on complete and
high-quality spatiotemporal traffic data to achieve optimal performance.
Nevertheless, in real-word traffic data collection processes, issues such as
communication failures and sensor malfunctions often lead to incomplete or
corrupted datasets, thereby posing significant challenges to the advancement of
ITS. Among various methods for imputing missing spatiotemporal traffic data,
the latent factorization of tensors (LFT) model has emerged as a widely adopted
and effective solution. However, conventional LFT models typically employ the
standard L2-norm in their learning objective, which makes them vulnerable to
the influence of outliers. To overcome this limitation, this paper proposes a
threshold distance weighted (TDW) loss-incorporated Latent Factorization of
Tensors (TDWLFT) model. The proposed loss function effectively reduces the
model's sensitivity to outliers by assigning differentiated weights to
individual samples. Extensive experiments conducted on two traffic speed
datasets sourced from diverse urban environments confirm that the proposed
TDWLFT model consistently outperforms state-of-the-art approaches in terms of
both in both prediction accuracy and computational efficiency.

</details>


### [2] [Features-based embedding or Feature-grounding](https://arxiv.org/abs/2506.22442)
*Piotr Makarevich*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了如何在深度学习模型中重现基于知识的结构化思维，提出了一种构建特征嵌入的方法，将可操作词典的共享表示与可解释的领域特定概念特征对齐。


<details>
  <summary>更多</summary>
  
**动机:** 在日常推理中，当我们思考一个特定对象时，我们会将其与一组独特的预期属性相关联，这些属性由我们的先验知识和通过经验形成的概念类别塑造。作者希望在深度学习模型中重现这种基于知识的结构化思维方式。

**方法:** 引入一种特定方法来构建特征嵌入（feature-grounded embedding），以对齐可操作字典的共享表示与可解释的领域特定概念特征。

**结果:** 未提及具体结果，但该方法旨在提高深度学习模型对知识结构的表达能力。

**结论:** 该研究为深度学习模型中的知识结构化提供了新的思路，可能增强模型的可解释性和性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Features-based+embedding+or+Feature-grounding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22442，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22442&send_immediately=true&force_search=false)

**原文摘要:** In everyday reasoning, when we think about a particular object, we associate
it with a unique set of expected properties such as weight, size, or more
abstract attributes like density or horsepower. These expectations are shaped
by our prior knowledge and the conceptual categories we have formed through
experience. This paper investigates how such knowledge-based structured
thinking can be reproduced in deep learning models using features based
embeddings. Specially, it introduces an specific approach to build
feature-grounded embedding, aiming to align shareable representations of
operable dictionary with interpretable domain-specific conceptual features.

</details>


### [3] [Learning Interpretable Rules from Neural Networks: Neurosymbolic AI for Radar Hand Gesture Recognition](https://arxiv.org/abs/2506.22443)
*Sarah Seifi, Tobias Sukianto, Cecilia Carbonelli, Lorenzo Servadei, Robert Wille*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为RL-Net的神经符号规则学习网络，首次将其应用于基于雷达的手势识别任务。该模型在保持高精度（F1得分93.03%）的同时显著降低了规则复杂度，并解决了规则剪枝和层次偏差方面的优化挑战。与MIRA和XentricAI相比，RL-Net在透明性和性能之间找到了一个实用的平衡点，为可解释AI在边缘部署感知系统中的应用提供了新见解。


<details>
  <summary>更多</summary>
  
**动机:** 规则驱动模型虽然具有可解释性，但在处理复杂数据时表现不佳；而深度神经网络虽然性能优越，但缺乏透明性。因此需要一种能够在性能和可解释性之间取得平衡的新方法，特别是在手势识别等实际应用场景中。

**方法:** 研究提出了RL-Net，这是一种通过神经优化学习可解释规则列表的神经符号模型。将RL-Net应用于基于雷达的手势识别任务，并与完全透明的规则驱动系统（MIRA）和可解释的黑箱模型（XentricAI）进行对比，评估其准确性、可解释性和用户适应能力（通过迁移学习）。

**结果:** 实验结果表明，RL-Net在保持高性能（F1得分为93.03%）的同时显著减少了规则复杂度。此外，研究还发现了与规则剪枝和层次偏差相关的优化挑战，并提出了改进方案。

**结论:** RL-Net为手势识别任务提供了一个在透明性和性能之间平衡的实用解决方案，展示了神经符号模型在可解释AI领域的实际可行性，并为扩展到边缘部署感知系统提供了重要参考。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Interpretable+Rules+from+Neural+Networks%3A+Neurosymbolic+AI+for+Radar+Hand+Gesture+Recognition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22443，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22443&send_immediately=true&force_search=false)

**原文摘要:** Rule-based models offer interpretability but struggle with complex data,
while deep neural networks excel in performance yet lack transparency. This
work investigates a neuro-symbolic rule learning neural network named RL-Net
that learns interpretable rule lists through neural optimization, applied for
the first time to radar-based hand gesture recognition (HGR). We benchmark
RL-Net against a fully transparent rule-based system (MIRA) and an explainable
black-box model (XentricAI), evaluating accuracy, interpretability, and user
adaptability via transfer learning. Our results show that RL-Net achieves a
favorable trade-off, maintaining strong performance (93.03% F1) while
significantly reducing rule complexity. We identify optimization challenges
specific to rule pruning and hierarchy bias and propose stability-enhancing
modifications. Compared to MIRA and XentricAI, RL-Net emerges as a practical
middle ground between transparency and performance. This study highlights the
real-world feasibility of neuro-symbolic models for interpretable HGR and
offers insights for extending explainable AI to edge-deployable sensing
systems.

</details>


### [4] [Active Learning for Forecasting Severity among Patients with Post Acute Sequelae of SARS-CoV-2](https://arxiv.org/abs/2506.22444)
*Jing Wang, Amar Sra, Jeremy C. Weiss*

**主要类别:** cs.LG

**AI概要:** The paper introduces an Active Attention Network using a cohort of 18 PASC patients to predict clinical risk and identify progression events related to PASC by integrating human expertise with active learning.


<details>
  <summary>更多</summary>
  
**动机:** To address the challenge of accurately identifying progression events such as hospitalization and reinfection in PASC patients, which is crucial for effective patient management and resource allocation.

**方法:** Using a publicly available cohort of 18 PASC patients with text time series features based on Llama-3.1-70B-Instruct and clinical risk annotated by experts, an Active Attention Network was proposed to predict clinical risk and identify progression events related to the risk.

**结果:** The integration of human expertise with active learning enhances clinical risk prediction accuracy and enables progression event identification with fewer annotations.

**结论:** This study aims to improve patient care and decision-making for SARS-CoV-2 patients through better prediction models.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Active+Learning+for+Forecasting+Severity+among+Patients+with+Post+Acute+Sequelae+of+SARS-CoV-2，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22444，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22444&send_immediately=true&force_search=false)

**原文摘要:** The long-term effects of Postacute Sequelae of SARS-CoV-2, known as PASC,
pose a significant challenge to healthcare systems worldwide. Accurate
identification of progression events, such as hospitalization and reinfection,
is essential for effective patient management and resource allocation. However,
traditional models trained on structured data struggle to capture the nuanced
progression of PASC. In this study, we introduce the first publicly available
cohort of 18 PASC patients, with text time series features based on Large
Language Model Llama-3.1-70B-Instruct and clinical risk annotated by clinical
expert. We propose an Active Attention Network to predict the clinical risk and
identify progression events related to the risk. By integrating human expertise
with active learning, we aim to enhance clinical risk prediction accuracy and
enable progression events identification with fewer number of annotation. The
ultimate goal is to improves patient care and decision-making for SARS-CoV-2
patient.

</details>


### [5] [Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for Cyber-Physical Systems Security](https://arxiv.org/abs/2506.22445)
*Saad Alqithami*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种新的分层对抗弹性多智能体强化学习（HAMARL）框架，用于提升网络物理系统（CPS）的安全性和弹性。通过局部智能体和全局协调器的结合以及对抗性训练循环，该框架在实验中表现出色，显著提高了攻击检测精度、缩短了响应时间，并确保了操作连续性。


<details>
  <summary>更多</summary>
  
**动机:** 随着网络物理系统（CPS）连接性的增加，其面临复杂的网络威胁，如自适应和零日攻击，传统的基于规则的入侵检测和单智能体强化学习方法已无法满足需求。

**方法:** 论文引入了Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL)框架，采用分层结构，包含负责子系统安全的局部智能体和监督优化全局防御策略的全局协调器，同时整合了一个对抗性训练循环以模拟和预测演化中的网络威胁。

**结果:** 在模拟工业物联网测试平台上进行的广泛实验评估表明，与传统多智能体强化学习方法相比，HAMARL框架显著提高了攻击检测的准确性，减少了响应时间，并确保了操作的连续性。

**结论:** 结合分层多智能体协调与对抗性训练可以有效增强下一代CPS的弹性和安全性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hierarchical+Adversarially-Resilient+Multi-Agent+Reinforcement+Learning+for+Cyber-Physical+Systems+Security，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22445，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22445&send_immediately=true&force_search=false)

**原文摘要:** Cyber-Physical Systems play a critical role in the infrastructure of various
sectors, including manufacturing, energy distribution, and autonomous
transportation systems. However, their increasing connectivity renders them
highly vulnerable to sophisticated cyber threats, such as adaptive and zero-day
attacks, against which traditional security methods like rule-based intrusion
detection and single-agent reinforcement learning prove insufficient. To
overcome these challenges, this paper introduces a novel Hierarchical
Adversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL) framework.
HAMARL employs a hierarchical structure consisting of local agents dedicated to
subsystem security and a global coordinator that oversees and optimizes
comprehensive, system-wide defense strategies. Furthermore, the framework
incorporates an adversarial training loop designed to simulate and anticipate
evolving cyber threats, enabling proactive defense adaptation. Extensive
experimental evaluations conducted on a simulated industrial IoT testbed
indicate that HAMARL substantially outperforms traditional multi-agent
reinforcement learning approaches, significantly improving attack detection
accuracy, reducing response times, and ensuring operational continuity. The
results underscore the effectiveness of combining hierarchical multi-agent
coordination with adversarially-aware training to enhance the resilience and
security of next-generation CPS.

</details>


### [6] [EAGLE: Efficient Alignment of Generalized Latent Embeddings for Multimodal Survival Prediction with Interpretable Attribution Analysis](https://arxiv.org/abs/2506.22446)
*Aakash Tripathi, Asim Waqas, Matthew B. Schabath, Yasin Yilmaz, Ghulam Rasool*

**主要类别:** cs.LG

**AI概要:** EAGLE是一种新型深度学习框架，通过基于注意力的多模态融合和全面的归因分析来准确预测癌症生存率。它在三种癌症类型中进行了测试，并能识别具有临床意义的风险群体，从而为治疗决策提供信息。


<details>
  <summary>更多</summary>
  
**动机:** 现有的多模态方法在癌症生存预测中存在融合策略简单、计算需求大和缺乏可解释性的问题，这些问题阻碍了其在临床中的应用。

**方法:** EAGLE引入了四个关键创新：1）动态跨模态注意力机制以学习模态间的分层关系；2）大幅降低维度（99.96%），同时保持预测性能；3）三种互补的归因方法提供患者级别的可解释性；4）统一的管道实现跨癌症类型的无缝适应。

**结果:** 在911名患者中进行评估，包括胶质母细胞瘤（GBM）、粘液性乳头状导管内肿瘤（IPMN）和非小细胞肺癌（NSCLC）。高风险个体更依赖不良影像特征，而低风险患者则表现出平衡的模态贡献。风险分层识别出中位生存时间差异达4到5倍的临床有意义的群体。

**结论:** EAGLE结合了最先进的性能与临床可解释性，弥合了高级AI能力和实际医疗部署之间的差距，为多模态生存预测提供了可扩展的解决方案，提高了预后准确性和医生对自动化预测的信任。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EAGLE%3A+Efficient+Alignment+of+Generalized+Latent+Embeddings+for+Multimodal+Survival+Prediction+with+Interpretable+Attribution+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22446，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22446&send_immediately=true&force_search=false)

**原文摘要:** Accurate cancer survival prediction requires integration of diverse data
modalities that reflect the complex interplay between imaging, clinical
parameters, and textual reports. However, existing multimodal approaches suffer
from simplistic fusion strategies, massive computational requirements, and lack
of interpretability-critical barriers to clinical adoption. We present EAGLE
(Efficient Alignment of Generalized Latent Embeddings), a novel deep learning
framework that addresses these limitations through attention-based multimodal
fusion with comprehensive attribution analysis. EAGLE introduces four key
innovations: (1) dynamic cross-modal attention mechanisms that learn
hierarchical relationships between modalities, (2) massive dimensionality
reduction (99.96%) while maintaining predictive performance, (3) three
complementary attribution methods providing patient-level interpretability, and
(4) a unified pipeline enabling seamless adaptation across cancer types. We
evaluated EAGLE on 911 patients across three distinct malignancies:
glioblastoma (GBM, n=160), intraductal papillary mucinous neoplasms (IPMN,
n=171), and non-small cell lung cancer (NSCLC, n=580). Patient-level analysis
showed high-risk individuals relied more heavily on adverse imaging features,
while low-risk patients demonstrated balanced modality contributions. Risk
stratification identified clinically meaningful groups with 4-fold (GBM) to
5-fold (NSCLC) differences in median survival, directly informing treatment
intensity decisions. By combining state-of-the-art performance with clinical
interpretability, EAGLE bridges the gap between advanced AI capabilities and
practical healthcare deployment, offering a scalable solution for multimodal
survival prediction that enhances both prognostic accuracy and physician trust
in automated predictions.

</details>


### [7] [Vision Transformers for Multi-Variable Climate Downscaling: Emulating Regional Climate Models with a Shared Encoder and Multi-Decoder Architecture](https://arxiv.org/abs/2506.22447)
*Fabio Merizzi, Harilaos Loukos*

**主要类别:** cs.LG

**AI概要:** Global Climate Models (GCMs) have a coarse resolution, and Regional Climate Models (RCMs) are computationally expensive. Most deep learning approaches focus on single-variable models, which have limitations. This study proposes a multi-task, multi-variable Vision Transformer architecture that predicts three climate variables from GCM-resolution inputs for European downscaling, outperforming single-variable baselines while improving computational efficiency.


<details>
  <summary>更多</summary>
  
**动机:** To overcome the limitations of coarse resolution in GCMs, high computational cost of RCMs, and the inefficiencies of single-variable deep learning models, such as limited contextual awareness, redundant computation, and lack of cross-variable interaction.

**方法:** A multi-task, multi-variable Vision Transformer (ViT) architecture with a shared encoder and variable-specific decoders was developed. It jointly predicts three climate variables: surface temperature, wind speed, and 500 hPa geopotential height directly from GCM-resolution inputs.

**结果:** The multi-variable approach achieved positive cross-variable knowledge transfer and consistently outperformed single-variable baselines trained under identical conditions, while also enhancing computational efficiency.

**结论:** Multi-variable modeling using the proposed Vision Transformer architecture is effective for high-resolution climate downscaling.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Vision+Transformers+for+Multi-Variable+Climate+Downscaling%3A+Emulating+Regional+Climate+Models+with+a+Shared+Encoder+and+Multi-Decoder+Architecture，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22447，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22447&send_immediately=true&force_search=false)

**原文摘要:** Global Climate Models (GCMs) are critical for simulating large-scale climate
dynamics, but their coarse spatial resolution limits their applicability in
regional studies. Regional Climate Models (RCMs) refine this through dynamic
downscaling, albeit at considerable computational cost and with limited
flexibility. While deep learning has emerged as an efficient data-driven
alternative, most existing studies have focused on single-variable models that
downscale one variable at a time. This approach can lead to limited contextual
awareness, redundant computation, and lack of cross-variable interaction. Our
study addresses these limitations by proposing a multi-task, multi-variable
Vision Transformer (ViT) architecture with a shared encoder and
variable-specific decoders (1EMD). The proposed architecture jointly predicts
three key climate variables: surface temperature (tas), wind speed (sfcWind),
and 500 hPa geopotential height (zg500), directly from GCM-resolution inputs,
emulating RCM-scale downscaling over Europe. We show that our multi-variable
approach achieves positive cross-variable knowledge transfer and consistently
outperforms single-variable baselines trained under identical conditions, while
also improving computational efficiency. These results demonstrate the
effectiveness of multi-variable modeling for high-resolution climate
downscaling.

</details>


### [8] [Stabilization of industrial processes with time series machine learning](https://arxiv.org/abs/2506.22502)
*Matvei Anoshin, Olga Tsurkan, Vadim Lopatkin, Leonid Fedichkin*

**主要类别:** cs.LG

**AI概要:** This paper proposes a simple pipeline consisting of two neural networks which successfully improves stability in terms of the temperature control by about 3 times compared to ordinary solvers.


<details>
  <summary>更多</summary>
  
**动机:** The stabilization of time series processes is a crucial problem that is ubiquitous in various industrial fields. The application of machine learning can improve both the quality of the resulting stabilization with less computational resources required.

**方法:** A simple pipeline consisting of two neural networks: the oracle predictor and the optimizer, proposing a substitution of the point-wise values optimization to the problem of the neural network training.

**结果:** Improves stability in terms of the temperature control by about 3 times compared to ordinary solvers.

**结论:** The proposed pipeline improves stability in terms of temperature control by about 3 times compared to ordinary solvers.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Stabilization+of+industrial+processes+with+time+series+machine+learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22502，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22502&send_immediately=true&force_search=false)

**原文摘要:** The stabilization of time series processes is a crucial problem that is
ubiquitous in various industrial fields. The application of machine learning to
its solution can have a decisive impact, improving both the quality of the
resulting stabilization with less computational resources required. In this
work, we present a simple pipeline consisting of two neural networks: the
oracle predictor and the optimizer, proposing a substitution of the point-wise
values optimization to the problem of the neural network training, which
successfully improves stability in terms of the temperature control by about 3
times compared to ordinary solvers.

</details>


### [9] [Task-Agnostic Contrastive Pretraining for Relational Deep Learning](https://arxiv.org/abs/2506.22530)
*Jakub Peleška, Gustav Šír*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种新颖的与任务无关的对比预训练方法，用于关系深度学习（RDL），通过三个层次的对比目标进行数据库范围的表征学习，实验结果表明微调预训练模型比从头开始训练表现更好。


<details>
  <summary>更多</summary>
  
**动机:** 现有的RDL模型通常依赖于特定任务的监督学习，需要为每个预测任务单独训练模型，这可能会影响可扩展性和重用性。

**方法:** 提出了一种新颖的任务无关的对比预训练方法，引入了三个层次的对比目标（行级、链接级和上下文级），并通过模块化的RDL架构和高效的采样策略实现预训练方法。

**结果:** 初步的标准RDL基准测试结果显示，微调预训练模型明显优于从头开始训练。

**结论:** 提出的对比预训练方法在学习可转移的关系数据表征方面具有潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Task-Agnostic+Contrastive+Pretraining+for+Relational+Deep+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22530，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22530&send_immediately=true&force_search=false)

**原文摘要:** Relational Deep Learning (RDL) is an emerging paradigm that leverages Graph
Neural Network principles to learn directly from relational databases by
representing them as heterogeneous graphs. However, existing RDL models
typically rely on task-specific supervised learning, requiring training
separate models for each predictive task, which may hamper scalability and
reuse.
  In this work, we propose a novel task-agnostic contrastive pretraining
approach for RDL that enables database-wide representation learning. For that
aim, we introduce three levels of contrastive objectives$-$row-level,
link-level, and context-level$-$designed to capture the structural and semantic
heterogeneity inherent to relational data. We implement the respective
pretraining approach through a modular RDL architecture and an efficient
sampling strategy tailored to the heterogeneous database setting. Our
preliminary results on standard RDL benchmarks demonstrate that fine-tuning the
pretrained models measurably outperforms training from scratch, validating the
promise of the proposed methodology in learning transferable representations
for relational data.

</details>


### [10] [Exploration Behavior of Untrained Policies](https://arxiv.org/abs/2506.22566)
*Jacob Adamczyk*

**主要类别:** cs.LG

**AI概要:** 研究了深度神经策略架构在训练前如何隐式地影响探索行为，提出了生成未训练策略轨迹的策略，并通过无限宽度网络理论和连续时间极限展示了未训练策略的相关动作及状态访问分布。


<details>
  <summary>更多</summary>
  
**动机:** 强化学习中，在稀疏或对抗性奖励结构的环境中探索是一个基本挑战，因此需要研究深度神经策略架构在训练前如何隐式地塑造探索行为。

**方法:** 通过理论上和实证上展示从未训练策略生成弹道或扩散轨迹的策略，使用无限宽度网络理论和连续时间极限来分析未训练策略的动作相关性和状态访问分布。

**结果:** 揭示了标准架构对应的轨迹分布，提供了关于归纳偏置以解决探索问题的见解，并建立了使用策略初始化作为设计工具来理解早期训练中探索行为的理论和实验框架。

**结论:** 本研究为利用策略初始化设计工具理解强化学习中的探索行为奠定了理论和实验基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploration+Behavior+of+Untrained+Policies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22566，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22566&send_immediately=true&force_search=false)

**原文摘要:** Exploration remains a fundamental challenge in reinforcement learning (RL),
particularly in environments with sparse or adversarial reward structures. In
this work, we study how the architecture of deep neural policies implicitly
shapes exploration before training. We theoretically and empirically
demonstrate strategies for generating ballistic or diffusive trajectories from
untrained policies in a toy model. Using the theory of infinite-width networks
and a continuous-time limit, we show that untrained policies return correlated
actions and result in non-trivial state-visitation distributions. We discuss
the distributions of the corresponding trajectories for a standard
architecture, revealing insights into inductive biases for tackling
exploration. Our results establish a theoretical and experimental framework for
using policy initialization as a design tool to understand exploration behavior
in early training.

</details>


### [11] [The Hidden Link Between RLHF and Contrastive Learning](https://arxiv.org/abs/2506.22578)
*Xufei Lv, Haoyuan Sun, Xuefeng Bai, Min Zhang, Houde Liu, Kehai Chen*

**主要类别:** cs.LG

**AI概要:** 本论文重新解读了RLHF和DPO方法，发现它们可以通过对比学习视角理解，并与互信息最大化相关联。基于此洞察，提出了新的MIO方法，能够缓解DPO在后期出现的选择概率下降问题，并在多项基准测试中表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 当前LLM对齐方法如RLHF和DPO已取得显著成果，但其理论基础和潜在局限性仍需深入研究。作者试图从互信息最大化的角度统一解释这两种方法，并探索改进空间以提升模型推理能力。

**方法:** 作者首先将RLHF和DPO方法解释为基于正负样本的对比学习框架，利用Donsker-Varadhan (DV)下界或MINE估计器进行互信息最大化。接着提出用Jensen-Shannon MI估计器替代DV/MINE，引入Mutual Information Optimization (MIO)方法。该方法通过优化互信息来实现更好的对齐效果。

**结果:** 理论分析表明MIO可以缓解DPO在训练后期出现的选择概率下降问题。实证结果表明，MIO在多个复杂的推理和数学基准测试中表现出与现有方法相当或更优的性能。

**结论:** MIO提供了一种新的LLM对齐范式，有效缓解了DPO方法的缺陷，并揭示了RLHF可能无法本质提升LLM推理能力的原因。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Hidden+Link+Between+RLHF+and+Contrastive+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22578，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22578&send_immediately=true&force_search=false)

**原文摘要:** Alignment of large language models (LLMs) with human values has recently
garnered significant attention, with prominent examples including the canonical
yet costly Reinforcement Learning from Human Feedback (RLHF) and the simple
Direct Preference Optimization (DPO). In this work, we demonstrate that both
RLHF and DPO can be interpreted from the perspective of mutual information (MI)
maximization, uncovering a profound connection to contrastive learning. Within
this framework, both RLHF and DPO can be viewed as methods that perform
contrastive learning based on the positive and negative samples derived from
the base model, leveraging the Donsker-Varadhan (DV) lower bound on MI
(equivalently, the MINE estimator). This paradigm further explains why RLHF may
not intrinsically incentivize reasoning capacities in LLMs beyond what is
already present in the base model. Building on this perspective, we replace the
DV/MINE bound with the Jensen-Shannon MI estimator and propose Mutual
Information Optimization (MIO). Comprehensive theoretical analysis and
extensive empirical evaluations demonstrate that MIO mitigates the late-stage
decline in chosen-likelihood observed in DPO, achieving competitive or superior
performance across various challenging reasoning and mathematical benchmarks.
We will release the model and code upon acceptance.

</details>


### [12] [Are Fast Methods Stable in Adversarially Robust Transfer Learning?](https://arxiv.org/abs/2506.22602)
*Joshua C. Zhao, Saurabh Bagchi*

**主要类别:** cs.LG

**AI概要:** 本文探讨了在对抗鲁棒转移学习中使用快速梯度符号法（FGSM）来降低计算成本的效果，发现其具有更高的稳定性，且性能接近于投影梯度下降（PGD）方法，但训练时间大幅减少。


<details>
  <summary>更多</summary>
  
**动机:** 对抗训练模型从头开始训练计算成本非常高，因此转移学习成为一种减少模型训练计算成本的有效方法。然而，高鲁棒性的转移学习仍然需要在微调阶段进行对抗训练，这比标准微调需要多一个数量级的时间。

**方法:** 研究重新审视了在鲁棒转移学习中使用快速梯度符号法（FGSM）以改善对抗微调的计算成本。研究发现FGSM在对抗微调时比从零训练时更加稳定，并且这种稳定性可以通过参数高效微调方法进一步增强。

**结果:** 与常用的投影梯度下降（PGD）方法相比，FGSM平均仅损失0.39%和1.39%的测试鲁棒性（对于ε=4和ε=8），但使用的训练时间仅为PGD的四分之一。此外，FGSM在多个数据集上的性能表现良好，显示出其不仅是一种更高效的替代方案，而且也是一种表现良好的方法。

**结论:** FGSM在对抗微调中的稳定性使其成为一种显著更高效且性能良好的对抗鲁棒转移学习方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Are+Fast+Methods+Stable+in+Adversarially+Robust+Transfer+Learning%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22602，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22602&send_immediately=true&force_search=false)

**原文摘要:** Transfer learning is often used to decrease the computational cost of model
training, as fine-tuning a model allows a downstream task to leverage the
features learned from the pre-training dataset and quickly adapt them to a new
task. This is particularly useful for achieving adversarial robustness, as
adversarially training models from scratch is very computationally expensive.
However, high robustness in transfer learning still requires adversarial
training during the fine-tuning phase, which requires up to an order of
magnitude more time than standard fine-tuning. In this work, we revisit the use
of the fast gradient sign method (FGSM) in robust transfer learning to improve
the computational cost of adversarial fine-tuning. We surprisingly find that
FGSM is much more stable in adversarial fine-tuning than when training from
scratch. In particular, FGSM fine-tuning does not suffer from any issues with
catastrophic overfitting at standard perturbation budgets of $\varepsilon=4$ or
$\varepsilon=8$. This stability is further enhanced with parameter-efficient
fine-tuning methods, where FGSM remains stable even up to $\varepsilon=32$ for
linear probing. We demonstrate how this stability translates into performance
across multiple datasets. Compared to fine-tuning with the more commonly used
method of projected gradient descent (PGD), on average, FGSM only loses 0.39%
and 1.39% test robustness for $\varepsilon=4$ and $\varepsilon=8$ while using
$4\times$ less training time. Surprisingly, FGSM may not only be a
significantly more efficient alternative to PGD in adversarially robust
transfer learning but also a well-performing one.

</details>


### [13] [Hierarchical Modeling and Architecture Optimization: Review and Unified Framework](https://arxiv.org/abs/2506.22621)
*Paul Saves, Edward Hallé-Hannan, Jasper Bussemaker, Youssef Diouane, Nathalie Bartoli*

**主要类别:** cs.LG

**AI概要:** This paper proposes a unified framework for handling mixed-variable input spaces in simulation-based problems, introducing meta and partially-decreed variables, design space graphs, and integrating hierarchical kernels for efficient modeling and optimization. Demonstrated through Bayesian optimization applications.


<details>
  <summary>更多</summary>
  
**动机:** Simulation-based problems often involve complex input spaces that are hierarchical, conditional, heterogeneous, or tree-structured, posing challenges for data representation, modeling, and optimization.

**方法:** The paper introduces a framework where input variables can be continuous, integer, or categorical. It defines meta variables that govern the presence of other decreed variables, and partially-decreed variables whose activation depends on contextual conditions. Design space graphs are used to capture inter-variable hierarchical relationships, combining principles from feature modeling and graph theory. The framework supports surrogate models with hierarchical kernels and distances.

**结果:** The methods were implemented in the open-source SMT 2.0 toolbox and demonstrated effectiveness in Bayesian optimization for complex system design, including a case study in green aircraft architecture.

**结论:** The proposed framework generalizes existing approaches for structured input spaces and provides tools for efficient modeling and optimization of complex hierarchical domains.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hierarchical+Modeling+and+Architecture+Optimization%3A+Review+and+Unified+Framework，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22621，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22621&send_immediately=true&force_search=false)

**原文摘要:** Simulation-based problems involving mixed-variable inputs frequently feature
domains that are hierarchical, conditional, heterogeneous, or tree-structured.
These characteristics pose challenges for data representation, modeling, and
optimization. This paper reviews extensive literature on these structured input
spaces and proposes a unified framework that generalizes existing approaches.
In this framework, input variables may be continuous, integer, or categorical.
A variable is described as meta if its value governs the presence of other
decreed variables, enabling the modeling of conditional and hierarchical
structures.
  We further introduce the concept of partially-decreed variables, whose
activation depends on contextual conditions. To capture these inter-variable
hierarchical relationships, we introduce design space graphs, combining
principles from feature modeling and graph theory. This allows the definition
of general hierarchical domains suitable for describing complex system
architectures. The framework supports the use of surrogate models over such
domains and integrates hierarchical kernels and distances for efficient
modeling and optimization. The proposed methods are implemented in the
open-source Surrogate Modeling Toolbox (SMT 2.0), and their capabilities are
demonstrated through applications in Bayesian optimization for complex system
design, including a case study in green aircraft architecture.

</details>


### [14] [A hierarchical Vovk-Azoury-Warmuth forecaster with discounting for online regression in RKHS](https://arxiv.org/abs/2506.22631)
*Dmitry B. Rokhlin*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种新的在线回归算法H-VAW-D，结合了折扣Vovk-Azoury-Warmuth框架与随机特征逼近，在非参数域中实现了最优动态后悔界。


<details>
  <summary>更多</summary>
  
**动机:** 在线回归问题中，针对来自再生核希尔伯特空间（RKHS）的时间变化函数序列，需要一种能够在无约束二次损失下实现最优动态后悔界的算法。现有的有限维方法不适用于非参数域，因此需要扩展和改进方法以适应更复杂的场景。

**方法:** 作者通过将折扣Vovk-Azoury-Warmuth（DVAW）框架与随机特征逼近相结合，提出了一种完全自适应的分层算法H-VAW-D。该算法能够同时学习折扣因子和随机特征的数量，并在每次迭代中的计算复杂度为$O(T\ln T)$。

**结果:** 所提出的H-VAW-D算法在动态后悔界上达到了$O(T^{2/3}P_T^{1/3} + \sqrt{T}\ln T)$，其中$P_T$是比较器序列的功能路径长度。这表明该算法在非参数域中具有良好的性能和理论保证。

**结论:** H-VAW-D算法成功地将有限维的折扣Vovk-Azoury-Warmuth方法扩展到非参数域，并提供了高效的计算复杂度和理论上可证明的动态后悔界，为在线回归问题提供了一种新的解决途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+hierarchical+Vovk-Azoury-Warmuth+forecaster+with+discounting+for+online+regression+in+RKHS，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22631，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22631&send_immediately=true&force_search=false)

**原文摘要:** We study the problem of online regression with the unconstrained quadratic
loss against a time-varying sequence of functions from a Reproducing Kernel
Hilbert Space (RKHS). Recently, Jacobsen and Cutkosky (2024) introduced a
discounted Vovk-Azoury-Warmuth (DVAW) forecaster that achieves optimal dynamic
regret in the finite-dimensional case. In this work, we lift their approach to
the non-parametric domain by synthesizing the DVAW framework with a random
feature approximation. We propose a fully adaptive, hierarchical algorithm,
which we call H-VAW-D (Hierarchical Vovk-Azoury-Warmuth with Discounting), that
learns both the discount factor and the number of random features. We prove
that this algorithm, which has a per-iteration computational complexity of
$O(T\ln T)$, achieves an expected dynamic regret of $O(T^{2/3}P_T^{1/3} +
\sqrt{T}\ln T)$, where $P_T$ is the functional path length of a comparator
sequence.

</details>


### [15] [Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training](https://arxiv.org/abs/2506.22638)
*Aadim Nepal, Safal Shrestha, Anubhav Shrestha, Minwu Kim, Keith Ross*

**主要类别:** cs.LG

**AI概要:** 大型语言模型在经过指令微调、强化学习或知识蒸馏的后训练后，其数学推理能力有所提高。但这些改进是源于transformer层的重大变化还是细微调整尚不清楚。通过逐层消融实验，我们发现数学推理会产生特定的层重要性结构，这种结构在所有后训练范式中都持续存在。移除这些层会导致高达80%的准确率下降。相比之下，非数学任务（如事实回忆）没有关键层。这表明数学推理需要在预训练期间出现的专门层，而其他非推理任务则不需要。从信息论的角度来看，这些关键层也是发生重大表示转换的层。


<details>
  <summary>更多</summary>
  
**动机:** 研究大型语言模型在后训练过程中，数学推理能力的提升是否来源于transformer层的重大变化还是细微调整。

**方法:** 通过系统性的逐层消融实验，对基础模型、指令微调、知识蒸馏和强化学习变体进行分析，使用数学推理基准进行测试。

**结果:** 数学推理产生特定的层重要性结构，该结构在所有后训练范式中持续存在；移除这些层会导致高达80%的准确率下降；非数学任务没有关键层；数学推理需要专门层，这些层在预训练期间出现。

**结论:** 数学推理需要专门层，这些层在预训练期间形成，并且在不同的后训练范式中保持不变。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Layer+Importance+for+Mathematical+Reasoning+is+Forged+in+Pre-Training+and+Invariant+after+Post-Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22638，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22638&send_immediately=true&force_search=false)

**原文摘要:** Large language models can exhibit improved mathematical reasoning
capabilities following post-training with instruction tuning, reinforcement
learning, or knowledge distillation. However, it remains unclear whether these
improvements are driven by major changes in transformer layers or from minor
adjustments that leave the relative layer importance structures of the base
model largely unchanged. We investigate this question through systematic
layer-wise ablation experiments, examining base, instruction-tuned,
knowledge-distilled, and reinforcement learning variants on mathematical
reasoning benchmarks. Our findings show that mathematical reasoning gives rise
to a specific layer importance structure, and this structure persists across
all post-training paradigms. Removal of such layers causes accuracy drops of up
to 80%. In contrast, non-mathematical tasks like factual recall exhibit no
critical layers. This distinction suggests that mathematical reasoning requires
specialized layers that emerge during pre-training, while other non-reasoning
tasks do not. From an information-theoretic perspective, we also observe that
these critical layers are the same layers where major representational
transformation occurs.

</details>


### [16] [Cost-effective Reduced-Order Modeling via Bayesian Active Learning](https://arxiv.org/abs/2506.22645)
*Amir Hossein Rahmati, Nathan M. Urban, Byung-Jun Yoon, Xiaoning Qian*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种名为BayPOD-AL的主动学习框架，结合了不确定性感知的贝叶斯正交分解方法，用于从高保真全阶模型中有效地学习降阶模型。实验表明，该方法在预测杆温演化时能有效选择信息数据并减少构建训练集的计算成本，并且具有良好的泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的机器学习代理方法需要大量训练数据来准确捕捉复杂系统动力学，这限制了其在实际问题中的应用。因此，研究者希望开发一种能够减少所需训练数据量同时保持高精度的方法。

**方法:** 提出了BayPOD-AL，这是一种基于不确定性感知的贝叶斯正交分解（POD）的主动学习框架，用于从高保真全阶模型中学习降阶模型。该方法通过主动学习策略选择最具信息量的数据点进行训练。

**结果:** 实验结果表明，BayPOD-AL在预测杆温演化时表现出色，与其它基于不确定性的主动学习策略相比，能够显著减少构建训练数据集的计算成本。此外，在更高时间分辨率的数据集上的测试进一步证明了该方法的泛化能力和效率。

**结论:** BayPOD-AL是一种有效的主动学习框架，可以在减少训练数据需求的同时，保持对复杂系统动力学的高精度建模能力，并具备良好的泛化性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cost-effective+Reduced-Order+Modeling+via+Bayesian+Active+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22645，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22645&send_immediately=true&force_search=false)

**原文摘要:** Machine Learning surrogates have been developed to accelerate solving systems
dynamics of complex processes in different science and engineering
applications. To faithfully capture governing systems dynamics, these methods
rely on large training datasets, hence restricting their applicability in
real-world problems. In this work, we propose BayPOD-AL, an active learning
framework based on an uncertainty-aware Bayesian proper orthogonal
decomposition (POD) approach, which aims to effectively learn reduced-order
models from high-fidelity full-order models representing complex systems.
Experimental results on predicting the temperature evolution over a rod
demonstrate BayPOD-AL's effectiveness in suggesting the informative data and
reducing computational cost related to constructing a training dataset compared
to other uncertainty-guided active learning strategies. Furthermore, we
demonstrate BayPOD-AL's generalizability and efficiency by evaluating its
performance on a dataset of higher temporal resolution than the training
dataset.

</details>


### [17] [Learning Stochastic Multiscale Models](https://arxiv.org/abs/2506.22655)
*Andrew F. Ilersich, Prasanth B. Nair*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种从观测数据中直接学习随机多尺度模型的方法，该方法通过引入辅助状态变量捕捉未解析尺度的影响，并使用现代无前向求解器的摊销变分推断方法学习多尺度模型参数。数值研究表明，所学的多尺度模型在等效分辨率下比直接数值模拟和封闭型模型具有更高的预测精度。


<details>
  <summary>更多</summary>
  
**动机:** 物理科学中充满了许多需要解析广泛长度和时间尺度的动力系统，直接数值模拟会导致高维状态空间，计算成本高昂。因此，需要一种能够从数据中直接学习随机多尺度模型的方法来解决这一问题。

**方法:** 提出了一种从观测数据中直接学习随机微分方程形式的随机多尺度模型的方法。该方法在粗网格上解析状态，并引入辅助状态以捕获未解析尺度的影响。使用现代无前向求解器的摊销变分推断方法学习多尺度模型参数。

**结果:** 数值研究显示，所提出的多尺度模型在等效分辨率下实现了比直接数值模拟和封闭型模型更高的预测准确性。

**结论:** 所提出的学习随机多尺度模型的方法能够有效提高预测准确性，为处理高维度动力系统提供了新的思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Stochastic+Multiscale+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22655，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22655&send_immediately=true&force_search=false)

**原文摘要:** The physical sciences are replete with dynamical systems that require the
resolution of a wide range of length and time scales. This presents significant
computational challenges since direct numerical simulation requires
discretization at the finest relevant scales, leading to a high-dimensional
state space. In this work, we propose an approach to learn stochastic
multiscale models in the form of stochastic differential equations directly
from observational data. Our method resolves the state on a coarse mesh while
introducing an auxiliary state to capture the effects of unresolved scales. We
learn the parameters of the multiscale model using a modern forward-solver-free
amortized variational inference method. Our approach draws inspiration from
physics-based multiscale modeling approaches, such as large-eddy simulation in
fluid dynamics, while learning directly from data. We present numerical studies
to demonstrate that our learned multiscale models achieve superior predictive
accuracy compared to direct numerical simulation and closure-type models at
equivalent resolution.

</details>


### [18] [DistShap: Scalable GNN Explanations with Distributed Shapley Values](https://arxiv.org/abs/2506.22668)
*Selahattin Akkas, Aditya Devarakonda, Ariful Azad*

**主要类别:** cs.LG

**AI概要:** DistShap是一种用于解释图神经网络（GNN）预测的并行算法，通过在多GPU上分布Shapley值计算，解决了大规模GNN解释中计算昂贵的问题，其在准确性和可扩展性上超越了现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 随着GNN的广泛应用，对其预测进行解释变得日益重要，但当前方法在归因于特定边或特征时计算成本过高，特别是在大规模节点和边的情况下。

**方法:** 提出了一种名为DistShap的并行算法，该算法通过分布式子图采样、跨多个GPU并行执行GNN推理以及解决分布式最小二乘问题来计算边的重要性分数。

**结果:** DistShap在准确性上优于大多数现有的GNN解释方法，并且是首个能够扩展到具有数百万特征的GNN模型的解释方法，使用多达128个GPU在NERSC Perlmutter超级计算机上实现。

**结论:** DistShap为大规模GNN模型的解释提供了高效、准确的解决方案，显著降低了计算成本并提高了可扩展性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DistShap%3A+Scalable+GNN+Explanations+with+Distributed+Shapley+Values，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22668，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22668&send_immediately=true&force_search=false)

**原文摘要:** With the growing adoption of graph neural networks (GNNs), explaining their
predictions has become increasingly important. However, attributing predictions
to specific edges or features remains computationally expensive. For example,
classifying a node with 100 neighbors using a 3-layer GNN may involve
identifying important edges from millions of candidates contributing to the
prediction. To address this challenge, we propose DistShap, a parallel
algorithm that distributes Shapley value-based explanations across multiple
GPUs. DistShap operates by sampling subgraphs in a distributed setting,
executing GNN inference in parallel across GPUs, and solving a distributed
least squares problem to compute edge importance scores. DistShap outperforms
most existing GNN explanation methods in accuracy and is the first to scale to
GNN models with millions of features by using up to 128 GPUs on the NERSC
Perlmutter supercomputer.

</details>


### [19] [Mitigating Semantic Collapse in Generative Personalization with a Surprisingly Simple Test-Time Embedding Adjustment](https://arxiv.org/abs/2506.22685)
*Anh Bui, Trang Vu, Trung Le, Junae Kim, Tamas Abraham, Rollin Omari, Amar Kaur, Dinh Phung*

**主要类别:** cs.LG

**AI概要:** 本文研究生成个性化中的语义塌缩问题，提出一种无需训练的方法调整预训练嵌入的大小和方向，有效缓解该问题，并提升文本-图像对齐效果。


<details>
  <summary>更多</summary>
  
**动机:** 生成个性化中存在语义塌缩问题，即学习到的视觉概念逐渐偏离其原始文本意义并主导多概念输入提示。这不仅减少了复杂输入提示的语义丰富性，还导致简化的输出图像无法捕捉目标概念。

**方法:** 通过识别无约束优化为根本原因，提出一种无需训练的方法，在推理时调整预训练嵌入的大小和方向，从而有效缓解语义塌缩问题。

**结果:** 该方法适用于不同的个性化方法，在各种用例中显著改善了文本-图像对齐效果。

**结论:** 提出的简单有效方法可以缓解语义塌缩问题，并在不同个性化方法中展现良好的性能，代码已匿名发布。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mitigating+Semantic+Collapse+in+Generative+Personalization+with+a+Surprisingly+Simple+Test-Time+Embedding+Adjustment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22685，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22685&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we investigate the semantic collapsing problem in generative
personalization, an under-explored topic where the learned visual concept
($V^*$) gradually shifts from its original textual meaning and comes to
dominate other concepts in multi-concept input prompts. This issue not only
reduces the semantic richness of complex input prompts like "a photo of $V^*$
wearing glasses and playing guitar" into simpler, less contextually rich forms
such as "a photo of $V^*$" but also leads to simplified output images that fail
to capture the intended concept.
  We identify the root cause as unconstrained optimisation, which allows the
learned embedding $V^*$ to drift arbitrarily in the embedding space, both in
direction and magnitude. To address this, we propose a simple yet effective
training-free method that adjusts the magnitude and direction of pre-trained
embedding at inference time, effectively mitigating the semantic collapsing
problem. Our method is broadly applicable across different personalization
methods and demonstrates significant improvements in text-image alignment in
diverse use cases. Our code is anonymously published at
https://anonymous.4open.science/r/Embedding-Adjustment.

</details>


### [20] [Generalized Linear Mode Connectivity for Transformers](https://arxiv.org/abs/2506.22712)
*Alexander Theus, Alessandro Cabodi, Sotiris Anagnostidis, Antonio Orvieto, Sidak Pal Singh, Valentina Boeva*

**主要类别:** cs.LG

**AI概要:** 理解神经网络损失景观的几何结构是深度学习中的核心问题，其中线性模式连接性（LMC）是一个显著现象。本文提出了一种统一框架，捕捉四种对称类别：置换、半置换、正交变换和一般可逆映射，首次实现了在独立训练的视觉Transformer和GPT-2模型之间发现低障碍和零障碍的线性插值路径。


<details>
  <summary>更多</summary>
  
**动机:** 理解神经网络损失景观的几何结构对于泛化和优化具有重要意义。然而，参数空间中的对称性（如神经元置换）使得功能上等价的模型看起来不同，这掩盖了线性模式连接性（LMC）的现象。现有的研究主要集中在神经元重排上，但这种方法范围有限，无法捕捉现代架构（如Transformer）所表现出的更丰富的对称性。

**方法:** 本文引入了一个统一框架，该框架捕捉了四种对称类别：置换、半置换、正交变换和一般可逆映射。这一框架扩展了有效的重新参数化的集合，并将许多先前的方法作为特殊情况包含在内。

**结果:** 通过该框架，首次发现了独立训练的视觉Transformer和GPT-2模型之间的低障碍和零障碍的线性插值路径。这些结果揭示了损失景观中更深的结构，并强调了对称性感知分析在理解模型空间几何结构中的重要性。

**结论:** 该研究表明，对称性感知分析对于理解神经网络损失景观的几何结构至关重要，而提出的统一框架为未来的研究提供了新的视角和工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generalized+Linear+Mode+Connectivity+for+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22712，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22712&send_immediately=true&force_search=false)

**原文摘要:** Understanding the geometry of neural network loss landscapes is a central
question in deep learning, with implications for generalization and
optimization. A striking phenomenon is linear mode connectivity (LMC), where
independently trained models can be connected by low- or zero-loss paths,
despite appearing to lie in separate loss basins. However, this is often
obscured by symmetries in parameter space -- such as neuron permutations --
which make functionally equivalent models appear dissimilar. Prior work has
predominantly focused on neuron re-ordering through permutations, but such
approaches are limited in scope and fail to capture the richer symmetries
exhibited by modern architectures such as Transformers. In this work, we
introduce a unified framework that captures four symmetry classes:
permutations, semi-permutations, orthogonal transformations, and general
invertible maps -- broadening the set of valid reparameterizations and
subsuming many previous approaches as special cases. Crucially, this
generalization enables, for the first time, the discovery of low- and
zero-barrier linear interpolation paths between independently trained Vision
Transformers and GPT-2 models. These results reveal deeper structure in the
loss landscape and underscore the importance of symmetry-aware analysis for
understanding model space geometry.

</details>


### [21] [Residual Matrix Transformers: Scaling the Size of the Residual Stream](https://arxiv.org/abs/2506.22696)
*Brian Mak, Jeffrey Flanigan*

**主要类别:** cs.LG

**AI概要:** The paper introduces Residual Matrix Transformer (RMT), which replaces the residual stream of transformers with an outer product memory matrix, leading to performance improvements, efficiency gains, and better scaling properties.


<details>
  <summary>更多</summary>
  
**动机:** Transformers use a residual stream for both storing and accessing features. The authors aim to improve this mechanism by replacing it with an outer product memory matrix.

**方法:** The residual stream in transformers is replaced with an outer product memory matrix, forming the Residual Matrix Transformer (RMT). This model is then theoretically analyzed and compared to traditional transformers.

**结果:** RMT offers several advantages: scalable residual stream size independent of compute and model size, achieving same loss as transformers with fewer FLOPS, parameters, and training tokens, and outperforming transformers on downstream tasks.

**结论:** RMT provides more efficient residual stream scaling and improved variance propagation properties compared to standard transformers.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Residual+Matrix+Transformers%3A+Scaling+the+Size+of+the+Residual+Stream，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22696，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22696&send_immediately=true&force_search=false)

**原文摘要:** The residual stream acts as a memory bus where transformer layers both store
and access features (Elhage et al., 2021). We consider changing the mechanism
for retrieving and storing information in the residual stream, and replace the
residual stream of the transformer with an outer product memory matrix
(Kohonen, 1972, Anderson, 1972). We call this model the Residual Matrix
Transformer (RMT). We find that the RMT enjoys a number of attractive
properties: 1) the size of the residual stream can be scaled independently of
compute and model size, improving performance, 2) the RMT can achieve the same
loss as the transformer with 58% fewer FLOPS, 25% fewer parameters, and 41%
fewer training tokens tokens, and 3) the RMT outperforms the transformer on
downstream evaluations. We theoretically analyze the transformer and the RMT,
and show that the RMT allows for more efficient scaling of the residual stream,
as well as improved variance propagation properties. Code for this project can
be found at https://github.com/bmac3/residual-matrix-transformer.

</details>


### [22] [Robust Tensor Completion via Gradient Tensor Nulclear L1-L2 Norm for Traffic Data Recovery](https://arxiv.org/abs/2506.22732)
*Hao Shu, Jicheng Li, Tianyv Lei, Lijun Sun*

**主要类别:** cs.LG

**AI概要:** 提出RTC-GTNLN模型，通过结合梯度张量核L1-L2范数解决交通数据中缺失值和噪声的双重退化问题。


<details>
  <summary>更多</summary>
  
**动机:** 现实场景中的时空交通数据经常受到传感器故障和通信失败导致的缺失值和噪声的双重退化影响，传统的张量补全方法无法有效处理这些问题。

**方法:** 引入了非凸张量秩代理——张量L1-L2范数，并进一步发展为梯度张量L1-L2范数，最终提出了RTC-GTNLN模型。

**结果:** 在多个真实世界交通数据集上的实验表明，RTC-GTNLN模型在复杂恢复场景中始终优于现有最先进方法。

**结论:** RTC-GTNLN模型能够充分利用全局低秩性和局部一致性，有效应对交通数据的双重退化挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robust+Tensor+Completion+via+Gradient+Tensor+Nulclear+L1-L2+Norm+for+Traffic+Data+Recovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22732，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22732&send_immediately=true&force_search=false)

**原文摘要:** In real-world scenarios, spatiotemporal traffic data frequently experiences
dual degradation from missing values and noise caused by sensor malfunctions
and communication failures. Therefore, effective data recovery methods are
essential to ensure the reliability of downstream data-driven applications.
while classical tensor completion methods have been widely adopted, they are
incapable of modeling noise, making them unsuitable for complex scenarios
involving simultaneous data missingness and noise interference. Existing Robust
Tensor Completion (RTC) approaches offer potential solutions by separately
modeling the actual tensor data and noise. However, their effectiveness is
often constrained by the over-relaxation of convex rank surrogates and the
suboptimal utilization of local consistency, leading to inadequate model
accuracy. To address these limitations, we first introduce the tensor L1-L2
norm, a novel non-convex tensor rank surrogate that functions as an effective
low-rank representation tool. Leveraging an advanced feature fusion strategy,
we further develop the gradient tensor L1-L2 norm by incorporating the tensor
L1-L2 norm in the gradient domain. By integrating the gradient tensor nuclear
L1-L2 norm into the RTC framework, we propose the Robust Tensor Completion via
Gradient Tensor Nuclear L1-L2 Norm (RTC-GTNLN) model, which not only fully
exploits both global low-rankness and local consistency without trade-off
parameter, but also effectively handles the dual degradation challenges of
missing data and noise in traffic data. Extensive experiments conducted on
multiple real-world traffic datasets demonstrate that the RTC-GTNLN model
consistently outperforms existing state-of-the-art methods in complex recovery
scenarios involving simultaneous missing values and noise.

</details>


### [23] [FairMarket-RL: LLM-Guided Fairness Shaping for Multi-Agent Reinforcement Learning in Peer-to-Peer Markets](https://arxiv.org/abs/2506.22708)
*Shrenik Jadhav, Birva Sevak, Srijita Das, Akhtar Hussain, Wencong Su, Van-Hai Bui*

**主要类别:** cs.LG

**AI概要:** 提出FairMarket-RL框架，结合大语言模型和强化学习，在P2P微电网交易中实现公平性感知的交易代理。通过公平性评估和自适应奖励调整机制，提升买家需求满足率、卖家利润公平性和整体交易公平性。该框架在扩展到大型电力分配系统时仍保持高效和适用性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的P2P交易机制缺乏确保公平性的稳健框架，特别是在多买家和多卖家的情况下。需要一种新方法来实现更公平的市场交易。

**方法:** 设计了FairMarket-RL框架，使用大语言模型作为实时公平性评估器，并通过强化学习训练交易代理。引入了两个公平性指标（FTB和FBS），并通过λ系数将这些指标整合到代理奖励中。采用独立近端策略优化（IPPO）进行代理训练。

**结果:** 代理能够满足超过90%的买家需求，维持卖家间的公平利润，并达到高于0.80的FTB和FBS分数。同时，公平性反馈提高了收敛速度，减少了买家短缺并缩小了卖家利润差异。

**结论:** FairMarket-RL提供了一种可扩展且以公平性为导向的解决方案，适用于去中心化能源系统的自主交易。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FairMarket-RL%3A+LLM-Guided+Fairness+Shaping+for+Multi-Agent+Reinforcement+Learning+in+Peer-to-Peer+Markets，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22708，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22708&send_immediately=true&force_search=false)

**原文摘要:** Peer-to-peer (P2P) trading is increasingly recognized as a key mechanism for
decentralized market regulation, yet existing approaches often lack robust
frameworks to ensure fairness. This paper presents FairMarket-RL, a novel
hybrid framework that combines Large Language Models (LLMs) with Reinforcement
Learning (RL) to enable fairness-aware trading agents. In a simulated P2P
microgrid with multiple sellers and buyers, the LLM acts as a real-time
fairness critic, evaluating each trading episode using two metrics:
Fairness-To-Buyer (FTB) and Fairness-Between-Sellers (FBS). These fairness
scores are integrated into agent rewards through scheduled
{\lambda}-coefficients, forming an adaptive LLM-guided reward shaping loop that
replaces brittle, rule-based fairness constraints. Agents are trained using
Independent Proximal Policy Optimization (IPPO) and achieve equitable outcomes,
fulfilling over 90% of buyer demand, maintaining fair seller margins, and
consistently reaching FTB and FBS scores above 0.80. The training process
demonstrates that fairness feedback improves convergence, reduces buyer
shortfalls, and narrows profit disparities between sellers. With its
language-based critic, the framework scales naturally, and its extension to a
large power distribution system with household prosumers illustrates its
practical applicability. FairMarket-RL thus offers a scalable, equity-driven
solution for autonomous trading in decentralized energy systems.

</details>


### [24] [Kernel Outlier Detection](https://arxiv.org/abs/2506.22994)
*Can Hakan Dağıdır, Mia Hubert, Peter J. Rousseeuw*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的异常检测方法——核异常检测（KOD），用于解决高维场景下的异常检测问题。该方法通过核变换和投影寻优，并引入新方向集合和结果组合方式，克服现有方法的局限性。实验结果表明KOD在大小数据集上均表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 现有的异常检测方法在高维场景下存在挑战，如依赖分布假设或难以调整的超参数等问题。

**方法:** KOD方法首先进行核变换，然后采用投影寻优方法。其创新点包括新的方向集合搜索以及结合不同类型方向结果的新方法。

**结果:** 在三个具有挑战性结构的小数据集和四个大型基准数据集上的实证评估表明，KOD方法在异常检测方面是有效的。

**结论:** KOD在小数据集和大数据集上的表现都很好，证明了其在高维异常检测中的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Kernel+Outlier+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22994，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22994&send_immediately=true&force_search=false)

**原文摘要:** A new anomaly detection method called kernel outlier detection (KOD) is
proposed. It is designed to address challenges of outlier detection in
high-dimensional settings. The aim is to overcome limitations of existing
methods, such as dependence on distributional assumptions or on hyperparameters
that are hard to tune. KOD starts with a kernel transformation, followed by a
projection pursuit approach. Its novelties include a new ensemble of directions
to search over, and a new way to combine results of different direction types.
This provides a flexible and lightweight approach for outlier detection. Our
empirical evaluations illustrate the effectiveness of KOD on three small
datasets with challenging structures, and on four large benchmark datasets.

</details>


### [25] [BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute](https://arxiv.org/abs/2506.22716)
*Dujian Ding, Ankur Mallick, Shaokun Zhang, Chi Wang, Daniel Madrigal, Mirian Del Carmen Hipolito Garcia, Menglin Xia, Laks V. S. Lakshmanan, Qingyun Wu, Victor Rühle*

**主要类别:** cs.LG

**AI概要:** BEST-Route是一种新的路由框架，它根据查询难度和质量阈值选择模型以及从中采样的响应数量，从而在实际数据集上最多降低成本60%，性能下降不到1%。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLMs）虽然功能强大，但部署成本高。现有的查询路由方法虽然可以动态分配查询以平衡成本和质量，但由于小型模型生成的单一响应质量不足以与大型模型媲美，导致大型模型被过度使用，错失了潜在的成本节约。然而，已知通过生成多个响应并选择最佳结果，小型模型可以在保持低成本的同时提高质量。

**方法:** 提出了一种名为BEST-Route的新颖路由框架，该框架根据查询难度和质量阈值来选择模型以及从所选模型中采样的响应数量。对于较难的查询，可能会选择大型模型或从小型模型中生成更多响应；对于简单的查询，则可能选择小型模型并生成较少响应。

**结果:** 在真实世界的数据集上的实验表明，BEST-Route方法能够将成本降低多达60%，同时性能下降不到1%。

**结论:** BEST-Route提供了一种有效的方法来平衡LLM应用中的成本和性能，通过灵活地选择模型和响应数量，显著降低了运行成本，同时仅带来轻微的性能损失。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BEST-Route%3A+Adaptive+LLM+Routing+with+Test-Time+Optimal+Compute，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22716，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22716&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are powerful tools but are often expensive to
deploy at scale. LLM query routing mitigates this by dynamically assigning
queries to models of varying cost and quality to obtain a desired trade-off.
Prior query routing approaches generate only one response from the selected
model and a single response from a small (inexpensive) model was often not good
enough to beat a response from a large (expensive) model due to which they end
up overusing the large model and missing out on potential cost savings.
However, it is well known that for small models, generating multiple responses
and selecting the best can enhance quality while remaining cheaper than a
single large-model response. We leverage this idea to propose BEST-Route, a
novel routing framework that chooses a model and the number of responses to
sample from it based on query difficulty and the quality thresholds.
Experiments on real-world datasets demonstrate that our method reduces costs by
up to 60% with less than 1% performance drop.

</details>


### [26] [Feature-Wise Mixing for Mitigating Contextual Bias in Predictive Supervised Learning](https://arxiv.org/abs/2506.23033)
*Yash Vardhan Tomar*

**主要类别:** cs.LG

**AI概要:** A feature-wise mixing framework is introduced to mitigate contextual bias in ML models, achieving significant bias reduction and improved performance without needing explicit bias attribute identification.


<details>
  <summary>更多</summary>
  
**动机:** Bias in predictive machine learning models leads to skewed or unfair outcomes. Current mitigation strategies either rely on post-hoc corrections or rigid constraints which can limit scalability and generalizability.

**方法:** Introduced a feature-wise mixing framework that redistributes feature representations across multiple contextual datasets. Assessed effectiveness by training four ML classifiers using cross-validation and evaluating with bias-sensitive loss functions including disparity metrics and MSE.

**结果:** Achieved an average bias reduction of 43.35% and a statistically significant decrease in MSE across all classifiers trained on mixed datasets. Outperformed SMOTE oversampling and demonstrated competitive effectiveness without requiring explicit bias attribute identification.

**结论:** Feature-wise mixing efficiently mitigates contextual bias without the computational overhead typically associated with fairness-aware learning algorithms. Suggests future application in real-world fields for accurate predictions.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Feature-Wise+Mixing+for+Mitigating+Contextual+Bias+in+Predictive+Supervised+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23033，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23033&send_immediately=true&force_search=false)

**原文摘要:** Bias in predictive machine learning (ML) models is a fundamental challenge
due to the skewed or unfair outcomes produced by biased models. Existing
mitigation strategies rely on either post-hoc corrections or rigid constraints.
However, emerging research claims that these techniques can limit scalability
and reduce generalizability. To address this, this paper introduces a
feature-wise mixing framework to mitigate contextual bias. This was done by
redistributing feature representations across multiple contextual datasets. To
assess feature-wise mixing's effectiveness, four ML classifiers were trained
using cross-validation and evaluated with bias-sensitive loss functions,
including disparity metrics and mean squared error (MSE), which served as a
standard measure of predictive performance. The proposed method achieved an
average bias reduction of 43.35% and a statistically significant decrease in
MSE across all classifiers trained on mixed datasets. Additionally,
benchmarking against established bias mitigation techniques found that
feature-wise mixing consistently outperformed SMOTE oversampling and
demonstrated competitive effectiveness without requiring explicit bias
attribute identification. Feature-wise mixing efficiently avoids the
computational overhead typically associated with fairness-aware learning
algorithms. Future work could explore applying feature-wise mixing for
real-world fields where accurate predictions are necessary.

</details>


### [27] [Efficient Algorithms for Learning and Compressing Monophonic Halfspaces in Graphs](https://arxiv.org/abs/2506.23186)
*Marco Bressan, Victor Chepoi, Emmanuel Esposito, Maximilian Thiessen*

**主要类别:** cs.LG

**AI概要:** 本论文探讨了图的顶点上的凸性概念及其对应的半空间概念，特别是在机器学习领域中受到关注的单调半空间。作者提出了一种基于2-可满足性（2-SAT）的分解定理，并利用该定理开发了高效且接近最优的学习算法，解决了教学、主动和在线学习等问题。此外，还提出了一个有效的样本压缩方案，使单调半空间在实际PAC设定下能够被正确学习。结果表明，与测地线半空间相比，这些问题对单调半空间来说是高效的，而对测地线半空间来说大多是NP难的。


<details>
  <summary>更多</summary>
  
**动机:** 研究图的顶点上凸性和半空间的概念，特别是通过闭合诱导路径定义的单调半空间，以期解决机器学习中的各类学习问题。

**方法:** 1. 提出基于2-可满足性的分解定理，用于表示单调半空间。
2. 利用该分解定理设计针对教学、主动和在线学习等任务的有效算法。
3. 提出一种有效的样本压缩方案，确保学习过程稳定且正确。

**结果:** 1. 开发了多项式时间算法以最小化经验风险。
2. 实现了在实际PAC设定下的有效学习，错误率线性为$1/\varepsilon$。
3. 与测地线半空间形成对比，展示了单调半空间学习问题的高效性。

**结论:** 单调半空间可以通过2-可满足性分解定理进行有效学习，解决了多个开放问题，并在多项式时间内实现了误差最小化。与测地线半空间相比，单调半空间的学习问题是高效的。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Algorithms+for+Learning+and+Compressing+Monophonic+Halfspaces+in+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23186，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23186&send_immediately=true&force_search=false)

**原文摘要:** Abstract notions of convexity over the vertices of a graph, and corresponding
notions of halfspaces, have recently gained attention from the machine learning
community. In this work we study monophonic halfspaces, a notion of graph
halfspaces defined through closure under induced paths. Our main result is a
$2$-satisfiability based decomposition theorem, which allows one to represent
monophonic halfspaces as a disjoint union of certain vertex subsets. Using this
decomposition, we achieve efficient and (nearly) optimal algorithms for various
learning problems, such as teaching, active, and online learning. Most notably,
we obtain a polynomial-time algorithm for empirical risk minimization.
Independently of the decomposition theorem, we obtain an efficient, stable, and
proper sample compression scheme. This makes monophonic halfspaces efficiently
learnable with proper learners and linear error rate $1/\varepsilon$ in the
realizable PAC setting. Our results answer open questions from the literature,
and show a stark contrast with geodesic halfspaces, for which most of the said
learning problems are NP-hard.

</details>


### [28] [FF-INT8: Efficient Forward-Forward DNN Training on Edge Devices with INT8 Precision](https://arxiv.org/abs/2506.22771)
*Jingxiao Ma, Priyadarshini Panda, Sherief Reda*

**主要类别:** cs.LG

**AI概要:** 提出了一种基于INT8量化和Forward-Forward(FF)算法的训练方法，该方法通过逐层策略稳定梯度量化，并引入了前瞻方案提升模型精度。实验表明，与现有方法相比，此方法在NVIDIA Jetson Orin Nano板上训练速度快4.6%，能耗降低8.3%，内存使用减少27.0%，同时保持较高的精度。


<details>
  <summary>更多</summary>
  
**动机:** 反向传播（Backpropagation）是神经网络训练的核心技术，但由于其时间和能源消耗效率低的问题，在资源受限的边缘设备上的适用性受到限制。虽然低精度神经网络量化已经被广泛研究以加速模型推理，但在训练中的应用较少。Forward-Forward (FF) 算法作为一种替代反向传播的新方法，通过用额外的前向传递代替后向传递来减少内存占用，非常适合嵌入式设备。

**方法:** 本论文提出了一种结合INT8量化的训练方法，利用FF算法的逐层策略来稳定梯度量化。此外，还提出了一个新颖的“前瞻”方案，用于解决FF算法的局限性并提高模型的准确性。

**结果:** 在NVIDIA Jetson Orin Nano板上进行的实验显示，所提出的方法比现有技术快4.6%，节省8.3%的能量，并将内存使用减少了27.0%，同时保持了与最先进的方法相当的精度。

**结论:** 提出的INT8量化训练方法结合FF算法及其前瞻方案，成功地提高了训练速度、降低了能耗和内存使用，同时维持了高精度，为资源受限设备提供了一种有效的训练解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FF-INT8%3A+Efficient+Forward-Forward+DNN+Training+on+Edge+Devices+with+INT8+Precision，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22771，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22771&send_immediately=true&force_search=false)

**原文摘要:** Backpropagation has been the cornerstone of neural network training for
decades, yet its inefficiencies in time and energy consumption limit its
suitability for resource-constrained edge devices. While low-precision neural
network quantization has been extensively researched to speed up model
inference, its application in training has been less explored. Recently, the
Forward-Forward (FF) algorithm has emerged as a promising alternative to
backpropagation, replacing the backward pass with an additional forward pass.
By avoiding the need to store intermediate activations for backpropagation, FF
can reduce memory footprint, making it well-suited for embedded devices. This
paper presents an INT8 quantized training approach that leverages FF's
layer-by-layer strategy to stabilize gradient quantization. Furthermore, we
propose a novel "look-ahead" scheme to address limitations of FF and improve
model accuracy. Experiments conducted on NVIDIA Jetson Orin Nano board
demonstrate 4.6% faster training, 8.3% energy savings, and 27.0% reduction in
memory usage, while maintaining competitive accuracy compared to the
state-of-the-art.

</details>


### [29] [Not All Explanations for Deep Learning Phenomena Are Equally Valuable](https://arxiv.org/abs/2506.23286)
*Alan Jeffares, Mihaela van der Schaar*

**主要类别:** cs.LG

**AI概要:** 近年来，深度学习研究中很大一部分致力于理解令人惊讶或反直觉的现象，如双重下降、grokking和彩票假设等。然而，这些现象在实际应用中的证据很少，因此不应将它们视为需要定制解决方案的孤立谜题。尽管如此，这些现象仍具有研究价值，因为它们提供了改进更广泛的深度学习原则解释理论的独特环境。本文通过分析几个显著例子的研究成果，重新审视了当前研究社区处理这些问题的规范，并提出了未来研究的实际建议，以确保深度学习现象的研究进展与整个深度学习领域的最终实用目标保持一致。


<details>
  <summary>更多</summary>
  
**动机:** 理解深度学习中的反直觉现象（例如双重下降、grokking和彩票假设）已成为该领域的重要部分。然而，许多这些现象可能并未在现实世界应用中广泛出现，专注于这些孤立现象可能效率低下，不利于推动整个深度学习领域的发展。

**方法:** 本文回顾并分析了近期文献中几个显著的深度学习现象的研究结果。通过对这些现象的研究背景、方法及结论进行综合评估，提出了一种新的视角：不将这些现象视为孤立问题，而是将其作为改进深度学习一般性原则解释理论的独特场景。此外，文章还重新审视了研究社区当前处理这些问题的规范，并提出了未来研究的建议。

**结果:** 研究表明，许多深度学习现象在现实应用中缺乏充分证据支持，因此不应过度关注这些现象的独立解释。相反，应利用这些现象提供的独特场景来改进对深度学习更广泛原则的理解。这有助于确保研究方向与整个深度学习领域的实际目标保持一致。

**结论:** 本文认为，深度学习现象的研究应超越对其孤立解释的关注，转而聚焦于如何通过这些现象改进对深度学习普遍原则的理解。同时，作者提出了未来研究的具体建议，以确保研究努力与深度学习领域的实际目标相一致。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Not+All+Explanations+for+Deep+Learning+Phenomena+Are+Equally+Valuable，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23286，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23286&send_immediately=true&force_search=false)

**原文摘要:** Developing a better understanding of surprising or counterintuitive phenomena
has constituted a significant portion of deep learning research in recent
years. These include double descent, grokking, and the lottery ticket
hypothesis -- among many others. Works in this area often develop ad hoc
hypotheses attempting to explain these observed phenomena on an isolated,
case-by-case basis. This position paper asserts that, in many prominent cases,
there is little evidence to suggest that these phenomena appear in real-world
applications and these efforts may be inefficient in driving progress in the
broader field. Consequently, we argue against viewing them as isolated puzzles
that require bespoke resolutions or explanations. However, despite this, we
suggest that deep learning phenomena do still offer research value by providing
unique settings in which we can refine our broad explanatory theories of more
general deep learning principles. This position is reinforced by analyzing the
research outcomes of several prominent examples of these phenomena from the
recent literature. We revisit the current norms in the research community in
approaching these problems and propose practical recommendations for future
research, aiming to ensure that progress on deep learning phenomena is well
aligned with the ultimate pragmatic goal of progress in the broader field of
deep learning.

</details>


### [30] [Multimodal Atmospheric Super-Resolution With Deep Generative Models](https://arxiv.org/abs/2506.22780)
*Dibyajyoti Chakraborty, Haiwen Guan, Jason Stock, Troy Arcomano, Guido Cervone, Romit Maulik*

**主要类别:** cs.LG

**AI概要:** Score-based diffusion modeling is used for super-resolution of high-dimensional dynamical systems, enabling zero-shot conditioning and accurate recovery from low-fidelity measurements.


<details>
  <summary>更多</summary>
  
**动机:** To apply score-based diffusion models for the super-resolution of high-dimensional dynamical systems using real-time low-resolution and sparse sensor data.

**方法:** Learn a score function (gradient of log-probability density) and reverse a noising process to generate new samples and enable zero-shot conditioning. Update implicitly learned distributions with online data in a Bayesian framework.

**结果:** Accurate recovery of high-dimensional states from multiple low-fidelity measurement sources; generative model balances influence of different dataset modalities during reconstructions.

**结论:** Score-based diffusion models show promise for super-resolution tasks, enabling fusion of data and models in a novel paradigm.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multimodal+Atmospheric+Super-Resolution+With+Deep+Generative+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22780，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22780&send_immediately=true&force_search=false)

**原文摘要:** Score-based diffusion modeling is a generative machine learning algorithm
that can be used to sample from complex distributions. They achieve this by
learning a score function, i.e., the gradient of the log-probability density of
the data, and reversing a noising process using the same. Once trained,
score-based diffusion models not only generate new samples but also enable
zero-shot conditioning of the generated samples on observed data. This promises
a novel paradigm for data and model fusion, wherein the implicitly learned
distributions of pretrained score-based diffusion models can be updated given
the availability of online data in a Bayesian formulation. In this article, we
apply such a concept to the super-resolution of a high-dimensional dynamical
system, given the real-time availability of low-resolution and experimentally
observed sparse sensor measurements from multimodal data. Additional analysis
on how score-based sampling can be used for uncertainty estimates is also
provided. Our experiments are performed for a super-resolution task that
generates the ERA5 atmospheric dataset given sparse observations from a
coarse-grained representation of the same and/or from unstructured experimental
observations of the IGRA radiosonde dataset. We demonstrate accurate recovery
of the high dimensional state given multiple sources of low-fidelity
measurements. We also discover that the generative model can balance the
influence of multiple dataset modalities during spatiotemporal reconstructions.

</details>


### [31] [Training of Spiking Neural Networks with Expectation-Propagation](https://arxiv.org/abs/2506.23757)
*Dan Yao, Steve McLaughlin, Yoann Altmann*

**主要类别:** cs.LG

**AI概要:** 提出了一种用于训练脉冲神经网络（SNNs）的统一消息传递框架，使用期望传播方法，无需梯度即可学习网络参数的边缘分布，并同时边缘化诸如隐藏层输出等干扰参数。此框架首次能够训练离散和连续权重的确定性和随机性SNNs，且在实际应用中比基于梯度的方法收敛更快，无需多次遍历训练数据。


<details>
  <summary>更多</summary>
  
**动机:** 当前缺乏一种统一的、高效的训练脉冲神经网络的方法，尤其是对于包含离散和连续权重的网络以及确定性和随机性网络。需要一种新方法来提高训练效率并减少对大量训练数据的需求。

**方法:** 提出了一种基于期望传播的消息传递框架，该方法无需梯度计算，可同时学习网络参数的边缘分布并边缘化干扰参数。适用于离散和连续权重的确定性和随机性SNNs，利用训练样本批次进行训练。

**结果:** 实验结果表明，该方法在分类和回归任务上表现良好，为深度贝叶斯网络的高效训练方法开辟了新的途径。相较于基于梯度的方法，其在实践中收敛速度更快。

**结论:** 所提出的无梯度期望传播框架为训练脉冲神经网络提供了一种新的有效方法，尤其适用于包含离散和连续权重的确定性和随机性网络。尽管算法的收敛性尚未完全保证，但其在实际应用中的表现优于基于梯度的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Training+of+Spiking+Neural+Networks+with+Expectation-Propagation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23757，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23757&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we propose a unifying message-passing framework for training
spiking neural networks (SNNs) using Expectation-Propagation. Our gradient-free
method is capable of learning the marginal distributions of network parameters
and simultaneously marginalizes nuisance parameters, such as the outputs of
hidden layers. This framework allows for the first time, training of discrete
and continuous weights, for deterministic and stochastic spiking networks,
using batches of training samples. Although its convergence is not ensured, the
algorithm converges in practice faster than gradient-based methods, without
requiring a large number of passes through the training data. The
classification and regression results presented pave the way for new efficient
training methods for deep Bayesian networks.

</details>


### [32] [Riemannian-Geometric Fingerprints of Generative Models](https://arxiv.org/abs/2506.22802)
*Hae Jin Song, Laurent Itti*

**主要类别:** cs.LG

**AI概要:** 生成模型(GMs)的归属和指纹问题日益重要，但理解其指纹仍存在差距。本文通过黎曼几何提出了一种新的工件和指纹定义方法，推广了先前的工作，并提出了基于梯度的算法来计算指纹。实验结果表明，该方法在区分不同GMs、数据集、模型架构和模态方面表现出色，显著提高了模型归属的性能和泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 生成模型的归属和指纹对于保护知识产权、验证生成内容来源以及区分合成与人类数据至关重要。然而，目前对生成模型指纹的理解存在不足，缺乏一个正式的框架来定义、表示和分析这些指纹。

**方法:** 采用几何方法，使用黎曼几何提出生成模型工件和指纹的新定义。通过从数据中学习黎曼度量，用测地线距离和基于kNN的黎曼质心取代欧几里得距离和最近邻搜索，将先前工作推广到非欧几里得流形。并开发了一个基于梯度的算法用于实际计算指纹。

**结果:** 该方法能够有效区分大量不同的生成模型，涵盖4个不同数据集、27种模型架构和2种模态。使用所提定义显著提高了模型归属性能，并在未见数据集、模型类型和模态上表现出良好的泛化能力。

**结论:** 提出的基于黎曼几何的生成模型指纹定义及其计算方法，在模型归属和泛化方面表现优异，具有实际应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Riemannian-Geometric+Fingerprints+of+Generative+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22802，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22802&send_immediately=true&force_search=false)

**原文摘要:** Recent breakthroughs and rapid integration of generative models (GMs) have
sparked interest in the problem of model attribution and their fingerprints.
For instance, service providers need reliable methods of authenticating their
models to protect their IP, while users and law enforcement seek to verify the
source of generated content for accountability and trust. In addition, a
growing threat of model collapse is arising, as more model-generated data are
being fed back into sources (e.g., YouTube) that are often harvested for
training ("regurgitative training"), heightening the need to differentiate
synthetic from human data. Yet, a gap still exists in understanding generative
models' fingerprints, we believe, stemming from the lack of a formal framework
that can define, represent, and analyze the fingerprints in a principled way.
To address this gap, we take a geometric approach and propose a new definition
of artifact and fingerprint of GMs using Riemannian geometry, which allows us
to leverage the rich theory of differential geometry. Our new definition
generalizes previous work (Song et al., 2024) to non-Euclidean manifolds by
learning Riemannian metrics from data and replacing the Euclidean distances and
nearest-neighbor search with geodesic distances and kNN-based Riemannian center
of mass. We apply our theory to a new gradient-based algorithm for computing
the fingerprints in practice. Results show that it is more effective in
distinguishing a large array of GMs, spanning across 4 different datasets in 2
different resolutions (64 by 64, 256 by 256), 27 model architectures, and 2
modalities (Vision, Vision-Language). Using our proposed definition
significantly improves the performance on model attribution, as well as a
generalization to unseen datasets, model types, and modalities, suggesting its
practical efficacy.

</details>


### [33] [Faster Diffusion Models via Higher-Order Approximation](https://arxiv.org/abs/2506.24042)
*Gen Li, Yuchen Zhou, Yuting Wei, Yuxin Chen*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Faster+Diffusion+Models+via+Higher-Order+Approximation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.24042，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.24042&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we explore provable acceleration of diffusion models without
any additional retraining. Focusing on the task of approximating a target data
distribution in $\mathbb{R}^d$ to within $\varepsilon$ total-variation
distance, we propose a principled, training-free sampling algorithm that
requires only the order of
  $$ d^{1+2/K} \varepsilon^{-1/K} $$
  score function evaluations (up to log factor) in the presence of accurate
scores, where $K$ is an arbitrarily large fixed integer. This result applies to
a broad class of target data distributions, without the need for assumptions
such as smoothness or log-concavity. Our theory is robust vis-a-vis inexact
score estimation, degrading gracefully as the score estimation error increases
-- without demanding higher-order smoothness on the score estimates as assumed
in previous work. The proposed algorithm draws insight from high-order ODE
solvers, leveraging high-order Lagrange interpolation and successive refinement
to approximate the integral derived from the probability flow ODE.

</details>


### [34] [BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters](https://arxiv.org/abs/2506.22809)
*Cooper Doyle*

**主要类别:** cs.LG

**AI概要:** 提出BayesLoRA框架，结合MC-Dropout与LoRA适配器，以提供任务特定的不确定性量化及可靠置信估计，助力代理决策。


<details>
  <summary>更多</summary>
  
**动机:** 现有的transformer不确定性方法通常是通用的，缺乏针对下游任务工作流的定制化解决方案。这使得在不确定性下进行自省和行为调节变得困难。

**方法:** 提出了一种名为BayesLoRA的任务特定不确定性量化框架，该框架将MC-Dropout整合到低秩适配器（LoRA）中。通过这种方式，BayesLoRA为下游工作流提供了定制化的保障。

**结果:** 理论上和实证上证明了LoRA适配器在微调分布之外表现出放大方差的特性，从而为代理决策提供了可靠的置信估计。

**结论:** BayesLoRA提供了一种有效的方法来处理特定任务的不确定性，并有助于代理在不确定性下的决策过程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BayesLoRA%3A+Task-Specific+Uncertainty+in+Low-Rank+Adapters，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22809，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22809&send_immediately=true&force_search=false)

**原文摘要:** We propose BayesLoRA, a task-specific uncertainty quantification framework
that integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike
general-purpose transformer uncertainty methods, BayesLoRA provides guardrails
tailored to downstream workflows, enabling agents to introspect and modulate
behavior under uncertainty. We demonstrate mathematically and empirically that
LoRA adapters exhibit amplified variance outside fine-tuning distributions,
yielding reliable confidence estimates for agentic decision-making.

</details>


### [35] [Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime](https://arxiv.org/abs/2506.24120)
*Yuqing Wang, Shangding Gu*

**主要类别:** cs.LG

**AI概要:** 选择更均匀分布的数据可以提高训练效率并增强模型性能，尤其是在复杂任务中。论文通过理论和实验验证了最大化数据点间最小距离（h_min）可加速梯度下降动态过程，并减少神经网络的近似误差。


<details>
  <summary>更多</summary>
  
**动机:** 尽管数据质量和多样性已被广泛研究以提升模型性能，但是否存在其他定量且通用的数据选择原则尚不明确，特别是在先验知识有限的复杂任务中。

**方法:** 1. 提出选择更均匀分布的数据的方法，基于数据点间最小距离(h_min)的指标。
2. 理论证明：较小的h_min会减慢梯度下降训练动态，而较大的h_min可减少神经网络的近似误差。
3. 提供超越神经切线核(NTK)机制的收敛框架，适用于包括Transformer在内的多种架构。
4. 通过监督微调进行综合实验，涵盖不同优化策略、模型大小和训练数据集。

**结果:** 实验结果表明，通过最大化数据点间的成对距离选择数据显著加速训练过程，并在各类数据集中实现相当或更好的LLM性能。

**结论:** 选择更均匀分布的数据能有效提升训练效率与模型性能，为复杂任务中的数据选择提供了新的理论依据和实践方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Data+Uniformity+Improves+Training+Efficiency+and+More%2C+with+a+Convergence+Framework+Beyond+the+NTK+Regime，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.24120，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.24120&send_immediately=true&force_search=false)

**原文摘要:** Data selection plays a crucial role in data-driven decision-making, including
in large language models (LLMs), and is typically task-dependent. Properties
such as data quality and diversity have been extensively studied and are known
to enhance model performance. However, it remains unclear whether there exist
other quantitative and general principles of data selection that can
consistently improve performance, especially for complex tasks with limited
prior knowledge. In this paper, we demonstrate that selecting more uniformly
distributed data can improve training efficiency while enhancing performance.
Specifically, we establish that more uniform (less biased) distribution leads
to a larger minimum pairwise distance between data points, denoted by
$h_{\min}$, and prove that a smaller $h_{\min}$ can slow down the training
dynamics of gradient descent (GD). Moreover, we theoretically show that the
approximation error of neural networks decreases as $h_{\min}$ increases. Our
analysis introduces a convergence framework for GD beyond the Neural Tangent
Kernel (NTK) regime, applicable to a broad class of architectures, including
transformers, without requiring Lipschitz smoothness. This framework further
provides theoretical justification for the use of residual connections and
function compositions in deep neural architectures. In the end, we conduct
comprehensive experiments for supervised fine-tuning across various settings,
including different optimization strategies, model sizes, and training
datasets. The results consistently demonstrate that selecting data by
maximizing pairwise distance significantly accelerates training and achieves
comparable or better performance in LLMs across diverse datasets. Code and
Datasets are available at the link:
https://github.com/SafeRL-Lab/data-uniformity.

</details>


### [36] [Deep learning 40 years of human migration](https://arxiv.org/abs/2506.22821)
*Thomas Gaskin, Guy J. Abel*

**主要类别:** cs.LG

**AI概要:** 作者提出了一种新的详细数据集，涵盖了1990年至今230个国家和地区的年度迁徙流动和存量。通过深度递归神经网络，结合18个协变量（包括地理、经济、文化、社会和政治信息），估计了迁移模式，并按出生国进一步细分。该模型能够捕捉长期时间相关性，并提供了所有估计值的置信区间。与传统方法相比，该方法在五年流估算上表现显著更优，同时提高了时间分辨率。此外，该模型完全开源，为未来的人类迁移研究提供了宝贵资源。


<details>
  <summary>更多</summary>
  
**动机:** 当前关于国际迁移的数据缺乏全面性和高时间分辨率，特别是在细化到出生国的迁移流动方面。为了填补这一空白并改进现有估算方法，作者开发了一个基于深度递归神经网络的新模型来分析和预测迁移模式。

**方法:** 作者使用深度递归神经网络训练模型，输入18个协变量（包括地理、经济、文化、社会和政治信息），以学习迁移模式。模型通过其递归架构考虑了历史数据对当前迁移模式的影响，从而捕捉长期时间相关性。通过训练神经网络集合，并对协变量引入不确定性传播，生成所有估计值的置信区间。

**结果:** 该模型在多个未见数据测试集中验证，结果表明它显著优于传统方法，尤其是在五年的迁移流估算上，同时提供了更高的时间分辨率。置信区间的提供有助于识别需要额外数据收集的地理区域。

**结论:** 本研究提出了一个全面且高时间分辨率的迁移数据集及模型，显著改进了迁移流的估算方法。模型及其训练数据、代码完全开源，为未来人类迁移研究提供了重要资源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep+learning+40+years+of+human+migration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22821，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22821&send_immediately=true&force_search=false)

**原文摘要:** We present a novel and detailed dataset on origin-destination annual
migration flows and stocks between 230 countries and regions, spanning the
period from 1990 to the present. Our flow estimates are further disaggregated
by country of birth, providing a comprehensive picture of migration over the
last 43 years. The estimates are obtained by training a deep recurrent neural
network to learn flow patterns from 18 covariates for all countries, including
geographic, economic, cultural, societal, and political information. The
recurrent architecture of the neural network means that the entire past can
influence current migration patterns, allowing us to learn long-range temporal
correlations. By training an ensemble of neural networks and additionally
pushing uncertainty on the covariates through the trained network, we obtain
confidence bounds for all our estimates, allowing researchers to pinpoint the
geographic regions most in need of additional data collection. We validate our
approach on various test sets of unseen data, demonstrating that it
significantly outperforms traditional methods estimating five-year flows while
delivering a significant increase in temporal resolution. The model is fully
open source: all training data, neural network weights, and training code are
made public alongside the migration estimates, providing a valuable resource
for future studies of human migration.

</details>


### [37] [xLSTMAD: A Powerful xLSTM-based Method for Anomaly Detection](https://arxiv.org/abs/2506.22837)
*Kamil Faber, Marcin Pietroń, Dominik Żurek, Roberto Corizzo*

**主要类别:** cs.LG

**AI概要:** 提出xLSTMAD，首个用于多变量时间序列异常检测的全编码器-解码器xLSTM架构方法。通过预测（xLSTMAD-F）和重建（xLSTMAD-R）两种方式，并结合MSE和SoftDTW损失函数评估模型性能。在TSB-AD-M基准上，xLSTM展现出超越23个现有基线的卓越准确性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管xLSTM模型在长时间序列预测、无损压缩及大规模语言建模任务中表现出色，但尚未有研究将其应用于异常检测领域。因此，本文旨在填补这一空白，探索xLSTM在异常检测中的潜力。

**方法:** 构建了xLSTMAD，一种专为多变量时间序列数据设计的异常检测方法。该方法采用完整的编码器-解码器xLSTM架构，其中编码器捕捉历史上下文，解码器分为两种变体：预测未来值（xLSTMAD-F）和重建输入时间序列（xLSTMAD-R）。同时，使用均方误差（MSE）和软动态时间规整（SoftDTW）两种损失函数来分别考虑局部重建保真度和全局序列对齐。

**结果:** 在包含17个真实世界数据集的TSB-AD-M基准上进行评估，xLSTMAD在VUS-PR等先进指标下展现了超越23种流行异常检测基线的最先进准确性。

**结论:** 本文首次揭示了xLSTM在异常检测方面的强大建模能力，为该领域的进一步发展奠定了基础。代码已公开。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是xLSTMAD%3A+A+Powerful+xLSTM-based+Method+for+Anomaly+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22837，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22837&send_immediately=true&force_search=false)

**原文摘要:** The recently proposed xLSTM is a powerful model that leverages expressive
multiplicative gating and residual connections, providing the temporal capacity
needed for long-horizon forecasting and representation learning. This
architecture has demonstrated success in time series forecasting, lossless
compression, and even large-scale language modeling tasks, where its linear
memory footprint and fast inference make it a viable alternative to
Transformers. Despite its growing popularity, no prior work has explored xLSTM
for anomaly detection. In this work, we fill this gap by proposing xLSTMAD, the
first anomaly detection method that integrates a full encoder-decoder xLSTM
architecture, purpose-built for multivariate time series data. Our encoder
processes input sequences to capture historical context, while the decoder is
devised in two separate variants of the method. In the forecasting approach,
the decoder iteratively generates forecasted future values xLSTMAD-F, while the
reconstruction approach reconstructs the input time series from its encoded
counterpart xLSTMAD-R. We investigate the performance of two loss functions:
Mean Squared Error (MSE), and Soft Dynamic Time Warping (SoftDTW) to consider
local reconstruction fidelity and global sequence alignment, respectively. We
evaluate our method on the comprehensive TSB-AD-M benchmark, which spans 17
real-world datasets, using state-of-the-art challenging metrics such as VUS-PR.
In our results, xLSTM showcases state-of-the-art accuracy, outperforming 23
popular anomaly detection baselines. Our paper is the first work revealing the
powerful modeling capabilities of xLSTM for anomaly detection, paving the way
for exciting new developments on this subject. Our code is available at:
https://github.com/Nyderx/xlstmad

</details>


### [38] [Quantum Neural Networks for Wind Energy Forecasting: A Comparative Study of Performance and Scalability with Classical Models](https://arxiv.org/abs/2506.22845)
*Batuhan Hangun, Oguz Altun, Onder Eyecioglu*

**主要类别:** cs.LG

**AI概要:** 本研究探讨了基于Z特征映射的六种量子神经网络(QNN)配置在风力涡轮机功率预测中的表现，通过交叉验证和测试集评估，证明QNNs在预测性能上可与经典方法竞争甚至略胜一筹，并分析了数据集大小和电路复杂性对性能和模拟时间的影响。


<details>
  <summary>更多</summary>
  
**动机:** 随着智能电网的普及以及可再生能源系统的整合，机器学习在电力需求预测和系统扰动检测中扮演着重要角色。因此，探索量子神经网络（QNNs）在这些任务中的潜力变得至关重要。

**方法:** 研究采用了基于Z特征映射的六种QNN配置，通过不同的假设结构进行实验。利用详细的交叉验证实验和未见测试集上的测试，评估了QNNs的预测性能和模拟时间。

**结果:** 实验结果表明，QNNs在预测性能上可以与经典方法竞争，在某些情况下甚至略胜一筹。此外，还揭示了数据集大小和电路复杂性对预测性能和模拟时间的影响。

**结论:** 本研究为希望将量子机器学习融入工作的能源领域研究人员提供了有价值的见解，展示了QNNs在风力涡轮机功率预测中的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quantum+Neural+Networks+for+Wind+Energy+Forecasting%3A+A+Comparative+Study+of+Performance+and+Scalability+with+Classical+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22845，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22845&send_immediately=true&force_search=false)

**原文摘要:** Quantum Neural Networks (QNNs), a prominent approach in Quantum Machine
Learning (QML), are emerging as a powerful alternative to classical machine
learning methods. Recent studies have focused on the applicability of QNNs to
various tasks, such as time-series forecasting, prediction, and classification,
across a wide range of applications, including cybersecurity and medical
imaging. With the increased use of smart grids driven by the integration of
renewable energy systems, machine learning plays an important role in
predicting power demand and detecting system disturbances. This study provides
an in-depth investigation of QNNs for predicting the power output of a wind
turbine. We assess the predictive performance and simulation time of six QNN
configurations that are based on the Z Feature Map for data encoding and
varying ansatz structures. Through detailed cross-validation experiments and
tests on an unseen hold-out dataset, we experimentally demonstrate that QNNs
can achieve predictive performance that is competitive with, and in some cases
marginally better than, the benchmarked classical approaches. Our results also
reveal the effects of dataset size and circuit complexity on predictive
performance and simulation time. We believe our findings will offer valuable
insights for researchers in the energy domain who wish to incorporate quantum
machine learning into their work.

</details>


### [39] [Scalable Structure Learning of Bayesian Networks by Learning Algorithm Ensembles](https://arxiv.org/abs/2506.22848)
*Shengcai Liu, Hui Ou-yang, Zhiyuan Wang, Cheng Chen, Qijun Cai, Yew-Soon Ong, Ke Tang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种自动结构学习集成（Auto-SLE）方法，通过结合多种贝叶斯网络（BN）结构学习算法，显著提高了大规模BN结构学习的准确性。相比单一算法的分而治之（D&D）方法，该方法在10,000变量的数据集上准确率提升了30%~225%，并且对更多变量（如30,000）和不同网络特征的数据集具有良好的泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的分而治之（D&D）策略虽然适用于大规模贝叶斯网络的学习，但在子问题中学习精度不稳定，限制了其性能。为了解决这一问题，需要一种能够稳定提高学习精度的方法。

**方法:** 引入结构学习集成（SLE）的概念，将多种BN结构学习算法组合起来以实现高学习精度；进一步提出自动学习SLE的方法（Auto-SLE），解决手动设计高质量SLE的挑战；最后将学习到的SLE集成到D&D方法中用于大规模BN结构学习。

**结果:** 实验表明，与基于单一BN结构学习算法的D&D方法相比，该方法在包含10,000个变量的数据集上显著提高了学习精度（提升幅度达30%~225%）。此外，该方法对更多变量（如30,000）和不同网络特性的数据集具有良好的泛化能力。

**结论:** 采用（自动学习的）结构学习集成（SLE）方法对于可扩展的贝叶斯网络结构学习具有重要潜力，能显著提高大规模BN结构学习的精度和稳定性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scalable+Structure+Learning+of+Bayesian+Networks+by+Learning+Algorithm+Ensembles，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22848，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22848&send_immediately=true&force_search=false)

**原文摘要:** Learning the structure of Bayesian networks (BNs) from data is challenging,
especially for datasets involving a large number of variables. The recently
proposed divide-and-conquer (D\&D) strategies present a promising approach for
learning large BNs. However, they still face a main issue of unstable learning
accuracy across subproblems. In this work, we introduce the idea of employing
structure learning ensemble (SLE), which combines multiple BN structure
learning algorithms, to consistently achieve high learning accuracy. We further
propose an automatic approach called Auto-SLE for learning near-optimal SLEs,
addressing the challenge of manually designing high-quality SLEs. The learned
SLE is then integrated into a D\&D method. Extensive experiments firmly show
the superiority of our method over D\&D methods with single BN structure
learning algorithm in learning large BNs, achieving accuracy improvement
usually by 30\%$\sim$225\% on datasets involving 10,000 variables. Furthermore,
our method generalizes well to datasets with many more (e.g., 30000) variables
and different network characteristics than those present in the training data
for learning the SLE. These results indicate the significant potential of
employing (automatic learning of) SLEs for scalable BN structure learning.

</details>


### [40] [P$^2$U: Progressive Precision Update For Efficient Model Distribution](https://arxiv.org/abs/2506.22871)
*Homayun Afrabandpey, Hamed Rezazadegan Tavakoli*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种名为Progressive Precision Update (P²U)的简单而有效的方法，用于在带宽受限环境中高效分发模型。通过传输低精度模型及与原始高精度模型之间的差异更新，P²U在不同模型架构和数据集上展现了更好的准确率、带宽使用和延迟权衡。此外，P²U可与其他压缩技术结合使用，适用于资源受限场景如联邦学习和边缘计算。


<details>
  <summary>更多</summary>
  
**动机:** 在带宽受限环境下，模型分发变得越来越关键，但传统方法直接传输高精度模型会消耗大量带宽和时间。因此需要一种更高效的模型分发方式。

**方法:** 提出了一种名为Progressive Precision Update (P²U)的方法，该方法传输低精度模型以及表示原始高精度模型与传输低精度版本之间差异的模型更新。

**结果:** 通过在多个数据集（如胸片X光、PASCAL-VOC和CIFAR-100）和不同规模模型上的广泛实验表明，P²U能够实现准确率、带宽使用和延迟之间的更好权衡，并且在优先考虑带宽或启动时间时，可以使用激进量化（例如4位）而不严重影响性能。

**结论:** P²U是一种有效且实用的解决方案，适用于资源受限环境下的可扩展和高效模型分发，包括联邦学习、边缘计算和物联网部署。并且由于它可以补充现有的压缩技术，潜在改进空间更大。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是P%24%5E2%24U%3A+Progressive+Precision+Update+For+Efficient+Model+Distribution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22871，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22871&send_immediately=true&force_search=false)

**原文摘要:** Efficient model distribution is becoming increasingly critical in
bandwidth-constrained environments. In this paper, we propose a simple yet
effective approach called Progressive Precision Update (P$^2$U) to address this
problem. Instead of transmitting the original high-precision model, P$^2$U
transmits a lower-bit precision model, coupled with a model update representing
the difference between the original high-precision model and the transmitted
low precision version. With extensive experiments on various model
architectures, ranging from small models ($1 - 6$ million parameters) to a
large model (more than $100$ million parameters) and using three different data
sets, e.g., chest X-Ray, PASCAL-VOC, and CIFAR-100, we demonstrate that P$^2$U
consistently achieves better tradeoff between accuracy, bandwidth usage and
latency. Moreover, we show that when bandwidth or startup time is the priority,
aggressive quantization (e.g., 4-bit) can be used without severely compromising
performance. These results establish P$^2$U as an effective and practical
solution for scalable and efficient model distribution in low-resource
settings, including federated learning, edge computing, and IoT deployments.
Given that P$^2$U complements existing compression techniques and can be
implemented alongside any compression method, e.g., sparsification,
quantization, pruning, etc., the potential for improvement is even greater.

</details>


### [41] [Interpretable Time Series Autoregression for Periodicity Quantification](https://arxiv.org/abs/2506.22895)
*Xinyu Chen, Vassilis Digalakis Jr, Lijun Ding, Dingyi Zhuang, Jinhua Zhao*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种新的稀疏自回归框架，通过ℓ0-范数约束增强了周期性量化模型的可解释性，并开发了子空间追踪决策变量剪枝策略（DVP）以加速优化过程。此外，针对多维时间序列，提出了一种时空变化的稀疏自回归模型和两阶段优化方案，能够处理大规模问题。实验表明，该方法在共享出行数据中揭示了日和周周期性以及人类移动规律的长期变化，在气候数据中发现了季节性和动态气候模式，如厄尔尼诺现象。


<details>
  <summary>更多</summary>
  
**动机:** 时间序列自回归是一种经典统计模型，用于捕捉自相关并识别时间模式，例如周期性和季节性。然而，传统模型在处理复杂的时间序列数据时面临挑战，尤其是在高维和非平稳数据上。因此，需要一种更高效且可解释的模型来分析这些复杂的模式。

**方法:** 1. 提出一种基于ℓ0-范数约束的稀疏自回归框架，增强模型对周期性的可解释性。
2. 对于时变时间序列数据，将稀疏自回归问题转化为混合整数优化（MIO）问题，并开发DVP策略减少搜索空间。
3. 针对多维时间序列，提出一种时空变化的稀疏自回归模型，并设计两阶段优化方案以解决相应MIO问题。
4. 该方案可扩展到具有数百万决策变量的大规模问题。

**结果:** 1. 实验证明，DVP策略可以显著加速MIO求解器，同时保持与完整MIO求解器相同的解质量。
2. 在共享出行数据中，发现日和周周期性，并揭示了人类移动规律的长期变化。
3. 在气候数据中，揭示了过去四十年温度和降水量等变量的季节性空间模式，发现了动态气候模式和厄尔尼诺现象。

**结论:** 提出的稀疏自回归框架及其优化方法在处理复杂时间序列数据方面表现出色，能够有效揭示周期性、长期变化和动态气候模式，为实际应用提供了有价值的工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interpretable+Time+Series+Autoregression+for+Periodicity+Quantification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22895，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22895&send_immediately=true&force_search=false)

**原文摘要:** Time series autoregression is a classical statistical model for capturing
auto-correlations and identifying temporal patterns such as periodicity and
seasonality. In this work, we propose a novel sparse autoregression framework
from an interpretable machine learning perspective and the model
interpretability for periodicity quantification is reinforced by $\ell_0$-norm
induced sparsity constraints. On the time-varying time series data, we
reformulate the sparse autoregression and convert the involved optimization
problem into a mixed-integer optimization (MIO). To accelerate it, we develop a
subspace pursuit based decision variable pruning (DVP) strategy to reduce the
search space. On the multidimensional time series that involves complicated
spatial and temporal dimensions, we propose a spatially- and time-varying
sparse autoregression model and resolve the corresponding MIO problem by
developing a two-stage optimization scheme. In particular, the proposed scheme
makes the model scalable to large problems even with millions of decision
variables. Empirically, we conduct extensive experiments to evaluate the
proposed models on real-world time series data. First, we demonstrate that the
MIO solver can be drastically accelerated through the DVP strategy, while
maintaining the same solution quality as a full MIO solver. Applying the
time-varying sparse autoregression model to ridesharing trip data, we uncover
both daily and weekly periodicities and reveal long-term changes in regularity
of human mobility. Second, we demonstrate the spatial patterns of yearly
seasonality in climate variable time series such as temperature and
precipitation across the past four decades, and our model allows to discover
dynamic climate patterns and identify climate phenomena such as El Nino in sea
surface temperature.

</details>


### [42] [Missing-Modality-Aware Graph Neural Network for Cancer Classification](https://arxiv.org/abs/2506.22901)
*Sina Tabakhi, Haiping Lu*

**主要类别:** cs.LG

**AI概要:** MAGNET是一种新的多模态生物数据学习方法，通过引入患者-模态多头注意力机制和构建患者图来解决模态缺失问题，其复杂度随模态数量线性增长，并在真实世界癌症分类任务中优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 当前融合方法处理模态缺失问题时存在局限性，难以应对多样化的缺失模式以及模态数量增加导致的指数级模式数增长。

**方法:** 提出MAGNET（Missing-modality-Aware Graph neural NETwork），通过患者-模态多头注意力机制融合低维模态嵌入，根据重要性和缺失性调整权重；随后构建患者图，以融合的多模态嵌入为节点特征，模态缺失性决定连通性，最后使用常规图神经网络生成预测。

**结果:** 在三个公开多组学数据集上的癌症分类实验表明，MAGNET在真实世界的缺失模式下优于最先进的融合方法。

**结论:** MAGNET有效解决了多模态生物数据中的模态缺失问题，具有线性复杂度增长和良好的适应性，适用于真实世界的多模态数据任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Missing-Modality-Aware+Graph+Neural+Network+for+Cancer+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22901，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22901&send_immediately=true&force_search=false)

**原文摘要:** A key challenge in learning from multimodal biological data is missing
modalities, where all data from some modalities are missing for some patients.
Current fusion methods address this by excluding patients with missing
modalities, imputing missing modalities, or making predictions directly with
partial modalities. However, they often struggle with diverse missing-modality
patterns and the exponential growth of the number of such patterns as the
number of modalities increases. To address these limitations, we propose MAGNET
(Missing-modality-Aware Graph neural NETwork) for direct prediction with
partial modalities, which introduces a patient-modality multi-head attention
mechanism to fuse lower-dimensional modality embeddings based on their
importance and missingness. MAGNET's complexity increases linearly with the
number of modalities while adapting to missing-pattern variability. To generate
predictions, MAGNET further constructs a patient graph with fused multimodal
embeddings as node features and the connectivity determined by the modality
missingness, followed by a conventional graph neural network. Experiments on
three public multiomics datasets for cancer classification, with real-world
instead of artificial missingness, show that MAGNET outperforms the
state-of-the-art fusion methods. The data and code are available at
https://github.com/SinaTabakhi/MAGNET.

</details>


### [43] [Towards Time Series Generation Conditioned on Unstructured Natural Language](https://arxiv.org/abs/2506.22927)
*Jaeyun Woo, Jiseok Lee, Brian Kenji Iwana*

**主要类别:** cs.LG

**AI概要:** 生成式人工智能（AI）在图像和文本生成方面取得了显著进展，但时间序列生成AI仍相对欠发达。本文提出了一种基于非结构化自然语言描述生成时间序列的新方法，结合扩散模型与语言模型实现从文本生成时间序列，展示了自然语言驱动的时间序列生成的可行性，并提出了包含63,010个时间序列-描述对的新公开数据集。


<details>
  <summary>更多</summary>
  
**动机:** 尽管生成式AI在许多领域取得了显著进步，但在金融、气候等关键领域中至关重要的时间序列生成AI却发展不足，因此需要一种新的方法来通过自然语言描述生成时间序列。

**方法:** 使用扩散模型与语言模型相结合的方法，根据非结构化的自然语言描述生成时间序列，并构建了一个包含63,010个时间序列-描述对的新公共数据集。

**结果:** 证明了基于自然语言生成时间序列的可行性，该方法可应用于定制预测、时间序列操控、数据增强和迁移学习等领域。

**结论:** 本研究提出的方法为时间序列生成提供了新的可能性，并为相关领域的进一步研究奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Time+Series+Generation+Conditioned+on+Unstructured+Natural+Language，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22927，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22927&send_immediately=true&force_search=false)

**原文摘要:** Generative Artificial Intelligence (AI) has rapidly become a powerful tool,
capable of generating various types of data, such as images and text. However,
despite the significant advancement of generative AI, time series generative AI
remains underdeveloped, even though the application of time series is essential
in finance, climate, and numerous fields. In this research, we propose a novel
method of generating time series conditioned on unstructured natural language
descriptions. We use a diffusion model combined with a language model to
generate time series from the text. Through the proposed method, we demonstrate
that time series generation based on natural language is possible. The proposed
method can provide various applications such as custom forecasting, time series
manipulation, data augmentation, and transfer learning. Furthermore, we
construct and propose a new public dataset for time series generation,
consisting of 63,010 time series-description pairs.

</details>


### [44] [Mathematical Computation on High-dimensional Data via Array Programming and Parallel Acceleration](https://arxiv.org/abs/2506.22929)
*Chen Zhang*

**主要类别:** cs.LG

**AI概要:** 提出了一种基于空间完备性的并行计算架构，将高维数据分解为维度独立的结构进行分布式处理，实现了数据挖掘与并行优化机器学习方法的无缝集成。


<details>
  <summary>更多</summary>
  
**动机:** 深度学习在自然图像和语言处理中表现出色，但在高维数据上的应用面临计算挑战，现有大数据工具缺乏对高级分析的数理统计支持。

**方法:** 提出了一个基于空间完备性的并行计算架构，将高维数据分解成维度独立的结构以进行分布式处理，从而整合数据挖掘和并行优化的机器学习方法。

**结果:** 该框架能够支持跨多种数据类型的科学计算，并在一个统一系统内实现。

**结论:** 此并行计算架构为高维数据分析提供了新的解决方案，可广泛应用于包括医疗和自然图像在内的各类数据类型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mathematical+Computation+on+High-dimensional+Data+via+Array+Programming+and+Parallel+Acceleration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22929，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22929&send_immediately=true&force_search=false)

**原文摘要:** While deep learning excels in natural image and language processing, its
application to high-dimensional data faces computational challenges due to the
dimensionality curse. Current large-scale data tools focus on business-oriented
descriptive statistics, lacking mathematical statistics support for advanced
analysis. We propose a parallel computation architecture based on space
completeness, decomposing high-dimensional data into dimension-independent
structures for distributed processing. This framework enables seamless
integration of data mining and parallel-optimized machine learning methods,
supporting scientific computations across diverse data types like medical and
natural images within a unified system.

</details>


### [45] [Infinite Sampling: Efficient and Stable Grouped RL Training for Large Language Models](https://arxiv.org/abs/2506.22950)
*Liangyu Wang, Huanyi Xie, Xinhai Wang, Tianjin Huang, Mengdi Li, Di Wang*

**主要类别:** cs.LG

**AI概要:** 提出Infinite Sampling框架，通过微采样组、连续采样和长度感知调度器减少内存使用并提高吞吐量，使Group-based强化学习算法在实际GPU内存限制下更高效稳定。


<details>
  <summary>更多</summary>
  
**动机:** 基于群体的强化学习算法（如GRPO）在使用人类反馈微调大型语言模型时效果显著，但随着样本组规模增加，生成和存储每个提示的多个响应会导致巨大的内存开销，限制了其在硬件受限情况下的可扩展性。

**方法:** 提出了Infinite Sampling框架，包括：(1) 微采样组，将大组分解为内存可行的轮次；(2) 连续采样，在组间交错生成以提高利用率；(3) 长度感知调度器，结合标记条件序列长度预测与两阶段计划（全局分组通过FPTAS，运行时补充通过SJF）。

**结果:** 实验表明，微采样组相比全组解码减少超过50%的峰值内存使用（例如，在Qwen3-1.7B上从21.55 GB降至10.64 GB）。在此基础上，Infinite Sampling比简单的微采样组方法提高了超过25%的吞吐量，减少了解码步骤，同时保持完整长度完成和内存使用。混合调度确保在实际GPU内存约束下更大组的高效稳定GRPO训练。

**结论:** Infinite Sampling框架通过解耦组大小与GPU内存使用，实现了高效且稳定的GRPO训练，从而在有限的硬件资源下提升了基于群体的强化学习算法的可扩展性和性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Infinite+Sampling%3A+Efficient+and+Stable+Grouped+RL+Training+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22950，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22950&send_immediately=true&force_search=false)

**原文摘要:** Group-based reinforcement learning algorithms such as Group Reward Policy
Optimization (GRPO) have proven effective for fine-tuning large language models
(LLMs) with human feedback. However, generating and storing multiple responses
per prompt incurs substantial memory overhead, especially as the sample group
size increases, limiting scalability under constrained hardware.
  We propose Infinite Sampling, a framework that enables efficient and stable
GRPO training by decoupling group size from GPU memory usage. It consists of:
(1) micro sampling groups that decompose large groups into memory-feasible
rounds; (2) continuous sampling that interleaves generation across groups to
improve utilization; and (3) a length-aware scheduler combining
token-conditioned sequence length prediction with a two-stage plan: global
grouping via FPTAS and runtime refill via SJF.
  Experiments show that our Micro Sampling Groups reduce peak memory usage by
over 50% compared to full-group decoding (e.g., from 21.55 GB to 10.64 GB on
Qwen3-1.7B). Building on this, Infinite Sampling improves throughput by over
25% compared to the naive micro sampling group method, reducing decoding steps
while maintaining full-length completions and memory usage. Our hybrid
scheduling ensures efficient and stable GRPO training with larger groups under
realistic GPU memory constraints.

</details>


### [46] [Cybersecurity-Focused Anomaly Detection in Connected Autonomous Vehicles Using Machine Learning](https://arxiv.org/abs/2506.22984)
*Prathyush Kumar Reddy Lebaku, Lu Gao, Yunpeng Zhang, Zhixia Li, Yongxin Liu, Tanvir Arafin*

**主要类别:** cs.LG

**AI概要:** 本研究通过模拟车辆行为生成数据集，使用堆叠LSTM和随机森林模型检测联网自动驾驶汽车（CAVs）中的异常驾驶模式。堆叠LSTM捕捉时间序列依赖性和序列异常，而随机森林提供基于集成的预测，提高模型解释性和性能。实验结果表明，这两种模型在预测车辆轨迹和检测自动驾驶场景中的异常方面具有很高的准确性。


<details>
  <summary>更多</summary>
  
**动机:** 联网自动驾驶汽车（CAVs）可能受到传感器故障、网络攻击和意外环境破坏的影响，因此在这些车辆中进行异常检测对于维持安全可靠的交通网络至关重要。

**方法:** 1. 模拟车辆行为生成数据集，包括位置、速度和加速度的时间序列数据。
2. 使用堆叠长短期记忆（LSTM）模型捕捉时间依赖性和序列异常。
3. 部署随机森林模型以提供基于集成的预测，增强模型可解释性和性能。

**结果:** - 随机森林模型：R2为0.9830，MAE为5.746，95百分位异常阈值为14.18。
- 堆叠LSTM模型：R2为0.9998，MAE为82.425，95百分位异常阈值为265.63。

**结论:** 机器学习模型在准确预测车辆轨迹和检测自动驾驶场景中的异常方面表现出色，证明了其在联网自动驾驶汽车异常检测中的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cybersecurity-Focused+Anomaly+Detection+in+Connected+Autonomous+Vehicles+Using+Machine+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22984，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22984&send_immediately=true&force_search=false)

**原文摘要:** Anomaly detection in connected autonomous vehicles (CAVs) is crucial for
maintaining safe and reliable transportation networks, as CAVs can be
susceptible to sensor malfunctions, cyber-attacks, and unexpected environmental
disruptions. This study explores an anomaly detection approach by simulating
vehicle behavior, generating a dataset that represents typical and atypical
vehicular interactions. The dataset includes time-series data of position,
speed, and acceleration for multiple connected autonomous vehicles. We utilized
machine learning models to effectively identify abnormal driving patterns.
First, we applied a stacked Long Short-Term Memory (LSTM) model to capture
temporal dependencies and sequence-based anomalies. The stacked LSTM model
processed the sequential data to learn standard driving behaviors.
Additionally, we deployed a Random Forest model to support anomaly detection by
offering ensemble-based predictions, which enhanced model interpretability and
performance. The Random Forest model achieved an R2 of 0.9830, MAE of 5.746,
and a 95th percentile anomaly threshold of 14.18, while the stacked LSTM model
attained an R2 of 0.9998, MAE of 82.425, and a 95th percentile anomaly
threshold of 265.63. These results demonstrate the models' effectiveness in
accurately predicting vehicle trajectories and detecting anomalies in
autonomous driving scenarios.

</details>


### [47] [A Reinforcement Learning Approach for Optimal Control in Microgrids](https://arxiv.org/abs/2506.22995)
*Davide Salaorni, Federico Bianchi, Francesco Trovò, Marcello Restelli*

**主要类别:** cs.LG

**AI概要:** The paper presents a novel RL-based methodology for optimizing microgrid energy management, validated through real-world data from an Italian power grid.


<details>
  <summary>更多</summary>
  
**动机:** The increasing integration of renewable energy sources (RESs) is transforming traditional power grid networks, requiring new approaches for managing decentralized energy production and consumption.

**方法:** A reinforcement learning (RL)-based methodology is proposed. An RL agent learns optimal energy trading and storage policies using historical data on energy production, consumption, and market prices. A digital twin (DT) simulates the energy storage system dynamics, incorporating degradation factors.

**结果:** The proposed RL-based strategy outperforms rule-based methods and existing RL benchmarks.

**结论:** The RL-based methodology offers a robust solution for intelligent microgrid management.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Reinforcement+Learning+Approach+for+Optimal+Control+in+Microgrids，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22995，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22995&send_immediately=true&force_search=false)

**原文摘要:** The increasing integration of renewable energy sources (RESs) is transforming
traditional power grid networks, which require new approaches for managing
decentralized energy production and consumption. Microgrids (MGs) provide a
promising solution by enabling localized control over energy generation,
storage, and distribution. This paper presents a novel reinforcement learning
(RL)-based methodology for optimizing microgrid energy management.
Specifically, we propose an RL agent that learns optimal energy trading and
storage policies by leveraging historical data on energy production,
consumption, and market prices. A digital twin (DT) is used to simulate the
energy storage system dynamics, incorporating degradation factors to ensure a
realistic emulation of the analysed setting. Our approach is validated through
an experimental campaign using real-world data from a power grid located in the
Italian territory. The results indicate that the proposed RL-based strategy
outperforms rule-based methods and existing RL benchmarks, offering a robust
solution for intelligent microgrid management.

</details>


### [48] [BWLer: Barycentric Weight Layer Elucidates a Precision-Conditioning Tradeoff for PINNs](https://arxiv.org/abs/2506.23024)
*Jerry Liu, Yasa Baig, Denise Hui Jean Lee, Rajat Vadiraj Dwaraknath, Atri Rudra, Chris Ré*

**主要类别:** cs.LG

**AI概要:** 本研究通过引入Barycentric Weight Layer (BWLer)来提高物理信息神经网络(PINNs)求解偏微分方程(PDEs)的精度，揭示了MLP架构中的基本精度限制，并展示了BWLer在多种PDE任务中的显著性能提升。


<details>
  <summary>更多</summary>
  
**动机:** 尽管PINNs提供了一种灵活的方法来用机器学习解决PDEs，但它们仍然远未达到许多科学任务所需的机器精度。因此，研究者探索了精度限制是否源于PDEs的病态性或典型的多层感知机(MLP)架构。

**方法:** 研究者提出了Barycentric Weight Layer (BWLer)，它通过重心多项式插值来建模PDE解。BWLer可以附加在现有的MLP之上（BWLer-hat）或完全替代MLP（显式BWLer）。这种方法将解的表示与PDE损失的导数计算分离。此外，研究者对MLP进行了基本精度限制的研究，并通过误差分解和训练过程中的预处理技术来优化线性PDE的学习。

**结果:** 在五个基准PDE上，添加BWLer可使MLP的RMSE大幅改善：对流问题提升30倍，反应问题提升10倍，波动方程提升1800倍。而完全替换MLP的显式BWLer，在对流、反应和波动问题中接近机器精度（比先前结果好达10亿倍），并在Burgers'和不规则几何Poisson问题中匹配标准PINNs的性能。

**结论:** BWLer为结合PINNs的灵活性和经典谱解算器的精度提供了实际路径，能够显著提高PDE求解的精度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BWLer%3A+Barycentric+Weight+Layer+Elucidates+a+Precision-Conditioning+Tradeoff+for+PINNs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23024，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23024&send_immediately=true&force_search=false)

**原文摘要:** Physics-informed neural networks (PINNs) offer a flexible way to solve
partial differential equations (PDEs) with machine learning, yet they still
fall well short of the machine-precision accuracy many scientific tasks demand.
In this work, we investigate whether the precision ceiling comes from the
ill-conditioning of the PDEs or from the typical multi-layer perceptron (MLP)
architecture. We introduce the Barycentric Weight Layer (BWLer), which models
the PDE solution through barycentric polynomial interpolation. A BWLer can be
added on top of an existing MLP (a BWLer-hat) or replace it completely
(explicit BWLer), cleanly separating how we represent the solution from how we
take derivatives for the PDE loss. Using BWLer, we identify fundamental
precision limitations within the MLP: on a simple 1-D interpolation task, even
MLPs with O(1e5) parameters stall around 1e-8 RMSE -- about eight orders above
float64 machine precision -- before any PDE terms are added. In PDE learning,
adding a BWLer lifts this ceiling and exposes a tradeoff between achievable
accuracy and the conditioning of the PDE loss. For linear PDEs we fully
characterize this tradeoff with an explicit error decomposition and navigate it
during training with spectral derivatives and preconditioning. Across five
benchmark PDEs, adding a BWLer on top of an MLP improves RMSE by up to 30x for
convection, 10x for reaction, and 1800x for wave equations while remaining
compatible with first-order optimizers. Replacing the MLP entirely lets an
explicit BWLer reach near-machine-precision on convection, reaction, and wave
problems (up to 10 billion times better than prior results) and match the
performance of standard PINNs on stiff Burgers' and irregular-geometry Poisson
problems. Together, these findings point to a practical path for combining the
flexibility of PINNs with the precision of classical spectral solvers.

</details>


### [49] [Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models](https://arxiv.org/abs/2506.23025)
*Tejas Vaidhya, Ayush Kaushal, Vineet Jain, Francis Couture Harpin, Prashant Shishodia, Majid Behbahani, Yuriy Nevmyvaka, Irina Rish*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了通过量化感知训练减少内存需求的三元语言模型（TriLMs），并引入了Spectra-1.1模型套件和加速推理的TriRun GPU内核。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型在研究和工业应用中越来越受欢迎，但其推理效率仍然存在重大挑战。现代GPU架构的计算能力持续提高，但其内存带宽和容量没有按比例扩展，导致推理过程中出现关键瓶颈。因此，需要一种方法来解决这一问题。

**方法:** 作者首先分析了三元语言模型（TriLMs）的可扩展性，并进行了缩放定律分析，发现TriLMs从增加训练数据中受益更多，而不是从模型参数扩展中受益。基于此观察，他们提出了Spectra-1.1，这是一个开放的TriLMs套件，使用多达1.2万亿个标记进行训练。此外，为了提高推理效率，他们提出了新的2位和1.6位三元权重打包方案，这些方案在各种CPU架构上展示了加速推理的能力。同时，基于2位打包，他们开发了一个名为TriRun的GPU内核，与浮点基线相比，该内核可以将端到端模型推理加速高达5倍。

**结果:** 实验结果表明，Spectra-1.1模型在大规模训练数据下表现出持续的性能提升。而TriRun GPU内核则显著提高了推理速度，最多可达浮点基线的5倍。

**结论:** 这项工作为构建和部署高效的大型语言模型奠定了基础，并为研究社区提供了一个宝贵的资源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Spectra+1.1%3A+Scaling+Laws+and+Efficient+Inference+for+Ternary+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23025，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23025&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are increasingly used across research and
industry applications, yet their inference efficiency remains a significant
challenge. As the computational power of modern GPU architectures continuously
improves, their memory bandwidth and capacity have not scaled proportionally,
creating a critical bottleneck during inference. To address this, we
investigate ternary language models (TriLMs) that employ quantization-aware
training to significantly reduce memory requirements. We first analyze the
scalability of TriLMs by conducting a scaling law analysis, revealing that
TriLMs benefit more from increasing training data than from scaling model
parameters. Based on this observation, we introduce Spectra-1.1, an open suite
of TriLMs trained on up to 1.2 trillion tokens, demonstrating sustained
performance gains at scale. Furthermore, to improve inference efficiency, we
propose novel 2-bit and 1.6-bit packing schemes for ternary weights, which
demonstrate accelerated inference across various CPU architectures. Also,
building on the 2-bit packing, we develop a GPU kernel called TriRun that
accelerates end-to-end model inference by up to 5 times compared to
floating-point baselines. To encourage further exploration and development of
TriLMs, we will release the Spectra-1.1 suite and TriRun inference kernels.
Overall, our work lays the foundation for building and deploying efficient
LLMs, providing a valuable resource for the research community.

</details>


### [50] [Fragile, Robust, and Antifragile: A Perspective from Parameter Responses in Reinforcement Learning Under Stress](https://arxiv.org/abs/2506.23036)
*Zain ul Abdeen, Ming Jin*

**主要类别:** cs.LG

**AI概要:** 这篇论文通过系统地分析网络参数在内外压力下的表现，探讨了强化学习（RL）策略的鲁棒性。受神经科学中突触可塑性的启发，论文提出了一种双重方法：通过选择性扰动参数引入内部压力，通过修改代理观察进行对抗性攻击以施加外部压力。这种方法能够将参数分类为脆弱、鲁棒或反脆弱，并通过参数得分来量化这些特性。实验在Mujoco连续控制环境中验证了使用PPO训练的代理，结果表明存在增强策略在压力下表现的反脆弱参数，展示了有针对性的过滤技术改善RL策略适应性的潜力，为未来设计鲁棒和反脆弱的RL系统提供了基础。


<details>
  <summary>更多</summary>
  
**动机:** 强化学习策略在实际应用中需要具备良好的鲁棒性以应对各种不确定性。然而，目前对策略鲁棒性的理解有限，尤其是在面对内部参数扰动和外部对抗攻击时的表现。因此，研究如何系统评估和提高RL策略的鲁棒性成为了一个重要的课题。

**方法:** 论文采用了双管齐下的方法：1) 突触过滤 - 受神经科学中突触可塑性的启发，通过选择性扰动网络参数引入内部压力；2) 对抗攻击 - 通过对代理观察进行修改施加外部压力。基于这两种压力源，论文定义了参数得分来衡量参数在干净环境和对抗环境下的影响，并据此将参数分为脆弱、鲁棒和反脆弱三类。

**结果:** 实验结果表明，在Mujoco连续控制环境中，存在一些反脆弱参数，它们在压力下能够提升策略的表现。这说明通过针对性的过滤技术可以改善RL策略的适应性。

**结论:** 本研究表明，通过系统分析网络参数在内外压力下的表现，可以有效识别出反脆弱参数，这些参数有助于提升RL策略在压力条件下的性能。这一发现为设计更加鲁棒和反脆弱的RL系统提供了新的思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fragile%2C+Robust%2C+and+Antifragile%3A+A+Perspective+from+Parameter+Responses+in+Reinforcement+Learning+Under+Stress，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23036，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23036&send_immediately=true&force_search=false)

**原文摘要:** This paper explores Reinforcement learning (RL) policy robustness by
systematically analyzing network parameters under internal and external
stresses. Inspired by synaptic plasticity in neuroscience, synaptic filtering
introduces internal stress by selectively perturbing parameters, while
adversarial attacks apply external stress through modified agent observations.
This dual approach enables the classification of parameters as fragile, robust,
or antifragile, based on their influence on policy performance in clean and
adversarial settings. Parameter scores are defined to quantify these
characteristics, and the framework is validated on PPO-trained agents in Mujoco
continuous control environments. The results highlight the presence of
antifragile parameters that enhance policy performance under stress,
demonstrating the potential of targeted filtering techniques to improve RL
policy adaptability. These insights provide a foundation for future
advancements in the design of robust and antifragile RL systems.

</details>


### [51] [ReMem: Mutual Information-Aware Fine-tuning of Pretrained Vision Transformers for Effective Knowledge Distillation](https://arxiv.org/abs/2506.23041)
*Chengyu Dong, Huan Gui, Noveen Sachdeva, Long Jin, Ke Yin, Jingbo Shang, Lichan Hong, Ed H. Chi, Zhe Zhao*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了从大规模预训练视觉模型（如Vision Transformers, ViTs）进行知识蒸馏时遇到的挑战，并提出了解决方案以改进小规模任务特定模型的效果。具体来说，作者提出了基于互信息感知优化的微调方法，以及针对小型或高度不平衡下游数据集的MLP块重新加权启发式方法，从而让小规模学生模型能够更有效地利用强大的预训练模型。


<details>
  <summary>更多</summary>
  
**动机:** 在使用大规模预训练模型进行知识蒸馏时，当源模型非常强大且基于大规模数据预训练时，知识转移的效果会显著下降。为了解决这一问题，特别是在Vision Transformers (ViTs)中，需要探索更有效的微调方法以改善知识转移效果。

**方法:** 作者提出了两种主要方法：1. 在微调过程中采用互信息感知优化（mutual information-aware optimization），以提高知识蒸馏的有效性；2. 针对小型或高度不平衡的下游数据集，引入了一种简单的启发式方法——重新加权MLP块，因为观察到顶部MLP块主要负责互信息损失。

**结果:** 实验结果表明，所提出的方法使小规模学生模型能够从最强的预训练模型中受益，提高了其在各种任务上的表现。尤其在小型或高度不平衡的数据集上，重新加权MLP块的启发式方法表现出了显著的优势。

**结论:** 通过结合互信息感知优化和MLP块重新加权技术，可以有效解决从强预训练模型进行知识蒸馏时遇到的挑战，进一步提升小规模学生模型的性能。这为未来在视觉任务中利用强大预训练模型提供了新的思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ReMem%3A+Mutual+Information-Aware+Fine-tuning+of+Pretrained+Vision+Transformers+for+Effective+Knowledge+Distillation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23041，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23041&send_immediately=true&force_search=false)

**原文摘要:** Knowledge distillation from pretrained visual representation models offers an
effective approach to improve small, task-specific production models. However,
the effectiveness of such knowledge transfer drops significantly when
distilling from strong models that are pretrained in a large scale. In this
paper, we address this challenge for pretrained Vision Transformers (ViTs) by
exploring methods to fine-tune them for more effective knowledge transfer.
Motivated by the connection between mutual information and distillation
effectiveness, we propose to employ mutual information-aware optimization
during finetuning. For small or highly-imbalanced downstream datasets where
such optimization becomes less effective, we introduce a simple yet effective
heuristic of reweighting MLP blocks. This approach is inspired by our
observation that top MLP blocks are primarily responsible for mutual
information loss. Our method enables small student models to benefit from those
pretrained models among the strongest.

</details>


### [52] [Double-Diffusion: Diffusion Conditioned Diffusion Probabilistic Model For Air Quality Prediction](https://arxiv.org/abs/2506.23053)
*Hanlin Dong, Arian Prabowo, Hao Xue, Flora D. Salim*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的扩散概率模型Double-Diffusion，结合已知物理原理引导空气质量预测中的不确定性。在两个真实数据集上，与其他概率模型相比，该模型在大多数评估场景中排名第一，并将推理时间减少了50%-30%，同时CRPS提高了3%-12%。


<details>
  <summary>更多</summary>
  
**动机:** 空气质量预测因其时空复杂性和固有的动态及不确定性而具有挑战性。当前模型通过图神经网络或已知物理原理来应对这些挑战，并通过概率网络（如扩散模型）量化随机性。然而，在确定性和不确定性之间找到正确的平衡点仍然是一个开放问题。

**方法:** 提出了Double-Diffusion模型，一种新颖的扩散概率模型，利用已知物理原理指导空气质量预测中的随机性。采用图像修复中的采样策略和新的去噪器架构。这是首次尝试使用物理作为条件生成方法进行空气质量预测。

**结果:** 在两个真实数据集上，与其他概率模型相比，Double-Diffusion在大多数评估场景中表现最佳，推理时间减少至原来的30%-50%，并且CRPS提高了3%-12%。

**结论:** Double-Diffusion模型在空气质量预测任务中表现出色，显著提高了预测精度并缩短了推理时间，为结合物理原理的概率模型提供了新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Double-Diffusion%3A+Diffusion+Conditioned+Diffusion+Probabilistic+Model+For+Air+Quality+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23053，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23053&send_immediately=true&force_search=false)

**原文摘要:** Air quality prediction is a challenging forecasting task due to its
spatio-temporal complexity and the inherent dynamics as well as uncertainty.
Most of the current models handle these two challenges by applying Graph Neural
Networks or known physics principles, and quantifying stochasticity through
probabilistic networks like Diffusion models. Nevertheless, finding the right
balancing point between the certainties and uncertainties remains an open
question. Therefore, we propose Double-Diffusion, a novel diffusion
probabilistic model that harnesses the power of known physics to guide air
quality forecasting with stochasticity. To the best of our knowledge, while
precedents have been made of using conditional diffusion models to predict air
pollution, this is the first attempt to use physics as a conditional generative
approach for air quality prediction. Along with a sampling strategy adopted
from image restoration and a new denoiser architecture, Double-Diffusion ranks
first in most evaluation scenarios across two real-life datasets compared with
other probabilistic models, it also cuts inference time by 50% to 30% while
enjoying an increase between 3-12% in Continuous Ranked Probabilistic Score
(CRPS).

</details>


### [53] [Measuring How LLMs Internalize Human Psychological Concepts: A preliminary analysis](https://arxiv.org/abs/2506.23055)
*Hiro Taiyo Hamada, Ippei Fujisawa, Genji Kawakita, Yuki Yamada*

**主要类别:** cs.LG

**AI概要:** 现代大语言模型（LLMs）能够以可测量的精度近似人类心理结构，为开发更可解释的AI系统提供了见解。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大语言模型（LLMs）如ChatGPT在生成类人文本方面表现出色，但尚不清楚这些模型对塑造人类思维和行为的概念内部化的准确性如何。

**方法:** 利用43个标准化心理学问卷，通过成对相似性分析评估语言模型重建和分类问卷项目的准确性，并使用层次聚类方法将结果聚类结构与原始类别标签进行比较。

**结果:** GPT-4模型在概念对齐评估中取得了66.2%的分类准确率，显著优于GPT-3.5（55.9%）和BERT（48.1%），且其估计的语义相似性与人类在多个心理学问卷中的响应具有相关性。

**结论:** 现代LLMs可以以可测量的精度近似人类心理结构，该框架提供了一种新方法来评估人类-LLM概念对齐并识别潜在表示偏差。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Measuring+How+LLMs+Internalize+Human+Psychological+Concepts%3A+A+preliminary+analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23055，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23055&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) such as ChatGPT have shown remarkable abilities
in producing human-like text. However, it is unclear how accurately these
models internalize concepts that shape human thought and behavior. Here, we
developed a quantitative framework to assess concept alignment between LLMs and
human psychological dimensions using 43 standardized psychological
questionnaires, selected for their established validity in measuring distinct
psychological constructs. Our method evaluates how accurately language models
reconstruct and classify questionnaire items through pairwise similarity
analysis. We compared resulting cluster structures with the original
categorical labels using hierarchical clustering. A GPT-4 model achieved
superior classification accuracy (66.2\%), significantly outperforming GPT-3.5
(55.9\%) and BERT (48.1\%), all exceeding random baseline performance (31.9\%).
We also demonstrated that the estimated semantic similarity from GPT-4 is
associated with Pearson's correlation coefficients of human responses in
multiple psychological questionnaires. This framework provides a novel approach
to evaluate the alignment of the human-LLM concept and identify potential
representational biases. Our findings demonstrate that modern LLMs can
approximate human psychological constructs with measurable accuracy, offering
insights for developing more interpretable AI systems.

</details>


### [54] [Curious Causality-Seeking Agents Learn Meta Causal World](https://arxiv.org/abs/2506.23068)
*Zhiyu Zhao, Haoxuan Li, Haifeng Zhang, Jun Wang, Francesco Faccio, Jürgen Schmidhuber, Mengyue Yang*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种名为Meta-Causal Graph的世界模型表示方法，能够有效编码潜在世界状态变化时因果结构的转变规则，并引入了Causality-Seeking Agent以识别和改进这些因果图。实验表明该方法可以稳健地捕捉因果动态的变化并推广到未见的情境。


<details>
  <summary>更多</summary>
  
**动机:** 传统构建世界模型的方法通常假设环境有一个单一不变的基本因果规则，但现实中即使是微妙的策略或环境状态的变化都可能改变观察到的因果机制。因此需要一种能适应不同潜在世界状态下的因果结构变化的模型。

**方法:** 作者提出了Meta-Causal Graph作为世界模型的统一表示，它由多个因果子图组成，每个子图由一个潜在的元状态触发。同时引入了一个Causality-Seeking Agent，其目标是：通过好奇驱动的干预策略识别触发每个子图的元状态、发现对应的因果关系并通过持续的好奇心驱动探索与经验迭代优化Meta-Causal Graph。

**结果:** 在合成任务和具有挑战性的机器人手臂操作任务中的实验证明，该方法能够稳健地捕捉因果动态的变化，并且在之前未见过的情境中具有良好的泛化能力。

**结论:** Meta-Causal Graph为建模因果结构在不同潜在世界状态下的变化提供了一种有效的表示方法，而Causality-Seeking Agent则可以通过好奇心驱动的探索不断改进这种表示，使得模型在面对新的情境时具备更强的适应性和泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Curious+Causality-Seeking+Agents+Learn+Meta+Causal+World，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23068，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23068&send_immediately=true&force_search=false)

**原文摘要:** When building a world model, a common assumption is that the environment has
a single, unchanging underlying causal rule, like applying Newton's laws to
every situation. In reality, what appears as a drifting causal mechanism is
often the manifestation of a fixed underlying mechanism seen through a narrow
observational window. This brings about a problem that, when building a world
model, even subtle shifts in policy or environment states can alter the very
observed causal mechanisms. In this work, we introduce the \textbf{Meta-Causal
Graph} as world models, a minimal unified representation that efficiently
encodes the transformation rules governing how causal structures shift across
different latent world states. A single Meta-Causal Graph is composed of
multiple causal subgraphs, each triggered by meta state, which is in the latent
state space. Building on this representation, we introduce a
\textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta
states that trigger each subgraph, (2) discover the corresponding causal
relationships by agent curiosity-driven intervention policy, and (3)
iteratively refine the Meta-Causal Graph through ongoing curiosity-driven
exploration and agent experiences. Experiments on both synthetic tasks and a
challenging robot arm manipulation task demonstrate that our method robustly
captures shifts in causal dynamics and generalizes effectively to previously
unseen contexts.

</details>


### [55] [ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks](https://arxiv.org/abs/2402.09146)
*Muhammad Kashif, Muhammad Shafique*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种新的框架，通过引入可训练的量子卷积层和残差量子神经网络（ResQuNNs）来增强量子卷积神经网络（QuNNs）的性能。实验结果表明，精确放置残差块可以显著提升QuNNs的性能，并为量子深度学习的理论发展和实际应用提供了新方向。


<details>
  <summary>更多</summary>
  
**动机:** 尽管传统的量子卷积层在特征提取方面具有优势，但其静态特性限制了灵活性和适应性。因此，需要一种方法来克服这一局限性，同时解决多层可训练量子卷积层中梯度优化的复杂性问题。

**方法:** 论文提出了一种新型框架，包括：1) 引入可训练的量子卷积层以增加灵活性；2) 提出残差量子神经网络（ResQuNNs），通过在量子卷积层之间插入残差块，改善梯度流动并优化训练性能。

**结果:** 通过广泛的实验验证，确定了一种高效的残差块配置，能够实现网络中所有层的梯度传播，从而显著提高训练效率和模型性能。

**结论:** 研究结果表明，残差块的精确位置对最大化QuNNs性能至关重要，这为量子深度学习的进一步发展和实际应用开辟了新的路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ResQuNNs%3ATowards+Enabling+Deep+Learning+in+Quantum+Convolution+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2402.09146，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2402.09146&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we present a novel framework for enhancing the performance of
Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional
layers and addressing the critical challenges associated with them. Traditional
quanvolutional layers, although beneficial for feature extraction, have largely
been static, offering limited adaptability. Unlike state-of-the-art, our
research overcomes this limitation by enabling training within these layers,
significantly increasing the flexibility and potential of QuNNs. However, the
introduction of multiple trainable quanvolutional layers induces complexities
in gradient-based optimization, primarily due to the difficulty in accessing
gradients across these layers. To resolve this, we propose a novel
architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging
the concept of residual learning, which facilitates the flow of gradients by
adding skip connections between layers. By inserting residual blocks between
quanvolutional layers, we ensure enhanced gradient access throughout the
network, leading to improved training performance. Moreover, we provide
empirical evidence on the strategic placement of these residual blocks within
QuNNs. Through extensive experimentation, we identify an efficient
configuration of residual blocks, which enables gradients across all the layers
in the network that eventually results in efficient training. Our findings
suggest that the precise location of residual blocks plays a crucial role in
maximizing the performance gains in QuNNs. Our results mark a substantial step
forward in the evolution of quantum deep learning, offering new avenues for
both theoretical development and practical quantum computing applications.

</details>


### [56] [Forget-MI: Machine Unlearning for Forgetting Multimodal Information in Healthcare Settings](https://arxiv.org/abs/2506.23145)
*Shahad Hardan, Darya Taratynova, Abdelmajid Essofi, Karthik Nandakumar, Mohammad Yaqub*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的机器遗忘方法Forget-MI，用于多模态医疗数据的隐私保护。该方法通过构建损失函数和扰动技术，能够在移除特定数据（被要求遗忘的数据）的同时保留其他数据的知识，并且在测试集上的表现与原始模型相当。实验结果表明，Forget-MI在降低成员推断攻击(MIA)的能力以及减少对遗忘数据集的表现方面优于现有方法，同时保持了测试集上的等效性能。


<details>
  <summary>更多</summary>
  
**动机:** 在AI领域，尤其是在医疗保健领域，隐私保护至关重要。然而，现有的机器遗忘方法难以从经过训练的多模态架构中移除患者数据。因此，需要一种新的方法来解决这一问题。

**方法:** 提出了名为Forget-MI的新方法，通过建立损失函数和扰动技术，实现对被要求遗忘的数据的单模态和联合表示的遗忘，同时保留其余数据的知识并维持与原始模型相当的性能。

**结果:** Forget-MI模型在降低成员推断攻击(MIA)的能力、减少对遗忘数据集的表现方面优于现有方法，同时在测试集上保持了等效性能。具体而言，MIA降低了0.202，遗忘数据集上的AUC和F1分数分别下降了0.221和0.305。此外，在测试集上的表现与重新训练的模型相当。

**结论:** Forget-MI是一种有效的多模态医疗数据机器遗忘方法，能够保护隐私并在测试集上维持良好的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Forget-MI%3A+Machine+Unlearning+for+Forgetting+Multimodal+Information+in+Healthcare+Settings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23145，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23145&send_immediately=true&force_search=false)

**原文摘要:** Privacy preservation in AI is crucial, especially in healthcare, where models
rely on sensitive patient data. In the emerging field of machine unlearning,
existing methodologies struggle to remove patient data from trained multimodal
architectures, which are widely used in healthcare. We propose Forget-MI, a
novel machine unlearning method for multimodal medical data, by establishing
loss functions and perturbation techniques. Our approach unlearns unimodal and
joint representations of the data requested to be forgotten while preserving
knowledge from the remaining data and maintaining comparable performance to the
original model. We evaluate our results using performance on the forget
dataset, performance on the test dataset, and Membership Inference Attack
(MIA), which measures the attacker's ability to distinguish the forget dataset
from the training dataset. Our model outperforms the existing approaches that
aim to reduce MIA and the performance on the forget dataset while keeping an
equivalent performance on the test set. Specifically, our approach reduces MIA
by 0.202 and decreases AUC and F1 scores on the forget set by 0.221 and 0.305,
respectively. Additionally, our performance on the test set matches that of the
retrained model, while allowing forgetting. Code is available at
https://github.com/BioMedIA-MBZUAI/Forget-MI.git

</details>


### [57] [maneuverRecognition -- A Python package for Timeseries Classification in the domain of Vehicle Telematics](https://arxiv.org/abs/2506.23147)
*Jonathan Schuster, Fabian Transchel*

**主要类别:** cs.LG

**AI概要:** A new python package named maneuverRecognition is developed to facilitate preprocessing, modelling and evaluation for driving maneuvers recognition. It also includes a modifiable LSTM-based network structure and was demonstrated using real driving data from smartphone sensors of three individuals.


<details>
  <summary>更多</summary>
  
**动机:** To address the practical need for python packages that can quickly transform telematic sensor data into the required structure for driving maneuvers recognition and allow building and evaluating predictive models.

**方法:** Development of the maneuverRecognition python package which provides functions for preprocessing, modelling and evaluation of driving maneuvers recognition. The package also includes a ready-to-use and modifiable LSTM-based network structure.

**结果:** The implementation of the maneuverRecognition package was successfully demonstrated using real driving data recorded via smartphone sensors from three different persons.

**结论:** The maneuverRecognition package offers necessary functions for preprocessing, modelling and evaluation in driving maneuvers recognition, fulfilling the practical needs in this domain.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是maneuverRecognition+--+A+Python+package+for+Timeseries+Classification+in+the+domain+of+Vehicle+Telematics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23147，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23147&send_immediately=true&force_search=false)

**原文摘要:** In the domain of vehicle telematics the automated recognition of driving
maneuvers is used to classify and evaluate driving behaviour. This not only
serves as a component to enhance the personalization of insurance policies, but
also to increase road safety, reduce accidents and the associated costs as well
as to reduce fuel consumption and support environmentally friendly driving. In
this context maneuver recognition technically requires a continuous application
of time series classification which poses special challenges to the transfer,
preprocessing and storage of telematic sensor data, the training of predictive
models, and the prediction itself. Although much research has been done in the
field of gathering relevant data or regarding the methods to build predictive
models for the task of maneuver recognition, there is a practical need for
python packages and functions that allow to quickly transform data into the
required structure as well as to build and evaluate such models. The
maneuverRecognition package was therefore developed to provide the necessary
functions for preprocessing, modelling and evaluation and also includes a ready
to use LSTM based network structure that can be modified. The implementation of
the package is demonstrated using real driving data of three different persons
recorded via smartphone sensors.

</details>


### [58] [Mirror Descent Policy Optimisation for Robust Constrained Markov Decision Processes](https://arxiv.org/abs/2506.23165)
*David Bossens, Atsushi Nitanda*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种用于鲁棒约束马尔可夫决策过程（RCMDP）的镜像下降策略优化方法，通过策略梯度技术优化策略和转移核，并在不同设置下获得了相应的收敛速率。实验表明该方法在约束和无约束优化中表现良好，并且比基线算法更具鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 强化学习系统中的安全性至关重要，而鲁棒约束马尔可夫决策过程框架可以学习满足长期约束的策略，并在认识不确定性下提供保证。因此，需要一种有效的优化方法来解决这一问题。

**方法:** 提出了一种基于策略梯度技术的镜像下降策略优化方法，用于鲁棒约束马尔可夫决策过程。该方法将策略视为最大化器，将转移核视为对抗最小化器，在拉格朗日函数上进行优化。分别分析了基于神谕和基于样本的RCMDP设置下的收敛速率。

**结果:** 在基于神谕的RCMDP设置下，获得了一个关于布雷格曼散度的平方距离的$\mathcal{O}\left(\frac{1}{T}\right)$收敛速率，以及一个关于熵正则化目标的$\mathcal{O}\left(e^{-T}\right)$收敛速率。在基于样本的RCMDP设置下，获得了一个$\tilde{\mathcal{O}}\left(\frac{1}{T^{1/3}}\right)$收敛速率。实验结果验证了该方法在约束和无约束优化中的有效性，并显示出更高的鲁棒性。

**结论:** 镜像下降策略优化方法在鲁棒约束马尔可夫决策过程中表现出色，具有良好的收敛性能和较高的鲁棒性，为强化学习系统的安全性提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mirror+Descent+Policy+Optimisation+for+Robust+Constrained+Markov+Decision+Processes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23165，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23165&send_immediately=true&force_search=false)

**原文摘要:** Safety is an essential requirement for reinforcement learning systems. The
newly emerging framework of robust constrained Markov decision processes allows
learning policies that satisfy long-term constraints while providing guarantees
under epistemic uncertainty. This paper presents mirror descent policy
optimisation for robust constrained Markov decision processes (RCMDPs), making
use of policy gradient techniques to optimise both the policy (as a maximiser)
and the transition kernel (as an adversarial minimiser) on the Lagrangian
representing a constrained MDP. In the oracle-based RCMDP setting, we obtain an
$\mathcal{O}\left(\frac{1}{T}\right)$ convergence rate for the squared distance
as a Bregman divergence, and an $\mathcal{O}\left(e^{-T}\right)$ convergence
rate for entropy-regularised objectives. In the sample-based RCMDP setting, we
obtain an $\tilde{\mathcal{O}}\left(\frac{1}{T^{1/3}}\right)$ convergence rate.
Experiments confirm the benefits of mirror descent policy optimisation in
constrained and unconstrained optimisation, and significant improvements are
observed in robustness tests when compared to baseline policy optimisation
algorithms.

</details>


### [59] [Data Can Speak for Itself: Quality-guided Utilization of Wireless Synthetic Data](https://arxiv.org/abs/2506.23174)
*Chen Gong, Bo Liang, Wei Gao, Chenren Xu*

**主要类别:** cs.LG

**AI概要:** 生成模型因其能够生成补充真实数据集数量的逼真合成数据而备受关注。然而，合成数据的质量难以预测，导致性能提升无法保证。本文提出了可衡量和通用的指标来量化合成数据的质量属性——亲和力与多样性，并发现当前无线合成数据普遍存在亲和力不足的问题。为解决这一问题，我们引入了 SynCheck，一种以质量为导向的合成数据利用方案，通过在任务模型训练过程中优化合成数据质量，显著提升了模型性能。实验表明，SynCheck 不仅优于不考虑质量的合成数据使用方法，还能在之前方法导致性能下降 13.4% 的情况下，实现 4.3% 的性能提升。


<details>
  <summary>更多</summary>
  
**动机:** 尽管将所有合成数据纳入训练集可以提高无线感知任务的性能，但合成数据的质量不可预测，导致性能提升无法保障。因此需要提出一种方法来评估并改进合成数据的质量。

**方法:** 1. 提出可衡量和通用的指标（亲和力与多样性）来量化合成数据质量。
2. 发现当前无线合成数据存在的亲和力不足问题。
3. 引入 SynCheck，一种质量导向的合成数据利用方案，在任务模型训练中优化合成数据质量。

**结果:** SynCheck 在实验中表现优异，始终优于不考虑质量的合成数据使用方法，并在后者导致性能下降 13.4% 的情况下，实现了 4.3% 的性能提升。

**结论:** SynCheck 是一种有效的合成数据利用方案，能够显著提升无线感知任务的性能，同时解决了合成数据质量不可预测的问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Data+Can+Speak+for+Itself%3A+Quality-guided+Utilization+of+Wireless+Synthetic+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23174，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23174&send_immediately=true&force_search=false)

**原文摘要:** Generative models have gained significant attention for their ability to
produce realistic synthetic data that supplements the quantity of real-world
datasets. While recent studies show performance improvements in wireless
sensing tasks by incorporating all synthetic data into training sets, the
quality of synthetic data remains unpredictable and the resulting performance
gains are not guaranteed. To address this gap, we propose tractable and
generalizable metrics to quantify quality attributes of synthetic data -
affinity and diversity. Our assessment reveals prevalent affinity limitation in
current wireless synthetic data, leading to mislabeled data and degraded task
performance. We attribute the quality limitation to generative models' lack of
awareness of untrained conditions and domain-specific processing. To mitigate
these issues, we introduce SynCheck, a quality-guided synthetic data
utilization scheme that refines synthetic data quality during task model
training. Our evaluation demonstrates that SynCheck consistently outperforms
quality-oblivious utilization of synthetic data, and achieves 4.3% performance
improvement even when the previous utilization degrades performance by 13.4%.

</details>


### [60] [Attribution assignment for deep-generative sequence models enables interpretability analysis using positive-only data](https://arxiv.org/abs/2506.23182)
*Robert Frank, Michael Widrich, Rahmad Akbar, Günter Klambauer, Geir Kjetil Sandve, Philippe A. Robert, Victor Greiff*

**主要类别:** cs.LG

**AI概要:** 生成模型在治疗设计中提供了一个强大的框架，通过有效地探索富含理想特性的生物序列空间。为了解决生成模型可解释性不足的问题，本文提出了Generative Attribution Metric Analysis (GAMA)，该方法基于Integrated Gradients，能够从生成模型中提取生物学洞见，并通过合成数据集和实验抗体-抗原结合数据进行了验证。


<details>
  <summary>更多</summary>
  
**动机:** 生成模型在生物序列探索中有巨大潜力，但缺乏有效的归因方法限制了其解释能力。而监督学习需要正负样本，在生物领域难以获取足够的负样本。因此，需要一种新的方法来提高生成模型的可解释性。

**方法:** 开发了一种名为GAMA（Generative Attribution Metric Analysis）的归因方法，基于Integrated Gradients，用于自回归生成模型。通过使用具有已知真实情况的合成数据集评估GAMA的统计行为并验证其恢复生物学相关特征的能力。同时将其应用于实验抗体-抗原结合数据以展示其实用性。

**结果:** GAMA成功地从生成模型中提取了可解释的生物学见解，并且无需负训练数据即可验证生成序列设计策略的有效性。

**结论:** GAMA为生成模型提供了可解释性，有助于验证生成序列设计策略，推动了生成模型在生物序列设计中的应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Attribution+assignment+for+deep-generative+sequence+models+enables+interpretability+analysis+using+positive-only+data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23182，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23182&send_immediately=true&force_search=false)

**原文摘要:** Generative machine learning models offer a powerful framework for therapeutic
design by efficiently exploring large spaces of biological sequences enriched
for desirable properties. Unlike supervised learning methods, which require
both positive and negative labeled data, generative models such as LSTMs can be
trained solely on positively labeled sequences, for example, high-affinity
antibodies. This is particularly advantageous in biological settings where
negative data are scarce, unreliable, or biologically ill-defined. However, the
lack of attribution methods for generative models has hindered the ability to
extract interpretable biological insights from such models. To address this
gap, we developed Generative Attribution Metric Analysis (GAMA), an attribution
method for autoregressive generative models based on Integrated Gradients. We
assessed GAMA using synthetic datasets with known ground truths to characterize
its statistical behavior and validate its ability to recover biologically
relevant features. We further demonstrated the utility of GAMA by applying it
to experimental antibody-antigen binding data. GAMA enables model
interpretability and the validation of generative sequence design strategies
without the need for negative training data.

</details>


### [61] [External Data-Enhanced Meta-Representation for Adaptive Probabilistic Load Forecasting](https://arxiv.org/abs/2506.23201)
*Haoran Li, Muhao Guo, Marija Ilic, Yang Weng, Guangchun Ruan*

**主要类别:** cs.LG

**AI概要:** 准确的家庭用电负荷预测对电力系统可靠性至关重要。然而，大多数统计和机器学习模型将外部因素（如天气、日历效应和定价）作为额外输入，忽略了它们的异质性。本文提出了一种范式转变：外部数据应作为元知识来动态调整预测模型本身。基于此思想，设计了一个使用超网络的元表示框架，该框架根据外部条件调制基础深度学习模型的选择参数，从而提供表达性和适应性。进一步集成了专家混合机制以通过选择性专家激活提高效率，并通过过滤冗余外部输入提高鲁棒性。最终模型（称为外部数据的元专家混合模型M2oE2）在有限的额外开销下显著提高了准确性和鲁棒性，超越了现有最先进的方法。


<details>
  <summary>更多</summary>
  
**动机:** 准确的住宅负荷预测对于电力系统的可靠性越来越重要，但传统方法对外部因素的处理方式限制了有用信息的提取。

**方法:** 提出了一个元表示框架，使用超网络调节基础深度学习模型的参数，并结合专家混合机制以增强效率和鲁棒性。

**结果:** 提出的模型M2oE2在多种负荷数据集上实现了显著的准确性和鲁棒性提升，优于现有的最先进方法。

**结论:** 所提出的M2oE2模型通过有限的额外开销提升了预测性能，且数据集和源代码已公开。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是External+Data-Enhanced+Meta-Representation+for+Adaptive+Probabilistic+Load+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23201，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23201&send_immediately=true&force_search=false)

**原文摘要:** Accurate residential load forecasting is critical for power system
reliability with rising renewable integration and demand-side flexibility.
However, most statistical and machine learning models treat external factors,
such as weather, calendar effects, and pricing, as extra input, ignoring their
heterogeneity, and thus limiting the extraction of useful external information.
We propose a paradigm shift: external data should serve as meta-knowledge to
dynamically adapt the forecasting model itself. Based on this idea, we design a
meta-representation framework using hypernetworks that modulate selected
parameters of a base Deep Learning (DL) model in response to external
conditions. This provides both expressivity and adaptability. We further
integrate a Mixture-of-Experts (MoE) mechanism to enhance efficiency through
selective expert activation, while improving robustness by filtering redundant
external inputs. The resulting model, dubbed as a Meta Mixture of Experts for
External data (M2oE2), achieves substantial improvements in accuracy and
robustness with limited additional overhead, outperforming existing
state-of-the-art methods in diverse load datasets. The dataset and source code
are publicly available at
https://github.com/haorandd/M2oE2\_load\_forecast.git.

</details>


### [62] [FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model](https://arxiv.org/abs/2506.23210)
*Taehwan Yoon, Bongjun Choi*

**主要类别:** cs.LG

**AI概要:** 联邦学习（FL）在分布式场景下训练AI模型，保护用户隐私。然而，模型性能可能无法满足用户的期望。本文提出了一种基于参考模型的联邦学习方法，用于最佳微调，克服灾难性遗忘问题，同时实现高性能和低计算成本。


<details>
  <summary>更多</summary>
  
**动机:** 尽管联邦学习确保了数据隐私，但AI模型性能可能无法充分满足用户的期望，并且难以满足多样化的需求。因此需要一种方法来优化模型性能，同时避免灾难性遗忘。

**方法:** 提出了一种基于参考模型的联邦学习方法，该方法源自贝叶斯参数高效迁移学习，包含最优近端项，利用参考模型整合先前模型参数，从而克服每轮迭代中的灾难性遗忘问题。

**结果:** 所提出的方法实现了高模型性能和低计算成本，有效地克服了灾难性遗忘问题。

**结论:** 基于参考模型的联邦学习方法是一种有效的解决方案，可以在保证高性能的同时降低计算成本，适用于优化联邦学习环境中的模型性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedRef%3A+Communication-Efficient+Bayesian+Fine+Tuning+with+Reference+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23210，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23210&send_immediately=true&force_search=false)

**原文摘要:** Federated learning(FL) is used for distributed scenarios to train artificial
intelligence(AI) models while ensuring users' privacy. In federated learning
scenario, the server generally never knows about users' data. This type of
concept makes the AI training process efficient in terms of data privacy.
However, regarding model performance, federated AI models may not sufficiently
satisfy AI users' expectations. Furthermore, AI users have a wide range of
different needs. It is not easy to satisfy the whole users needs. These types
of issues can be addressed through AI model optimization, fine-tuning, or
personalization to achieve optimal model performance. To address model
optimization challenges, we propose reference model-based federated learning
for optimal fine-tuning, which overcomes catastrophic forgetting in each round.
This method is derived from Bayesian parameter-efficient transfer learning,
which includes an optimal proximal term and enables overcoming the catastrophic
forgetting issue in each round by utilizing a reference model that incorporates
previous model parameters. As a result, this method achieves both high model
performance and low computing cost.

</details>


### [63] [Single Image Inpainting and Super-Resolution with Simultaneous Uncertainty Guarantees by Universal Reproducing Kernels](https://arxiv.org/abs/2506.23221)
*Bálint Horváth, Balázs Csanád Csáji*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种统计学习方法，用于估计图像缺失像素的问题，并提供了不确定性量化。该方法假设数据生成函数来自再生核希尔伯特空间（RKHS），并特别强调了带限函数。所提出的Simultaneously Guaranteed Kernel Interpolation（SGKI）方法不仅估计缺失像素，还为未观测值构建非渐进置信区间。此外，论文讨论了向量值函数的推广和一系列数值实验。


<details>
  <summary>更多</summary>
  
**动机:** 图像修复和超分辨率问题中需要有效估计图像的缺失像素，同时提供估计值的不确定性量化。

**方法:** 假设数据生成函数来自再生核希尔伯特空间（RKHS），特别关注带限函数。提出了Simultaneously Guaranteed Kernel Interpolation（SGKI）方法，扩展和改进了近期发展的核方法。利用Schur补高效计算置信区间，并推广到向量值函数。

**结果:** 通过在合成生成和基准图像数据集上的数值实验，验证了SGKI方法的有效性，能够准确估计缺失像素并提供可靠的置信区间。

**结论:** SGKI方法为图像缺失像素估计提供了有效的解决方案，并能同时为所有缺失像素提供置信区间保证，适用于图像修复和超分辨率问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Single+Image+Inpainting+and+Super-Resolution+with+Simultaneous+Uncertainty+Guarantees+by+Universal+Reproducing+Kernels，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23221，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23221&send_immediately=true&force_search=false)

**原文摘要:** The paper proposes a statistical learning approach to the problem of
estimating missing pixels of images, crucial for image inpainting and
super-resolution problems. One of the main novelties of the method is that it
also provides uncertainty quantifications together with the estimated values.
Our core assumption is that the underlying data-generating function comes from
a Reproducing Kernel Hilbert Space (RKHS). A special emphasis is put on
band-limited functions, central to signal processing, which form Paley-Wiener
type RKHSs. The proposed method, which we call Simultaneously Guaranteed Kernel
Interpolation (SGKI), is an extension and refinement of a recently developed
kernel method. An advantage of SGKI is that it not only estimates the missing
pixels, but also builds non-asymptotic confidence bands for the unobserved
values, which are simultaneously guaranteed for all missing pixels. We also
show how to compute these bands efficiently using Schur complements, we discuss
a generalization to vector-valued functions, and we present a series of
numerical experiments on various datasets containing synthetically generated
and benchmark images, as well.

</details>


### [64] [Masked Gated Linear Unit](https://arxiv.org/abs/2506.23225)
*Yukito Tajima, Nakamasa Inoue, Yusuke Sekikawa, Ikuro Sato, Rio Yokota*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种新型的GLU变体，称为Masked Gated Linear Units (MGLUs)，通过引入Mixture of Element-wise Gating (MoEG)架构和FlashMGLU内核，显著减少了内存传输需求，并在推理时间上实现了加速。此外，在大型语言模型实验中，Swish-激活的SwiMGLU变体不仅保留了其内存优势，还匹配甚至超越了SwiGLU基线的下游任务准确性。


<details>
  <summary>更多</summary>
  
**动机:** Gated Linear Units (GLUs)虽然在大型语言模型中表现优异，但它们需要比普通前馈层多一倍的内存读取操作，这是因为GLUs使用了独立的权重矩阵来处理门控和值流。为了解决这一瓶颈问题，本文提出了更高效的GLU变体。

**方法:** 1. 提出了Mixture of Element-wise Gating (MoEG) 架构，该架构通过学习多个二进制掩码，在单个共享权重矩阵上以元素级别决定门控或值分配，从而减少内存传输。
2. 开发了硬件友好的FlashMGLU内核，该内核在RTX5090 GPU上比普通的PyTorch MGLU快19.7倍，同时比标准GLU节省47%的内存并提升34%的速度。

**结果:** 在大型语言模型实验中，Swish激活的SwiMGLU变体在保持内存优势的同时，与SwiGLU基线相比，能够匹配甚至超过其下游任务的准确性。

**结论:** MGLUs通过引入MoEG架构和FlashMGLU内核，成功地减少了内存传输需求，并在推理时间上实现了显著加速。此外，SwiMGLU在大型语言模型中的表现证明了其在保持高效性的同时，还能达到甚至超越现有技术的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Masked+Gated+Linear+Unit，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23225，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23225&send_immediately=true&force_search=false)

**原文摘要:** Gated Linear Units (GLUs) have become essential components in the
feed-forward networks of state-of-the-art Large Language Models (LLMs).
However, they require twice as many memory reads compared to feed-forward
layers without gating, due to the use of separate weight matrices for the gate
and value streams. To address this bottleneck, we introduce Masked Gated Linear
Units (MGLUs), a novel family of GLUs with an efficient kernel implementation.
The core contribution of MGLUs include: (1) the Mixture of Element-wise Gating
(MoEG) architecture that learns multiple binary masks, each determining gate or
value assignments at the element level on a single shared weight matrix
resulting in reduced memory transfer, and (2) FlashMGLU, a hardware-friendly
kernel that yields up to a 19.7 $\times$ inference-time speed-up over a naive
PyTorch MGLU and is 47% more memory-efficient and 34% faster than standard GLUs
despite added architectural complexity on an RTX5090 GPU. In LLM experiments,
the Swish-activated variant SwiMGLU preserves its memory advantages while
matching - or even surpassing - the downstream accuracy of the SwiGLU baseline.

</details>


### [65] [Sub-MoE: Efficient Mixture-of-Expert LLMs Compression via Subspace Expert Merging](https://arxiv.org/abs/2506.23266)
*Lujun Li, Zhu Qiyuan, Jiacheng Wang, Wei Li, Hao Gu, Sirui Han, Yike Guo*

**主要类别:** cs.LG

**AI概要:** 本研究提出了Sub-MoE，一种通过子空间专家合并的MoE压缩框架。它通过联合奇异值分解（SVD）减少参数冲突，并包含自适应专家聚类和子空间专家合并两个阶段。实验表明，Sub-MoE在Mixtral、DeepSeek和Qwen-1.5|3 MoE大语言模型上显著优于现有的专家剪枝和合并方法。


<details>
  <summary>更多</summary>
  
**动机:** 混合专家（MoE）的大语言模型（LLMs）由于其庞大的参数规模，在内存、存储和部署方面面临重大挑战。尽管最近的专家合并方法承诺通过整合多个专家提高效率，但它们受到因专家专业化而产生的参数冲突的根本限制。

**方法:** Sub-MoE通过以下步骤实现：1) 自适应专家聚类：根据专家输出的余弦相似性使用K-means聚类将功能连贯的专家分组；2) 子空间专家合并：执行专家联合分解以提取共享的U矩阵，然后基于频率合并各个V矩阵，并使用合并后的V矩阵完成专家重建。

**结果:** 广泛的实验表明，Sub-MoE在Mixtral、DeepSeek和Qwen-1.5|3 MoE大语言模型上显著优于现有的专家剪枝和合并方法。特别地，在零样本基准测试中，Sub-MoE在Mixtral-8x7B上分别以96%和86%的原始性能实现了25%和50%的专家数量减少。

**结论:** Sub-MoE是一种有效的MoE压缩框架，能够通过子空间专家合并显著减少参数冲突并优化推理。该方法在减少专家数量的同时保持了较高的性能水平，适用于多种MoE大语言模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sub-MoE%3A+Efficient+Mixture-of-Expert+LLMs+Compression+via+Subspace+Expert+Merging，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23266，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23266&send_immediately=true&force_search=false)

**原文摘要:** Mixture of Experts (MoE) LLMs face significant obstacles due to their massive
parameter scale, which imposes memory, storage, and deployment challenges.
Although recent expert merging methods promise greater efficiency by
consolidating multiple experts, they are fundamentally hindered by parameter
conflicts arising from expert specialization. In this paper, we present
Sub-MoE, a novel MoE compression framework via Subspace Expert Merging. Our key
insight is to perform joint Singular Value Decomposition (SVD) on concatenated
expert weights, reducing conflicting parameters by extracting shared
$U$-matrices while enabling effective merging of the expert-specific $V$
components. Specifically, Sub-MoE consists of two innovative phases: (1)
Adaptive Expert Clustering, which groups functionally coherent experts via
K-means clustering based on cosine similarity of expert outputs; and (2)
Subspace Expert Merging, which first enforces Experts Union Decomposition to
derive the shared $U$-matrix across experts in the same group, then pursues
frequency-based merging for individual $V$-matrices, and finalizes expert
reconstruction using the merged $V$-matrix. In this way, we align and fuse
experts in a shared subspace, and can be extended with intra-expert compression
for further inference optimization. Extensive experiments on Mixtral, DeepSeek,
and Qwen-1.5|3 MoE LLMs demonstrate that our Sub-MoE significantly outperforms
existing expert pruning and merging methods. Notably, our Sub-MoE maintains
96\%|86\% of original performance with 25\%|50\% expert reduction on
Mixtral-8x7B in zero-shot benchmarks. Code will be released at
https://github.com/lliai/MoERazor.

</details>


### [66] [Predicting thinking time in Reasoning models](https://arxiv.org/abs/2506.23274)
*Hans Peter Lynsgøe Raaschou-jensen, Constanza Fierro, Anders Søgaard*

**主要类别:** cs.LG

**AI概要:** 论文探索了预测模型思考时间的方法，旨在为推理过程提供实用的进度条，改善用户体验。


<details>
  <summary>更多</summary>
  
**动机:** 当前推理模型在处理复杂任务时表现出色，但用户无法预知模型完成推理所需的时间，这种不可预测性可能导致用户不满。

**方法:** 研究在线和离线预测模型思考时间的方法，开发出一个实际可用的“推理进度条”。

**结果:** 提出了预测模型思考时间的方法，并讨论了其对用户交互的影响及未来研究方向。

**结论:** 开发推理进度条能够提升用户体验，需要进一步研究以优化预测方法和增强用户交互设计。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Predicting+thinking+time+in+Reasoning+models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23274，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23274&send_immediately=true&force_search=false)

**原文摘要:** Reasoning models that produce long, hidden chains of thought have emerged as
powerful tools for complex, reasoning-intensive
tasks\citep{deepseekai2025deepseekr1incentivizingreasoningcapability,
openai2024openaio1card}. However, this paradigm introduces a new user
experience challenge: users have little insight into how much time the model
will spend reasoning before returning an answer. This unpredictability, can
lead to user frustration and is likely to compound as LLMs can produce
increasingly long tasks asynchronously
\citep{kwa2025measuringaiabilitycomplete}. In this paper, we introduce and
evaluate methods for both online and offline prediction of model "thinking
time," aiming to develop a practical "progress bar for reasoning." We discuss
the implications for user interaction and future research directions.

</details>


### [67] [BAPE: Learning an Explicit Bayes Classifier for Long-tailed Visual Recognition](https://arxiv.org/abs/2506.23280)
*Chaoqun Du, Yulin Wang, Shiji Song, Gao Huang*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种新方法（BAPE），通过显式建模后验概率参数并使用点估计求解，直接学习贝叶斯分类器，从而缓解梯度不平衡问题并确保贝叶斯最优决策规则。此外，还提出了分布调整技术以适应任意不平衡因子的测试数据分布，提高泛化性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前深度学习算法在处理长尾数据分布时存在梯度不平衡问题，无法确保贝叶斯最优决策规则。

**方法:** 提出一种新方法（BAPE）显式建模后验概率参数并使用点估计求解，同时提出分布调整技术以适应测试数据分布。

**结果:** 在CIFAR-10-LT、CIFAR-100-LT、ImageNet-LT和iNaturalist等数据集上的广泛实证评估表明，该方法显著提高了流行深度网络的泛化性能。

**结论:** 所提出的方法简单有效，能够显著改善长尾数据场景下的模型泛化能力，且与现有方法正交。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BAPE%3A+Learning+an+Explicit+Bayes+Classifier+for+Long-tailed+Visual+Recognition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23280，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23280&send_immediately=true&force_search=false)

**原文摘要:** Bayesian decision theory advocates the Bayes classifier as the optimal
approach for minimizing the risk in machine learning problems. Current deep
learning algorithms usually solve for the optimal classifier by
\emph{implicitly} estimating the posterior probabilities, \emph{e.g.}, by
minimizing the Softmax cross-entropy loss. This simple methodology has been
proven effective for meticulously balanced academic benchmark datasets.
However, it is not applicable to the long-tailed data distributions in the real
world, where it leads to the gradient imbalance issue and fails to ensure the
Bayes optimal decision rule. To address these challenges, this paper presents a
novel approach (BAPE) that provides a more precise theoretical estimation of
the data distributions by \emph{explicitly} modeling the parameters of the
posterior probabilities and solving them with point estimation. Consequently,
our method directly learns the Bayes classifier without gradient descent based
on Bayes' theorem, simultaneously alleviating the gradient imbalance and
ensuring the Bayes optimal decision rule. Furthermore, we propose a
straightforward yet effective \emph{distribution adjustment} technique. This
method enables the Bayes classifier trained from the long-tailed training set
to effectively adapt to the test data distribution with an arbitrary imbalance
factor, thereby enhancing performance without incurring additional
computational costs. In addition, we demonstrate the gains of our method are
orthogonal to existing learning approaches for long-tailed scenarios, as they
are mostly designed under the principle of \emph{implicitly} estimating the
posterior probabilities. Extensive empirical evaluations on CIFAR-10-LT,
CIFAR-100-LT, ImageNet-LT, and iNaturalist demonstrate that our method
significantly improves the generalization performance of popular deep networks,
despite its simplicity.

</details>


### [68] [Hierarchical Quantized Diffusion Based Tree Generation Method for Hierarchical Representation and Lineage Analysis](https://arxiv.org/abs/2506.23287)
*Zelin Zang, WenZhe Li, Fei Chen, Yongjie Xu, Chang Yu, Zhen Lei, Stan Z. Li*

**主要类别:** cs.LG

**AI概要:** 在单细胞研究中，追踪和分析高通量的单细胞分化轨迹对于理解复杂的生物过程至关重要。传统的建模方法存在计算成本、性能、生成能力和稳定性方面的限制。基于VAE的方法虽然有所改进，但仍然需要针对每个树分支的专用网络模块，这限制了其稳定性和捕捉深层层次关系的能力。为了解决这些挑战，我们引入了一种基于扩散的方法HDTree。HDTree通过统一的分层代码库和量化扩散过程，在分层潜在空间中捕捉树状关系，并通过消除分支特定模块和模拟渐进层次变化来增强生成能力。HDTree在通用和单细胞数据集上的表现优于现有方法，为分层谱系分析提供了新工具，能够更准确和高效地建模细胞分化路径，并为下游生物任务提供洞察力。


<details>
  <summary>更多</summary>
  
**动机:** 在单细胞研究中，追踪和分析高通量单细胞分化轨迹是关键，但传统方法和基于VAE的方法存在各种局限性，包括计算成本、性能、生成能力和稳定性方面的问题。因此，需要一种新的方法来克服这些限制，特别是在捕捉深层层次关系和提高稳定性方面。

**方法:** HDTree是一种基于扩散的方法，它通过统一的分层代码库和量化扩散过程，在分层潜在空间中捕捉树状关系。该方法通过消除分支特定模块来提高稳定性，并通过扩散过程模拟的渐进层次变化来增强生成能力。

**结果:** HDTree在通用和单细胞数据集上的表现优于现有方法，显示出更高的准确性和性能。

**结论:** HDTree为分层谱系分析提供了新工具，能够更准确和高效地建模细胞分化路径，并为下游生物任务提供洞察力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hierarchical+Quantized+Diffusion+Based+Tree+Generation+Method+for+Hierarchical+Representation+and+Lineage+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23287，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23287&send_immediately=true&force_search=false)

**原文摘要:** In single-cell research, tracing and analyzing high-throughput single-cell
differentiation trajectories is crucial for understanding complex biological
processes. Key to this is the modeling and generation of hierarchical data that
represents the intrinsic structure within datasets. Traditional methods face
limitations in terms of computational cost, performance, generative capacity,
and stability. Recent VAEs based approaches have made strides in addressing
these challenges but still require specialized network modules for each tree
branch, limiting their stability and ability to capture deep hierarchical
relationships. To overcome these challenges, we introduce diffusion-based
approach called HDTree. HDTree captures tree relationships within a
hierarchical latent space using a unified hierarchical codebook and quantized
diffusion processes to model tree node transitions. This method improves
stability by eliminating branch-specific modules and enhancing generative
capacity through gradual hierarchical changes simulated by the diffusion
process. HDTree's effectiveness is demonstrated through comparisons on both
general-purpose and single-cell datasets, where it outperforms existing methods
in terms of accuracy and performance. These contributions provide a new tool
for hierarchical lineage analysis, enabling more accurate and efficient
modeling of cellular differentiation paths and offering insights for downstream
biological tasks. The code of HDTree is available at anonymous link
https://anonymous.4open.science/r/code_HDTree_review-A8DB.

</details>


### [69] [VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular Design](https://arxiv.org/abs/2506.23339)
*Malikussaid, Hilal Hudan Nuha*

**主要类别:** cs.LG

**AI概要:** VALID-Mol 是一个结合化学验证与大型语言模型（LLMs）驱动的分子设计框架，通过精细提示工程、自动化化学验证和领域适配的 LLM 提升了生成有效化学结构的比例从 3% 到 83%，并展示了高达 17 倍的目标亲和力预测改进。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大语言模型在科学发现中具有巨大潜力，但在需要事实准确性及领域特定约束的应用中仍存在挑战。特别是在药物分子设计中，虽然 LLMs 可以提出创新的分子修改方案，但常常产生化学上无效或不实用的结构。因此，需要一种系统化的方法来整合化学验证与 LLM 驱动的分子设计。

**方法:** 作者开发了 VALID-Mol 框架，该方法结合了精细提示工程、自动化的化学验证以及领域适配的微调 LLM。这种方法确保了可合成分子的可靠生成，并改善了分子属性。此外，他们还提供了一个通用方法论，适用于其他需要科学约束的 LLM 应用领域。

**结果:** 通过使用 VALID-Mol 框架，生成有效化学结构的成功率从 3% 提高到了 83%。计算预测表明，生成的分子在目标亲和力方面有高达 17 倍的提升，同时保持了合成可达性。

**结论:** VALID-Mol 不仅提高了分子设计中生成有效化学结构的成功率，还为其他需要领域特定验证的科学应用提供了可重复的蓝图。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VALID-Mol%3A+a+Systematic+Framework+for+Validated+LLM-Assisted+Molecular+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23339，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23339&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) demonstrate remarkable potential for scientific
discovery, but their application in domains requiring factual accuracy and
domain-specific constraints remains challenging. In molecular design for drug
discovery, LLMs can suggest creative molecular modifications but often produce
chemically invalid or impractical structures. We present VALID-Mol, a
systematic framework for integrating chemical validation with LLM-driven
molecular design that increases the rate of generating valid chemical
structures from 3% to 83%. Our approach combines methodical prompt engineering,
automated chemical validation, and a fine-tuned domain-adapted LLM to ensure
reliable generation of synthesizable molecules with improved properties. Beyond
the specific implementation, we contribute a generalizable methodology for
scientifically-constrained LLM applications, with quantifiable reliability
improvements. Computational predictions suggest our framework can generate
promising candidates for synthesis with up to 17-fold computationally predicted
improvements in target affinity while maintaining synthetic accessibility. We
provide a detailed analysis of our prompt engineering process, validation
architecture, and fine-tuning approach, offering a reproducible blueprint for
applying LLMs to other scientific domains where domain-specific validation is
essential.

</details>


### [70] [A case for data valuation transparency via DValCards](https://arxiv.org/abs/2506.23349)
*Keziah Naggita, Julienne LaChance*

**主要类别:** cs.LG

**AI概要:** 数据估值方法在简单的算法设计选择下存在偏差和不稳定，可能导致技术和伦理问题。本文通过分析9个表格分类数据集和6种数据估值方法，揭示了数据预处理、子采样和数据代表性对数据估值的影响，并提出了Data Valuation Cards (DValCards)框架以提高透明度，减少误用并建立信任。


<details>
  <summary>更多</summary>
  
**动机:** 数据驱动的机器学习日益流行，数据估值方法被提出用于量化数据点对模型性能的贡献。此外，在数据市场中，数据买方可能利用这些方法公平补偿数据所有者。然而，现有方法可能存在偏差和不稳定性，需要进一步研究其影响及改进措施。

**方法:** 分析9个表格分类数据集和6种数据估值方法，评估以下方面：(1) 数据预处理技术对估值的影响；(2) 子采样是否增加类别不平衡；(3) 是否低估少数群体的数据价值。基于此，提出DValCards框架以提升数据估值的透明度。

**结果:** (1) 常见且低成本的数据预处理技术显著改变数据估值；(2) 使用数据估值指标进行子采样可能增加类别不平衡；(3) 数据估值方法可能低估少数群体的数据价值。

**结论:** 数据估值方法存在固有偏差和不稳定，可能导致技术与伦理问题。为减少误用并增强信任，需提高数据估值的透明度，而DValCards框架为此提供了一种解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+case+for+data+valuation+transparency+via+DValCards，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23349，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23349&send_immediately=true&force_search=false)

**原文摘要:** Following the rise in popularity of data-centric machine learning (ML),
various data valuation methods have been proposed to quantify the contribution
of each datapoint to desired ML model performance metrics (e.g., accuracy).
Beyond the technical applications of data valuation methods (e.g., data
cleaning, data acquisition, etc.), it has been suggested that within the
context of data markets, data buyers might utilize such methods to fairly
compensate data owners. Here we demonstrate that data valuation metrics are
inherently biased and unstable under simple algorithmic design choices,
resulting in both technical and ethical implications. By analyzing 9 tabular
classification datasets and 6 data valuation methods, we illustrate how (1)
common and inexpensive data pre-processing techniques can drastically alter
estimated data values; (2) subsampling via data valuation metrics may increase
class imbalance; and (3) data valuation metrics may undervalue underrepresented
group data. Consequently, we argue in favor of increased transparency
associated with data valuation in-the-wild and introduce the novel Data
Valuation Cards (DValCards) framework towards this aim. The proliferation of
DValCards will reduce misuse of data valuation metrics, including in data
pricing, and build trust in responsible ML systems.

</details>


### [71] [Federated Timeline Synthesis: Scalable and Private Methodology For Model Training and Deployment](https://arxiv.org/abs/2506.23358)
*Pawel Renc, Michal K. Grzeszczyk, Linglong Qian, Nassim Oufattole, Jeff Rasley, Arkadiusz Sitek*

**主要类别:** cs.LG

**AI概要:** 本研究提出了Federated Timeline Synthesis (FTS)，一种用于在分布式时间序列数据上训练生成性基础模型的新框架，特别适用于电子健康记录（EHR）。通过表示患者历史为标记化的患者健康时间线（PHTs），各机构在本地训练自回归变压器并仅传输模型权重到中央服务器。服务器使用生成器合成大量轨迹以训练全局生成器（GG），并通过蒙特卡洛模拟未来PHTs进行零样本推断。在MIMIC-IV数据上的五个临床预测任务中，基于GG生成的合成数据训练的模型表现与基于真实数据训练的模型相当。FTS提供了强大的隐私保护、跨机构可扩展性，并可扩展到多样化的预测和模拟任务，特别是在医疗保健领域，包括反事实推断、早期预警检测和合成试验设计。


<details>
  <summary>更多</summary>
  
**动机:** 当前在电子健康记录（EHR）中的分布式时间序列数据分析面临隐私保护、跨机构协作及数据可用性等挑战。为了克服这些限制，需要一个既能保护隐私又能利用多机构数据进行高效训练和推理的新框架。

**方法:** FTS将患者的病史编码为时间化、分类和连续临床信息的时间序列（PHTs）。每个机构在其本地数据上训练自回归变换器，并仅共享模型权重至中央服务器。中央服务器利用接收到的权重合成大规模轨迹，并训练全局生成器（GG）。通过GG，可以对未来的PHTs进行蒙特卡洛模拟以实现零样本推断。

**结果:** 在MIMIC-IV数据集上进行了五个临床预测任务的评估，结果显示使用由GG生成的合成数据训练的模型性能与使用真实数据训练的模型相当。这表明FTS在隐私保护和跨机构数据协作方面具有显著优势。

**结论:** FTS提供了一种有效的方法来解决分布式时间序列数据在隐私保护和跨机构协作中的挑战，同时展示了其在多样化预测和模拟任务中的潜力，特别是在医疗保健领域，如反事实推断、早期预警检测和合成试验设计。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Federated+Timeline+Synthesis%3A+Scalable+and+Private+Methodology+For+Model+Training+and+Deployment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23358，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23358&send_immediately=true&force_search=false)

**原文摘要:** We present Federated Timeline Synthesis (FTS), a novel framework for training
generative foundation models across distributed timeseries data applied to
electronic health records (EHR). At its core, FTS represents patient history as
tokenized Patient Health Timelines (PHTs), language-agnostic sequences encoding
temporal, categorical, and continuous clinical information. Each institution
trains an autoregressive transformer on its local PHTs and transmits only model
weights to a central server. The server uses the generators to synthesize a
large corpus of trajectories and train a Global Generator (GG), enabling
zero-shot inference via Monte Carlo simulation of future PHTs. We evaluate FTS
on five clinically meaningful prediction tasks using MIMIC-IV data, showing
that models trained on synthetic data generated by GG perform comparably to
those trained on real data. FTS offers strong privacy guarantees, scalability
across institutions, and extensibility to diverse prediction and simulation
tasks especially in healthcare, including counterfactual inference, early
warning detection, and synthetic trial design.

</details>


### [72] [When Additive Noise Meets Unobserved Mediators: Bivariate Denoising Diffusion for Causal Discovery](https://arxiv.org/abs/2506.23374)
*Dominik Meier, Sujai Hiremath, Promit Ghosal, Kyra Gan*

**主要类别:** cs.LG

**AI概要:** 这篇论文主要研究了在存在未观测中介变量的情况下，如何改进二元因果关系发现的问题。作者提出了Bivariate Denoising Diffusion (BiDD) 方法，并通过实验证明该方法在处理受中介变量干扰的数据时表现优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 传统的加性噪声模型（ANM）方法在存在未观测中介变量时失效，因此需要一种新的方法来解决这个问题。

**方法:** 提出了一种名为Bivariate Denoising Diffusion (BiDD)的新方法。该方法通过在对每个变量进行加噪和去噪的过程中，以另一个变量为条件输入，评估预测噪声与该输入的独立性，从而推断因果方向。

**结果:** 实验结果表明，BiDD方法在处理受中介变量干扰的数据时表现出色，优于现有方法；同时在无中介变量的情况下也保持了良好的性能。

**结论:** BiDD方法在理论上和实验上都证明了其在二元因果关系发现中的有效性和优越性，特别是在存在未观测中介变量的情况下。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+Additive+Noise+Meets+Unobserved+Mediators%3A+Bivariate+Denoising+Diffusion+for+Causal+Discovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23374，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23374&send_immediately=true&force_search=false)

**原文摘要:** Distinguishing cause and effect from bivariate observational data is a
foundational problem in many disciplines, but challenging without additional
assumptions. Additive noise models (ANMs) are widely used to enable
sample-efficient bivariate causal discovery. However, conventional ANM-based
methods fail when unobserved mediators corrupt the causal relationship between
variables. This paper makes three key contributions: first, we rigorously
characterize why standard ANM approaches break down in the presence of
unmeasured mediators. Second, we demonstrate that prior solutions for hidden
mediation are brittle in finite sample settings, limiting their practical
utility. To address these gaps, we propose Bivariate Denoising Diffusion (BiDD)
for causal discovery, a method designed to handle latent noise introduced by
unmeasured mediators. Unlike prior methods that infer directionality through
mean squared error loss comparisons, our approach introduces a novel
independence test statistic: during the noising and denoising processes for
each variable, we condition on the other variable as input and evaluate the
independence of the predicted noise relative to this input. We prove asymptotic
consistency of BiDD under the ANM, and conjecture that it performs well under
hidden mediation. Experiments on synthetic and real-world data demonstrate
consistent performance, outperforming existing methods in mediator-corrupted
settings while maintaining strong performance in mediator-free settings.

</details>


### [73] [Do LLMs Dream of Discrete Algorithms?](https://arxiv.org/abs/2506.23408)
*Claudionor Coelho Jr, Yanen Li, Philip Tee*

**主要类别:** cs.LG

**AI概要:** Large Language Models (LLMs) are powerful but struggle with strict logical reasoning. This paper proposes augmenting LLMs with logic-based reasoning modules using Prolog predicates to improve their reliability and interpretability.


<details>
  <summary>更多</summary>
  
**动机:** LLMs have revolutionized AI but face challenges in domains requiring strict logical reasoning, discrete decision-making, and robust interpretability due to their reliance on probabilistic inference.

**方法:** The paper investigates the limitations of LLMs and proposes a neurosymbolic approach that integrates first-order logic and explicit rule systems, using Prolog predicates and composable toolsets to augment LLMs.

**结果:** Experiments on the DABStep benchmark demonstrate improved precision, coverage, and system documentation in multi-step reasoning tasks.

**结论:** Combining LLMs with modular logic reasoning enhances engineering rigor, system reliability, and offers a scalable path toward trustworthy, interpretable AI agents.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Do+LLMs+Dream+of+Discrete+Algorithms%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23408，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23408&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have rapidly transformed the landscape of
artificial intelligence, enabling natural language interfaces and dynamic
orchestration of software components. However, their reliance on probabilistic
inference limits their effectiveness in domains requiring strict logical
reasoning, discrete decision-making, and robust interpretability. This paper
investigates these limitations and proposes a neurosymbolic approach that
augments LLMs with logic-based reasoning modules, particularly leveraging
Prolog predicates and composable toolsets. By integrating first-order logic and
explicit rule systems, our framework enables LLMs to decompose complex queries
into verifiable sub-tasks, orchestrate reliable solutions, and mitigate common
failure modes such as hallucination and incorrect step decomposition. We
demonstrate the practical benefits of this hybrid architecture through
experiments on the DABStep benchmark, showing improved precision, coverage, and
system documentation in multi-step reasoning tasks. Our results indicate that
combining LLMs with modular logic reasoning restores engineering rigor,
enhances system reliability, and offers a scalable path toward trustworthy,
interpretable AI agents across complex domains.

</details>


### [74] [BenchMake: Turn any scientific data set into a reproducible benchmark](https://arxiv.org/abs/2506.23419)
*Amanda S Barnard*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种新工具BenchMake，它能够将公开的科学数据集转化为基准测试集。该工具使用非负矩阵分解来识别和隔离具有挑战性的边界案例，并划分测试集以最大化差异和统计显著性。通过与已建立的分割和随机分割进行比较，证明了BenchMake在多种类型数据集上的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 计算科学中基准数据集的稀缺性使得评估新方法变得困难，因此需要一种工具将越来越多的公开科学数据集转化为可供社区使用的基准。

**方法:** 开发了一个名为BenchMake的新工具，利用非负矩阵分解确定性地识别和隔离凸壳上的挑战性边缘案例，并将匹配的数据实例划分为测试集，从而在表格、图、图像、信号和文本模态上最大化差异和统计显著性。

**结果:** 通过对十个来自不同科学领域的公共可用基准数据集进行比较，结果表明BenchMake分割相较于已建立的分割和随机分割具有优势。

**结论:** BenchMake提供了一种有效的方法，可以将公开的科学数据集转化为可靠的基准测试集，为计算科学家提供了新的评价工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BenchMake%3A+Turn+any+scientific+data+set+into+a+reproducible+benchmark，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23419，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23419&send_immediately=true&force_search=false)

**原文摘要:** Benchmark data sets are a cornerstone of machine learning development and
applications, ensuring new methods are robust, reliable and competitive. The
relative rarity of benchmark sets in computational science, due to the
uniqueness of the problems and the pace of change in the associated domains,
makes evaluating new innovations difficult for computational scientists. In
this paper a new tool is developed and tested to potentially turn any of the
increasing numbers of scientific data sets made openly available into a
benchmark accessible to the community. BenchMake uses non-negative matrix
factorisation to deterministically identify and isolate challenging edge cases
on the convex hull (the smallest convex set that contains all existing data
instances) and partitions a required fraction of matched data instances into a
testing set that maximises divergence and statistical significance, across
tabular, graph, image, signal and textual modalities. BenchMake splits are
compared to establish splits and random splits using ten publicly available
benchmark sets from different areas of science, with different sizes, shapes,
distributions.

</details>


### [75] [Accurate Parameter-Efficient Test-Time Adaptation for Time Series Forecasting](https://arxiv.org/abs/2506.23424)
*Heitor R. Medeiros, Hossein Sharifi-Noghabi, Gabriel L. Oliveira, Saghar Irandoust*

**主要类别:** cs.LG

**AI概要:** 提出了一种参数高效的测试时间适应方法PETSA，通过仅更新输入和输出的小校准模块来适应预测器。实验结果表明，PETSA在所有预测范围内实现了与基线相当或更好的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界中的时间序列通常表现出非平稳性，这会降低预训练预测模型的性能。现有的TTA方法通常更新整个模型，增加了内存和计算成本。

**方法:** PETSA使用低秩适配器和动态门控机制，在不重新训练的情况下调整表示。为了在有限的适应能力下保持准确性，引入了一个专门的损失函数，结合了三个组件：稳健项、频域项和块状结构项。

**结果:** PETSA提高了各种预测主干的适应性，同时比基线需要更少的参数。实验结果表明，PETSA在所有预测范围内实现了与基线相当或更好的性能。

**结论:** PETSA是一种参数高效的测试时间适应方法，能够在减少参数需求的同时提高预测模型的适应性和性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Accurate+Parameter-Efficient+Test-Time+Adaptation+for+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23424，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23424&send_immediately=true&force_search=false)

**原文摘要:** Real-world time series often exhibit a non-stationary nature, degrading the
performance of pre-trained forecasting models. Test-Time Adaptation (TTA)
addresses this by adjusting models during inference, but existing methods
typically update the full model, increasing memory and compute costs. We
propose PETSA, a parameter-efficient method that adapts forecasters at test
time by only updating small calibration modules on the input and output. PETSA
uses low-rank adapters and dynamic gating to adjust representations without
retraining. To maintain accuracy despite limited adaptation capacity, we
introduce a specialized loss combining three components: (1) a robust term, (2)
a frequency-domain term to preserve periodicity, and (3) a patch-wise
structural term for structural alignment. PETSA improves the adaptability of
various forecasting backbones while requiring fewer parameters than baselines.
Experimental results on benchmark datasets show that PETSA achieves competitive
or better performance across all horizons. Our code is available at:
https://github.com/BorealisAI/PETSA

</details>


### [76] [Enhancing Insider Threat Detection Using User-Based Sequencing and Transformer Encoders](https://arxiv.org/abs/2506.23446)
*Mohamed Elbasheer, Adewale Akinfaderin*

**主要类别:** cs.LG

**AI概要:** 内部威胁检测由于恶意行为者的授权状态和异常行为的微妙性而面临独特挑战。本文提出了一种基于用户的序列化（UBS）方法，将CERT内部威胁数据集转换为适合深度序列建模的结构化时间序列，并使用Transformer Encoder架构对良性用户活动进行建模，通过其重建误差作为异常评分。该方法在四个严格设计的测试集中均达到了最先进的性能，准确率为96.61%，召回率为99.43%，F1得分为96.38%，AUROC为95.00%，且假阴性和假阳性率极低。


<details>
  <summary>更多</summary>
  
**动机:** 现有的机器学习方法通常将用户活动视为孤立事件，未能利用用户行为中的顺序依赖关系，因此需要一种能够捕捉用户行为时序特性的新方法来提升内部威胁检测的准确性。

**方法:** 提出了一种User-Based Sequencing (UBS) 方法，将内部威胁数据集转化为结构化的时间序列；采用Transformer Encoder架构对良性用户活动进行建模，并通过其重建误差生成异常评分；最后使用三种无监督异常检测算法（One-Class SVM、Local Outlier Factor 和 Isolation Forest）对评分进行评估。

**结果:** 在四个精心设计的测试集中，UBS-Transformer管道表现出卓越性能：准确率达到96.61%，召回率达到99.43%，F1得分为96.38%，AUROC达到95.00%，并且假阴性率和假阳性率非常低（分别为0.0057和0.0571）。相比传统的表格数据和自动编码器基线模型，该方法显著提升了检测效果。

**结论:** 基于用户行为序列化的建模方法和先进的异常检测技术在内部威胁检测领域具有显著优势，能够有效捕捉用户行为的时序特性并实现高精度的异常检测。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+Insider+Threat+Detection+Using+User-Based+Sequencing+and+Transformer+Encoders，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23446，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23446&send_immediately=true&force_search=false)

**原文摘要:** Insider threat detection presents unique challenges due to the authorized
status of malicious actors and the subtlety of anomalous behaviors. Existing
machine learning methods often treat user activity as isolated events, thereby
failing to leverage sequential dependencies in user behavior. In this study, we
propose a User-Based Sequencing (UBS) methodology, transforming the CERT
insider threat dataset into structured temporal sequences suitable for deep
sequential modeling. We deploy a Transformer Encoder architecture to model
benign user activity and employ its reconstruction errors as anomaly scores.
These scores are subsequently evaluated using three unsupervised outlier
detection algorithms: One-Class SVM (OCSVM), Local Outlier Factor (LOF), and
Isolation Forest (iForest). Across four rigorously designed test sets,
including combinations of multiple CERT dataset releases, our UBS-Transformer
pipeline consistently achieves state-of-the-art performance - notably 96.61%
accuracy, 99.43% recall, 96.38% F1-score, 95.00% AUROC, and exceptionally low
false negative (0.0057) and false positive (0.0571) rates. Comparative analyses
demonstrate that our approach substantially outperforms tabular and
conventional autoencoder baselines, underscoring the efficacy of sequential
user modeling and advanced anomaly detection in the insider threat domain.

</details>


### [77] [Can We Predict the Unpredictable? Leveraging DisasterNet-LLM for Multimodal Disaster Classification](https://arxiv.org/abs/2506.23462)
*Manaswi Kulahara, Gautam Siddharth Kashyap, Nipun Joshi, Arpita Soni*

**主要类别:** cs.LG

**AI概要:** 提出DisasterNet-LLM，专为灾害分析设计的大型语言模型，利用先进的预训练、跨模态注意力机制和自适应变压器，在多模态灾害分类任务中表现出色，准确率达到89.5%。


<details>
  <summary>更多</summary>
  
**动机:** 有效的灾害管理需要及时和准确的见解，但传统方法难以整合多模态数据（如图像、天气记录和文本报告）。

**方法:** 提出了DisasterNet-LLM，一个专门用于全面灾害分析的大型语言模型，通过利用先进的预训练、跨模态注意力机制和自适应变压器进行灾害分类。

**结果:** 实验结果表明，该模型在多模态灾害分类任务中优于最先进的模型，准确率为89.5%，F1得分为88.0%，AUC为0.92%，BERTScore为0.88%。

**结论:** DisasterNet-LLM在多模态灾害分类任务中的表现优于现有模型，可提供更准确和及时的灾害管理见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Can+We+Predict+the+Unpredictable%3F+Leveraging+DisasterNet-LLM+for+Multimodal+Disaster+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23462，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23462&send_immediately=true&force_search=false)

**原文摘要:** Effective disaster management requires timely and accurate insights, yet
traditional methods struggle to integrate multimodal data such as images,
weather records, and textual reports. To address this, we propose
DisasterNet-LLM, a specialized Large Language Model (LLM) designed for
comprehensive disaster analysis. By leveraging advanced pretraining,
cross-modal attention mechanisms, and adaptive transformers, DisasterNet-LLM
excels in disaster classification. Experimental results demonstrate its
superiority over state-of-the-art models, achieving higher accuracy of 89.5%,
an F1 score of 88.0%, AUC of 0.92%, and BERTScore of 0.88% in multimodal
disaster classification tasks.

</details>


### [78] [Reconciling Attribute and Structural Anomalies for Improved Graph Anomaly Detection](https://arxiv.org/abs/2506.23469)
*Chunjing Xiao, Jiahui Lu, Xovee Xu, Fan Zhou, Tianshu Xie, Wei Lu, Lifeng Xu*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为TripleAD的三通道图异常检测框架，通过三个不同的估计模块分别识别属性、结构和混合异常，并引入相互蒸馏策略促进模块间的协作。实验表明该模型在多个基准上表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 图异常检测在医疗保健和经济学等领域至关重要，可以预防重大损失。然而，现有的无监督方法在同时检测属性和结构异常时面临性能次优的问题，因为两种异常类型之间存在干扰。

**方法:** TripleAD框架包含三个估计模块：1) 多尺度属性估计模块用于捕捉广泛的节点交互并缓解过平滑问题；2) 链接增强结构估计模块用于更好地识别结构异常，促进孤立节点的信息流动；3) 属性-混合曲率模块作为新指标，结合属性和结构信息以区分混合异常。此外，还引入了相互蒸馏策略以鼓励三个模块之间的通信与协作。

**结果:** 广泛的实验结果证明了TripleAD模型的有效性，其在多种基准测试中优于强大的基线模型。

**结论:** TripleAD通过设计专门的模块来解决属性和结构异常之间的干扰问题，并通过相互蒸馏策略提升了整体性能，为图异常检测提供了有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reconciling+Attribute+and+Structural+Anomalies+for+Improved+Graph+Anomaly+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23469，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23469&send_immediately=true&force_search=false)

**原文摘要:** Graph anomaly detection is critical in domains such as healthcare and
economics, where identifying deviations can prevent substantial losses.
Existing unsupervised approaches strive to learn a single model capable of
detecting both attribute and structural anomalies. However, they confront the
tug-of-war problem between two distinct types of anomalies, resulting in
suboptimal performance. This work presents TripleAD, a mutual
distillation-based triple-channel graph anomaly detection framework. It
includes three estimation modules to identify the attribute, structural, and
mixed anomalies while mitigating the interference between different types of
anomalies. In the first channel, we design a multiscale attribute estimation
module to capture extensive node interactions and ameliorate the over-smoothing
issue. To better identify structural anomalies, we introduce a link-enhanced
structure estimation module in the second channel that facilitates information
flow to topologically isolated nodes. The third channel is powered by an
attribute-mixed curvature, a new indicator that encapsulates both attribute and
structural information for discriminating mixed anomalies. Moreover, a mutual
distillation strategy is introduced to encourage communication and
collaboration between the three channels. Extensive experiments demonstrate the
effectiveness of the proposed TripleAD model against strong baselines.

</details>


### [79] [Sample Margin-Aware Recalibration of Temperature Scaling](https://arxiv.org/abs/2506.23492)
*Haolan Guo, Linwei Tao, Haoyang Luo, Minjing Dong, Chang Xu*

**主要类别:** cs.LG

**AI概要:** SMART是一种轻量级、数据高效的方法，通过基于logit gap的重新校准和新颖的SoftECE目标函数，在显著减少参数的情况下实现了最先进的模型校准性能。


<details>
  <summary>更多</summary>
  
**动机:** 现代神经网络存在系统性过度自信的问题，而现有的后验校准方法要么引入高偏差（全局方法），要么因高维噪声输入和有限验证数据而产生高方差（表达能力强的方法）。

**方法:** 提出了一种名为SMART的方法，该方法根据前两大logits之间的差距（logit gap）精确缩放logits，并使用一种新的SoftECE目标函数，通过自适应分箱平衡模型偏差和方差。

**结果:** 在多个数据集和架构上的广泛评估表明，SMART即使在参数数量显著减少的情况下也能实现最先进的校准性能。

**结论:** SMART提供了一种有原则、稳健且高效的解决方案，用于实际应用中的神经网络预测不确定性量化。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sample+Margin-Aware+Recalibration+of+Temperature+Scaling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23492，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23492&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in deep learning have significantly improved predictive
accuracy. However, modern neural networks remain systematically overconfident,
posing risks for deployment in safety-critical scenarios. Current post-hoc
calibration methods face a fundamental dilemma: global approaches like
Temperature Scaling apply uniform adjustments across all samples, introducing
high bias despite computational efficiency, while more expressive methods that
operate on full logit distributions suffer from high variance due to noisy
high-dimensional inputs and insufficient validation data. To address these
challenges, we propose Sample Margin-Aware Recalibration of Temperature
(SMART), a lightweight, data-efficient recalibration method that precisely
scales logits based on the margin between the top two logits -- termed the
logit gap. Specifically, the logit gap serves as a denoised, scalar signal
directly tied to decision boundary uncertainty, providing a robust indicator
that avoids the noise inherent in high-dimensional logit spaces while
preserving model prediction invariance. Meanwhile, SMART employs a novel
soft-binned Expected Calibration Error (SoftECE) objective that balances model
bias and variance through adaptive binning, enabling stable parameter updates
even with extremely limited calibration data. Extensive evaluations across
diverse datasets and architectures demonstrate that SMART achieves
state-of-the-art calibration performance even with substantially fewer
parameters compared to existing parametric methods, offering a principled,
robust, and highly efficient solution for practical uncertainty quantification
in neural network predictions. The source code is available at:
https://anonymous.4open.science/r/SMART-8B11.

</details>


### [80] [FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization](https://arxiv.org/abs/2506.23516)
*Seung-Wook Kim, Seongyeol Kim, Jiah Kim, Seowon Ji, Se-Ho Lee*

**主要类别:** cs.LG

**AI概要:** 在联邦学习（FL）中，数据异构性和通信约束常常导致性能下降。为了解决这些问题，本文提出了一个名为FedWSQ的新框架，该框架集成了权重标准化（WS）和分布感知非均匀量化（DANUQ）。实验表明，FedWSQ在各种具有挑战性的FL环境中，包括极端数据异构性和超低比特通信场景，均优于现有的FL方法。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习经常由于数据异构性和通信限制而遭受性能下降的问题。为了应对这些挑战，需要一种新的框架来提升联邦学习的性能。

**方法:** 提出了一种新的联邦学习框架FedWSQ，它结合了权重标准化（WS）和分布感知非均匀量化（DANUQ）。WS通过过滤训练过程中本地更新中的偏差成分来增强模型对数据异构性和不稳定的客户端参与的鲁棒性。DANUQ利用本地模型更新的统计特性来最小化量化误差。

**结果:** FedWSQ显著减少了通信开销，同时保持了较高的模型准确性。广泛的实验表明，在各种具有挑战性的联邦学习设置中，FedWSQ始终优于现有的联邦学习方法。

**结论:** FedWSQ是一种有效的新型联邦学习框架，可以在减少通信开销的同时提高模型精度，尤其适用于极端数据异构性和超低比特通信场景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedWSQ%3A+Efficient+Federated+Learning+with+Weight+Standardization+and+Distribution-Aware+Non-Uniform+Quantization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23516，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23516&send_immediately=true&force_search=false)

**原文摘要:** Federated learning (FL) often suffers from performance degradation due to key
challenges such as data heterogeneity and communication constraints. To address
these limitations, we present a novel FL framework called FedWSQ, which
integrates weight standardization (WS) and the proposed distribution-aware
non-uniform quantization (DANUQ). WS enhances FL performance by filtering out
biased components in local updates during training, thereby improving the
robustness of the model against data heterogeneity and unstable client
participation. In addition, DANUQ minimizes quantization errors by leveraging
the statistical properties of local model updates. As a result, FedWSQ
significantly reduces communication overhead while maintaining superior model
accuracy. Extensive experiments on FL benchmark datasets demonstrate that
FedWSQ consistently outperforms existing FL methods across various challenging
FL settings, including extreme data heterogeneity and ultra-low-bit
communication scenarios.

</details>


### [81] [Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size](https://arxiv.org/abs/2506.23544)
*Kento Imaizumi, Hideaki Iiduka*

**主要类别:** cs.LG

**AI概要:** 在本文中，作者研究了使用增大的batch size的mini-batch QHM的渐近和非渐近收敛性。结果表明，在不减少学习率的情况下，增大batch size是一种有效的策略。实验表明，即使batch size有限增加，也能为神经网络训练带来好处。


<details>
  <summary>更多</summary>
  
**动机:** 动量方法最初是为了在具有凸目标函数的确定性环境中优于随机梯度下降（SGD）而引入的。然而，尽管它们被广泛应用于深度神经网络（典型的随机非凸优化情况），但对其在这种环境下的有效性理论解释仍然有限。

**方法:** 作者提供了关于增大批size的mini-batch QHM的渐近和非渐近收敛结果。研究表明，实现渐近收敛需要衰减的学习率或增大的batch size。由于衰减的学习率对非渐近收敛有负面影响，因此展示了使用增大的batch size而不衰减学习率可能是更有效的策略。

**结果:** 理论和实验证明，即使batch size有限增加，也能为神经网络训练带来好处。具体来说，使用增大的batch size而不衰减学习率可以更有效地提升QHM的性能。

**结论:** 作者得出结论，增大的batch size可以在不降低学习率的情况下提高QHM的收敛性能，这对于深度神经网络的训练具有重要意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Both+Asymptotic+and+Non-Asymptotic+Convergence+of+Quasi-Hyperbolic+Momentum+using+Increasing+Batch+Size，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23544，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23544&send_immediately=true&force_search=false)

**原文摘要:** Momentum methods were originally introduced for their superiority to
stochastic gradient descent (SGD) in deterministic settings with convex
objective functions. However, despite their widespread application to deep
neural networks -- a representative case of stochastic nonconvex optimization
-- the theoretical justification for their effectiveness in such settings
remains limited. Quasi-hyperbolic momentum (QHM) is an algorithm that
generalizes various momentum methods and has been studied to better understand
the class of momentum-based algorithms as a whole. In this paper, we provide
both asymptotic and non-asymptotic convergence results for mini-batch QHM with
an increasing batch size. We show that achieving asymptotic convergence
requires either a decaying learning rate or an increasing batch size. Since a
decaying learning rate adversely affects non-asymptotic convergence, we
demonstrate that using mini-batch QHM with an increasing batch size -- without
decaying the learning rate -- can be a more effective strategy. Our experiments
show that even a finite increase in batch size can provide benefits for
training neural networks.

</details>


### [82] [A unified framework on the universal approximation of transformer-type architectures](https://arxiv.org/abs/2506.23551)
*Jingpu Cheng, Qianxiao Li, Ting Lin, Zuowei Shen*

**主要类别:** cs.LG

**AI概要:** 本文研究了Transformer架构的通用近似性质（UAP），提供了一个统一的理论框架，将残差网络的结果扩展到包含注意力机制的模型。


<details>
  <summary>更多</summary>
  
**动机:** 现有的关于残差网络的通用近似性质的研究尚未涵盖包含注意力机制的模型，因此需要一个更广泛的理论框架来分析Transformer架构的UAP。

**方法:** 作者首先确定了令牌可区分性作为UAP的基本要求，并提出了一种适用于广泛架构的充分条件。通过假设注意力层的解析性，简化了该条件的验证过程。接着，证明了具有不同注意力机制（如基于核和稀疏注意力）的Transformer满足UAP。

**结果:** 该框架不仅推广了先前的工作，还为具有特定功能对称性的新型Transformer架构的设计提供了理论基础。

**结论:** 本文提出的理论框架能够用于证明多种注意力机制下的Transformer具备UAP，并为设计具有内在UAP保证的新架构提供了指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+unified+framework+on+the+universal+approximation+of+transformer-type+architectures，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23551，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23551&send_immediately=true&force_search=false)

**原文摘要:** We investigate the universal approximation property (UAP) of transformer-type
architectures, providing a unified theoretical framework that extends prior
results on residual networks to models incorporating attention mechanisms. Our
work identifies token distinguishability as a fundamental requirement for UAP
and introduces a general sufficient condition that applies to a broad class of
architectures. Leveraging an analyticity assumption on the attention layer, we
can significantly simplify the verification of this condition, providing a
non-constructive approach in establishing UAP for such architectures. We
demonstrate the applicability of our framework by proving UAP for transformers
with various attention mechanisms, including kernel-based and sparse attention
mechanisms. The corollaries of our results either generalize prior works or
establish UAP for architectures not previously covered. Furthermore, our
framework offers a principled foundation for designing novel transformer
architectures with inherent UAP guarantees, including those with specific
functional symmetries. We propose examples to illustrate these insights.

</details>


### [83] [Transition Matching: Scalable and Flexible Generative Modeling](https://arxiv.org/abs/2506.23589)
*Neta Shaul, Uriel Singer, Itai Gat, Yaron Lipman*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的生成范式Transition Matching (TM)，统一并推进了扩散/流模型和连续自回归(AR)生成方法。通过三种TM变体，展示了其在图像质量和文本生成上的优越性及采样效率的提升。


<details>
  <summary>更多</summary>
  
**动机:** 现有的扩散和流匹配模型设计空间已被充分探索，限制了进一步改进的可能性；而连续标记生成的自回归(AR)模型为统一文本和媒体生成提供了有希望的方向。因此需要一种新的生成范式来结合两者的优点。

**方法:** 提出了Transition Matching (TM)，一种离散时间、连续状态的生成范式，将复杂生成任务分解为简单的马尔可夫转移。具体包括：Difference Transition Matching (DTM)，直接学习转移概率；Autoregressive Transition Matching (ARTM)，部分因果模型；Full History Transition Matching (FHTM)，完全因果模型。

**结果:** DTM在图像质量和文本一致性上达到最先进的水平，并提高了采样效率；ARTM和FHTM实现了与非因果方法相当的连续因果AR生成质量，且FHTM是首个在连续域文本到图像任务中性能媲美甚至超过基于流的方法的完全因果模型。

**结论:** 通过大规模对比实验验证了TM变体的有效性，展示了其在生成任务中的灵活性和潜力，为未来的研究开辟了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Transition+Matching%3A+Scalable+and+Flexible+Generative+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23589，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23589&send_immediately=true&force_search=false)

**原文摘要:** Diffusion and flow matching models have significantly advanced media
generation, yet their design space is well-explored, somewhat limiting further
improvements. Concurrently, autoregressive (AR) models, particularly those
generating continuous tokens, have emerged as a promising direction for
unifying text and media generation. This paper introduces Transition Matching
(TM), a novel discrete-time, continuous-state generative paradigm that unifies
and advances both diffusion/flow models and continuous AR generation. TM
decomposes complex generation tasks into simpler Markov transitions, allowing
for expressive non-deterministic probability transition kernels and arbitrary
non-continuous supervision processes, thereby unlocking new flexible design
avenues. We explore these choices through three TM variants: (i) Difference
Transition Matching (DTM), which generalizes flow matching to discrete-time by
directly learning transition probabilities, yielding state-of-the-art image
quality and text adherence as well as improved sampling efficiency. (ii)
Autoregressive Transition Matching (ARTM) and (iii) Full History Transition
Matching (FHTM) are partially and fully causal models, respectively, that
generalize continuous AR methods. They achieve continuous causal AR generation
quality comparable to non-causal approaches and potentially enable seamless
integration with existing AR text generation techniques. Notably, FHTM is the
first fully causal model to match or surpass the performance of flow-based
methods on text-to-image task in continuous domains. We demonstrate these
contributions through a rigorous large-scale comparison of TM variants and
relevant baselines, maintaining a fixed architecture, training data, and
hyperparameters.

</details>


### [84] [When Will It Fail?: Anomaly to Prompt for Forecasting Future Anomalies in Time Series](https://arxiv.org/abs/2506.23596)
*Min-Yeong Park, Won-Jeong Lee, Seong Tae Kim, Gyeong-Moon Park*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为Anomaly to Prompt (A2P)的新框架，用于解决异常预测（AP）任务。该框架由异常感知预测（AAF）和合成异常提示（SAP）组成，通过学习异常关系和使用信号自适应提示来模拟多样异常模式。实验表明，A2P在预测未来异常方面优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的时间序列数据处理方法在异常预测（AP）任务中表现不佳，只能检测即时异常或无法准确预测未来的异常。

**方法:** 提出了一个新框架Anomaly to Prompt (A2P)，包含Anomaly-Aware Forecasting (AAF) 和 Synthetic Anomaly Prompting (SAP)。AAF通过学习异常关系来预测异常时间点，而SAP引入了一个可学习的Anomaly Prompt Pool (APP)，用信号自适应提示来模拟多种异常模式。

**结果:** 在多个真实世界的数据集上的综合实验表明，A2P在异常预测任务上优于最先进的方法。

**结论:** A2P框架显著提高了未来异常事件的预测能力，并且其代码已在GitHub上开源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+Will+It+Fail%3F%3A+Anomaly+to+Prompt+for+Forecasting+Future+Anomalies+in+Time+Series，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23596，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23596&send_immediately=true&force_search=false)

**原文摘要:** Recently, forecasting future abnormal events has emerged as an important
scenario to tackle real-world necessities. However, the solution of predicting
specific future time points when anomalies will occur, known as Anomaly
Prediction (AP), remains under-explored. Existing methods dealing with time
series data fail in AP, focusing only on immediate anomalies or failing to
provide precise predictions for future anomalies. To address the AP task, we
propose a novel framework called Anomaly to Prompt (A2P), comprised of
Anomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP). To
enable the forecasting model to forecast abnormal time points, we adopt a
strategy to learn the relationships of anomalies. For the robust detection of
anomalies, our proposed SAP introduces a learnable Anomaly Prompt Pool (APP)
that simulates diverse anomaly patterns using signal adaptive prompt.
Comprehensive experiments on multiple real-world datasets demonstrate the
superiority of A2P over state-of-the-art methods, showcasing its ability to
predict future anomalies. Our implementation code is available at
https://github.com/KU-VGI/AP.

</details>


### [85] [A Nonlinear Low-rank Representation Model with Convolutional Neural Network for Imputing Water Quality Data](https://arxiv.org/abs/2506.23629)
*Xin Liao, Bing Yang, Cai Yu*

**主要类别:** cs.LG

**AI概要:** 提出了一种结合卷积神经网络的非线性低秩表示模型（NLR），用于填补水质数据中的缺失值。通过实验验证，该模型在估计精度上显著优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 水质数据完整性对环境监测至关重要，但实际中常因传感器故障和通信延迟等问题导致大量数据缺失，且数据呈现高维稀疏特性。传统填补方法难以捕捉深层数据特征，性能不理想。

**方法:** 提出一种非线性低秩表示模型（NLR）结合卷积神经网络（CNN）。利用CNN实现两个目标：1) 融合时间特征以建模时隙间的数据时间依赖性；2) 提取非线性交互和局部模式以挖掘高阶关系特征，实现多维信息的深度融合。

**结果:** 在三个真实水质数据集上的实验研究表明，所提出的模型在估计精度方面显著优于现有的最先进的数据填补模型。

**结论:** 该模型为处理复杂动态环境下的水质监测数据提供了一种有效的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Nonlinear+Low-rank+Representation+Model+with+Convolutional+Neural+Network+for+Imputing+Water+Quality+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23629，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23629&send_immediately=true&force_search=false)

**原文摘要:** The integrity of Water Quality Data (WQD) is critical in environmental
monitoring for scientific decision-making and ecological protection. However,
water quality monitoring systems are often challenged by large amounts of
missing data due to unavoidable problems such as sensor failures and
communication delays, which further lead to water quality data becoming
High-Dimensional and Sparse (HDS). Traditional data imputation methods are
difficult to depict the potential dynamics and fail to capture the deep data
features, resulting in unsatisfactory imputation performance. To effectively
address the above issues, this paper proposes a Nonlinear Low-rank
Representation model (NLR) with Convolutional Neural Networks (CNN) for
imputing missing WQD, which utilizes CNNs to implement two ideas: a) fusing
temporal features to model the temporal dependence of data between time slots,
and b) Extracting nonlinear interactions and local patterns to mine
higher-order relationships features and achieve deep fusion of multidimensional
information. Experimental studies on three real water quality datasets
demonstrate that the proposed model significantly outperforms existing
state-of-the-art data imputation models in terms of estimation accuracy. It
provides an effective approach for handling water quality monitoring data in
complex dynamic environments.

</details>


### [86] [Learning Modular Exponentiation with Transformers](https://arxiv.org/abs/2506.23679)
*David Demitri Africa, Sara M. Kapoor, Theo Simon Sorg*

**主要类别:** cs.LG

**AI概要:** 本研究通过训练一个4层的Transformer模型进行模指数运算，发现互惠操作数训练可显著提升性能，并揭示了模型内部算术结构的学习机制。最后一层的注意力头子图足以完成常规指数运算任务，表明Transformer通过专用计算电路学习模算术。


<details>
  <summary>更多</summary>
  
**动机:** 模指数运算在数论和密码学中至关重要，但从机械可解释性的角度尚未深入研究。

**方法:** 训练了一个4层的编码器-解码器Transformer模型执行模指数运算，并使用原则性采样策略、PCA嵌入分析和激活补丁方法研究数值推理在训练过程中的出现。

**结果:** 互惠操作数训练导致性能显著提升，并且在相关模数间有突然的泛化现象；最后一层的注意力头子图足以实现完全性能。

**结论:** Transformer模型通过专用计算电路学习模算术，为更可解释和高效的神经方法铺平道路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Modular+Exponentiation+with+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23679，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23679&send_immediately=true&force_search=false)

**原文摘要:** Modular exponentiation is crucial to number theory and cryptography, yet
remains largely unexplored from a mechanistic interpretability standpoint. We
train a 4-layer encoder-decoder Transformer model to perform this operation and
investigate the emergence of numerical reasoning during training. Utilizing
principled sampling strategies, PCA-based embedding analysis, and activation
patching, we examine how number-theoretic properties are encoded within the
model. We find that reciprocal operand training leads to strong performance
gains, with sudden generalization across related moduli. These synchronized
accuracy surges reflect grokking-like dynamics, suggesting the model
internalizes shared arithmetic structure. We also find a subgraph consisting
entirely of attention heads in the final layer sufficient to achieve full
performance on the task of regular exponentiation. These results suggest that
transformer models learn modular arithmetic through specialized computational
circuits, paving the way for more interpretable and efficient neural approaches
to modular exponentiation.

</details>


### [87] [DABstep: Data Agent Benchmark for Multi-step Reasoning](https://arxiv.org/abs/2506.23719)
*Alex Egg, Martin Iglesias Goyanes, Friso Kingma, Andreu Mora, Leandro von Werra, Thomas Wolf*

**主要类别:** cs.LG

**AI概要:** 提出DABstep，一个用于评估AI在多步骤数据分析任务上的新基准。它包含450多个来自金融分析平台的真实挑战，需要模型结合基于代码的数据处理与上下文推理能力。尽管领先的语言模型在最难的任务上准确率仅为14.55%，但该基准提供了客观评分和自动正确性检查。


<details>
  <summary>更多</summary>
  
**动机:** 当前缺乏有效评估AI在真实世界、多步骤数据分析任务上的工具，因此需要一个新的基准来推动这一领域的发展。

**方法:** 构建了一个名为DABstep的基准测试集，包含超过450个从金融分析平台提取的实际挑战，要求模型整合代码数据处理和异构文档的上下文推理能力，并采用迭代、多步骤的问题解决方法。

**结果:** 顶尖LLM代理在最难任务上的准确率为14.55%，显示了现有技术与理想性能之间的显著差距，同时详细报告了基准设计、数据集构成、任务公式化、评估协议及基线结果。

**结论:** DABstep为加速自主数据分析研究提供了一个公共排行榜和工具包，揭示了现有AI代理在复杂数据分析任务中的局限性，指出了未来改进的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DABstep%3A+Data+Agent+Benchmark+for+Multi-step+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23719，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23719&send_immediately=true&force_search=false)

**原文摘要:** We introduce DABstep, a novel benchmark for evaluating AI agents on realistic
multi-step data analysis tasks. DABstep comprises over 450 real-world
challenges derived from a financial analytics platform, requiring models to
combine code-based data processing with contextual reasoning over heterogeneous
documentation. Each task demands an iterative, multi-step problem-solving
approach, testing capabilities in data manipulation, cross-referencing multiple
sources, and precise result reporting. The benchmark provides a factoid-style
answer format with automatic correctness checks for objective scoring at scale.
We evaluate leading LLM-based agents, revealing a substantial performance gap:
even the best agent achieves only 14.55% accuracy on the hardest tasks. We
detail our benchmark's design, dataset composition, task formulation,
evaluation protocol, report baseline results and analyze failure modes. DABstep
is released with a public leaderboard and toolkit to accelerate research in
autonomous data analysis.

</details>


### [88] [System-Embedded Diffusion Bridge Models](https://arxiv.org/abs/2506.23726)
*Bartlomiej Sobieski, Matthew Tivnan, Yuang Wang, Siyeop Yoon, Pengfei Jin, Dufan Wu, Quanzheng Li, Przemyslaw Biecek*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种新的监督桥接方法，称为系统嵌入扩散桥模型（SDBs），通过将已知的线性测量系统显式地嵌入到矩阵值SDE的系数中，解决了各种线性逆问题，并在训练和部署之间的系统错配下表现出稳健的泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的解决逆问题的方法主要分为无监督和监督两类，但都存在不足：无监督方法通常需要知道测量模型，而监督方法大多忽略了结构信息。因此，需要一种能结合测量模型结构信息的新方法来提升性能和泛化能力。

**方法:** 引入了系统嵌入扩散桥模型（SDBs），这是一种新的监督桥接方法，它将已知的线性测量系统明确地嵌入到矩阵值随机微分方程（SDE）的系数中。这种方法可以更好地利用测量模型的结构信息。

**结果:** SDBs在各种线性逆问题上提供了显著的改进，并且在训练和部署之间存在系统错配的情况下展示了强大的泛化能力。

**结论:** SDBs为实际应用提供了一个有希望的解决方案，特别是在涉及线性逆问题的任务中表现优异，并具备良好的泛化性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是System-Embedded+Diffusion+Bridge+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23726，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23726&send_immediately=true&force_search=false)

**原文摘要:** Solving inverse problems -- recovering signals from incomplete or noisy
measurements -- is fundamental in science and engineering. Score-based
generative models (SGMs) have recently emerged as a powerful framework for this
task. Two main paradigms have formed: unsupervised approaches that adapt
pretrained generative models to inverse problems, and supervised bridge methods
that train stochastic processes conditioned on paired clean and corrupted data.
While the former typically assume knowledge of the measurement model, the
latter have largely overlooked this structural information. We introduce System
embedded Diffusion Bridge Models (SDBs), a new class of supervised bridge
methods that explicitly embed the known linear measurement system into the
coefficients of a matrix-valued SDE. This principled integration yields
consistent improvements across diverse linear inverse problems and demonstrates
robust generalization under system misspecification between training and
deployment, offering a promising solution to real-world applications.

</details>


### [89] [Radioactive Watermarks in Diffusion and Autoregressive Image Generative Models](https://arxiv.org/abs/2506.23731)
*Michel Meintz, Jan Dubiński, Franziska Boenisch, Adam Dziedzic*

**主要类别:** cs.LG

**AI概要:** 生成模型（如扩散模型和自回归模型）的水印技术对于检测未经授权的图像使用至关重要。然而，现有方法在扩散模型中无法保留放射性特征，而对于自回归模型则尚无放射性水印方法。本文提出了一种针对自回归模型的新型水印技术，其灵感来源于大型语言模型，并通过实验验证了该方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 训练图像生成模型需要大量数据集，成本高昂。一些方可能利用已有的生成图像作为训练数据，因此需要一种方法来检测未经授权的图像使用。水印技术可以实现这一目标，但前提是水印在新模型的输出中仍然可识别，即具备放射性特性。

**方法:** 作者分析了扩散模型和自回归模型中的水印放射性问题，发现扩散模型中的现有水印方法无法保留放射性。针对自回归模型，作者提出了受大型语言模型启发的新型水印方法，旨在解决放射性问题。

**结果:** 实验结果表明，所提出的方法在自回归模型中有效地保留了放射性，能够实现稳健的来源追踪，并防止未经授权的图像使用。

**结论:** 本文提出的水印方法为自回归模型提供了有效的放射性水印解决方案，填补了当前技术空白，并有助于保护生成图像的知识产权。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Radioactive+Watermarks+in+Diffusion+and+Autoregressive+Image+Generative+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23731，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23731&send_immediately=true&force_search=false)

**原文摘要:** Image generative models have become increasingly popular, but training them
requires large datasets that are costly to collect and curate. To circumvent
these costs, some parties may exploit existing models by using the generated
images as training data for their own models. In general, watermarking is a
valuable tool for detecting unauthorized use of generated images. However, when
these images are used to train a new model, watermarking can only enable
detection if the watermark persists through training and remains identifiable
in the outputs of the newly trained model - a property known as radioactivity.
We analyze the radioactivity of watermarks in images generated by diffusion
models (DMs) and image autoregressive models (IARs). We find that existing
watermarking methods for DMs fail to retain radioactivity, as watermarks are
either erased during encoding into the latent space or lost in the
noising-denoising process (during the training in the latent space). Meanwhile,
despite IARs having recently surpassed DMs in image generation quality and
efficiency, no radioactive watermarking methods have been proposed for them. To
overcome this limitation, we propose the first watermarking method tailored for
IARs and with radioactivity in mind - drawing inspiration from techniques in
large language models (LLMs), which share IARs' autoregressive paradigm. Our
extensive experimental evaluation highlights our method's effectiveness in
preserving radioactivity within IARs, enabling robust provenance tracking, and
preventing unauthorized use of their generated images.

</details>


### [90] [Model-driven Stochastic Trace Clustering](https://arxiv.org/abs/2506.23776)
*Jari Peeperkorn, Johannes De Smedt, Jochen De Weerdt*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的模型驱动的迹线聚类方法，通过优化每个聚类中的随机过程模型，提高了模型的可解释性和表示能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的迹线聚类技术要么没有使用过程模型发现，要么使用非随机模型，忽略了活动和转换的频率或概率，限制了其捕捉现实世界执行动态的能力。

**方法:** 该方法利用熵相关性（一种基于直接跟随概率的随机一致性度量）来指导迹线分配。这种方法允许聚类决策同时考虑与聚类过程模型的结构对齐以及迹线来源于给定随机过程模型的可能性。

**结果:** 广泛的实验证明，该方法在表示过程行为方面优于现有方法，并揭示了在考虑随机性时聚类性能排名的变化。

**结论:** 所提出的方法计算效率高，随输入规模线性扩展，并通过生成具有更清晰控制流模式的聚类，提高了模型的可解释性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Model-driven+Stochastic+Trace+Clustering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23776，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23776&send_immediately=true&force_search=false)

**原文摘要:** Process discovery algorithms automatically extract process models from event
logs, but high variability often results in complex and hard-to-understand
models. To mitigate this issue, trace clustering techniques group process
executions into clusters, each represented by a simpler and more understandable
process model. Model-driven trace clustering improves on this by assigning
traces to clusters based on their conformity to cluster-specific process
models. However, most existing clustering techniques rely on either no process
model discovery, or non-stochastic models, neglecting the frequency or
probability of activities and transitions, thereby limiting their capability to
capture real-world execution dynamics. We propose a novel model-driven trace
clustering method that optimizes stochastic process models within each cluster.
Our approach uses entropic relevance, a stochastic conformance metric based on
directly-follows probabilities, to guide trace assignment. This allows
clustering decisions to consider both structural alignment with a cluster's
process model and the likelihood that a trace originates from a given
stochastic process model. The method is computationally efficient, scales
linearly with input size, and improves model interpretability by producing
clusters with clearer control-flow patterns. Extensive experiments on public
real-life datasets show that our method outperforms existing alternatives in
representing process behavior and reveals how clustering performance rankings
can shift when stochasticity is considered.

</details>


### [91] [Calibrating Graph Neural Networks with Wavelet-Aware Temperature Scaling](https://arxiv.org/abs/2506.23782)
*Xiaoyang Li, Linwei Tao, Haohui Lu, Minjing Dong, Junbin Gao, Chang Xu*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种名为Wavelet-Aware Temperature Scaling (WATS)的后处理校准框架，通过可调谐热核图小波特征为节点分配特定温度，从而改进了GNNs预测置信度的估计。实验表明，WATS在多个基准数据集上表现出最低的预期校准误差(ECE)，并且计算效率高。


<details>
  <summary>更多</summary>
  
**动机:** 尽管现有的图感知校准方法试图缓解GNN预测置信度与实际预测准确性之间的不一致问题，但它们主要依赖于粗略的一阶统计量或潜在节点嵌入，忽略了图拓扑中的细粒度结构异质性。

**方法:** 提出了Wavelet-Aware Temperature Scaling (WATS)，一种基于可调谐热核图小波特征为节点分配特定温度的后处理校准框架。该方法利用了图小波的可扩展性和拓扑敏感性来改进置信度估计，无需模型重新训练或访问邻近logits或预测。

**结果:** 在七个具有不同图结构的基准数据集和两个GNN骨干网络上的广泛评估表明，WATS在所有比较方法中实现了最低的ECE，比经典和图特定基线高出最多42.3%的ECE，并且将校准方差平均减少了17.24%。此外，WATS保持了计算效率，能够很好地扩展到不同大小和密度的图。

**结论:** WATS提供了一种有效的后处理校准解决方案，适用于不同的图结构和GNN模型，显著提高了预测置信度的校准性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Calibrating+Graph+Neural+Networks+with+Wavelet-Aware+Temperature+Scaling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23782，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23782&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) have demonstrated strong predictive performance
on relational data; however, their confidence estimates often misalign with
actual predictive correctness, posing significant limitations for deployment in
safety-critical settings. While existing graph-aware calibration methods seek
to mitigate this limitation, they primarily depend on coarse one-hop
statistics, such as neighbor-predicted confidence, or latent node embeddings,
thereby neglecting the fine-grained structural heterogeneity inherent in graph
topology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a
post-hoc calibration framework that assigns node-specific temperatures based on
tunable heat-kernel graph wavelet features. Specifically, WATS harnesses the
scalability and topology sensitivity of graph wavelets to refine confidence
estimates, all without necessitating model retraining or access to neighboring
logits or predictions. Extensive evaluations across seven benchmark datasets
with varying graph structures and two GNN backbones demonstrate that WATS
achieves the lowest Expected Calibration Error (ECE) among all compared
methods, outperforming both classical and graph-specific baselines by up to
42.3\% in ECE and reducing calibration variance by 17.24\% on average compared
with graph-specific methods. Moreover, WATS remains computationally efficient,
scaling well across graphs of diverse sizes and densities. Code will be
released based on publication.

</details>


### [92] [KAIROS: Scalable Model-Agnostic Data Valuation](https://arxiv.org/abs/2506.23799)
*Jiongli Zhu, Parjanya Prajakta Prashant, Alex Cloninger, Babak Salimi*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为KAIROS的可扩展、与模型无关的数据估值框架，通过分配每个样本分布影响分数来评估其对训练数据的贡献，并在准确性和运行时方面优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的数据估值方法存在不足：基于模型的方法依赖单一模型并继承其偏差；基于算法的方法（如Data Shapley）需要昂贵的大规模重新训练；Wasserstein-based方法的近似值可能导致错误排序。因此，需要一种更高效和准确的数据估值方法。

**方法:** KAIROS是一种可扩展且与模型无关的估值框架，它为每个样本分配一个分布影响分数，表示其对经验训练分布与干净参考集之间的最大均值差异（MMD）的贡献。该方法具有闭式解，无需重新训练，并能自然扩展到条件核以检测标签和特征错误。此外，KAIROS支持高效的在线更新。

**结果:** 在噪声、错误标记和中毒基准测试中的实证评估表明，KAIROS在准确性和运行时间上始终优于最先进的模型、Shapley和Wasserstein-based基线方法。

**结论:** KAIROS提供了一种新的数据估值方法，具有更高的效率和准确性，并附有严格的理论保证，包括可重复的排序对称性和可解释的密度分离阈值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是KAIROS%3A+Scalable+Model-Agnostic+Data+Valuation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23799，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23799&send_immediately=true&force_search=false)

**原文摘要:** Training data increasingly shapes not only model accuracy but also regulatory
compliance and market valuation of AI assets. Yet existing valuation methods
remain inadequate: model-based techniques depend on a single fitted model and
inherit its biases, while algorithm-based approaches such as Data Shapley
require costly retrainings at web scale. Recent Wasserstein-based
model-agnostic methods rely on approximations that misrank examples relative to
their true leave-one-out (LOO) utility. We introduce KAIROS, a scalable,
model-agnostic valuation framework that assigns each example a distributional
influence score: its contribution to the Maximum Mean Discrepancy (MMD) between
the empirical training distribution and a clean reference set. Unlike
Wasserstein surrogates, our MMD-based influence admits a closed-form solution
that faithfully approximates the exact LOO ranking within $O(1/N^2)$ error,
requires no retraining, and naturally extends to conditional kernels for
unified label- and feature-error detection. Moreover, KAIROS supports efficient
online updates: when a new batch of size m arrives, all scores can be updated
in $O(mN)$ time, delivering up to 50x speedup without compromising ranking
quality. Empirical evaluations on noise, mislabeling, and poisoning benchmarks
show that KAIROS consistently outperforms state-of-the-art model-, Shapley-,
and Wasserstein-based baselines in both accuracy and runtime. We provide
rigorous theoretical guarantees, including symmetry for reproducible rankings
and density-separation for interpretable thresholds.

</details>


### [93] [Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts](https://arxiv.org/abs/2506.23845)
*Kenny Peng, Rajiv Movva, Jon Kleinberg, Emma Pierson, Nikhil Garg*

**主要类别:** cs.LG

**AI概要:** 尽管稀疏自编码器（SAEs）引发了极大的关注，但一系列负面结果增加了对其有用性的怀疑。本文通过概念区分调和了围绕SAEs的竞争性叙述，指出SAEs在处理已知概念时可能效果较差，但在发现未知概念方面非常强大。此研究还概述了SAEs在机器学习可解释性、公平性、审计和安全性以及社会科学和健康科学中的应用。


<details>
  <summary>更多</summary>
  
**动机:** 为了调和关于稀疏自编码器（SAEs）的正面和负面观点，并明确其在不同场景下的适用性，特别是在发现未知概念方面的潜力。

**方法:** 提出概念区分以理解SAEs的有效性；强调SAEs在发现未知概念上的能力，并探讨其在机器学习解释性、公平性、审计和安全性以及社会科学和健康科学中的潜在应用。

**结果:** 成功地将SAEs的正面与负面结果分开，并提出了SAEs在多个领域的具体使用案例。

**结论:** 稀疏自编码器在处理已知概念时可能不如其他方法有效，但在探索未知概念方面具有强大的潜力，适用于机器学习和跨学科领域。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Use+Sparse+Autoencoders+to+Discover+Unknown+Concepts%2C+Not+to+Act+on+Known+Concepts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23845，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23845&send_immediately=true&force_search=false)

**原文摘要:** While sparse autoencoders (SAEs) have generated significant excitement, a
series of negative results have added to skepticism about their usefulness.
Here, we establish a conceptual distinction that reconciles competing
narratives surrounding SAEs. We argue that while SAEs may be less effective for
acting on known concepts, SAEs are powerful tools for discovering unknown
concepts. This distinction cleanly separates existing negative and positive
results, and suggests several classes of SAE applications. Specifically, we
outline use cases for SAEs in (i) ML interpretability, explainability,
fairness, auditing, and safety, and (ii) social and health sciences.

</details>


### [94] [Towards the Training of Deeper Predictive Coding Neural Networks](https://arxiv.org/abs/2506.23800)
*Chang Qi, Matteo Forasassi, Thomas Lukasiewicz, Tommaso Salvatori*

**主要类别:** cs.LG

**AI概要:** 预测编码网络通过平衡传播训练，在浅层架构中表现良好，但在超过五到七层的深度网络中性能显著下降。本文提出两种新方法优化潜在变量，并引入一种新的权重更新机制以减少深层中的误差累积，从而在图像分类任务上显著提高测试准确性，性能接近反向传播方法。


<details>
  <summary>更多</summary>
  
**动机:** 尽管预测编码网络在浅层架构中表现出色，但随着网络深度增加，其性能显著下降，主要原因是层间误差不平衡以及前一层预测无法有效引导深层更新。

**方法:** 1. 引入两种基于精度加权的新方法来重新平衡松弛阶段各层之间的能量分布。2. 提出一种新的权重更新机制以减少深层中的误差累积。

**结果:** 在多个图像分类任务中测试表明，对于超过七层的网络，测试准确率有显著提升，性能与类似模型的反向传播方法相当。

**结论:** 更好地理解松弛阶段对于大规模使用平衡传播训练模型至关重要，这为复杂任务中的应用开辟了新可能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+the+Training+of+Deeper+Predictive+Coding+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23800，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23800&send_immediately=true&force_search=false)

**原文摘要:** Predictive coding networks trained with equilibrium propagation are neural
models that perform inference through an iterative energy minimization process.
Previous studies have demonstrated their effectiveness in shallow
architectures, but show significant performance degradation when depth exceeds
five to seven layers. In this work, we show that the reason behind this
degradation is due to exponentially imbalanced errors between layers during
weight updates, and predictions from the previous layer not being effective in
guiding updates in deeper layers. We address the first issue by introducing two
novel methods to optimize the latent variables that use precision-weighting to
re-balance the distribution of energy among layers during the `relaxation
phase', and the second issue by proposing a novel weight update mechanism that
reduces error accumulation in deeper layers. Empirically, we test our methods
on a large number of image classification tasks, resulting in large
improvements in test accuracy across networks with more than seven layers, with
performances comparable to those of backprop on similar models. These findings
suggest that a better understanding of the relaxation phase is important to
train models using equilibrium propagation at scale, and open new possibilities
for their application in complex tasks.

</details>


### [95] [Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic](https://arxiv.org/abs/2506.23875)
*Yuta Sato, Kazuhiko Kawamoto, Hiroshi Kera*

**主要类别:** cs.LG

**AI概要:** 研究通过重新排列解码器输入标记，为Transformer找到更易于学习的序列顺序，并在算术任务中验证了该方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** Transformer中的思维链对于逐步推理至关重要，而这些中间步骤的顺序会显著影响推理难度。因此，研究者提出了一种新的任务——解开思维链，即重新排列解码器输入标记以形成更易于Transformer学习的序列。

**方法:** 首先在一个由不同顺序的目标序列组成的混合数据集上训练Transformer，然后将那些在训练初期损失下降迅速的顺序识别为有利顺序。由于搜索空间随着序列长度呈阶乘增长，研究者提出了一个两阶段的分层方法，分别进行块间和块内的重新排序。

**结果:** 在四个对顺序敏感的算术任务上的实验表明，该方法可以从数十亿候选顺序中识别出有利于学习的顺序。特别是在乘法任务上，它恢复了之前研究中报告的反向数字顺序。

**结论:** 本研究提出了一种有效的方法来重新排列解码器输入标记，从而帮助Transformer更好地学习算术任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Chain+of+Thought+in+Order%3A+Discovering+Learning-Friendly+Orders+for+Arithmetic，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23875，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23875&send_immediately=true&force_search=false)

**原文摘要:** The chain of thought is fundamental in Transformers, which is to perform
step-by-step reasoning. Besides what intermediate steps work, the order of
these steps critically affects the difficulty of the reasoning. This study
addresses a novel task of unraveling chain of thought - reordering decoder
input tokens to a learning-friendly sequence for Transformers to learn
arithmetic tasks. The proposed pipeline first trains a Transformer on a mixture
of target sequences arranged in different orders and then identifies benign
orders as those with fast loss drops in the early stage. As the search space
grows factorially with sequence length, we propose a two-stage hierarchical
approach for inter- and intra-block reordering. Experiments on four
order-sensitive arithmetic tasks show that our method identifies a
learning-friendly order out of a few billion candidates. Notably, on the
multiplication task, it recovered the reverse-digit order reported in prior
studies.

</details>


### [96] [Adaptive Out-of-Control Point Pattern Detection in Sequential Random Finite Set Observations](https://arxiv.org/abs/2506.23802)
*Konstantinos Bourazas, Savvas Papaioannou, Panayiotis Kolios*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种新的自适应异常检测框架，用于监控顺序随机有限集（RFS）观测值。通过引入基于RFS的后验分布新类别（Power Discounting Posteriors, PD），该方法不仅能够在线学习数据生成过程的正常行为，还能动态适应行为变化以准确识别异常点模式。大量的定性和定量模拟实验证明了该方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 当前在监控顺序随机有限集（RFS）观测值时，缺乏一种能够动态适应并准确识别异常点模式的框架。因此，需要开发一种可以在线学习正常行为并适应行为变化的新方法。

**方法:** 1. 开发了一种创新的基于RFS的框架，可在线学习数据生成过程的正常行为。
2. 引入了名为Power Discounting Posteriors (PD)的新类别的RFS基础后验分布，用于适应数据系统性变化。
3. 利用预测后验密度函数实现点模式数据的异常检测。

**结果:** 通过广泛的定性和定量模拟实验，证明了所提出的框架在异常检测方面的有效性。

**结论:** 所提出的基于RFS的自适应异常检测框架成功实现了对顺序RFS观测值中异常点模式的准确识别，并能动态适应行为变化。这为相关领域的监控和异常检测提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptive+Out-of-Control+Point+Pattern+Detection+in+Sequential+Random+Finite+Set+Observations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23802，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23802&send_immediately=true&force_search=false)

**原文摘要:** In this work we introduce a novel adaptive anomaly detection framework
specifically designed for monitoring sequential random finite set (RFS)
observations. Our approach effectively distinguishes between In-Control data
(normal) and Out-Of-Control data (anomalies) by detecting deviations from the
expected statistical behavior of the process. The primary contributions of this
study include the development of an innovative RFS-based framework that not
only learns the normal behavior of the data-generating process online but also
dynamically adapts to behavioral shifts to accurately identify abnormal point
patterns. To achieve this, we introduce a new class of RFS-based posterior
distributions, named Power Discounting Posteriors (PD), which facilitate
adaptation to systematic changes in data while enabling anomaly detection of
point pattern data through a novel predictive posterior density function. The
effectiveness of the proposed approach is demonstrated by extensive qualitative
and quantitative simulation experiments.

</details>


### [97] [Reinforcement Learning for Synchronised Flow Control in a Dual-Gate Resin Infusion System](https://arxiv.org/abs/2506.23923)
*Miguel Camacho-Sánchez, Fernando García-Torres, Jesper John Lisegaard, Rocío del Amor, Sankhya Mohanty, Valery Naranjo*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种基于强化学习（RL）的策略，用于同步具有两个树脂入口和一个出口的灌注场景中的不同树脂流动前沿。通过使用近端策略优化（PPO），该方法在部分可观测环境中解决了流体动力学管理的挑战。结果表明，RL方法在实现精确的流动收敛方面是有效的，展示了其在复合材料制造中改进过程控制和产品质量的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 树脂注入（RI）和树脂传递模塑（RTM）是制造高性能纤维增强聚合物复合材料的关键工艺，特别是在大型应用如风力涡轮机叶片中。控制这些过程中的树脂流动动态对于确保纤维增强材料的均匀浸润至关重要，从而防止影响最终部件结构完整性的残余孔隙率和干斑。

**方法:** 本研究采用基于强化学习（RL）的策略，利用过程模拟建立模型，以同步涉及两个树脂入口和一个出口的灌注场景中的不同树脂流动前沿。具体使用了近端策略优化（PPO）来应对部分可观测环境中的流体动力学管理挑战。

**结果:** 结果证明了RL方法在实现精确流动收敛方面的有效性，突显了其在改善复合材料制造过程中控制和产品质量方面的潜力。

**结论:** 基于强化学习的方法在控制树脂流动动态、改善复合材料制造过程中的控制和产品质量方面具有显著潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reinforcement+Learning+for+Synchronised+Flow+Control+in+a+Dual-Gate+Resin+Infusion+System，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23923，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23923&send_immediately=true&force_search=false)

**原文摘要:** Resin infusion (RI) and resin transfer moulding (RTM) are critical processes
for the manufacturing of high-performance fibre-reinforced polymer composites,
particularly for large-scale applications such as wind turbine blades.
Controlling the resin flow dynamics in these processes is critical to ensure
the uniform impregnation of the fibre reinforcements, thereby preventing
residual porosities and dry spots that impact the consequent structural
integrity of the final component. This paper presents a reinforcement learning
(RL) based strategy, established using process simulations, for synchronising
the different resin flow fronts in an infusion scenario involving two resin
inlets and a single outlet. Using Proximal Policy Optimisation (PPO), our
approach addresses the challenge of managing the fluid dynamics in a partially
observable environment. The results demonstrate the effectiveness of the RL
approach in achieving an accurate flow convergence, highlighting its potential
towards improving process control and product quality in composites
manufacturing.

</details>


### [98] [SGD with Adaptive Preconditioning: Unified Analysis and Momentum Acceleration](https://arxiv.org/abs/2506.23803)
*Dmitry Kovalev*

**主要类别:** cs.LG

**AI概要:** 本研究重新审视了带有AdaGrad型预调节的随机梯度下降(SGD)方法。首先，提出了一种统一的收敛性分析框架，涵盖了具有各向异性或矩阵平滑性和噪声假设的自适应预调节SGD。其次，揭示了Scion和DASGO之间的基本联系，并首次为后者提供了理论保证。最后，证明了通过Nesterov动量可以加速AdaGrad和DASGO等方法的收敛，从而为Adam在实际应用中的高效性提供了理论支持。


<details>
  <summary>更多</summary>
  
**动机:** 尽管AdaGrad、Adam等自适应梯度方法在实践中表现优异，但其理论性能仍需进一步探索。特别是关于自适应预调节与动量结合的效果尚未有明确的理论解释。因此，本文旨在深入分析这些方法的收敛性，并探讨如何进一步提升其性能。

**方法:** 1. 提出了一种统一的收敛性分析框架，适用于具有各向异性或矩阵平滑性和噪声假设的自适应预调节SGD。
2. 建立了Scion和DASGO之间的理论联系，并为DASGO提供了首个理论保证。
3. 证明了通过引入Nesterov动量，可以加速AdaGrad和DASGO等方法的收敛。

**结果:** 1. 恢复并改进了现有自适应梯度方法（如AdaGrad-Norm、AdaGrad和ASGO/One-sided Shampoo）的最新收敛结果。
2. 首次为DASGO提供了理论保证。
3. 证实了Nesterov动量能够加速AdaGrad和DASGO等方法的收敛。
4. 为Adam在实际应用中的高效性提供了理论依据。

**结论:** 本研究不仅扩展了对自适应梯度方法的理解，还为这些方法的实际高效性提供了理论支持。通过统一的分析框架和动量技术的应用，进一步提升了这些方法的收敛性能，推动了优化算法的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SGD+with+Adaptive+Preconditioning%3A+Unified+Analysis+and+Momentum+Acceleration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23803，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23803&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we revisit stochastic gradient descent (SGD) with AdaGrad-type
preconditioning. Our contributions are twofold. First, we develop a unified
convergence analysis of SGD with adaptive preconditioning under anisotropic or
matrix smoothness and noise assumptions. This allows us to recover
state-of-the-art convergence results for several popular adaptive gradient
methods, including AdaGrad-Norm, AdaGrad, and ASGO/One-sided Shampoo. In
addition, we establish the fundamental connection between two recently proposed
algorithms, Scion and DASGO, and provide the first theoretical guarantees for
the latter. Second, we show that the convergence of methods like AdaGrad and
DASGO can be provably accelerated beyond the best-known rates using Nesterov
momentum. Consequently, we obtain the first theoretical justification that
AdaGrad-type algorithms can simultaneously benefit from both diagonal
preconditioning and momentum, which may provide an ultimate explanation for the
practical efficiency of Adam.

</details>


### [99] [ADReFT: Adaptive Decision Repair for Safe Autonomous Driving via Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.23960)
*Mingfei Cheng, Xiaofei Xie, Renzhi Wang, Yuan Zhou, Ming Hu*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的在线修复方法ADReFT，通过离线学习失败测试来识别关键状态并生成适当的缓解措施，提升自动驾驶系统的安全性。该方法结合了Transformer模型和强化学习，相较于现有方法具有更好的修复性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的在线修复方案通常缺乏通用性和适应性，且过于保守，导致修复效果不佳，既无法充分缓解安全风险，又降低了驾驶体验。

**方法:** 1. 提出ADReFT方法：通过离线学习从失败测试中识别安全关键状态，并生成适当的缓解动作。
2. 使用双联合头（State Monitor和Decision Adapter）的Transformer模型捕获复杂驾驶环境交互。
3. 利用监督学习进行预训练，粗略标注状态以建立基础能力。
4. 使用强化学习微调模型，生成更精确和上下文相关的修复决策。

**结果:** 评估结果表明，ADReFT在修复性能上优于现有方法。

**结论:** ADReFT是一种新颖且有效的在线修复方法，能够通过离线学习和强化学习提升自动驾驶系统的运行时安全性和可靠性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ADReFT%3A+Adaptive+Decision+Repair+for+Safe+Autonomous+Driving+via+Reinforcement+Fine-Tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23960，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23960&send_immediately=true&force_search=false)

**原文摘要:** Autonomous Driving Systems (ADSs) continue to face safety-critical risks due
to the inherent limitations in their design and performance capabilities.
Online repair plays a crucial role in mitigating such limitations, ensuring the
runtime safety and reliability of ADSs. Existing online repair solutions
enforce ADS compliance by transforming unacceptable trajectories into
acceptable ones based on predefined specifications, such as rule-based
constraints or training datasets. However, these approaches often lack
generalizability, adaptability and tend to be overly conservative, resulting in
ineffective repairs that not only fail to mitigate safety risks sufficiently
but also degrade the overall driving experience. To address this issue, we
propose Adaptive Decision Repair (ADReFT), a novel and effective repair method
that identifies safety-critical states through offline learning from failed
tests and generates appropriate mitigation actions to improve ADS safety.
Specifically, ADReFT incorporates a transformer-based model with two joint
heads, State Monitor and Decision Adapter, designed to capture complex driving
environment interactions to evaluate state safety severity and generate
adaptive repair actions. Given the absence of oracles for state safety
identification, we first pretrain ADReFT using supervised learning with coarse
annotations, i.e., labeling states preceding violations as positive samples and
others as negative samples. It establishes ADReFT's foundational capability to
mitigate safety-critical violations, though it may result in somewhat
conservative mitigation strategies. Therefore, we subsequently finetune ADReFT
using reinforcement learning to improve its initial capability and generate
more precise and contextually appropriate repair decisions. Our evaluation
results illustrate that ADReFT achieves better repair performance.

</details>


### [100] [Supercm: Revisiting Clustering for Semi-Supervised Learning](https://arxiv.org/abs/2506.23824)
*Durgesh Singh, Ahcene Boubekki, Robert Jenssen, Michael C. Kampffmeyer*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的半监督学习方法，通过扩展可微聚类模块，将底层聚类假设显式地纳入SSL中，利用标注数据指导聚类中心，形成一种简单且端到端可训练的深度SSL方法。实验表明该模型优于仅监督基线，并且可以与其他SSL方法结合以进一步提高性能。


<details>
  <summary>更多</summary>
  
**动机:** 近年来，半监督学习主要集中在一致性正则化和熵最小化方法上，这通常导致复杂的训练策略。为了简化模型并更直接地利用聚类假设，作者提出了一个新方法。

**方法:** 通过扩展最近提出的可微聚类模块，将聚类假设显式地纳入半监督学习中。利用标注数据指导聚类中心，从而形成一种简单且端到端可训练的深度SSL方法。

**结果:** 所提出的模型在性能上超过了仅监督基线，并且可以与其他SSL方法结合使用，进一步提升其性能。

**结论:** 本文提出的方法不仅简化了模型结构，还提高了半监督学习的效果，证明了将聚类假设显式纳入SSL的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Supercm%3A+Revisiting+Clustering+for+Semi-Supervised+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23824，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23824&send_immediately=true&force_search=false)

**原文摘要:** The development of semi-supervised learning (SSL) has in recent years largely
focused on the development of new consistency regularization or entropy
minimization approaches, often resulting in models with complex training
strategies to obtain the desired results. In this work, we instead propose a
novel approach that explicitly incorporates the underlying clustering
assumption in SSL through extending a recently proposed differentiable
clustering module. Leveraging annotated data to guide the cluster centroids
results in a simple end-to-end trainable deep SSL approach. We demonstrate that
the proposed model improves the performance over the supervised-only baseline
and show that our framework can be used in conjunction with other SSL methods
to further boost their performance.

</details>


### [101] [Bridging Theory and Practice in Link Representation with Graph Neural Networks](https://arxiv.org/abs/2506.24018)
*Veronica Lachi, Francesco Ferrini, Antonio Longa, Bruno Lepri, Andrea Passerini, Manfred Jaeger*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了图神经网络（GNNs）在链接表示上的表达能力，提出了一个统一的框架来比较现有模型的表达能力，并通过实验表明在对称性较高的数据集上，表达能力强的模型表现更好。


<details>
  <summary>更多</summary>
  
**动机:** 尽管GNNs被广泛应用于计算节点对的表示，但对其表达能力的研究主要集中在图级别的表示，而缺乏对链接表示的深入研究。

**方法:** 作者引入了一个统一的框架——$k_ho$-$k_ho$-$m$框架，该框架涵盖了现有的消息传递链接模型，并允许进行正式的表达能力比较。此外，还提出了一个合成评估协议，包括专门为评估链接级别表达能力设计的第一个基准。

**结果:** 通过提出的框架和评估协议，作者推导出了当前方法的层次结构，并提供了分析未来架构的理论工具。实验结果表明，在标准基准上，表达能力强的模型可能表现不佳，但在对称性较高的数据集上，它们显著优于简单的模型。

**结论:** 表达能力在实际应用中确实重要，尤其是在处理高对称性的数据集时。这强调了根据数据集特性选择合适模型的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bridging+Theory+and+Practice+in+Link+Representation+with+Graph+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.24018，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.24018&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) are widely used to compute representations of
node pairs for downstream tasks such as link prediction. Yet, theoretical
understanding of their expressive power has focused almost entirely on
graph-level representations. In this work, we shift the focus to links and
provide the first comprehensive study of GNN expressiveness in link
representation. We introduce a unifying framework, the $k_\phi$-$k_\rho$-$m$
framework, that subsumes existing message-passing link models and enables
formal expressiveness comparisons. Using this framework, we derive a hierarchy
of state-of-the-art methods and offer theoretical tools to analyze future
architectures. To complement our analysis, we propose a synthetic evaluation
protocol comprising the first benchmark specifically designed to assess
link-level expressiveness. Finally, we ask: does expressiveness matter in
practice? We use a graph symmetry metric that quantifies the difficulty of
distinguishing links and show that while expressive models may underperform on
standard benchmarks, they significantly outperform simpler ones as symmetry
increases, highlighting the need for dataset-aware model selection.

</details>


### [102] [EFPI: Elastic Formation and Position Identification in Football (Soccer) using Template Matching and Linear Assignment](https://arxiv.org/abs/2506.23843)
*Joris Bekkers*

**主要类别:** cs.LG

**AI概要:** This paper introduces EFPI, a method for recognizing football team formations and assigning player positions using predefined templates and spatiotemporal tracking data. It minimizes assignment costs by scaling actual player positions to template dimensions and optionally incorporates a stability parameter.


<details>
  <summary>更多</summary>
  
**动机:** To enhance tactical analysis in football by accurately recognizing team formations and assigning player positions.

**方法:** EFPI uses linear sum assignment to match players to positions within predefined static formation templates by minimizing total distance between actual player locations and template positions. A stability parameter can be used to prevent unnecessary formation changes.

**结果:** The method effectively recognizes formations and assigns player positions both on individual frames and across larger game segments, with the option of adding a stability parameter.

**结论:** EFPI offers a flexible approach for football formation recognition and is available as open-source code through the unravelsports Python package.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EFPI%3A+Elastic+Formation+and+Position+Identification+in+Football+%28Soccer%29+using+Template+Matching+and+Linear+Assignment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23843，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23843&send_immediately=true&force_search=false)

**原文摘要:** Understanding team formations and player positioning is crucial for tactical
analysis in football (soccer). This paper presents a flexible method for
formation recognition and player position assignment in football using
predefined static formation templates and cost minimization from spatiotemporal
tracking data, called EFPI. Our approach employs linear sum assignment to
optimally match players to positions within a set of template formations by
minimizing the total distance between actual player locations and template
positions, subsequently selecting the formation with the lowest assignment
cost. To improve accuracy, we scale actual player positions to match the
dimensions of these formation templates in both width and length. While the
method functions effectively on individual frames, it extends naturally to
larger game segments such as complete periods, possession sequences or specific
intervals (e.g. 10 second intervals, 5 minute intervals etc.). Additionally, we
incorporate an optional stability parameter that prevents unnecessary formation
changes when assignment costs differ only marginally between time segments.
EFPI is available as open-source code through the unravelsports Python package.

</details>


### [103] [Development of Hybrid Artificial Intelligence Training on Real and Synthetic Data: Benchmark on Two Mixed Training Strategies](https://arxiv.org/abs/2506.24093)
*Paul Wachter, Lukas Niehaus, Julius Schöning*

**主要类别:** cs.LG

**AI概要:** 本研究通过分析两种常用的混合策略在三种流行架构和三个不同混合数据集上的表现，探讨了合成数据与真实数据比例对ANN训练过程的影响，为优化合成数据的使用提供了有价值的见解。


<details>
  <summary>更多</summary>
  
**动机:** 尽管混合合成数据与真实数据的策略已被证明可以缓解领域差距，但这些策略在各种任务和架构中的泛化能力和鲁棒性尚未得到系统评估。

**方法:** 全面分析两种广泛使用的混合策略在三种流行架构和三个不同混合数据集上的表现，并采样具有不同合成到真实数据比例的子集以研究合成和真实成分的影响。

**结果:** 研究结果提供了关于如何优化ANN训练过程中合成数据使用的宝贵见解，有助于提高其鲁棒性和效率。

**结论:** 通过系统评估不同的混合策略、架构和数据集组合，本研究强调了优化合成数据使用的重要性，这对于提升ANN在实际场景中的性能和泛化能力至关重要。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Development+of+Hybrid+Artificial+Intelligence+Training+on+Real+and+Synthetic+Data%3A+Benchmark+on+Two+Mixed+Training+Strategies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.24093，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.24093&send_immediately=true&force_search=false)

**原文摘要:** Synthetic data has emerged as a cost-effective alternative to real data for
training artificial neural networks (ANN). However, the disparity between
synthetic and real data results in a domain gap. That gap leads to poor
performance and generalization of the trained ANN when applied to real-world
scenarios. Several strategies have been developed to bridge this gap, which
combine synthetic and real data, known as mixed training using hybrid datasets.
While these strategies have been shown to mitigate the domain gap, a systematic
evaluation of their generalizability and robustness across various tasks and
architectures remains underexplored. To address this challenge, our study
comprehensively analyzes two widely used mixing strategies on three prevalent
architectures and three distinct hybrid datasets. From these datasets, we
sample subsets with varying proportions of synthetic to real data to
investigate the impact of synthetic and real components. The findings of this
paper provide valuable insights into optimizing the use of synthetic data in
the training process of any ANN, contributing to enhancing robustness and
efficacy.

</details>


### [104] [When Plants Respond: Electrophysiology and Machine Learning for Green Monitoring Systems](https://arxiv.org/abs/2506.23872)
*Eduard Buss, Till Aust, Heiko Hamann*

**主要类别:** cs.LG

**AI概要:** 论文开发了一种名为PhytoNode的可穿戴设备，用于连续记录常春藤（Hedera helix）的电生理活动，并通过自动机器学习（AutoML）分析数据，将植物的电生理模式与环境条件相关联。研究在户外环境中进行了五个月的数据收集，分类模型在二分类任务中达到了高达95%的宏观F1分数。该研究展示了可持续环境监测的生物混合系统的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 为了利用植物作为自然传感器的能力，探索其在环境监测和精准农业中的应用潜力，同时建立植物与人工设备之间新的生理信号流通道。

**方法:** 将PhytoNode设备安装在常春藤上以记录其电生理活动；在不受控的户外环境中部署植物并收集五个月的数据；使用先进的自动机器学习技术对数据进行分析，将电生理模式与环境条件关联起来。

**结果:** 分类模型表现出高精度，二分类任务中宏F1分数达到95%；自动机器学习方法优于手动调参，选择统计特征子集进一步提高了准确性。

**结论:** 本研究证明了在恶劣现实条件下监测植物电生理学的可行性，为可持续环境监测的可扩展、自维持、植物集成的生物混合系统提供了新方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+Plants+Respond%3A+Electrophysiology+and+Machine+Learning+for+Green+Monitoring+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23872，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23872&send_immediately=true&force_search=false)

**原文摘要:** Living plants, while contributing to ecological balance and climate
regulation, also function as natural sensors capable of transmitting
information about their internal physiological states and surrounding
conditions. This rich source of data provides potential for applications in
environmental monitoring and precision agriculture. With integration into
biohybrid systems, we establish novel channels of physiological signal flow
between living plants and artificial devices. We equipped *Hedera helix* with a
plant-wearable device called PhytoNode to continuously record the plant's
electrophysiological activity. We deployed plants in an uncontrolled outdoor
environment to map electrophysiological patterns to environmental conditions.
Over five months, we collected data that we analyzed using state-of-the-art and
automated machine learning (AutoML). Our classification models achieve high
performance, reaching macro F1 scores of up to 95 percent in binary tasks.
AutoML approaches outperformed manual tuning, and selecting subsets of
statistical features further improved accuracy. Our biohybrid living system
monitors the electrophysiology of plants in harsh, real-world conditions. This
work advances scalable, self-sustaining, and plant-integrated living biohybrid
systems for sustainable environmental monitoring.

</details>


### [105] [Bridging the Gap with Retrieval-Augmented Generation: Making Prosthetic Device User Manuals Available in Marginalised Languages](https://arxiv.org/abs/2506.23958)
*Ikechukwu Ogbonna, Lesley Davidson, Soumya Banerjee, Abhishek Dasgupta, Laurence Kenney, Vikranth Harthikote Nagaraja*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种AI驱动的框架，用于将复杂的医学文档（如假肢设备用户手册）翻译成边缘化语言，特别是非洲国家中由于语言和读写能力差距而面临医疗保健获取障碍的人群。该系统通过整合检索增强生成（RAG）管道和先进的自然语言处理（NLP）模型，实现了上传的手册处理、语义理解、生成性问答和多语言翻译功能，从而为用户提供准确、本地化的实时答案，确保设备说明、治疗协议和安全信息的可访问性，帮助患者和临床医生做出明智的医疗决策。


<details>
  <summary>更多</summary>
  
**动机:** 非洲国家数百万人因语言和读写能力差距而难以获得医疗保健。特别是在接受捐赠的假肢设备时，可能缺乏相应的用户文档，或者在线提供的文档仅以当地人群无法访问的格式（例如英语、高资源环境/文化背景）存在。因此，需要一种方法将复杂医学文档转化为边缘化语言，以便于服务不足的人群使用。

**方法:** 该研究采用广泛使用的皮金（Pidgin）方言作为案例，展示了一种开源框架，可以快速扩展到其他语言或方言。技术上，系统集成了检索增强生成（RAG）管道以处理和理解上传的手册，并利用先进的自然语言处理（NLP）模型进行生成性问答和多语言翻译。用户可以上传英文医学设备手册，用母语提问，并实时获得准确、本地化的答案。

**结果:** 此框架成功地将复杂医学文档转换为边缘化语言，使用户能够以自己的语言获取设备说明、治疗协议和安全信息，提高了医疗保健的可及性和适用性。具体而言，该系统不仅实现了简单翻译，还确保了对医疗内容的全面理解，从而增强了患者的自主权和临床医生的决策能力。

**结论:** 这项工作证明了AI驱动框架在解决语言和读写障碍方面的潜力，特别是在改善医疗保健获取方面。通过提供准确、本地化的医疗信息，该系统有助于缩小全球医疗保健差距，特别是在服务不足的社区中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bridging+the+Gap+with+Retrieval-Augmented+Generation%3A+Making+Prosthetic+Device+User+Manuals+Available+in+Marginalised+Languages，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23958，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23958&send_immediately=true&force_search=false)

**原文摘要:** Millions of people in African countries face barriers to accessing healthcare
due to language and literacy gaps. This research tackles this challenge by
transforming complex medical documents -- in this case, prosthetic device user
manuals -- into accessible formats for underserved populations. This case study
in cross-cultural translation is particularly pertinent/relevant for
communities that receive donated prosthetic devices but may not receive the
accompanying user documentation. Or, if available online, may only be available
in formats (e.g., language and readability) that are inaccessible to local
populations (e.g., English-language, high resource settings/cultural context).
The approach is demonstrated using the widely spoken Pidgin dialect, but our
open-source framework has been designed to enable rapid and easy extension to
other languages/dialects. This work presents an AI-powered framework designed
to process and translate complex medical documents, e.g., user manuals for
prosthetic devices, into marginalised languages. The system enables users --
such as healthcare workers or patients -- to upload English-language medical
equipment manuals, pose questions in their native language, and receive
accurate, localised answers in real time. Technically, the system integrates a
Retrieval-Augmented Generation (RAG) pipeline for processing and semantic
understanding of the uploaded manuals. It then employs advanced Natural
Language Processing (NLP) models for generative question-answering and
multilingual translation. Beyond simple translation, it ensures accessibility
to device instructions, treatment protocols, and safety information, empowering
patients and clinicians to make informed healthcare decisions.

</details>


### [106] [UMA: A Family of Universal Models for Atoms](https://arxiv.org/abs/2506.23971)
*Brandon M. Wood, Misko Dzamba, Xiang Fu, Meng Gao, Muhammed Shuaibi, Luis Barroso-Luque, Kareem Abdelmaqsoud, Vahe Gharakhanyan, John R. Kitchin, Daniel S. Levine, Kyle Michel, Anuroop Sriram, Taco Cohen, Abhishek Das, Ammar Rizvi, Sushree Jagriti Sahoo, Zachary W. Ulissi, C. Lawrence Zitnick*

**主要类别:** cs.LG

**AI概要:** Meta FAIR提出了一个名为UMA的原子通用模型家族，这些模型基于半亿独特的3D原子结构进行训练，横跨多个化学领域如分子、材料和催化剂。通过开发经验性缩放定律来理解如何随数据集大小增加模型容量以达到最佳准确性。UMA模型在多个领域的应用中表现出色，无需微调即可媲美甚至超越专门模型。


<details>
  <summary>更多</summary>
  
**动机:** 快速且精确地从原子模拟中计算属性对于化学和材料科学中的许多应用（包括药物发现、能源存储和半导体制造）至关重要。目前需要一种能够提升速度、准确性和泛化的模型。

**方法:** UMA模型通过整合多个化学领域的数据进行训练，并采用一种称为线性专家混合的新颖架构设计，这使得模型能够在不牺牲速度的情况下增加容量。例如，UMA-medium有14亿参数，但每个原子结构仅激活约5000万参数。

**结果:** UMA模型在多个领域的多样化应用中表现优异，单一模型无需任何微调即可与专门模型相媲美或更优。

**结论:** Meta FAIR发布了UMA代码、权重和相关数据，以加速计算工作流程，并使社区能够继续构建越来越强大的AI模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是UMA%3A+A+Family+of+Universal+Models+for+Atoms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23971，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23971&send_immediately=true&force_search=false)

**原文摘要:** The ability to quickly and accurately compute properties from atomic
simulations is critical for advancing a large number of applications in
chemistry and materials science including drug discovery, energy storage, and
semiconductor manufacturing. To address this need, Meta FAIR presents a family
of Universal Models for Atoms (UMA), designed to push the frontier of speed,
accuracy, and generalization. UMA models are trained on half a billion unique
3D atomic structures (the largest training runs to date) by compiling data
across multiple chemical domains, e.g. molecules, materials, and catalysts. We
develop empirical scaling laws to help understand how to increase model
capacity alongside dataset size to achieve the best accuracy. The UMA small and
medium models utilize a novel architectural design we refer to as mixture of
linear experts that enables increasing model capacity without sacrificing
speed. For example, UMA-medium has 1.4B parameters but only ~50M active
parameters per atomic structure. We evaluate UMA models on a diverse set of
applications across multiple domains and find that, remarkably, a single model
without any fine-tuning can perform similarly or better than specialized
models. We are releasing the UMA code, weights, and associated data to
accelerate computational workflows and enable the community to continue to
build increasingly capable AI models.

</details>


### [107] [A Scalable Approach for Safe and Robust Learning via Lipschitz-Constrained Networks](https://arxiv.org/abs/2506.23977)
*Zain ul Abdeen, Vassilis Kekatos, Ming Jin*

**主要类别:** cs.LG

**AI概要:** 提出了一种凸训练框架，通过半定松弛来强制全局Lipschitz约束，并开发了RS-LMI方法以分解全局约束，从而实现平滑且内存高效的训练目标。实验结果表明该框架在保证鲁棒性的同时显著改善了Lipschitz界限和运行时性能。


<details>
  <summary>更多</summary>
  
**动机:** 在安全关键应用中部署神经网络需要具备认证鲁棒性，而限制网络的全局Lipschitz常数是实现这一目标的主要方法。然而，现有的准确方法通常因依赖全局半定规划（SDP）而导致非凸公式化和可扩展性差的问题。

**方法:** 论文提出了一种凸训练框架，通过半定松弛来强制全局Lipschitz约束。首先，使用环路变换重新参数化神经网络，推导出一个凸可接受条件，使得训练既可行又可认证。此外，为了克服全局SDP规模的限制，开发了一种随机子空间线性矩阵不等式（RS-LMI）方法，将全局约束分解为投影到低维子空间上的逐层约束，从而形成平滑且内存高效的训练目标。

**结果:** 在MNIST、CIFAR-10和ImageNet上的实证结果表明，所提出的框架在保持竞争力准确性的同时，显著改善了Lipschitz界限和运行时性能。

**结论:** 所提出的凸训练框架结合RS-LMI方法能够在保证神经网络认证鲁棒性的同时，提供更优的可扩展性和效率。这为在安全关键应用中部署神经网络提供了新的可能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Scalable+Approach+for+Safe+and+Robust+Learning+via+Lipschitz-Constrained+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23977，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23977&send_immediately=true&force_search=false)

**原文摘要:** Certified robustness is a critical property for deploying neural networks
(NN) in safety-critical applications. A principle approach to achieving such
guarantees is to constrain the global Lipschitz constant of the network.
However, accurate methods for Lipschitz-constrained training often suffer from
non-convex formulations and poor scalability due to reliance on global
semidefinite programs (SDPs). In this letter, we propose a convex training
framework that enforces global Lipschitz constraints via semidefinite
relaxation. By reparameterizing the NN using loop transformation, we derive a
convex admissibility condition that enables tractable and certifiable training.
While the resulting formulation guarantees robustness, its scalability is
limited by the size of global SDP. To overcome this, we develop a randomized
subspace linear matrix inequalities (RS-LMI) approach that decomposes the
global constraints into sketched layerwise constraints projected onto
low-dimensional subspaces, yielding a smooth and memory-efficient training
objective. Empirical results on MNIST, CIFAR-10, and ImageNet demonstrate that
the proposed framework achieves competitive accuracy with significantly
improved Lipschitz bounds and runtime performance.

</details>


### [108] [LLM Agents Are the Antidote to Walled Gardens](https://arxiv.org/abs/2506.23978)
*Samuele Marro, Philip Torr*

**主要类别:** cs.LG

**AI概要:** 论文提出，基于大型语言模型（LLM）的代理可以打破当前应用层被封闭专有平台主导的局面，通过自动转换数据格式和与人类设计的界面交互，实现低成本且不可避免的通用互操作性。这种互操作性削弱垄断行为、促进数据可移植性，但也可能带来新的安全风险和技术债务。作者认为，机器学习社区应接纳这一发展，并构建适当的框架以缓解负面影响。


<details>
  <summary>更多</summary>
  
**动机:** 互联网的核心基础设施原本设计为开放和普遍，但目前的应用层却被封闭、专有的平台所主导。开放和互操作的API需要大量投资，市场领导者几乎没有动力去实现数据交换，因为这可能会削弱他们对用户的锁定效应。因此，需要一种新的方法来打破这种现状。

**方法:** 利用LLM-based agents，这些代理能够自动在不同数据格式之间进行翻译，并与为人设计的界面进行交互，从而显著降低互操作性的成本并使其不可避免。这种能力被称为通用互操作性，它允许任何两个数字服务通过AI中介适配器无缝交换数据。

**结果:** 这种方法削弱了垄断行为，促进了数据的可移植性，同时指出也可能导致新的安全风险和技术债务。

**结论:** 机器学习社区应该接受这一发展，并建立适当的框架来减轻其潜在的负面影响，以便利用AI恢复用户自由和竞争市场，而不牺牲安全性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM+Agents+Are+the+Antidote+to+Walled+Gardens，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23978，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23978&send_immediately=true&force_search=false)

**原文摘要:** While the Internet's core infrastructure was designed to be open and
universal, today's application layer is dominated by closed, proprietary
platforms. Open and interoperable APIs require significant investment, and
market leaders have little incentive to enable data exchange that could erode
their user lock-in. We argue that LLM-based agents fundamentally disrupt this
status quo. Agents can automatically translate between data formats and
interact with interfaces designed for humans: this makes interoperability
dramatically cheaper and effectively unavoidable. We name this shift universal
interoperability: the ability for any two digital services to exchange data
seamlessly using AI-mediated adapters. Universal interoperability undermines
monopolistic behaviours and promotes data portability. However, it can also
lead to new security risks and technical debt. Our position is that the ML
community should embrace this development while building the appropriate
frameworks to mitigate the downsides. By acting now, we can harness AI to
restore user freedom and competitive markets without sacrificing security.

</details>


### [109] [The Jacobian and Hessian of the Kullback-Leibler Divergence between Multivariate Gaussian Distributions (Technical Report)](https://arxiv.org/abs/2506.23996)
*Juan Maroñas*

**主要类别:** cs.LG

**AI概要:** 这篇论文展示了如何利用一阶和二阶微分来获取两个多元高斯分布之间Kullback-Leibler散度的Jacobian矩阵和Hessian矩阵。推导基于[magnus99]中的理论，并从[minka]的一些推导中获得了极大的灵感。为了尽可能做到易懂，文档分为结果总结和每个参与元素的详细推导两部分，具体提到了推导中使用的技巧以及许多基础概念。


<details>
  <summary>更多</summary>
  
**动机:** 动机是提供一种方法来计算两个多元高斯分布之间Kullback-Leibler散度的Jacobian和Hessian矩阵，这对于理解模型参数对散度的影响非常重要。

**方法:** 使用一阶和二阶微分来计算Kullback-Leibler散度的Jacobian和Hessian矩阵。推导过程参考了[magnus99]的理论，并受到[minka]的启发。

**结果:** 成功地推导出了两个多元高斯分布之间Kullback-Leibler散度的Jacobian和Hessian矩阵的表达式。

**结论:** 通过详细的推导，本文提供了计算Kullback-Leibler散度相关矩阵的方法，并为读者提供了清晰的结果总结和推导细节。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Jacobian+and+Hessian+of+the+Kullback-Leibler+Divergence+between+Multivariate+Gaussian+Distributions+%28Technical+Report%29，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23996，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23996&send_immediately=true&force_search=false)

**原文摘要:** This document shows how to obtain the Jacobian and Hessian matrices of the
Kullback-Leibler divergence between two multivariate Gaussian distributions,
using the first and second-order differentials. The presented derivations are
based on the theory presented by \cite{magnus99}. I've also got great
inspiration from some of the derivations in \cite{minka}.
  Since I pretend to be at most didactic, the document is split into a summary
of results and detailed derivations on each of the elements involved, with
specific references to the tricks used in the derivations, and to many of the
underlying concepts.

</details>


### [110] [The Illusion of Progress? A Critical Look at Test-Time Adaptation for Vision-Language Models](https://arxiv.org/abs/2506.24000)
*Lijun Sheng, Jian Liang, Ran He, Zilei Wang, Tieniu Tan*

**主要类别:** cs.LG

**AI概要:** TTA-VLM是一个全面的基准测试平台，用于评估视觉-语言模型（VLMs）上的测试时适应（TTA）方法。它解决了当前TTA研究中存在的一些问题，例如基线结果重复、评估指标有限等。通过广泛的实验，作者发现现有的TTA方法改进有限，并且与训练时微调方法协作不佳，同时可能降低模型的可信度。


<details>
  <summary>更多</summary>
  
**动机:** 目前的TTA研究存在几个主要限制：1) 基线结果重复；2) 评估指标有限；3) 实验设置不一致；4) 分析不足。这些问题阻碍了TTA方法之间的公平比较，并模糊了它们的实际优缺点。

**方法:** TTA-VLM在统一和可重现的框架内实现了8种情景TTA和7种在线TTA方法，并在15个广泛使用的数据集上进行评估。除了CLIP，还扩展到SigLIP模型，并包括CoOp、MaPLe和TeCoA等训练时间调整方法来评估通用性。此外，TTA-VLM还结合了各种评估指标，如稳健性、校准、异常检测和稳定性，以提供更全面的评估。

**结果:** 实验结果表明：1) 现有的TTA方法相比早期开创性工作改进有限；2) 当前TTA方法与训练时微调方法协作不佳；3) 准确率的提升通常伴随着模型可信度的下降。

**结论:** 作者发布了TTA-VLM，旨在为VLMs的TTA方法提供公平的比较和全面的评估，并希望鼓励社区开发更可靠和可泛化的TTA策略。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Illusion+of+Progress%3F+A+Critical+Look+at+Test-Time+Adaptation+for+Vision-Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.24000，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.24000&send_immediately=true&force_search=false)

**原文摘要:** Test-time adaptation (TTA) methods have gained significant attention for
enhancing the performance of vision-language models (VLMs) such as CLIP during
inference, without requiring additional labeled data. However, current TTA
researches generally suffer from major limitations such as duplication of
baseline results, limited evaluation metrics, inconsistent experimental
settings, and insufficient analysis. These problems hinder fair comparisons
between TTA methods and obscure their practical strengths and weaknesses. To
address these challenges, we introduce TTA-VLM, a comprehensive benchmark for
evaluating TTA methods on VLMs. Our benchmark implements 8 episodic TTA and 7
online TTA methods within a unified and reproducible framework, and evaluates
them across 15 widely used datasets. Unlike prior studies focused solely on
CLIP, we extend the evaluation to SigLIP--a model trained with a Sigmoid
loss--and include training-time tuning methods such as CoOp, MaPLe, and TeCoA
to assess generality. Beyond classification accuracy, TTA-VLM incorporates
various evaluation metrics, including robustness, calibration,
out-of-distribution detection, and stability, enabling a more holistic
assessment of TTA methods. Through extensive experiments, we find that 1)
existing TTA methods produce limited gains compared to the previous pioneering
work; 2) current TTA methods exhibit poor collaboration with training-time
fine-tuning methods; 3) accuracy gains frequently come at the cost of reduced
model trustworthiness. We release TTA-VLM to provide fair comparison and
comprehensive evaluation of TTA methods for VLMs, and we hope it encourages the
community to develop more reliable and generalizable TTA strategies.

</details>


### [111] [Provably Efficient and Agile Randomized Q-Learning](https://arxiv.org/abs/2506.24005)
*He Wang, Xingyu Xu, Yuejie Chi*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Provably+Efficient+and+Agile+Randomized+Q-Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.24005，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.24005&send_immediately=true&force_search=false)

**原文摘要:** While Bayesian-based exploration often demonstrates superior empirical
performance compared to bonus-based methods in model-based reinforcement
learning (RL), its theoretical understanding remains limited for model-free
settings. Existing provable algorithms either suffer from computational
intractability or rely on stage-wise policy updates which reduce responsiveness
and slow down the learning process. In this paper, we propose a novel variant
of Q-learning algorithm, refereed to as RandomizedQ, which integrates
sampling-based exploration with agile, step-wise, policy updates, for episodic
tabular RL. We establish an $\widetilde{O}(\sqrt{H^5SAT})$ regret bound, where
$S$ is the number of states, $A$ is the number of actions, $H$ is the episode
length, and $T$ is the total number of episodes. In addition, we present a
logarithmic regret bound under a mild positive sub-optimality condition on the
optimal Q-function. Empirically, RandomizedQ exhibits outstanding performance
compared to existing Q-learning variants with both bonus-based and
Bayesian-based exploration on standard benchmarks.

</details>


### [112] [Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives](https://arxiv.org/abs/2506.24124)
*Dong Sixun, Fan Wei, Teresa Wu, Fu Yanjie*

**主要类别:** cs.LG

**AI概要:** 提出了一种多模态对比学习框架，将时间序列转换为结构化的视觉和文本视角，并通过对比学习对齐这些视角以捕捉更丰富的表示。实验表明，该方法在短期和长期预测基准上均优于单模态和跨模态基线。


<details>
  <summary>更多</summary>
  
**动机:** 传统的时间序列预测依赖于单一模式的数值输入，难以捕捉高层次的语义模式。虽然最近的方法探索了使用大语言模型将时间序列表示为文本，但这些方法仍然受到离散令牌序列的限制，并缺乏人类通常应用的感知直觉（如解释视觉模式）。

**方法:** 提出了一种多模态对比学习框架，将原始时间序列转换为结构化的视觉和文本视角。然后通过对比学习在共享语义空间中对齐这些视图，使模型能够捕捉更丰富、更互补的表示。此外，引入了一个变量选择模块，利用对齐的表示来识别多变量预测中最相关的变量。

**结果:** 在十五个短期和六个长期预测基准上的广泛实验证明，该方法始终优于强大的单模态和跨模态基线。

**结论:** 多模态对齐可以有效增强时间序列预测。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Teaching+Time+Series+to+See+and+Speak%3A+Forecasting+with+Aligned+Visual+and+Textual+Perspectives，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.24124，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.24124&send_immediately=true&force_search=false)

**原文摘要:** Time series forecasting traditionally relies on unimodal numerical inputs,
which often struggle to capture high-level semantic patterns due to their dense
and unstructured nature. While recent approaches have explored representing
time series as text using large language models (LLMs), these methods remain
limited by the discrete nature of token sequences and lack the perceptual
intuition humans typically apply, such as interpreting visual patterns. In this
paper, we propose a multimodal contrastive learning framework that transforms
raw time series into structured visual and textual perspectives. Rather than
using natural language or real-world images, we construct both modalities
directly from numerical sequences. We then align these views in a shared
semantic space via contrastive learning, enabling the model to capture richer
and more complementary representations. Furthermore, we introduce a variate
selection module that leverages the aligned representations to identify the
most informative variables for multivariate forecasting. Extensive experiments
on fifteen short-term and six long-term forecasting benchmarks demonstrate that
our approach consistently outperforms strong unimodal and cross-modal
baselines, highlighting the effectiveness of multimodal alignment in enhancing
time series forecasting. Code is available at:
https://github.com/Ironieser/TimesCLIP.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [113] [Bootstrapping Human-Like Planning via LLMs](https://arxiv.org/abs/2506.22604)
*David Porfirio, Vincent Hsiao, Morgan Fine-Morris, Leslie Smith, Laura M. Hiatt*

**主要类别:** cs.AI

**AI概要:** 本研究探讨了将自然语言编程与拖放式编程结合的可能性，构建了一个基于大型语言模型（LLM）的管道，该管道接受自然语言作为输入，并输出类似人类的动作序列。通过与手动指定的动作序列数据集进行比较，结果表明较大的模型在生成类似人类动作序列方面表现更好，但较小的模型也能达到满意的效果。


<details>
  <summary>更多</summary>
  
**动机:** 为了满足机器人终端用户对易用性任务规范手段的需求，探索自然语言编程和拖放式编程两种范式的结合可能性，以充分利用两者的优点：自然语言接口的直观性和拖放接口的精确性。

**方法:** 构建了一个基于大型语言模型（LLM）的管道，该管道接受自然语言输入并生成具有人类级别细致程度的动作序列。然后将这些生成的动作序列与另一组手工指定的动作序列数据集进行比较。

**结果:** 结果表明，较大的模型在生成类似人类动作序列方面的表现优于较小的模型，但较小的模型仍然能够实现令人满意的性能。

**结论:** 自然语言和拖放式编程可以有效结合，通过使用大型语言模型，可以从自然语言输入生成类似于人类指定的动作序列，为机器人任务编程提供了一种新的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bootstrapping+Human-Like+Planning+via+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22604，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22604&send_immediately=true&force_search=false)

**原文摘要:** Robot end users increasingly require accessible means of specifying tasks for
robots to perform. Two common end-user programming paradigms include
drag-and-drop interfaces and natural language programming. Although natural
language interfaces harness an intuitive form of human communication,
drag-and-drop interfaces enable users to meticulously and precisely dictate the
key actions of the robot's task. In this paper, we investigate the degree to
which both approaches can be combined. Specifically, we construct a large
language model (LLM)-based pipeline that accepts natural language as input and
produces human-like action sequences as output, specified at a level of
granularity that a human would produce. We then compare these generated action
sequences to another dataset of hand-specified action sequences. Although our
results reveal that larger models tend to outperform smaller ones in the
production of human-like action sequences, smaller models nonetheless achieve
satisfactory performance.

</details>


### [114] [Ludax: A GPU-Accelerated Domain Specific Language for Board Games](https://arxiv.org/abs/2506.22609)
*Graham Todd, Alexander G. Padula, Dennis J. N. J. Soemers, Julian Togelius*

**主要类别:** cs.AI

**AI概要:** 本论文提出了一种名为Ludax的框架，这是一个针对棋盘游戏的领域特定语言，可以自动编译为硬件加速代码。Ludax结合了游戏描述语言的通用性和现代并行处理硬件的速度，并且适合集成到现有的深度学习管道中。论文提供了Ludax描述语言的详细分解、技术编译说明、速度基准测试和训练强化学习代理的演示。


<details>
  <summary>更多</summary>
  
**动机:** 随着硬件加速的进步和游戏作为人工智能研究的基准环境的重要性增加，需要一种能够自动编译为硬件加速代码的游戏描述语言来支持更高效的研究。

**方法:** 开发了一个名为Ludax的框架，该框架是一种针对棋盘游戏的领域特定语言，能够自动编译为硬件加速代码。Ludax设计为与现有深度学习管道无缝集成，并提供灵活的表示方案以加速游戏研究。

**结果:** Ludax框架实现了快速模拟和RL代理训练的能力，并通过速度基准测试证明了其性能提升。此外，Ludax是开源的，提供了现有棋盘游戏的实现。

**结论:** Ludax作为一个工具，可以帮助加速从强化学习到认知科学的游戏研究，提供快速模拟和灵活的表示方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Ludax%3A+A+GPU-Accelerated+Domain+Specific+Language+for+Board+Games，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22609，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22609&send_immediately=true&force_search=false)

**原文摘要:** Games have long been used as benchmarks and testing environments for research
in artificial intelligence. A key step in supporting this research was the
development of game description languages: frameworks that compile
domain-specific code into playable and simulatable game environments, allowing
researchers to generalize their algorithms and approaches across multiple games
without having to manually implement each one. More recently, progress in
reinforcement learning (RL) has been largely driven by advances in hardware
acceleration. Libraries like JAX allow practitioners to take full advantage of
cutting-edge computing hardware, often speeding up training and testing by
orders of magnitude. Here, we present a synthesis of these strands of research:
a domain-specific language for board games which automatically compiles into
hardware-accelerated code. Our framework, Ludax, combines the generality of
game description languages with the speed of modern parallel processing
hardware and is designed to fit neatly into existing deep learning pipelines.
We envision Ludax as a tool to help accelerate games research generally, from
RL to cognitive science, by enabling rapid simulation and providing a flexible
representation scheme. We present a detailed breakdown of Ludax's description
language and technical notes on the compilation process, along with speed
benchmarking and a demonstration of training RL agents. The Ludax framework,
along with implementations of existing board games, is open-source and freely
available.

</details>


### [115] [URSA: The Universal Research and Scientific Agent](https://arxiv.org/abs/2506.22653)
*Michael Grosskopf, Russell Bent, Rahul Somasundaram, Isaac Michaud, Arthur Lui, Nathan Debardeleben, Earl Lawrence*

**主要类别:** cs.AI

**AI概要:** URSA是一个包含模块化代理和工具的科学代理生态系统，可以结合先进物理模拟代码，解决复杂科学问题，加速研究任务。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLMs）已经超越了简单的聊天机器人形式，能够执行复杂的推理、规划、写作、编码和研究任务，这些技能与人类科学家日常解决问题的技能有很大重叠，因此在'agentic' AI中使用LLMs有可能革新现代科学并消除进步的瓶颈。

**方法:** 提出了URSA，一个科学代理生态系统，包括一组模块化代理和工具，可以结合以解决不同复杂性和影响的科学问题。

**结果:** 展示了URSA的架构以及突出该系统潜力的例子。

**结论:** URSA有潜力通过加速研究任务来推动科学发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是URSA%3A+The+Universal+Research+and+Scientific+Agent，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22653，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22653&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have moved far beyond their initial form as
simple chatbots, now carrying out complex reasoning, planning, writing, coding,
and research tasks. These skills overlap significantly with those that human
scientists use day-to-day to solve complex problems that drive the cutting edge
of research. Using LLMs in "agentic" AI has the potential to revolutionize
modern science and remove bottlenecks to progress. In this work, we present
URSA, a scientific agent ecosystem for accelerating research tasks. URSA
consists of a set of modular agents and tools, including coupling to advanced
physics simulation codes, that can be combined to address scientific problems
of varied complexity and impact. This work highlights the architecture of URSA,
as well as examples that highlight the potential of the system.

</details>


### [116] [Explanations are a means to an end](https://arxiv.org/abs/2506.22740)
*Jessica Hullman, Ziyang Guo, Berk Ustun*

**主要类别:** cs.AI

**AI概要:** 这篇论文提出了解释性机器学习方法应该根据具体目标来设计和评估，并通过统计决策理论框架展示其在不同应用场景中的应用，同时定义了评估解释价值的理论与实证结合的新视角。


<details>
  <summary>更多</summary>
  
**动机:** 当前可解释机器学习方法缺乏对实际应用场景的深入考虑，可能导致解释的模糊性和误用。

**方法:** 基于统计决策理论构建一个框架，用于明确解释的目标并将其应用于临床决策支持、提供补救措施或调试等场景，同时分析理想决策者在特定任务中可能获得的最大性能提升。

**结果:** 该方法能够防止因模糊性导致的误用，并促使研究者明确具体的使用案例以便更好地评估解释的价值。

**结论:** 评估解释的价值应结合理论和实证视角，论文贡献了跨越这两种视角的定义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Explanations+are+a+means+to+an+end，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22740，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22740&send_immediately=true&force_search=false)

**原文摘要:** Modern methods for explainable machine learning are designed to describe how
models map inputs to outputs--without deep consideration of how these
explanations will be used in practice. This paper argues that explanations
should be designed and evaluated with a specific end in mind. We describe how
to formalize this end in a framework based in statistical decision theory. We
show how this functionally-grounded approach can be applied across diverse use
cases, such as clinical decision support, providing recourse, or debugging. We
demonstrate its use to characterize the maximum "boost" in performance on a
particular task that an explanation could provide an idealized decision-maker,
preventing misuse due to ambiguity by forcing researchers to specify concrete
use cases that can be analyzed in light of models of expected explanation use.
We argue that evaluation should meld theoretical and empirical perspectives on
the value of explanation, and contribute definitions that span these
perspectives.

</details>


### [117] [Bridging Ethical Principles and Algorithmic Methods: An Alternative Approach for Assessing Trustworthiness in AI Systems](https://arxiv.org/abs/2506.22774)
*Michael Papademas, Xenia Ziouvelou, Antonis Troumpoukis, Vangelis Karkaletsis*

**主要类别:** cs.AI

**AI概要:** 这篇论文提出了一种结合PageRank和TrustRank算法的评估方法，旨在通过提供定量分析来减少当前AI系统信任度自我评估中的主观性，并综合考虑理论指导和技术工具的优点。


<details>
  <summary>更多</summary>
  
**动机:** 人工智能（AI）系统的复杂性和广泛的社会影响带来了重大挑战，尤其是其运行可能超出人类直接监督或理解范围。尽管已有理论工具和指南以及技术工具分别从不同角度应对这些挑战，但前者缺乏量化手段，而后者缺乏全面视角。

**方法:** 论文引入了一种评估方法，将Trustworthy AI的伦理要素与PageRank和TrustRank的算法过程相结合，创建一个评估框架。该框架通过引入算法标准来最小化领域内普遍存在的自我评估技术的主观性。

**结果:** 研究表明，通过应用所提出的方法，可以在考虑相关指南理论内容的同时，提供定量见解，从而实现对AI系统信任度的全面评估。

**结论:** 结合PageRank和TrustRank算法的评估方法能够有效减少自我评估中的主观性，并为AI系统的信任度提供全面且量化的评估视角。这有助于推动更可信的AI技术的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bridging+Ethical+Principles+and+Algorithmic+Methods%3A+An+Alternative+Approach+for+Assessing+Trustworthiness+in+AI+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22774，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22774&send_immediately=true&force_search=false)

**原文摘要:** Artificial Intelligence (AI) technology epitomizes the complex challenges
posed by human-made artifacts, particularly those widely integrated into
society and exert significant influence, highlighting potential benefits and
their negative consequences. While other technologies may also pose substantial
risks, AI's pervasive reach makes its societal effects especially profound. The
complexity of AI systems, coupled with their remarkable capabilities, can lead
to a reliance on technologies that operate beyond direct human oversight or
understanding. To mitigate the risks that arise, several theoretical tools and
guidelines have been developed, alongside efforts to create technological tools
aimed at safeguarding Trustworthy AI. The guidelines take a more holistic view
of the issue but fail to provide techniques for quantifying trustworthiness.
Conversely, while technological tools are better at achieving such
quantification, they lack a holistic perspective, focusing instead on specific
aspects of Trustworthy AI. This paper aims to introduce an assessment method
that combines the ethical components of Trustworthy AI with the algorithmic
processes of PageRank and TrustRank. The goal is to establish an assessment
framework that minimizes the subjectivity inherent in the self-assessment
techniques prevalent in the field by introducing algorithmic criteria. The
application of our approach indicates that a holistic assessment of an AI
system's trustworthiness can be achieved by providing quantitative insights
while considering the theoretical content of relevant guidelines.

</details>


### [118] [ReasonBridge: Efficient Reasoning Transfer from Closed to Open-Source Language Models](https://arxiv.org/abs/2506.22865)
*Ziqi Zhong, Xunzhu Tang*

**主要类别:** cs.AI

**AI概要:** Recent advancements in Large Language Models (LLMs) have shown a significant performance gap between closed-source and open-source models. This paper introduces ReasonBridge, which efficiently transfers reasoning capabilities from closed-source to open-source models through a hierarchical knowledge distillation framework. The transfer learning approach incorporates a hierarchical distillation process, a sparse reasoning-focused adapter architecture, and a test-time compute scaling mechanism. Evaluations demonstrate that ReasonBridge improves reasoning capabilities in open-source models by up to 23% on benchmark tasks.


<details>
  <summary>更多</summary>
  
**动机:** The motivation of this paper is to address the significant performance gap between closed-source and open-source LLMs, particularly in tasks requiring complex reasoning and precise instruction following.

**方法:** The method introduced in this paper is called ReasonBridge, which uses a novel hierarchical knowledge distillation framework to transfer reasoning capabilities from powerful closed-source models to open-source models. This includes a hierarchical distillation process capturing both strategic abstraction and tactical implementation patterns, a sparse reasoning-focused adapter architecture requiring only 0.3% additional trainable parameters, and a test-time compute scaling mechanism using guided inference interventions.

**结果:** Comprehensive evaluations show that ReasonBridge improves reasoning capabilities in open-source models by up to 23% on benchmark tasks, significantly narrowing the gap with closed-source models. Notably, the enhanced Qwen2.5-14B outperforms Claude-Sonnet3.5 on MATH500 and matches its performance on competition-level AIME problems.

**结论:** ReasonBridge establishes a sample-efficient approach to reasoning enhancement for instruction following in open-source models, effectively generalizing across diverse reasoning domains and model architectures.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ReasonBridge%3A+Efficient+Reasoning+Transfer+from+Closed+to+Open-Source+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22865，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22865&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in Large Language Models (LLMs) have revealed a
significant performance gap between closed-source and open-source models,
particularly in tasks requiring complex reasoning and precise instruction
following. This paper introduces ReasonBridge, a methodology that efficiently
transfers reasoning capabilities from powerful closed-source to open-source
models through a novel hierarchical knowledge distillation framework. We
develop a tailored dataset Reason1K with only 1,000 carefully curated reasoning
traces emphasizing difficulty, diversity, and quality. These traces are
filtered from across multiple domains using a structured multi-criteria
selection algorithm. Our transfer learning approach incorporates: (1) a
hierarchical distillation process capturing both strategic abstraction and
tactical implementation patterns, (2) a sparse reasoning-focused adapter
architecture requiring only 0.3% additional trainable parameters, and (3) a
test-time compute scaling mechanism using guided inference interventions.
Comprehensive evaluations demonstrate that ReasonBridge improves reasoning
capabilities in open-source models by up to 23% on benchmark tasks,
significantly narrowing the gap with closed-source models. Notably, the
enhanced Qwen2.5-14B outperforms Claude-Sonnet3.5 on MATH500 and matches its
performance on competition-level AIME problems. Our methodology generalizes
effectively across diverse reasoning domains and model architectures,
establishing a sample-efficient approach to reasoning enhancement for
instruction following.

</details>


### [119] [Agentic Enterprise: AI-Centric User to User-Centric AI](https://arxiv.org/abs/2506.22893)
*Arpit Narechania, Alex Endert, Atanu R Sinha*

**主要类别:** cs.AI

**AI概要:** 在企业中，人工智能（AI）可以提升决策生产力。本文提出了以用户为中心的AI六项原则，并提倡通过市场机制将AI设计和交付与企业用户的需求对齐。


<details>
  <summary>更多</summary>
  
**动机:** 探讨AI在企业中的潜力，特别是其在提高决策生产力方面的作用。

**方法:** 分析当前AI-Centric用户范式存在的不足，并提出转向User-Centric AI的六项原则。

**结果:** 明确了六个成功要素，以促进AI在企业决策中的应用，并强调了市场机制的重要性。

**结论:** 应以用户为中心设计AI，并通过市场机制确保AI满足企业用户需求，从而提升企业决策效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Agentic+Enterprise%3A+AI-Centric+User+to+User-Centric+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22893，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22893&send_immediately=true&force_search=false)

**原文摘要:** After a very long winter, the Artificial Intelligence (AI) spring is here.
Or, so it seems over the last three years. AI has the potential to impact many
areas of human life - personal, social, health, education, professional. In
this paper, we take a closer look at the potential of AI for Enterprises, where
decision-making plays a crucial and repeated role across functions, tasks, and
operations. We consider Agents imbued with AI as means to increase
decision-productivity of enterprises. We highlight six tenets for Agentic
success in enterprises, by drawing attention to what the current, AI-Centric
User paradigm misses, in the face of persistent needs of and usefulness for
Enterprise Decision-Making. In underscoring a shift to User-Centric AI, we
offer six tenets and promote market mechanisms for platforms, aligning the
design of AI and its delivery by Agents to the cause of enterprise users.

</details>


### [120] [Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning](https://arxiv.org/abs/2506.22919)
*Sanskar Pandey, Ruhaan Chopra, Saad Murtaza Bhat, Ark Abhyudaya*

**主要类别:** cs.AI

**AI概要:** Hecto是一种轻量级的MoE架构，通过结合GRU和FFNN专家来处理不同类型的推理任务，在多个基准测试中表现接近同质化基线，并在较大批次规模下性能更佳。


<details>
  <summary>更多</summary>
  
**动机:** 现有的MoE模型中的专家依赖相同的归纳偏置，限制了表示多样性，且静态计算路径对需要不同类型推理的输入效率低下，限制了专业化和可解释性。

**方法:** 提出了一种名为Hecto的轻量级MoE架构，该架构通过稀疏Top-1门机制结合了一个用于时间推理的GRU专家和一个用于静态抽象的FFNN专家，利用了结构异质性。

**结果:** 在三个推理基准（AG News、SST-2、HotpotQA）和一个回归任务（STS-B）上进行评估时，尽管接收到孤立的输入表示，Hecto的表现与同质化基线相当或接近；并且实现了清晰的专家专业化，每个专家与不同的推理类型（时间 vs 静态）对齐。在较大的批次规模下，由于计算约束的放松，Hecto表现出更好的性能。消融结果表明，Hecto的稳定性和可解释性源于其架构多样性。

**结论:** Hecto确立了条件计算的新基准，提供了一个有原则的框架，在低资源条件下通过有原则的专业化实现专门推理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hecto%3A+Modular+Sparse+Experts+for+Adaptive+and+Interpretable+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22919，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22919&send_immediately=true&force_search=false)

**原文摘要:** Mixture-of-Experts (MoE) models enable conditional computation by routing
inputs to specialized experts, but these experts rely on identical inductive
biases, thus limiting representational diversity. This static computation
pathway is inefficient for inputs that require different types of reasoning and
limits specialization and interpretability. We propose Hecto, a lightweight MoE
architecture that leverages architectural heterogeneity by combining a GRU
expert for temporal reasoning and an FFNN expert for static abstraction under a
sparse Top-1 gating mechanism. Evaluated on three reasoning benchmarks (AG
News, SST-2, HotpotQA) and a regression task (STS-B), Hecto matches or closely
trails homogeneous baselines in performance despite receiving isolated input
representations, while achieving clear expert specialization, with each expert
aligning to distinct reasoning types (temporal vs static). At larger batch
sizes, Hecto exhibits improved performance, benefiting from relaxed
computational constraints that allow its heterogeneous architecture to optimize
more effectively. Ablation results isolate architectural diversity as the
source of Hecto's stability and interpretability across diverse reasoning
tasks. Overall, Hecto establishes itself as a new benchmark for conditional
computation, offering a principled framework for specialized reasoning in
low-resource regimes with its model strength derived from principled
specialization.

</details>


### [121] [Improving Rationality in the Reasoning Process of Language Models through Self-playing Game](https://arxiv.org/abs/2506.22920)
*Pinzheng Wang, Juntao Li, Zecheng Tang, Haijia Gui, Min zhang*

**主要类别:** cs.AI

**AI概要:** 通过自我对弈机制，特别是设计的Critic-Discernment Game(CDG)，可以提升大型语言模型在无监督情况下的推理能力。实验表明这种方法在数学推理、逐步错误检测、自我修正和长链推理任务中显著提高了LLM对自身推理过程的理解。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型在许多任务中表现出强大的推理能力，但它们对自己推理过程的理解仍然有限。为了解决这个问题，并且不依赖人类或更优模型的监督，研究者探索了通过自对弈来增强模型推理合理性的方式。

**方法:** 研究者设计了一个名为Critic-Discernment Game(CDG)的游戏，在这个游戏中，一个‘证明者’先提供一个问题的解决方案，然后接受来自‘批评者’的挑战。这些批评可能是建设性的（帮助改进）或误导性的（试图混淆）。‘证明者’需要在面对误导性评论时保持正确答案，同时根据建设性反馈纠正错误。

**结果:** 实验结果表明，CDG训练方法能够显著提高已良好对齐的大型语言模型在理解自身推理过程方面的能力。这种提升在涉及数学推理、逐步错误检测、自我修正以及长链推理的任务中得到了验证。

**结论:** 通过引入Critic-Discernment Game，可以在没有人类或高级模型监督的情况下，有效提升大型语言模型对其推理过程的理解和合理性。这为未来进一步优化LLMs的推理能力提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+Rationality+in+the+Reasoning+Process+of+Language+Models+through+Self-playing+Game，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22920，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22920&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have demonstrated considerable reasoning
abilities in various tasks such as mathematics and coding. However, recent
studies indicate that even the best models lack true comprehension of their
reasoning processes. In this paper, we explore how self-play can enhance the
rationality of models in the reasoning process without supervision from humans
or superior models. We design a Critic-Discernment Game(CDG) in which a prover
first provides a solution to a given problem and is subsequently challenged by
critiques of its solution. These critiques either aim to assist or mislead the
prover. The objective of the prover is to maintain the correct answer when
faced with misleading comments, while correcting errors in response to
constructive feedback. Our experiments on tasks involving mathematical
reasoning, stepwise error detection, self-correction, and long-chain reasoning
demonstrate that CDG training can significantly improve the ability of
well-aligned LLMs to comprehend their reasoning process.

</details>


### [122] [MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning](https://arxiv.org/abs/2506.22992)
*Yulun Jiang, Yekun Chai, Maria Brbić, Michael Moor*

**主要类别:** cs.AI

**AI概要:** 本论文提出了一种名为MARBLE的多模态推理基准，用于评估多模态语言模型在复杂多模态问题中的逐步推理能力。当前最先进的12个模型在MARBLE的任务中表现不佳，显示出复杂推理和感知信息提取仍然是多模态语言模型的重大挑战。


<details>
  <summary>更多</summary>
  
**动机:** 现有的推理基准主要集中在纯文本推理或多模态问题上，但这些问题可以通过直接从非文本模态中检索信息来回答，因此对复杂多模态推理的理解仍然不足。

**方法:** 作者设计了一个名为MARBLE的多模态推理基准，包括两个极具挑战性的任务M-Portal和M-Cube，这些任务需要在空间、视觉和物理约束下制定和理解多步骤计划。通过这些任务评估多模态语言模型的逐步推理能力。

**结果:** 实验结果表明，当前12个先进的多模态语言模型在MARBLE上的表现接近随机水平，在M-Cube任务上准确率为0%。仅在简化子任务中，部分模型超过了随机基线。这说明复杂推理仍是对现有模型的重大挑战。此外，感知仍然是一个瓶颈，模型偶尔无法从视觉输入中提取信息。

**结论:** MARBLE揭示了多模态语言模型在复杂推理和感知方面的局限性，希望借此推动下一代能够跨多种多模态推理步骤进行推理和规划的模型的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MARBLE%3A+A+Hard+Benchmark+for+Multimodal+Spatial+Reasoning+and+Planning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22992，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22992&send_immediately=true&force_search=false)

**原文摘要:** The ability to process information from multiple modalities and to reason
through it step-by-step remains a critical challenge in advancing artificial
intelligence. However, existing reasoning benchmarks focus on text-only
reasoning, or employ multimodal questions that can be answered by directly
retrieving information from a non-text modality. Thus, complex reasoning
remains poorly understood in multimodal domains. Here, we present MARBLE, a
challenging multimodal reasoning benchmark that is designed to scrutinize
multimodal language models (MLLMs) in their ability to carefully reason
step-by-step through complex multimodal problems and environments. MARBLE is
composed of two highly challenging tasks, M-Portal and M-Cube, that require the
crafting and understanding of multistep plans under spatial, visual, and
physical constraints. We find that current MLLMs perform poorly on MARBLE --
all the 12 advanced models obtain near-random performance on M-Portal and 0%
accuracy on M-Cube. Only in simplified subtasks some models outperform the
random baseline, indicating that complex reasoning is still a challenge for
existing MLLMs. Moreover, we show that perception remains a bottleneck, where
MLLMs occasionally fail to extract information from the visual inputs. By
shedding a light on the limitations of MLLMs, we hope that MARBLE will spur the
development of the next generation of models with the ability to reason and
plan across many, multimodal reasoning steps.

</details>


### [123] [AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks](https://arxiv.org/abs/2506.23049)
*Leander Melroy Maben, Gayathri Ganesh Lakshmy, Srijith Radhakrishnan, Siddhant Arora, Shinji Watanabe*

**主要类别:** cs.AI

**AI概要:** 尽管语言和语音技术有所进步，但没有开源系统能够实现完整的语音到语音、多轮对话，并结合工具使用和代理推理。本文介绍了AURA（用于理解、推理和自动工具使用的代理），这是第一个开源的、以语音为核心的助手，能够通过动态工具调用和多轮对话完成复杂的、目标驱动的任务。AURA在级联管道中结合了开放权重的ASR、TTS和LLM，并支持日历预订、联系人查找、网络搜索和电子邮件等工具。其模块化设计允许使用自然语言提示和动作类轻松集成新工具。在VoiceBench上，AURA在OpenBookQA上的得分为92.75%，优于所有开放权重系统并接近GPT-4o，在AlpacaEval上的得分为4.39，与其它开放权重系统竞争。人类评估显示，AURA在复杂的、多轮语音任务上的成功率为90%。


<details>
  <summary>更多</summary>
  
**动机:** 当前没有一个开源系统可以实现完整的语音到语音、多轮对话，并且能够结合工具使用和代理推理。这促使研究者开发一个能够满足这些需求的系统。

**方法:** AURA是一个开源的语音核心助手，它通过结合开放权重的ASR、TTS和LLM在一个级联管道中工作。它支持多种工具如日历预订、联系人查找、网络搜索和电子邮件，并且具有模块化设计，允许轻松集成新的工具。

**结果:** AURA在VoiceBench上的表现优异，OpenBookQA得分92.75%，接近GPT-4o；AlpacaEval得分为4.39，与其它开放权重系统竞争。人类评估显示任务成功率达到90%。

**结论:** AURA是首个开源的、以语音为核心的助手，能够通过动态工具调用和多轮对话完成复杂的、目标驱动的任务，其性能接近闭源顶级模型，并在复杂语音任务中表现出高成功率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AURA%3A+Agent+for+Understanding%2C+Reasoning%2C+and+Automated+Tool+Use+in+Voice-Driven+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23049，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23049&send_immediately=true&force_search=false)

**原文摘要:** Despite advances in language and speech technologies, no open-source system
enables full speech-to-speech, multi-turn dialogue with integrated tool use and
agentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and
Automated Tool Use), the first open-source, speech-native assistant capable of
completing complex, goal-driven tasks through dynamic tool invocation and
multi-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a
cascaded pipeline and supports tools such as calendar booking, contact lookup,
web search, and email. Its modular design allows easy integration of new tools
using natural language prompts and action classes. On VoiceBench, AURA scores
92.75% on OpenBookQA-outperforming all open-weight systems and nearing
GPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.
Human evaluation shows 90% task success on complex, multi-turn speech tasks.

</details>


### [124] [AI's Euclid's Elements Moment: From Language Models to Computable Thought](https://arxiv.org/abs/2506.23080)
*Xinmin Fang, Lingfeng Tao, Zhengxiong Li*

**主要类别:** cs.AI

**AI概要:** 本文提出了一个全面的五阶段进化框架，用以理解人工智能的发展，并表明其轨迹与人类认知技术的历史进步相呼应。


<details>
  <summary>更多</summary>
  
**动机:** 为了提供一种系统性的跨学科模型来解释AI过去的架构转变，并为未来的研究和开发提供具体可行的策略。

**方法:** 通过提出“认知几何”框架，描述AI从专家系统到Transformer的演变，并预测未来的神经符号架构和程序合成。

**结果:** 展示了AI的进化并非线性而是自反性的，工具和见解的反馈循环重塑了AI的基础架构，并且目前正进入具有自我反思能力的'元语言时刻'。

**结论:** 本研究作为三部曲的方法论总结，探讨了AI的经济驱动因素（为什么）和认知本质（是什么），并提供了构建下一代智能系统的理论基础和实际策略。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI%27s+Euclid%27s+Elements+Moment%3A+From+Language+Models+to+Computable+Thought，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23080，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23080&send_immediately=true&force_search=false)

**原文摘要:** This paper presents a comprehensive five-stage evolutionary framework for
understanding the development of artificial intelligence, arguing that its
trajectory mirrors the historical progression of human cognitive technologies.
We posit that AI is advancing through distinct epochs, each defined by a
revolutionary shift in its capacity for representation and reasoning, analogous
to the inventions of cuneiform, the alphabet, grammar and logic, mathematical
calculus, and formal logical systems. This "Geometry of Cognition" framework
moves beyond mere metaphor to provide a systematic, cross-disciplinary model
that not only explains AI's past architectural shifts-from expert systems to
Transformers-but also charts a concrete and prescriptive path forward.
Crucially, we demonstrate that this evolution is not merely linear but
reflexive: as AI advances through these stages, the tools and insights it
develops create a feedback loop that fundamentally reshapes its own underlying
architecture. We are currently transitioning into a "Metalinguistic Moment,"
characterized by the emergence of self-reflective capabilities like
Chain-of-Thought prompting and Constitutional AI. The subsequent stages, the
"Mathematical Symbolism Moment" and the "Formal Logic System Moment," will be
defined by the development of a computable calculus of thought, likely through
neuro-symbolic architectures and program synthesis, culminating in provably
aligned and reliable AI that reconstructs its own foundational representations.
This work serves as the methodological capstone to our trilogy, which
previously explored the economic drivers ("why") and cognitive nature ("what")
of AI. Here, we address the "how," providing a theoretical foundation for
future research and offering concrete, actionable strategies for startups and
developers aiming to build the next generation of intelligent systems.

</details>


### [125] [Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural Study](https://arxiv.org/abs/2506.23107)
*Bing Song, Jianing Liu, Sisi Jian, Chenyang Wu, Vinayak Dixit*

**主要类别:** cs.AI

**AI概要:** 大型语言模型（LLMs）在模拟人类风险决策行为方面显示出潜力，但也存在局限性。研究发现，ChatGPT 4o和o1-mini在多项抽彩任务中表现出比人类更保守的风险偏好，其中o1-mini与人类决策更接近。此外，模型在中文环境下的预测偏差较大，提示语言可能影响模拟效果。


<details>
  <summary>更多</summary>
  
**动机:** 尽管LLMs在许多领域取得了显著进展，但在复杂决策（如风险决策）方面的可靠性仍受到质疑。因此，本研究旨在评估LLMs在模拟人类风险决策行为中的表现。

**方法:** 通过提供人口统计信息给两个LLM（ChatGPT 4o和o1-mini），让它们预测个体在一系列基于彩票的任务中的选择，并将模型生成的决策与来自悉尼、达卡、香港和南京参与者的实际人类反应进行比较。使用CRRA框架分析风险偏好。

**结果:** 两个模型均表现出比人类更保守的风险偏好，其中o1-mini的表现更接近人类决策。此外，中文环境下模型预测与实际反应的偏差更大，表明提示语言可能影响模型性能。

**结论:** LLMs在复制人类风险行为方面具有潜力，但目前仍存在局限性，特别是在语言和文化背景不同的情况下。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Can+Large+Language+Models+Capture+Human+Risk+Preferences%3F+A+Cross-Cultural+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23107，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23107&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have made significant strides, extending their
applications to dialogue systems, automated content creation, and
domain-specific advisory tasks. However, as their use grows, concerns have
emerged regarding their reliability in simulating complex decision-making
behavior, such as risky decision-making, where a single choice can lead to
multiple outcomes. This study investigates the ability of LLMs to simulate
risky decision-making scenarios. We compare model-generated decisions with
actual human responses in a series of lottery-based tasks, using transportation
stated preference survey data from participants in Sydney, Dhaka, Hong Kong,
and Nanjing. Demographic inputs were provided to two LLMs -- ChatGPT 4o and
ChatGPT o1-mini -- which were tasked with predicting individual choices. Risk
preferences were analyzed using the Constant Relative Risk Aversion (CRRA)
framework. Results show that both models exhibit more risk-averse behavior than
human participants, with o1-mini aligning more closely with observed human
decisions. Further analysis of multilingual data from Nanjing and Hong Kong
indicates that model predictions in Chinese deviate more from actual responses
compared to English, suggesting that prompt language may influence simulation
performance. These findings highlight both the promise and the current
limitations of LLMs in replicating human-like risk behavior, particularly in
linguistic and cultural settings.

</details>


### [126] [The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy](https://arxiv.org/abs/2506.23123)
*Rishi Bommasani*

**主要类别:** cs.AI

**AI概要:** 这篇论文探讨了人工智能基础模型的能力、风险及其对社会的影响，并提出了通过科学研究和政策接口改善AI治理以实现更好的社会结果的必要性。


<details>
  <summary>更多</summary>
  
**动机:** 人工智能基础模型技术虽然充满潜力，但同时也带来了许多困惑和危害，对其理解不足促使研究其与社会的共同演化过程。

**方法:** 论文围绕三个主题展开：1) 概念框架：基础模型的能力、风险和供应链；2) 实证见解：通过模型评估和组织指数建立透明度；3) 理解到行动的转变：基于对基础模型社会影响的理解推进证据驱动的AI政策。

**结果:** 论文为在AI时代实现更好的社会结果提供了科学基础和研究-政策接口。

**结论:** 通过构建科学基础和研究-政策接口，可以更好地治理AI，从而在AI时代实现更优的社会成果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Societal+Impact+of+Foundation+Models%3A+Advancing+Evidence-based+AI+Policy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23123，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23123&send_immediately=true&force_search=false)

**原文摘要:** Artificial intelligence is humanity's most promising technology because of
the remarkable capabilities offered by foundation models. Yet, the same
technology brings confusion and consternation: foundation models are poorly
understood and they may precipitate a wide array of harms. This dissertation
explains how technology and society coevolve in the age of AI, organized around
three themes. First, the conceptual framing: the capabilities, risks, and the
supply chain that grounds foundation models in the broader economy. Second, the
empirical insights that enrich the conceptual foundations: transparency created
via evaluations at the model level and indexes at the organization level.
Finally, the transition from understanding to action: superior understanding of
the societal impact of foundation models advances evidence-based AI policy.
View together, this dissertation makes inroads into achieving better societal
outcomes in the age of AI by building the scientific foundations and
research-policy interface required for better AI governance.

</details>


### [127] [Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons](https://arxiv.org/abs/2506.23128)
*Chi Chiu So, Yueyue Sun, Jun-Min Wang, Siu Pang Yung, Anthony Wai Keung Loh, Chun Pong Chau*

**主要类别:** cs.AI

**AI概要:** 研究评估了三个前沿的大语言模型（DeepSeek-R1、DeepSeek-V3和GPT-4o）在家族树和通用图推理基准任务中的表现。实验表明，DeepSeek-R1在多项任务中表现出最高的F1分数，但在问题复杂度增加时，所有模型都面临显著挑战。分析还揭示了DeepSeek-R1独特的规划与验证策略，但也存在不连贯或不完整的推理情况。研究提出了未来工作的关键方向，包括多模态推理和系统性分析推理失败的原因。


<details>
  <summary>更多</summary>
  
**动机:** 了解大语言模型在深度关系推理方面的性能，并比较不同模型的能力，特别是在家族树和通用图推理任务中的表现。

**方法:** 通过一系列精心设计的基准任务来评估和比较DeepSeek-R1、DeepSeek-V3和GPT-4o这三个模型的表现。这些任务涉及家族树和通用图推理。

**结果:** DeepSeek-R1在多个任务和问题规模上实现了最高的F1分数，展现出强大的逻辑推理能力。然而，随着问题复杂性的增加，所有模型都遇到显著困难，主要归因于标记长度限制和输出结构不完整。此外，DeepSeek-R1的长链思维响应显示了独特的规划和验证策略，但也存在不连贯或不完整的推理实例。

**结论:** 该研究提供了关于提升大语言模型推理能力的经验见解和理论意义，特别是对于需要结构化、多步骤逻辑推理的任务。同时指出了未来研究的关键方向，如多模态推理和推理失败的系统性检查。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Are+Large+Language+Models+Capable+of+Deep+Relational+Reasoning%3F+Insights+from+DeepSeek-R1+and+Benchmark+Comparisons，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23128，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23128&send_immediately=true&force_search=false)

**原文摘要:** How far are Large Language Models (LLMs) in performing deep relational
reasoning? In this paper, we evaluate and compare the reasoning capabilities of
three cutting-edge LLMs, namely, DeepSeek-R1, DeepSeek-V3 and GPT-4o, through a
suite of carefully designed benchmark tasks in family tree and general graph
reasoning. Our experiments reveal that DeepSeek-R1 consistently achieves the
highest F1-scores across multiple tasks and problem sizes, demonstrating strong
aptitude in logical deduction and relational inference. However, all evaluated
models, including DeepSeek-R1, struggle significantly as problem complexity
increases, largely due to token length limitations and incomplete output
structures. A detailed analysis of DeepSeek-R1's long Chain-of-Thought
responses uncovers its unique planning and verification strategies, but also
highlights instances of incoherent or incomplete reasoning, calling attention
to the need for deeper scrutiny into LLMs' internal inference dynamics. We
further discuss key directions for future work, including the role of
multimodal reasoning and the systematic examination of reasoning failures. Our
findings provide both empirical insights and theoretical implications for
advancing LLMs' reasoning abilities, particularly in tasks that demand
structured, multi-step logical inference. Our code repository will be publicly
available at https://github.com/kelvinhkcs/Deep-Relational-Reasoning.

</details>


### [128] [Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing](https://arxiv.org/abs/2506.23141)
*Siyuan Li, Ruitong Liu, Yan Wen, Te Sun*

**主要类别:** cs.AI

**AI概要:** 提出了一种语义感知的关系消息传递方法，通过Top-K邻居选择策略和多头注意力聚合器，有效减轻噪声和过平滑问题，提升知识图谱补全性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统基于节点的消息传递机制在知识图谱中引入噪声，导致信息稀释或过平滑问题，无法准确捕捉预测所需的语义上下文。

**方法:** 提出语义感知的Top-K邻居选择策略，在共享潜在空间中评估中心节点与关联边的语义相关性，选出最相关的Top-K边；使用多头注意力聚合器将这些边的信息与中心节点表示融合，生成语义聚焦的节点消息。

**结果:** 大量实验表明，该方法在多个已建立的基准测试中优于现有方法。

**结论:** 所提出的方法能够更准确地捕捉和传播与特定链接预测任务最相关的上下文信息，有效缓解无关信息的干扰。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Context-Driven+Knowledge+Graph+Completion+with+Semantic-Aware+Relational+Message+Passing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23141，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23141&send_immediately=true&force_search=false)

**原文摘要:** Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge
Graph Completion (KGC), providing vital cues for prediction. However,
traditional node-based message passing mechanisms, when applied to knowledge
graphs, often introduce noise and suffer from information dilution or
over-smoothing by indiscriminately aggregating information from all neighboring
edges. To address this challenge, we propose a semantic-aware relational
message passing. A core innovation of this framework is the introduction of a
\textbf{semantic-aware Top-K neighbor selection strategy}. Specifically, this
strategy first evaluates the semantic relevance between a central node and its
incident edges within a shared latent space, selecting only the Top-K most
pertinent ones. Subsequently, information from these selected edges is
effectively fused with the central node's own representation using a
\textbf{multi-head attention aggregator} to generate a semantically focused
node message. In this manner, our model not only leverages the structure and
features of edges within the knowledge graph but also more accurately captures
and propagates the contextual information most relevant to the specific link
prediction task, thereby effectively mitigating interference from irrelevant
information. Extensive experiments demonstrate that our method achieves
superior performance compared to existing approaches on several established
benchmarks.

</details>


### [129] [Rises for Measuring Local Distributivity in Lattices](https://arxiv.org/abs/2506.23168)
*Mohammad Abdulla, Tobias Hille, Dominik Dürrschnabel, Gerd Stumme*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种通过概念格中的升幅来评估分配性的方法，并发现实际数据中的概念格具有高度的并分配性，但交分配性较低。


<details>
  <summary>更多</summary>
  
**动机:** 在形式概念分析（FCA）中，尽管晶格经常表现出高度的分配性，但目前尚无标准化的度量方法来量化这一性质。

**方法:** 引入了概念格中的升幅（rises）作为评估分配性的手段，研究了升幅与经典交并分配性的关系，并观察了实际数据中概念格的分配性特征。

**结果:** 证明了一个晶格是分配性的当且仅当没有非单位升幅发生；实际数据中的概念格高度并分配，但交分配性较低。

**结论:** 升幅可以作为评估晶格分配性的有效工具，揭示了实际数据中晶格分配性的不对称性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rises+for+Measuring+Local+Distributivity+in+Lattices，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23168，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23168&send_immediately=true&force_search=false)

**原文摘要:** Distributivity is a well-established and extensively studied notion in
lattice theory. In the context of data analysis, particularly within Formal
Concept Analysis (FCA), lattices are often observed to exhibit a high degree of
distributivity. However, no standardized measure exists to quantify this
property. In this paper, we introduce the notion of rises in (concept) lattices
as a means to assess distributivity. Rises capture how the number of attributes
or objects in covering concepts change within the concept lattice. We show that
a lattice is distributive if and only if no non-unit rises occur. Furthermore,
we relate rises to the classical notion of meet- and join distributivity. We
observe that concept lattices from real-world data are to a high degree
join-distributive, but much less meet-distributive. We additionally study how
join-distributivity manifests on the level of ordered sets.

</details>


### [130] [FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis](https://arxiv.org/abs/2506.23273)
*Quang Hung Nguyen, Phuong Anh Trinh, Phan Quoc Hung Mai, Tuan Phong Trinh*

**主要类别:** cs.AI

**AI概要:** 尽管大型语言模型取得了进展，但text2sql在复杂和特定领域的查询中仍面临许多挑战。特别是在金融领域，由于数据库设计和财务报告布局在不同金融机构和国家之间差异很大，使得text2sql更具挑战性。本文介绍了FinStat2SQL，这是一个轻量级的text2sql管道，允许对财务报表进行自然语言查询。它针对本地标准（如VAS）进行了定制，在多代理设置中结合了大、小语言模型，用于实体提取、SQL生成和自我修正。我们构建了一个特定领域的数据库，并在一个合成的QA数据集上评估模型。经过微调的7B模型在消费级硬件上实现了61.33%的准确率，响应时间不到4秒，表现优于GPT-4o-mini。FinStat2SQL提供了一个可扩展且成本效益高的金融分析解决方案，使越南企业能够使用AI驱动的查询。


<details>
  <summary>更多</summary>
  
**动机:** 金融领域的数据库设计和财务报告布局在不同金融机构和国家之间差异很大，这使得text2sql更具挑战性。

**方法:** 提出FinStat2SQL，一个轻量级的text2sql管道，该方法针对本地标准进行了定制，在多代理设置中结合了大、小语言模型，用于实体提取、SQL生成和自我修正。

**结果:** 经过微调的7B模型在消费级硬件上实现了61.33%的准确率，响应时间不到4秒，表现优于GPT-4o-mini。

**结论:** FinStat2SQL提供了一个可扩展且成本效益高的金融分析解决方案，使越南企业能够使用AI驱动的查询。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FinStat2SQL%3A+A+Text2SQL+Pipeline+for+Financial+Statement+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23273，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23273&send_immediately=true&force_search=false)

**原文摘要:** Despite the advancements of large language models, text2sql still faces many
challenges, particularly with complex and domain-specific queries. In finance,
database designs and financial reporting layouts vary widely between financial
entities and countries, making text2sql even more challenging. We present
FinStat2SQL, a lightweight text2sql pipeline enabling natural language queries
over financial statements. Tailored to local standards like VAS, it combines
large and small language models in a multi-agent setup for entity extraction,
SQL generation, and self-correction. We build a domain-specific database and
evaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves
61.33\% accuracy with sub-4-second response times on consumer hardware,
outperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient
solution for financial analysis, making AI-powered querying accessible to
Vietnamese enterprises.

</details>


### [131] [Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games](https://arxiv.org/abs/2506.23276)
*David Guzman Piedrahita, Yongjin Yang, Mrinmaya Sachan, Giorgia Ramponi, Bernhard Schölkopf, Zhijing Jin*

**主要类别:** cs.AI

**AI概要:** 大型语言模型（LLMs）在多智能体系统中的合作和社会机制研究。通过公共物品博弈实验，发现不同LLMs在合作行为上表现出四种模式：持续合作、波动、逐渐下降和固定策略。令人惊讶的是，强调推理能力的LLMs在合作方面表现不佳，而一些传统LLMs则表现出高水平的合作。这表明提高推理能力不一定能促进合作。


<details>
  <summary>更多</summary>
  
**动机:** 随着LLMs越来越多地作为自主代理部署，理解它们的合作与社会机制变得越来越重要。特别是在多智能体LLM系统中，如何平衡自身利益与集体福祉是一个关键挑战。本文探讨了在这种情境下成本制裁的难题，即一个智能体是否应该投资自己的资源来激励合作或惩罚背叛。

**方法:** 作者改编了行为经济学中的公共物品博弈，并引入制度选择元素，以观察不同LLMs在重复交互中如何应对社会困境。这种方法使研究人员能够分析这些模型在面对合作与背叛时的行为模式。

**结果:** 研究揭示了四种不同的行为模式：一些模型始终保持高水平的合作；另一些模型在参与和退出之间波动；还有一些模型随着时间推移逐渐减少合作行为；最后，有些模型无论结果如何都坚持固定的策略。值得注意的是，具有推理能力的LLMs（如o1系列）在合作方面存在显著困难，而某些传统LLMs则始终维持高合作水平。

**结论:** 当前提升LLMs的方法主要集中在增强其推理能力，但这并不一定导致更好的合作。这一发现为在需要持续协作的环境中部署LLM代理提供了宝贵的见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Corrupted+by+Reasoning%3A+Reasoning+Language+Models+Become+Free-Riders+in+Public+Goods+Games，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23276，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23276&send_immediately=true&force_search=false)

**原文摘要:** As large language models (LLMs) are increasingly deployed as autonomous
agents, understanding their cooperation and social mechanisms is becoming
increasingly important. In particular, how LLMs balance self-interest and
collective well-being is a critical challenge for ensuring alignment,
robustness, and safe deployment. In this paper, we examine the challenge of
costly sanctioning in multi-agent LLM systems, where an agent must decide
whether to invest its own resources to incentivize cooperation or penalize
defection. To study this, we adapt a public goods game with institutional
choice from behavioral economics, allowing us to observe how different LLMs
navigate social dilemmas over repeated interactions. Our analysis reveals four
distinct behavioral patterns among models: some consistently establish and
sustain high levels of cooperation, others fluctuate between engagement and
disengagement, some gradually decline in cooperative behavior over time, and
others rigidly follow fixed strategies regardless of outcomes. Surprisingly, we
find that reasoning LLMs, such as the o1 series, struggle significantly with
cooperation, whereas some traditional LLMs consistently achieve high levels of
cooperation. These findings suggest that the current approach to improving
LLMs, which focuses on enhancing their reasoning capabilities, does not
necessarily lead to cooperation, providing valuable insights for deploying LLM
agents in environments that require sustained collaboration. Our code is
available at https://github.com/davidguzmanp/SanctSim

</details>


### [132] [GATSim: Urban Mobility Simulation with Generative Agents](https://arxiv.org/abs/2506.23306)
*Qi Liu, Can Li, Wanjing Ma*

**主要类别:** cs.AI

**AI概要:** 传统的基于代理的城市移动模拟依赖于无法捕捉人类旅行决策复杂性、适应性和行为多样性的刚性规则系统。本文提出GATSim（生成型代理交通模拟），通过结合城市移动基础模型、代理认知系统和交通模拟环境的综合架构，创建具有丰富行为特征的生成型代理。这些代理具备社会经济属性、个人生活方式和不断演变的偏好，能够通过心理知情的记忆系统、工具使用能力和终身学习机制影响其移动决策。实验表明，生成型代理在移动场景中的表现与人工标注者相当，并能自然生成宏观交通演化模式。


<details>
  <summary>更多</summary>
  
**动机:** 传统基于代理的城市移动模拟方法存在局限性，无法充分捕捉人类旅行决策的复杂性、适应性和多样性。因此需要一种新的框架来改进这一问题。

**方法:** GATSim框架结合了城市移动基础模型、代理认知系统和交通模拟环境，创建生成型代理。这些代理具备多样化社会经济属性、个人生活方式和不断演变的偏好，通过心理知情记忆系统、工具使用能力和终身学习机制影响其移动决策。

**结果:** 实验表明，生成型代理在移动场景中表现与人工标注者相当，并能自然生成宏观交通演化模式。此外，生成型代理可以通过设计的反思过程，将特定旅行体验转化为普遍见解，实现随时间推移的行为适应。

**结论:** GATSim框架为城市移动模拟提供了新途径，生成型代理表现出可信的旅行行为，具备活动规划和实时反应能力，适应城市移动情境。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GATSim%3A+Urban+Mobility+Simulation+with+Generative+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23306，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23306&send_immediately=true&force_search=false)

**原文摘要:** Traditional agent-based urban mobility simulations rely on rigid rule-based
systems that fail to capture the complexity, adaptability, and behavioral
diversity characteristic of human travel decision-making. Recent advances in
large language models and AI agent technology offer opportunities to create
agents with reasoning capabilities, persistent memory, and adaptive learning
mechanisms. We propose GATSim (Generative-Agent Transport Simulation), a novel
framework that leverages these advances to create generative agents with rich
behavioral characteristics for urban mobility simulation. Unlike conventional
approaches, GATSim agents possess diverse socioeconomic attributes, individual
lifestyles, and evolving preferences that shape their mobility decisions
through psychologically-informed memory systems, tool usage capabilities, and
lifelong learning mechanisms. The main contributions of this study include: (1)
a comprehensive architecture combining an urban mobility foundation model with
agent cognitive systems and transport simulation environment, (2) a fully
functional prototype implementation, and (3) systematic validation
demonstrating that generative agents produce believable travel behaviors.
Through designed reflection processes, generative agents in this study can
transform specific travel experiences into generalized insights, enabling
realistic behavioral adaptation over time with specialized mechanisms for
activity planning and real-time reactive behaviors tailored to urban mobility
contexts. Experiments show that generative agents perform competitively with
human annotators in mobility scenarios while naturally producing macroscopic
traffic evolution patterns. The code for the prototype system is shared at
https://github.com/qiliuchn/gatsim.

</details>


### [133] [The Confidence Paradox: Can LLM Know When It's Wrong](https://arxiv.org/abs/2506.23464)
*Sahil Tripathi, Md Tabrez Nafis, Imran Hussain, Jiechao Gao*

**主要类别:** cs.AI

**AI概要:** HonestVQA是一种新的自我监督诚实校准框架，用于道德对齐的文档视觉问答系统。它通过量化不确定性、调整模型置信度和实际正确性以及强制执行道德响应行为来改善现有系统的伦理响应能力。实验证明，HonestVQA在多个数据集上提高了准确率和F1分数，并减少了过度自信的问题。


<details>
  <summary>更多</summary>
  
**动机:** 当前的DocVQA系统虽然性能优越，但在伦理透明度方面存在不足，经常对模糊问题产生过于自信的答案或无法以可信方式传达不确定性。这在需要伦理责任的领域中带来了显著风险。

**方法:** 引入了HonestVQA框架，这是一种与模型无关的方法，包含：1) 量化不确定性以识别知识空白；2) 使用加权损失函数将模型置信度与实际正确性对齐；3) 通过对比学习强制执行伦理响应行为。此外，提出了两个评估指标：诚实分数（H-Score）和伦理置信指数（ECI）。

**结果:** HonestVQA在SpDocVQA、InfographicsVQA和SROIE数据集上将DocVQA准确率提升了高达4.3%，F1分数也提升了4.3%。同时降低了过自信，H-Score和ECI分别下降了0.072和0.078。在跨域评估中，准确率达到78.9%，F1分数达到76.1%。消融实验表明，如果没有对齐或对比损失，准确率会下降3.8%。

**结论:** HonestVQA有效地解决了现有DocVQA系统在伦理响应方面的不足，提升了系统准确性和可靠性，同时减少了过度自信的问题，展示了强大的泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Confidence+Paradox%3A+Can+LLM+Know+When+It%27s+Wrong，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23464，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23464&send_immediately=true&force_search=false)

**原文摘要:** Document Visual Question Answering (DocVQA) systems are increasingly deployed
in real world applications, yet they remain ethically opaque-often producing
overconfident answers to ambiguous questions or failing to communicate
uncertainty in a trustworthy manner. This misalignment between model confidence
and actual knowledge poses significant risks, particularly in domains requiring
ethical accountability. Existing approaches such as LayoutLMv3, UDOP, and DONUT
have advanced SOTA performance by focusing on architectural sophistication and
accuracy; however, they fall short in ethical responsiveness.
  To address these limitations, we introduce HonestVQA, a self-supervised
honesty calibration framework for ethically aligned DocVQA. Our model-agnostic
method quantifies uncertainty to identify knowledge gaps, aligns model
confidence with actual correctness using weighted loss functions, and enforces
ethical response behavior via contrastive learning. We further introduce two
principled evaluation metrics--Honesty Score (H-Score) and Ethical Confidence
Index (ECI)--to benchmark alignment between confidence, accuracy, and ethical
communication. Empirically, HonestVQA improves DocVQA accuracy by up to 4.3%
and F1 by 4.3% across SpDocVQA, InfographicsVQA, and SROIE datasets. It reduces
overconfidence, lowering H-Score and ECI by 0.072 and 0.078, respectively. In
cross domain evaluation, it achieves up to 78.9% accuracy and 76.1% F1-score,
demonstrating strong generalization. Ablation shows a 3.8% drop in accuracy
without alignment or contrastive loss.

</details>


### [134] [Data Augmentation for Cognitive Behavioral Therapy: Leveraging ERNIE Language Models using Artificial Intelligence](https://arxiv.org/abs/2506.23503)
*Bosubabu Sambana, Kondreddygari Archana, Suram Indhra Sena Reddy, Shaik Meethaigar Jameer Basha, Shaik Karishma*

**主要类别:** cs.AI

**AI概要:** 论文提出了一种基于认知行为疗法（CBT）的框架，结合接受、承诺和数据增强技术，用于分析文本和视觉内容中的负面情绪及认知扭曲。该系统利用了BERT、RoBERTa进行情感分析，T5、PEGASUS进行文本摘要，mT5进行多语言翻译，旨在检测社交媒体数据中的负面情绪和认知扭曲，并预测可能的心理健康问题，如恐惧症和饮食失调等，为心理治疗师提供早期干预工具。


<details>
  <summary>更多</summary>
  
**动机:** 目前缺乏有效的方法来分析社交媒体上的认知路径，而这些路径对于心理治疗师及时有效的在线干预至关重要。

**方法:** 该方法采用CBT框架，结合接受、承诺和数据增强技术，使用自然语言处理模型（如BERT、RoBERTa、T5、PEGASUS和mT5），对文本和视觉内容进行分类和处理，检测负面情绪和认知扭曲，并预测其他潜在的心理健康问题。

**结果:** 该系统不仅能够识别负面思维，还能预测额外的负面副作用和其他潜在的心理健康障碍，从而增强了对心理问题的理解和干预策略。

**结论:** 所提出的系统为心理治疗师提供了强大的工具，可用于早期检测和治疗各种心理问题，提升了CBT在数字时代的效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Data+Augmentation+for+Cognitive+Behavioral+Therapy%3A+Leveraging+ERNIE+Language+Models+using+Artificial+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23503，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23503&send_immediately=true&force_search=false)

**原文摘要:** Cognitive Behavioral Therapy (CBT) is a proven approach for addressing the
irrational thought patterns associated with mental health disorders, but its
effectiveness relies on accurately identifying cognitive pathways to provide
targeted treatment. In today's digital age, individuals often express negative
emotions on social media, where they may reveal cognitive distortions, and in
severe cases, exhibit suicidal tendencies. However, there is a significant gap
in methodologies designed to analyze these cognitive pathways, which could be
critical for psychotherapists aiming to deliver timely and effective
interventions in online environments. Cognitive Behavioral Therapy (CBT)
framework leveraging acceptance, commitment and data augmentation to categorize
and address both textual and visual content as positive or negative.
Specifically, the system employs BERT, RoBERTa for Sentiment Analysis and T5,
PEGASUS for Text Summarization, mT5 for Text Translation in Multiple Languages
focusing on detecting negative emotions and cognitive distortions within social
media data. While existing models are primarily designed to identify negative
thoughts, the proposed system goes beyond this by predicting additional
negative side effects and other potential mental health disorders likes
Phobias, Eating Disorders. This enhancement allows for a more comprehensive
understanding and intervention strategy, offering psychotherapists a powerful
tool for early detection and treatment of various psychological issues.

</details>


### [135] [Hybrid Approach for Electricity Price Forecasting using AlexNet and LSTM](https://arxiv.org/abs/2506.23504)
*Bosubabu Sambana, Kotamsetty Geethika Devi, Bandi Rajeswara Reddy, Galeti Mohammad Hussain, Gownivalla Siddartha*

**主要类别:** cs.AI

**AI概要:** 本论文提出了一种结合AlexNet和LSTM算法的混合模型，用于提高电力价格预测的准确性。通过使用需求、温度、阳光和降雨等重要元素的历史数据，并采用最小-最大缩放和时间窗口等方法，该模型在预测未来电力价格方面表现出比传统单一模型更高的准确性，准确率达到了97.08%。


<details>
  <summary>更多</summary>
  
**动机:** 传统方法在预测电力价格时只关注需求和价格，导致对数据的分析不充分，尤其是在处理外汇时间序列数据时效果不佳。因此，需要一种新的方法来提高预测的准确性。

**方法:** 结合AlexNet和LSTM算法构建一个混合模型，利用历史数据中的关键因素（如需求、温度、阳光、降雨），并采用最小-最大缩放和时间窗口等技术进行电力价格预测。

**结果:** 实验结果表明，该混合模型在预测准确性上优于单独使用的RNN和ANN模型，准确率分别提高了97.08%，而RNN和ANN的准确率为96.64%和96.63%。

**结论:** 提出的混合模型在电力价格预测方面具有更高的准确性，能够更好地处理影响价格的外部变量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hybrid+Approach+for+Electricity+Price+Forecasting+using+AlexNet+and+LSTM，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23504，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23504&send_immediately=true&force_search=false)

**原文摘要:** The recent development of advanced machine learning methods for hybrid models
has greatly addressed the need for the correct prediction of electrical prices.
This method combines AlexNet and LSTM algorithms, which are used to introduce a
new model with higher accuracy in price forecasting. Despite RNN and ANN being
effective, they often fail to deal with forex time sequence data. The
traditional methods do not accurately forecast the prices. These traditional
methods only focus on demand and price which leads to insufficient analysis of
data. To address this issue, using the hybrid approach, which focuses on
external variables that also effect the predicted prices. Nevertheless, due to
AlexNet's excellent feature extraction and LSTM's learning sequential patterns,
the prediction accuracy is vastly increased. The model is built on the past
data, which has been supplied with the most significant elements like demand,
temperature, sunlight, and rain. For example, the model applies methods, such
as minimum-maximum scaling and a time window, to predict the electricity prices
of the future. The results show that this hybrid model is good than the
standalone ones in terms of accuracy. Although we got our accuracy rating of
97.08, it shows higher accompaniments than remaining models RNN and ANN with
accuracies of 96.64 and 96.63 respectively.

</details>


### [136] [Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays](https://arxiv.org/abs/2506.23517)
*Selin Dik, Osman Erdem, Mehmet Dik*

**主要类别:** cs.AI

**AI概要:** 尽管GPTZero在检测纯AI生成内容方面效果显著，但在区分人类撰写的文章时可靠性有限，教育者应谨慎使用AI检测工具。


<details>
  <summary>更多</summary>
  
**动机:** 随着学生使用AI工具的普遍化，教师开始使用如GPTZero和QuillBot等AI检测工具来识别AI编写的内容，但这些检测器的可靠性仍不确定。

**方法:** 研究主要关注GPTZero成功识别AI生成文本的比率，基于不同长度（短、中、长）随机提交的论文进行实验。收集了28篇AI生成和50篇人类撰写的文章数据集，并用GPTZero分别检测其AI生成比例和置信度。

**结果:** 绝大多数AI生成的文章被准确检测（AI生成可信度为91-100%），而人类撰写的文章检测结果波动，存在少量误报。

**结论:** GPTZero在检测纯AI生成内容方面有效，但在区分人类撰写的文章时可靠性有限，教育者不应完全依赖AI检测工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Assessing+GPTZero%27s+Accuracy+in+Identifying+AI+vs.+Human-Written+Essays，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23517，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23517&send_immediately=true&force_search=false)

**原文摘要:** As the use of AI tools by students has become more prevalent, instructors
have started using AI detection tools like GPTZero and QuillBot to detect AI
written text. However, the reliability of these detectors remains uncertain. In
our study, we focused mostly on the success rate of GPTZero, the most-used AI
detector, in identifying AI-generated texts based on different lengths of
randomly submitted essays: short (40-100 word count), medium (100-350 word
count), and long (350-800 word count). We gathered a data set consisting of
twenty-eight AI-generated papers and fifty human-written papers. With this
randomized essay data, papers were individually plugged into GPTZero and
measured for percentage of AI generation and confidence. A vast majority of the
AI-generated papers were detected accurately (ranging from 91-100% AI believed
generation), while the human generated essays fluctuated; there were a handful
of false positives. These findings suggest that although GPTZero is effective
at detecting purely AI-generated content, its reliability in distinguishing
human-authored texts is limited. Educators should therefore exercise caution
when relying solely on AI detection tools.

</details>


### [137] [ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions with LLM-Generated Data](https://arxiv.org/abs/2506.23520)
*Yu Zhang, Ruijie Yu, Jidong Tian, Feng Zhu, Jiapeng Liu, Xiaokang Yang, Yaohui Jin, Yanyan Xu*

**主要类别:** cs.AI

**AI概要:** 提出ChemActor，一个完全微调的大语言模型（LLM），作为化学执行器，用于将非结构化实验程序转换为结构化动作序列。通过引入顺序LLM生成的数据框架和多轮LLM循环审查指标，解决了注释数据不足和质量低的问题，并在反应到描述（R2D）和描述到动作（D2A）任务中取得了最先进的性能。


<details>
  <summary>更多</summary>
  
**动机:** 随着有机化学背景下对机器人合成的兴趣增加，从文献中自动提取化学程序变得至关重要。然而，由于化学语言的固有歧义和开发可靠计算机辅助提取协议所需的人工注释成本高，这一任务仍然具有挑战性。

**方法:** 提出了ChemActor，一个完全微调的大语言模型（LLM），用作化学执行器，以在非结构化实验程序和结构化动作序列之间进行转换。提出了一个顺序LLM生成的数据框架，该框架集成了基于分布差异选择数据的数据选择模块，与通用目的LLM结合，从单一分子输入生成机器可执行的动作。此外，还引入了一个新的多轮LLMs圈审度量标准。

**结果:** 在反应到描述（R2D）和描述到动作（D2A）任务中的广泛实验表明，由LLM生成的数据增强的ChemActor实现了最先进的性能，比基线模型高出10%。

**结论:** ChemActor通过使用LLM生成的数据框架和多轮LLMs圈审度量标准，成功解决了注释数据不足和质量低的问题，并在化学程序的自动提取方面取得了显著进展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ChemActor%3A+Enhancing+Automated+Extraction+of+Chemical+Synthesis+Actions+with+LLM-Generated+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23520，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23520&send_immediately=true&force_search=false)

**原文摘要:** With the increasing interest in robotic synthesis in the context of organic
chemistry, the automated extraction of chemical procedures from literature is
critical. However, this task remains challenging due to the inherent ambiguity
of chemical language and the high cost of human annotation required for
developing reliable computer-aided extraction protocols. Here, we present
ChemActor, a fully fine-tuned large language model (LLM), as a chemical
executor to convert between unstructured experimental procedures and structured
action sequences. We propose a sequential LLM-generated data framework to
address the challenges of insufficient and low-quality annotated data. This
framework integrates a data selection module that selects data based on
distribution divergence, with a general-purpose LLM, to generate
machine-executable actions from a single molecule input. Additionally, we
introduce a novel multi-round LLMs circle review metric, which reflects the
model's advanced understanding of chemical experimental procedures. Extensive
experiments on reaction-to-description (R2D) and description-to-action (D2A)
tasks demonstrate that ChemActor, augmented by LLM-generated data, achieves
state-of-the-art performance, outperforming the baseline model by 10%. The code
is available at: https://github.com/Zhanghahah/ChemActor.

</details>


### [138] [CooT: Learning to Coordinate In-Context with Coordination Transformers](https://arxiv.org/abs/2506.23549)
*Huai-Chih Wang, Hsiang-Chun Chuang, Hsi-Chun Cheng, Dai-Jie Wu, Shao-Hua Sun*

**主要类别:** cs.AI

**AI概要:** 提出了一种新的框架Coordination Transformers (CooT)，通过使用交互历史来快速适应未见过的伙伴，克服现有方法在泛化和训练需求上的限制。CooT在没有显式监督或微调的情况下，学习有效的协调策略，并在实验中显著优于基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的多智能体系统中的协调方法，如自我对弈和基于种群的方法，在面对未见过的伙伴时泛化能力差，或者需要大量的训练。为了克服这些局限性，提出了新的协调框架。

**方法:** 提出Coordination Transformers (CooT)，利用最近的交互历史来快速适应未见过的伙伴行为。该方法通过预测与观察到的伙伴交互一致的动作，专注于适应新的伙伴行为，而不是增加训练伙伴的多样性。CooT在具有互补行为的不同代理对收集的交互轨迹上进行训练。

**结果:** 在Overcooked基准测试中的评估表明，CooT在涉及之前未见过的伙伴的协调任务中显著优于基线方法。人类评估进一步确认了CooT是最有效的协作伙伴，广泛的消融实验突显了其鲁棒性、灵活性和对上下文的敏感性。

**结论:** CooT作为一种新颖的协调框架，能够快速学习有效的协调策略，无需显式监督或微调，并且在多智能体场景中表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CooT%3A+Learning+to+Coordinate+In-Context+with+Coordination+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23549，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23549&send_immediately=true&force_search=false)

**原文摘要:** Effective coordination among artificial agents in dynamic and uncertain
environments remains a significant challenge in multi-agent systems. Existing
approaches, such as self-play and population-based methods, either generalize
poorly to unseen partners or require extensive training. To overcome these
limitations, we propose Coordination Transformers (CooT), a novel in-context
coordination framework that uses recent interaction histories to adapt to
unseen partners rapidly. Unlike previous approaches that primarily aim to
increase the diversity of training partners, CooT explicitly focuses on
adapting to new partner behaviors by predicting actions aligned with observed
partner interactions. Trained on interaction trajectories collected from
diverse pairs of agents with complementary behaviors, CooT quickly learns
effective coordination strategies without explicit supervision or fine-tuning.
Evaluations on the Overcooked benchmark demonstrate that CooT significantly
outperforms baseline methods in coordination tasks involving previously unseen
partners. Human evaluations further confirm CooT as the most effective
collaborative partner, while extensive ablations highlight its robustness,
flexibility, and sensitivity to context in multi-agent scenarios.

</details>


### [139] [MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI](https://arxiv.org/abs/2506.23563)
*Huanjin Yao, Jiaxing Huang, Yawen Qiu, Michael K. Chen, Wenzheng Liu, Wei Zhang, Wenjie Zeng, Xikun Zhang, Jingyi Zhang, Yuxin Song, Wenhao Wu, Dacheng Tao*

**主要类别:** cs.AI

**AI概要:** 提出MMReason，一个新基准来评估多模态大语言模型的长链推理能力，具有挑战性、开放性和多样性的问题。


<details>
  <summary>更多</summary>
  
**动机:** 当前多模态大语言模型（MLLM）基准在评估长链推理能力方面存在不足：缺乏难度和多样性、容易猜测和记忆、以及对中间推理步骤评估不足。

**方法:** 从六个学科收集多难度级别的问题，采用开放式问题形式并通过多模型投票技术过滤掉猜测和记忆相关的问题，提供详细的分步解决方案，并设计基于参考的三元评分机制来评估中间推理步骤。

**结果:** 使用MMReason对主流MLLM进行基准测试并深入分析其推理能力。

**结论:** 期望MMReason能成为推动MLLM推理研究的宝贵资源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MMReason%3A+An+Open-Ended+Multi-Modal+Multi-Step+Reasoning+Benchmark+for+MLLMs+Toward+AGI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23563，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23563&send_immediately=true&force_search=false)

**原文摘要:** Reasoning plays a crucial role in advancing Multimodal Large Language Models
(MLLMs) toward Artificial General Intelligence. However, existing MLLM
benchmarks often fall short in precisely and comprehensively evaluating
long-chain reasoning abilities from three key aspects: (1) lack of difficulty
and diversity, (2) susceptibility to guessability and memorization, (3)
inadequate assessment of intermediate reasoning steps. To fill this gap, we
introduce MMReason, a new benchmark designed to precisely and comprehensively
evaluate MLLM long-chain reasoning capability with diverse, open-ended,
challenging questions. First, we curate challenging questions requiring
multi-step reasoning from various fields (i.e., 6 disciplines) and multiple
difficulty levels (i.e., from pre-university to university, and from
foundational to competition tiers). Second, these questions are reformulated
into an open-ended format and filtered using a multi-model voting technique to
eliminate shortcut cases related to guessing and memorization, ensuring robust
reasoning evaluations. Third, we annotate the questions with detailed
step-by-step solutions, and design a reference-based ternary scoring mechanism
to reliably assess intermediate reasoning steps. With MMReason, we benchmark
popular leading MLLMs and provide an in-depth analysis of their reasoning
capabilities. We hope MMReason will serve as a valuable resource for advancing
MLLM reasoning research. Code will be available at
https://github.com/HJYao00/MMReason.

</details>


### [140] [Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models](https://arxiv.org/abs/2506.23576)
*Maria Carolina Cornelia Wit, Jun Pang*

**主要类别:** cs.AI

**AI概要:** 最近大语言模型（LLM）的进步引发了关于越狱攻击的担忧，即绕过安全机制的提示。本文研究了使用多代理LLM系统作为防御此类攻击的方法。我们评估了三种越狱策略，包括原始的AutoDefense攻击和Deepleaps的两种：BetterDan和JB。通过重现AutoDefense框架，我们将单代理设置与双代理和三代理配置进行了比较。我们的结果表明，多代理系统通过减少假阴性增强了对越狱的抵抗力。然而，其有效性因攻击类型而异，并引入了增加假阳性和计算开销等权衡。这些发现指出了当前自动化防御的局限性，并为提高未来LLM系统的对齐鲁棒性提出了方向。


<details>
  <summary>更多</summary>
  
**动机:** 由于大语言模型的快速发展，越狱攻击成为了一个重要的安全问题，这促使研究者探索有效的防御方法。现有的防御机制存在不足，特别是在面对复杂的越狱策略时。因此，有必要研究多代理系统是否能增强LLM的安全性并减少潜在的漏洞。

**方法:** 研究采用了实验对比的方法，首先重现了AutoDefense框架，然后分别测试了单代理、双代理和三代理配置在应对三种不同越狱策略（AutoDefense、BetterDan、JB）时的表现。通过分析不同配置下的检测结果，评估多代理系统在减少假阴性、提高安全性方面的效果。

**结果:** 实验结果显示，多代理系统能够显著提高对越狱攻击的抵抗能力，特别是减少了假阴性的发生率。然而，不同类型的攻击对其有效性的影响各异，同时多代理系统也带来了假阳性增加和计算成本上升的问题。

**结论:** 虽然多代理LLM系统可以有效提升对越狱攻击的防御能力，但其实际效果受到攻击类型的影响，并伴随着一定的副作用，如假阳性和计算开销的增加。这些结果揭示了当前自动化防御机制的局限性，也为未来改进LLM系统的对齐鲁棒性提供了启示。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+Multi-Agent+Defences+Against+Jailbreaking+Attacks+on+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23576，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23576&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in large language models (LLMs) have raised concerns about
jailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper
investigates the use of multi-agent LLM systems as a defence against such
attacks. We evaluate three jailbreaking strategies, including the original
AutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the
AutoDefense framework, we compare single-agent setups with two- and three-agent
configurations. Our results show that multi-agent systems enhance resistance to
jailbreaks, especially by reducing false negatives. However, its effectiveness
varies by attack type, and it introduces trade-offs such as increased false
positives and computational overhead. These findings point to the limitations
of current automated defences and suggest directions for improving alignment
robustness in future LLM systems.

</details>


### [141] [Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games](https://arxiv.org/abs/2506.23626)
*António Afonso, Iolanda Leite, Alessandro Sestini, Florian Fuchs, Konrad Tollmar, Linus Gisslén*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种基于语言模型的自动化方法，用于迭代微调强化学习代理的奖励函数权重。在赛车任务中，该方法显著提高了代理的成功率，并与人类专家设计的权重表现相当。


<details>
  <summary>更多</summary>
  
**动机:** 强化学习在游戏中的应用虽然广泛，但部署RL代理时存在两个主要挑战：1）设计有效的奖励函数通常需要RL专家；2）当游戏内容或机制改变时，先前调整好的奖励权重可能不再是最优的。为了解决第二个问题，提出了本研究。

**方法:** 提出了一种自动化方法，利用语言模型（LM）根据用户定义的行为目标和之前训练轮次的性能统计摘要，迭代更新RL代理的奖励函数权重。通过闭环过程，LM能够自我纠正和优化其输出，从而无需手动奖励工程即可生成更符合目标的行为。

**结果:** 在赛车任务中评估了该方法，结果显示每次迭代都能持续提高代理性能。仅一次迭代，代理成功率从9%提升到74%。与人类专家手动设计的权重相比，最终迭代中LM调整的代理达到了80%的成功率，平均用时855步，与专家调整代理的94%成功率和850步的表现相当。

**结论:** 基于语言模型的自动化方法可以有效微调强化学习代理的奖励函数权重，显著提升代理性能，且能与人类专家设计的权重相媲美。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-correcting+Reward+Shaping+via+Language+Models+for+Reinforcement+Learning+Agents+in+Games，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23626，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23626&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning (RL) in games has gained significant momentum in
recent years, enabling the creation of different agent behaviors that can
transform a player's gaming experience. However, deploying RL agents in
production environments presents two key challenges: (1) designing an effective
reward function typically requires an RL expert, and (2) when a game's content
or mechanics are modified, previously tuned reward weights may no longer be
optimal. Towards the latter challenge, we propose an automated approach for
iteratively fine-tuning an RL agent's reward function weights, based on a
user-defined language based behavioral goal. A Language Model (LM) proposes
updated weights at each iteration based on this target behavior and a summary
of performance statistics from prior training rounds. This closed-loop process
allows the LM to self-correct and refine its output over time, producing
increasingly aligned behavior without the need for manual reward engineering.
We evaluate our approach in a racing task and show that it consistently
improves agent performance across iterations. The LM-guided agents show a
significant increase in performance from $9\%$ to $74\%$ success rate in just
one iteration. We compare our LM-guided tuning against a human expert's manual
weight design in the racing task: by the final iteration, the LM-tuned agent
achieved an $80\%$ success rate, and completed laps in an average of $855$ time
steps, a competitive performance against the expert-tuned agent's peak $94\%$
success, and $850$ time steps.

</details>


### [142] [HASD: Hierarchical Adaption for pathology Slide-level Domain-shift](https://arxiv.org/abs/2506.23673)
*Jingsong Liu, Han Li, Chen Yang, Michael Deutges, Ario Sadafi, Xin You, Katharina Breininger, Nassir Navab, Peter J. Schüffler*

**主要类别:** cs.AI

**AI概要:** 提出了一种分层适应框架HASD，用于解决病理学AI中的幻灯片级别领域转移问题，通过多尺度特征一致性及高效计算的幻灯片级别领域适配，在乳腺癌和UCEC数据集上验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 病理学AI面临领域转移的关键问题，现有方法专注于图像块而非WSI（全切片影像），无法捕捉临床场景中所需的全局WSI特征。

**方法:** HASD框架包含两个关键部分：1）分层适应框架，整合领域对齐求解器、幻灯片级几何不变性正则化以及块级注意力一致性正则化；2）原型选择机制，降低计算开销。

**结果:** 在五个数据集上的两个幻灯片级别任务中验证了该方法，分别在乳腺癌HER2分级队列中提高了4.1%的AUROC，在UCEC生存预测队列中提升了3.9%的C-index。

**结论:** HASD为病理学机构提供了一个实用且可靠的幻灯片级别领域适配解决方案，同时减少了计算和标注成本。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HASD%3A+Hierarchical+Adaption+for+pathology+Slide-level+Domain-shift，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23673，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23673&send_immediately=true&force_search=false)

**原文摘要:** Domain shift is a critical problem for pathology AI as pathology data is
heavily influenced by center-specific conditions. Current pathology domain
adaptation methods focus on image patches rather than WSI, thus failing to
capture global WSI features required in typical clinical scenarios. In this
work, we address the challenges of slide-level domain shift by proposing a
Hierarchical Adaptation framework for Slide-level Domain-shift (HASD). HASD
achieves multi-scale feature consistency and computationally efficient
slide-level domain adaptation through two key components: (1) a hierarchical
adaptation framework that integrates a Domain-level Alignment Solver for
feature alignment, a Slide-level Geometric Invariance Regularization to
preserve the morphological structure, and a Patch-level Attention Consistency
Regularization to maintain local critical diagnostic cues; and (2) a prototype
selection mechanism that reduces computational overhead. We validate our method
on two slide-level tasks across five datasets, achieving a 4.1\% AUROC
improvement in a Breast Cancer HER2 Grading cohort and a 3.9\% C-index gain in
a UCEC survival prediction cohort. Our method provides a practical and reliable
slide-level domain adaption solution for pathology institutions, minimizing
both computational and annotation costs.

</details>


### [143] [PokéAI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red](https://arxiv.org/abs/2506.23689)
*Zihao Liu, Xinhang Sui, Yueran Song, Siwen Wang*

**主要类别:** cs.AI

**AI概要:** 本论文介绍了PokéAI，一个基于文本的多智能体大型语言模型框架，用于自主玩通关《精灵宝可梦 红》。系统由三个专门的智能体组成：规划、执行和评估智能体，形成闭环决策系统。战斗模块作为初步步骤被开发，结果表明其平均胜率为80.8%，与经验丰富的玩家差距仅为6%。此外，模型的语言能力与其战略推理能力有显著关联，且每个模型展现出独特的游戏风格。


<details>
  <summary>更多</summary>
  
**动机:** 为自主玩游戏创建一个多智能体系统，并探索大型语言模型在游戏中的应用潜力，特别是在战斗场景中的表现及与人类玩家的对比。

**方法:** 设计了一个包括规划、执行和评估三个智能体的系统，其中执行智能体中包含一个战斗模块。通过闭环决策系统进行任务生成、执行和评价。使用大型语言模型进行任务处理并分析其在游戏中的表现。

**结果:** 战斗AI在50次野生战斗中取得了80.8%的平均胜率，仅比有经验的人类玩家低6%。模型在语言相关任务上的得分与其战斗表现有很强的相关性。此外，不同模型展现出独特的游戏风格。

**结论:** PokéAI展示了一个成功的多智能体系统在游戏中的应用，证明了大型语言模型在战略推理和游戏表现方面的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pok%C3%A9AI%3A+A+Goal-Generating%2C+Battle-Optimizing+Multi-agent+System+for+Pokemon+Red，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23689，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23689&send_immediately=true&force_search=false)

**原文摘要:** We introduce Pok\'eAI, the first text-based, multi-agent large language model
(LLM) framework designed to autonomously play and progress through Pok\'emon
Red. Our system consists of three specialized agents-Planning, Execution, and
Critique-each with its own memory bank, role, and skill set. The Planning Agent
functions as the central brain, generating tasks to progress through the game.
These tasks are then delegated to the Execution Agent, which carries them out
within the game environment. Upon task completion, the Critique Agent evaluates
the outcome to determine whether the objective was successfully achieved. Once
verification is complete, control returns to the Planning Agent, forming a
closed-loop decision-making system.
  As a preliminary step, we developed a battle module within the Execution
Agent. Our results show that the battle AI achieves an average win rate of
80.8% across 50 wild encounters, only 6% lower than the performance of an
experienced human player. Furthermore, we find that a model's battle
performance correlates strongly with its LLM Arena score on language-related
tasks, indicating a meaningful link between linguistic ability and strategic
reasoning. Finally, our analysis of gameplay logs reveals that each LLM
exhibits a unique playstyle, suggesting that individual models develop distinct
strategic behaviors.

</details>


### [144] [Agent4S: The Transformation of Research Paradigms from the Perspective of Large Language Models](https://arxiv.org/abs/2506.23692)
*Boyuan Zheng, Zerui Fang, Zhe Xu, Rui Wang, Yiwen Chen, Cunshi Wang, Mengwei Qu, Lei Lei, Zhen Feng, Yan Liu, Yuyang Li, Mingzhou Tan, Jiaji Wu, Jianwei Shuai, Jia Li, Fangfu Ye*

**主要类别:** cs.AI

**AI概要:** 提出Agent for Science (Agent4S)，作为第五科学范式，通过LLM驱动的代理自动完成整个研究工作流，并引入五级分类法描绘从简单任务自动化到完全自主协作的“AI科学家”的路线图。


<details>
  <summary>更多</summary>
  
**动机:** 当前AI for Science仅作为分析工具，未能解决科学研究的核心低效问题。

**方法:** 提出Agent4S概念及五级分类法，涵盖从简单任务自动化到创建完全自主协作的“AI科学家”的完整路径。

**结果:** 定义了科学发现的下一个革命性步骤，提供了清晰的发展蓝图。

**结论:** Agent4S代表了真正的第五科学范式，有潜力彻底改变科学研究的方式。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Agent4S%3A+The+Transformation+of+Research+Paradigms+from+the+Perspective+of+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23692，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23692&send_immediately=true&force_search=false)

**原文摘要:** While AI for Science (AI4S) serves as an analytical tool in the current
research paradigm, it doesn't solve its core inefficiency. We propose "Agent
for Science" (Agent4S)-the use of LLM-driven agents to automate the entire
research workflow-as the true Fifth Scientific Paradigm. This paper introduces
a five-level classification for Agent4S, outlining a clear roadmap from simple
task automation to fully autonomous, collaborative "AI Scientists." This
framework defines the next revolutionary step in scientific discovery.

</details>


### [145] [A New Perspective On AI Safety Through Control Theory Methodologies](https://arxiv.org/abs/2506.23703)
*Lars Ullrich, Walter Zimmer, Ross Greer, Knut Graichen, Alois C. Knoll, Mohan Trivedi*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种基于系统理论和系统分析的新视角，称为数据控制，以促进人工智能安全的发展，并通过跨学科方法利用现有的安全性分析和保障手段。


<details>
  <summary>更多</summary>
  
**动机:** 当前AI系统的安全性保证存在不足，尤其是在关键的现实世界网络物理系统中。虽然数据驱动控制利用了AI的最新进展来改进控制系统，但控制理论也可以反过来被用来提高AI的安全性。

**方法:** 通过跨学科的数据生成过程解释和AI系统抽象，结合系统理论启发和系统分析驱动的方法，提出了数据控制这一新视角。采用自顶向下的方法，在抽象层面上为特定AI系统和应用提供了可细化的安全性分析和保障基础。

**结果:** 提出了数据控制的概念，旨在推动AI工程学利用现有的安全性分析和保障手段，从而推动数据控制范式的进步。

**结论:** 数据控制提供了一个通用的基础，可以为具体的AI系统和应用进行细化，并且准备迎接未来的创新。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+New+Perspective+On+AI+Safety+Through+Control+Theory+Methodologies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23703，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23703&send_immediately=true&force_search=false)

**原文摘要:** While artificial intelligence (AI) is advancing rapidly and mastering
increasingly complex problems with astonishing performance, the safety
assurance of such systems is a major concern. Particularly in the context of
safety-critical, real-world cyber-physical systems, AI promises to achieve a
new level of autonomy but is hampered by a lack of safety assurance. While
data-driven control takes up recent developments in AI to improve control
systems, control theory in general could be leveraged to improve AI safety.
Therefore, this article outlines a new perspective on AI safety based on an
interdisciplinary interpretation of the underlying data-generation process and
the respective abstraction by AI systems in a system theory-inspired and system
analysis-driven manner. In this context, the new perspective, also referred to
as data control, aims to stimulate AI engineering to take advantage of existing
safety analysis and assurance in an interdisciplinary way to drive the paradigm
of data control. Following a top-down approach, a generic foundation for safety
analysis and assurance is outlined at an abstract level that can be refined for
specific AI systems and applications and is prepared for future innovation.

</details>


### [146] [Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments](https://arxiv.org/abs/2506.23706)
*Christoph Schnabl, Daniel Hugenroth, Bill Marino, Alastair R. Beresford*

**主要类别:** cs.AI

**AI概要:** 提出了一种名为可验证审计（Attestable Audits）的方法，可在不互信的情况下保护模型和数据的敏感性，并在典型审计基准上针对Llama-3.1进行了原型可行性验证。


<details>
  <summary>更多</summary>
  
**动机:** 当前的基准测试虽然能够大规模评估AI模型的安全性和合规性，但缺乏可验证的结果，并且在模型知识产权和基准数据集方面缺乏保密性。

**方法:** 通过在可信执行环境中运行的可验证审计（Attestable Audits），使用户能够在与符合规定的AI模型交互时进行验证，同时保护敏感数据，即使在模型提供者和审计者之间不存在信任关系。

**结果:** 构建了一个原型，展示了在典型审计基准上对Llama-3.1进行可行性的验证。

**结论:** 解决了最近AI治理框架中提出的验证挑战，证明了方法的可行性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Attestable+Audits%3A+Verifiable+AI+Safety+Benchmarks+Using+Trusted+Execution+Environments，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23706，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23706&send_immediately=true&force_search=false)

**原文摘要:** Benchmarks are important measures to evaluate safety and compliance of AI
models at scale. However, they typically do not offer verifiable results and
lack confidentiality for model IP and benchmark datasets. We propose Attestable
Audits, which run inside Trusted Execution Environments and enable users to
verify interaction with a compliant AI model. Our work protects sensitive data
even when model provider and auditor do not trust each other. This addresses
verification challenges raised in recent AI governance frameworks. We build a
prototype demonstrating feasibility on typical audit benchmarks against
Llama-3.1.

</details>


### [147] [BayesL: Towards a Logical Framework for Bayesian Networks](https://arxiv.org/abs/2506.23773)
*Stefano M. Nicoletti, Mariëlle Stoelinga*

**主要类别:** cs.AI

**AI概要:** 论文介绍了BayesL，一种用于贝叶斯网络的新逻辑框架，支持查询、验证和假设情景评估。


<details>
  <summary>更多</summary>
  
**动机:** 现有的方法可能无法轻松处理贝叶斯网络中的因果和基于证据的关系推理，也不能方便地进行假设情景评估。

**方法:** 引入了BayesL，这是一种用于指定、查询和验证贝叶斯网络行为的新型逻辑框架。

**结果:** BayesL允许创建关于贝叶斯网络的查询，促进了关于因果和基于证据关系的灵活推理，并允许在不修改模型的情况下进行假设情景评估。

**结论:** BayesL提供了一种新的方法来指定、查询和验证贝叶斯网络的行为，无需对模型进行手动修改即可进行全面的假设情景评估。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BayesL%3A+Towards+a+Logical+Framework+for+Bayesian+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23773，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23773&send_immediately=true&force_search=false)

**原文摘要:** We introduce BayesL, a novel logical framework for specifying, querying, and
verifying the behaviour of Bayesian networks (BNs). BayesL (pronounced "Basil")
is a structured language that allows for the creation of queries over BNs. It
facilitates versatile reasoning concerning causal and evidence-based
relationships, and permits comprehensive what-if scenario evaluations without
the need for manual modifications to the model.

</details>


### [148] [When GNNs Met a Word Equations Solver: Learning to Rank Equations (Extended Technical Report)](https://arxiv.org/abs/2506.23784)
*Parosh Aziz Abdulla, Mohamed Faouzi Atig, Julie Cailler, Chencheng Liang, Philipp Rümmer*

**主要类别:** cs.AI

**AI概要:** 探索了使用图神经网络（GNNs）在求解词方程过程中对词方程进行排序的方法，提出了一种新的基于图的表示方法和三种适应多分类任务以解决方程排序问题的方法。实验结果表明，与最先进的字符串求解器相比，新框架在基准测试中解决了更多问题。


<details>
  <summary>更多</summary>
  
**动机:** 在通过Nielsen变换求解词方程时，方程的处理顺序会显著影响求解器的性能。因此，需要一种方法来优化方程的处理顺序，从而提高求解效率。

**方法:** 1. 提出了一种新的基于图的表示方法，用于表示词方程，保留全局信息。
2. 提出了三种方法将多分类任务适应于方程排序问题。
3. 使用词方程的最小不可满足子集（MUSes）来训练GNN。

**结果:** 实验结果表明，在每个变量在每个方程中最多出现一次的基准测试中，新框架比最先进的字符串求解器解决了更多问题。

**结论:** 使用GNNs对词方程进行排序可以有效提高求解器的性能，特别是在特定类型的基准测试中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+GNNs+Met+a+Word+Equations+Solver%3A+Learning+to+Rank+Equations+%28Extended+Technical+Report%29，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23784，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23784&send_immediately=true&force_search=false)

**原文摘要:** Nielsen transformation is a standard approach for solving word equations: by
repeatedly splitting equations and applying simplification steps, equations are
rewritten until a solution is reached. When solving a conjunction of word
equations in this way, the performance of the solver will depend considerably
on the order in which equations are processed. In this work, the use of Graph
Neural Networks (GNNs) for ranking word equations before and during the solving
process is explored. For this, a novel graph-based representation for word
equations is presented, preserving global information across conjuncts,
enabling the GNN to have a holistic view during ranking. To handle the variable
number of conjuncts, three approaches to adapt a multi-classification task to
the problem of ranking equations are proposed. The training of the GNN is done
with the help of minimum unsatisfiable subsets (MUSes) of word equations. The
experimental results show that, compared to state-of-the-art string solvers,
the new framework solves more problems in benchmarks where each variable
appears at most once in each equation.

</details>


### [149] [Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning](https://arxiv.org/abs/2506.23793)
*Anton Andreychuk, Konstantin Yakovlev, Aleksandr Panov, Alexey Skrynnik*

**主要类别:** cs.AI

**AI概要:** 本研究提出了MAPF-GPT-DDG，一种通过集中式专家数据微调预训练MAPF模型的方法。借助新颖的delta-data生成机制，该方法在测试时显著提升了性能，并在解决方案质量上超越了所有现有的基于学习的MAPF求解器，包括原始的MAPF-GPT。它能够处理单个环境中多达1百万个代理的MAPF实例，为MAPF领域的可扩展性设定了新标准。


<details>
  <summary>更多</summary>
  
**动机:** 多智能体路径寻找（MAPF）问题是多机器人轨迹规划问题的常见抽象，解决其最优解已被证明是NP难的问题。然而，为了实际应用（如物流、搜救等），需要高效且可扩展的求解器。因此，利用机器学习的去中心化次优MAPF求解器逐渐受到关注。

**方法:** 基于最近引入的纯模仿学习求解器MAPF-GPT，研究提出了一种新的方法MAPF-GPT-DDG。该方法利用集中式专家数据对预训练的MAPF模型进行有效的微调，并通过一种新颖的delta-data生成机制加速训练过程。

**结果:** 实验结果表明，MAPF-GPT-DDG在许多测试场景中，解决方案的质量超过了所有现有的基于学习的MAPF求解器，包括原始的MAPF-GPT。此外，它可以处理涉及多达1百万个代理的MAPF实例。

**结论:** MAPF-GPT-DDG在性能和可扩展性方面取得了显著进步，为MAPF领域设定了新的里程碑。这一成果表明，结合集中式专家数据和delta-data生成机制的微调方法在提升MAPF求解器性能方面的潜力巨大。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Advancing+Learnable+Multi-Agent+Pathfinding+Solvers+with+Active+Fine-Tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23793，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23793&send_immediately=true&force_search=false)

**原文摘要:** Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot
trajectory planning problems, where multiple homogeneous robots simultaneously
move in the shared environment. While solving MAPF optimally has been proven to
be NP-hard, scalable, and efficient, solvers are vital for real-world
applications like logistics, search-and-rescue, etc. To this end, decentralized
suboptimal MAPF solvers that leverage machine learning have come on stage.
Building on the success of the recently introduced MAPF-GPT, a pure imitation
learning solver, we introduce MAPF-GPT-DDG. This novel approach effectively
fine-tunes the pre-trained MAPF model using centralized expert data. Leveraging
a novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training
while significantly improving performance at test time. Our experiments
demonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF
solvers, including the original MAPF-GPT, regarding solution quality across
many testing scenarios. Remarkably, it can work with MAPF instances involving
up to 1 million agents in a single environment, setting a new milestone for
scalability in MAPF domains.

</details>


### [150] [A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents](https://arxiv.org/abs/2506.23844)
*Hang Su, Jun Luo, Chang Liu, Xiao Yang, Yichi Zhang, Yinpeng Dong, Jun Zhu*

**主要类别:** cs.AI

**AI概要:** 近期大型语言模型（LLMs）的发展推动了自主AI代理的兴起，这些代理能够在动态、开放的环境中感知、推理和行动。然而，这些能力也引入了新的安全风险。本文分析了代理自主性的基础能力及其对应的安全漏洞，并回顾了最近在不同自治层部署的防御策略，提出了Reflective Risk-Aware Agent Architecture (R2A2)统一认知框架。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型的进步使得能够感知、推理和行动于动态环境中的自主AI代理成为可能，但这些进步带来了记忆中毒、工具误用等新型安全风险。

**方法:** 首先考察支撑代理自主性的结构性基础和关键能力，然后分析代理堆栈中的相应安全漏洞，最后系统性回顾了在不同自治层部署的防御策略，并提出R2A2框架以实现主动安全性。

**结果:** 识别出代理自主性相关的能力和安全漏洞，并通过R2A2框架实现了原则性和主动性安全措施。

**结论:** 为了应对自主AI代理带来的安全挑战，需要从架构上改进，并采用如R2A2这样的统一认知框架来实现主动安全性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Survey+on+Autonomy-Induced+Security+Risks+in+Large+Model-Based+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23844，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23844&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in large language models (LLMs) have catalyzed the rise of
autonomous AI agents capable of perceiving, reasoning, and acting in dynamic,
open-ended environments. These large-model agents mark a paradigm shift from
static inference systems to interactive, memory-augmented entities. While these
capabilities significantly expand the functional scope of AI, they also
introduce qualitatively novel security risks - such as memory poisoning, tool
misuse, reward hacking, and emergent misalignment - that extend beyond the
threat models of conventional systems or standalone LLMs. In this survey, we
first examine the structural foundations and key capabilities that underpin
increasing levels of agent autonomy, including long-term memory retention,
modular tool use, recursive planning, and reflective reasoning. We then analyze
the corresponding security vulnerabilities across the agent stack, identifying
failure modes such as deferred decision hazards, irreversible tool chains, and
deceptive behaviors arising from internal state drift or value misalignment.
These risks are traced to architectural fragilities that emerge across
perception, cognition, memory, and action modules. To address these challenges,
we systematically review recent defense strategies deployed at different
autonomy layers, including input sanitization, memory lifecycle control,
constrained decision-making, structured tool invocation, and introspective
reflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a
unified cognitive framework grounded in Constrained Markov Decision Processes
(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,
and joint reward-risk optimization to enable principled, proactive safety
across the agent's decision-making loop.

</details>


### [151] [Beyond Statistical Learning: Exact Learning Is Essential for General Intelligence](https://arxiv.org/abs/2506.23908)
*András György, Tor Lattimore, Nevena Lazić, Csaba Szepesvári*

**主要类别:** cs.AI

**AI概要:** 当前AI系统在演绎推理任务上的表现不佳，尽管在数学和科学领域取得了重大进展。最先进的系统仍然在简单的演绎推理问题上频繁失败。作者认为这是由于统计学习方法的局限性。为了实现可靠且正确的演绎推理，AI研究需要从优化统计性能转向更雄心勃勃的确切学习范式，该范式要求在所有输入上都正确。


<details>
  <summary>更多</summary>
  
**动机:** 即使是最先进的AI系统也在容易解决的演绎推理任务上经常失败，这使得它们无法实现具备可靠演绎推理能力的人工通用智能。

**方法:** 研究人员应从根本上转变思维方式，从针对推理问题和算法任务的分布优化统计性能，转而采用确切学习范式，该范式要求算法在所有输入上都能保证正确性。

**结果:** 通过采用确切学习范式，可以实现可靠且正确的演绎推理能力，从而推动人工智能向通用智能迈进。

**结论:** 确切学习范式是实现可靠演绎推理的必要条件，也是可行的目标，应该指导未来算法的设计。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Statistical+Learning%3A+Exact+Learning+Is+Essential+for+General+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23908，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23908&send_immediately=true&force_search=false)

**原文摘要:** Sound deductive reasoning -- the ability to derive new knowledge from
existing facts and rules -- is an indisputably desirable aspect of general
intelligence. Despite the major advances of AI systems in areas such as math
and science, especially since the introduction of transformer architectures, it
is well-documented that even the most advanced frontier systems regularly and
consistently falter on easily-solvable deductive reasoning tasks. Hence, these
systems are unfit to fulfill the dream of achieving artificial general
intelligence capable of sound deductive reasoning. We argue that their unsound
behavior is a consequence of the statistical learning approach powering their
development. To overcome this, we contend that to achieve reliable deductive
reasoning in learning-based AI systems, researchers must fundamentally shift
from optimizing for statistical performance against distributions on reasoning
problems and algorithmic tasks to embracing the more ambitious exact learning
paradigm, which demands correctness on all inputs. We argue that exact learning
is both essential and possible, and that this ambitious objective should guide
algorithm design.

</details>


### [152] [Performance of LLMs on Stochastic Modeling Operations Research Problems: From Theory to Practice](https://arxiv.org/abs/2506.23924)
*Akshit Kumar, Tianyi Peng, Yuhang Wu, Assaf Zeevi*

**主要类别:** cs.AI

**AI概要:** 尽管需要进一步努力来实现现实中的随机建模自动化，但最先进的大型语言模型（LLMs）在课堂和实际场景中表现出与人类专家相当的专业能力。这些结果表明了构建辅助运筹学（OR）研究人员并增强其现实世界影响的AI代理人的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 评估大型语言模型（LLMs）解决运筹学（OR）问题的能力，特别是涉及不确定性分析和优化的随机建模问题。这是由于目前对于LLMs在运筹学领域的应用探索不足。

**方法:** 收集研究生级别的作业和博士资格考试问题，并使用开源库SimOpt测试LLMs解决这些问题的能力。通过这些问题和工具，研究LLMs在不确定条件下进行现实决策的能力。

**结果:** 研究表明，虽然要实现随机建模管道的可靠自动化仍需大量工作，但最先进的LLMs在课堂和实际环境中展现出与人类专家相当的专业水平。

**结论:** LLMs有潜力协助运筹学研究人员，并通过自动化增强运筹学在现实世界中的影响力。然而，完全自动化的随机建模过程还需要进一步的研究和发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Performance+of+LLMs+on+Stochastic+Modeling+Operations+Research+Problems%3A+From+Theory+to+Practice，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23924，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23924&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have exhibited expert-level capabilities across
various domains. However, their abilities to solve problems in Operations
Research (OR) -- the analysis and optimization of mathematical models derived
from real-world problems or their verbal descriptions -- remain underexplored.
In this work, we take a first step toward evaluating LLMs' abilities to solve
stochastic modeling problems, a core class of OR problems characterized by
uncertainty and typically involving tools from probability, statistics, and
stochastic processes. We manually procure a representative set of
graduate-level homework and doctoral qualification-exam problems and test LLMs'
abilities to solve them. We further leverage SimOpt, an open-source library of
simulation-optimization problems and solvers, to investigate LLMs' abilities to
make real-world decisions under uncertainty. Our results show that, though a
nontrivial amount of work is still needed to reliably automate the stochastic
modeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on
par with human experts in both classroom and practical settings. These findings
highlight the potential of building AI agents that assist OR researchers and
amplify the real-world impact of OR through automation.

</details>


### [153] [Industrial brain: a human-like autonomous neuro-symbolic cognitive decision-making system](https://arxiv.org/abs/2506.23926)
*Junping Wang, Bicheng Wang, Yibo Xuea, Yuan Xie*

**主要类别:** cs.AI

**AI概要:** 为了应对工业链弹性预测和规划中的挑战，本文提出了一个名为工业大脑的框架，它通过整合神经网络和符号推理技术，实现了从观测数据中直接进行弹性规划，实验表明其性能优于现有方法并具备良好的泛化能力和稳健性。


<details>
  <summary>更多</summary>
  
**动机:** 工业链的弹性非平衡测量对于科学管理和工程应用至关重要，但目前的端到端深度学习方法在面对复杂多变的数据时表现不佳。

**方法:** 提出了一种名为工业大脑（industrial brain）的人类自主认知决策和规划框架，该框架结合了高阶活动驱动的神经网络和CT-OODA符号推理技术，能够直接从全局变量观测数据中自主规划弹性。此框架无需简化假设即可理解并建模节点活动动态结构和网络共演化拓扑，并揭示复杂网络背后的潜在规律。

**结果:** 实验结果表明，工业大脑显著优于现有的弹性预测和规划方法，相较于GoT和OlaGPT框架准确率提高了10.8%，相较于谱维数约简方法提高了11.03%。此外，该模型对未见过的拓扑和动力学具有泛化能力，并在观察干扰下仍能保持稳健性能。

**结论:** 研究发现，工业大脑填补了工业链弹性预测与规划领域的重要空白，为实际应用提供了更准确、更稳健的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Industrial+brain%3A+a+human-like+autonomous+neuro-symbolic+cognitive+decision-making+system，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23926，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23926&send_immediately=true&force_search=false)

**原文摘要:** Resilience non-equilibrium measurement, the ability to maintain fundamental
functionality amidst failures and errors, is crucial for scientific management
and engineering applications of industrial chain. The problem is particularly
challenging when the number or types of multiple co-evolution of resilience
(for example, randomly placed) are extremely chaos. Existing end-to-end deep
learning ordinarily do not generalize well to unseen full-feld reconstruction
of spatiotemporal co-evolution structure, and predict resilience of network
topology, especially in multiple chaos data regimes typically seen in
real-world applications. To address this challenge, here we propose industrial
brain, a human-like autonomous cognitive decision-making and planning framework
integrating higher-order activity-driven neuro network and CT-OODA symbolic
reasoning to autonomous plan resilience directly from observational data of
global variable. The industrial brain not only understands and model structure
of node activity dynamics and network co-evolution topology without simplifying
assumptions, and reveal the underlying laws hidden behind complex networks, but
also enabling accurate resilience prediction, inference, and planning.
Experimental results show that industrial brain significantly outperforms
resilience prediction and planning methods, with an accurate improvement of up
to 10.8\% over GoT and OlaGPT framework and 11.03\% over spectral dimension
reduction. It also generalizes to unseen topologies and dynamics and maintains
robust performance despite observational disturbances. Our findings suggest
that industrial brain addresses an important gap in resilience prediction and
planning for industrial chain.

</details>


### [154] [AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models](https://arxiv.org/abs/2506.23949)
*Anthony M. Barrett, Jessica Newman, Brandie Nonnecke, Nada Madkour, Dan Hendrycks, Evan R. Murphy, Krystal Jackson, Deepika Raman*

**主要类别:** cs.AI

**AI概要:** 日益多用途的AI模型（如大型语言模型或其他通用AI模型）在提供多种有益功能的同时，也伴随着严重的不良事件风险。本文档为识别、分析和缓解这些模型的风险提供了管理实践和控制措施，主要面向大型、尖端的通用AI/基础模型开发者，并且对构建在其上的应用开发者也有指导意义。本文档结合了NIST AI风险管理框架和ISO/IEC 23894等标准，专注于通用AI/基础模型开发者面临的独特问题。


<details>
  <summary>更多</summary>
  
**动机:** 描述日益多用途的AI模型（如大型语言模型或通用AI模型）所带来的风险和益处，强调需要一种系统的方法来管理和缓解这些模型可能带来的严重不良后果。

**方法:** 通过制定风险管理和控制措施文档，帮助开发者识别、分析和缓解通用AI/基础模型的风险，同时与现有AI风险管理标准（如NIST和ISO/IEC 23894）相结合，提出针对通用AI/基础模型的独特问题的具体解决方案。

**结果:** 为通用AI/基础模型的开发者提供了明确的风险管理指南，促进符合相关标准，并帮助下游开发者构建更安全的应用程序。

**结论:** 本文档为通用AI/基础模型的风险管理提供了重要指导，结合现有标准并解决特定问题，有助于推动AI技术的安全和负责任的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI+Risk-Management+Standards+Profile+for+General-Purpose+AI+%28GPAI%29+and+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23949，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23949&send_immediately=true&force_search=false)

**原文摘要:** Increasingly multi-purpose AI models, such as cutting-edge large language
models or other 'general-purpose AI' (GPAI) models, 'foundation models,'
generative AI models, and 'frontier models' (typically all referred to
hereafter with the umbrella term 'GPAI/foundation models' except where greater
specificity is needed), can provide many beneficial capabilities but also risks
of adverse events with profound consequences. This document provides
risk-management practices or controls for identifying, analyzing, and
mitigating risks of GPAI/foundation models. We intend this document primarily
for developers of large-scale, state-of-the-art GPAI/foundation models; others
that can benefit from this guidance include downstream developers of end-use
applications that build on a GPAI/foundation model. This document facilitates
conformity with or use of leading AI risk management-related standards,
adapting and building on the generic voluntary guidance in the NIST AI Risk
Management Framework and ISO/IEC 23894, with a focus on the unique issues faced
by developers of GPAI/foundation models.

</details>


### [155] [Harnessing AI Agents to Advance Research on Refugee Child Mental Health](https://arxiv.org/abs/2506.23992)
*Aditya Shrivastava, Komal Gupta, Shraddha Arora*

**主要类别:** cs.AI

**AI概要:** The research proposes an AI-based framework to process refugee health data focusing on child mental health. It compares two RAG models (Zephyr-7B-beta and DeepSeek R1-7B), finding that DeepSeek R1 is superior with an accuracy of 0.91 in answer relevance.


<details>
  <summary>更多</summary>
  
**动机:** To address the psychological trauma experienced by displaced children due to the international refugee crisis, and to provide a scalable strategy to assist policymakers, mental health practitioners, and humanitarian agencies.

**方法:** Comparison of two Retrieval-Augmented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, for processing unstructured refugee health data while avoiding hallucination hazards.

**结果:** Both models worked properly, but DeepSeek R1 showed significantly better performance with an accuracy of 0.91 in answer relevance.

**结论:** An AI-based framework combining advanced AI methods with migration research and child psychology can effectively assist in recognizing and improving the mental wellbeing of displaced children.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Harnessing+AI+Agents+to+Advance+Research+on+Refugee+Child+Mental+Health，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23992，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23992&send_immediately=true&force_search=false)

**原文摘要:** The international refugee crisis deepens, exposing millions of dis placed
children to extreme psychological trauma. This research suggests a com pact,
AI-based framework for processing unstructured refugee health data and
distilling knowledge on child mental health. We compare two Retrieval-Aug
mented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, to
determine how well they process challenging humanitarian datasets while avoid
ing hallucination hazards. By combining cutting-edge AI methods with migration
research and child psychology, this study presents a scalable strategy to
assist policymakers, mental health practitioners, and humanitarian agencies to
better assist displaced children and recognize their mental wellbeing. In
total, both the models worked properly but significantly Deepseek R1 is
superior to Zephyr with an accuracy of answer relevance 0.91

</details>


### [156] [Constructing Non-Markovian Decision Process via History Aggregator](https://arxiv.org/abs/2506.24026)
*Yongyi Wang, Wenxin Li*

**主要类别:** cs.AI

**AI概要:** 论文提出了一种基于范畴论的通用方法，建立了马尔可夫决策过程（MDP）和非马尔可夫决策过程（NMDP）之间的等价关系，并引入了历史聚合器状态（HAS）以精确控制决策问题的时间序列依赖结构。这种方法有助于更严格、灵活地评估决策算法。


<details>
  <summary>更多</summary>
  
**动机:** 当前基准测试在全面评估决策算法处理非马尔可夫动态的能力方面存在不足。

**方法:** 通过范畴论构建马尔可夫决策过程（MDP）和非马尔可夫决策过程（NMDP）的范畴，并证明其等价关系；引入历史聚合器状态（HAS），以在时间序列中精确控制决策问题的状态依赖结构。

**结果:** 该方法能够表示广泛的非马尔可夫动态，促进对决策算法更严格和灵活的评估。

**结论:** 所提出的方法为理解和解决非马尔可夫动态提供了新视角，并有效提升了决策算法的评估能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Constructing+Non-Markovian+Decision+Process+via+History+Aggregator，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.24026，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.24026&send_immediately=true&force_search=false)

**原文摘要:** In the domain of algorithmic decision-making, non-Markovian dynamics manifest
as a significant impediment, especially for paradigms such as Reinforcement
Learning (RL), thereby exerting far-reaching consequences on the advancement
and effectiveness of the associated systems. Nevertheless, the existing
benchmarks are deficient in comprehensively assessing the capacity of decision
algorithms to handle non-Markovian dynamics. To address this deficiency, we
have devised a generalized methodology grounded in category theory. Notably, we
established the category of Markov Decision Processes (MDP) and the category of
non-Markovian Decision Processes (NMDP), and proved the equivalence
relationship between them. This theoretical foundation provides a novel
perspective for understanding and addressing non-Markovian dynamics. We further
introduced non-Markovianity into decision-making problem settings via the
History Aggregator for State (HAS). With HAS, we can precisely control the
state dependency structure of decision-making problems in the time series. Our
analysis demonstrates the effectiveness of our method in representing a broad
range of non-Markovian dynamics. This approach facilitates a more rigorous and
flexible evaluation of decision algorithms by testing them in problem settings
where non-Markovian dynamics are explicitly constructed.

</details>


### [157] [SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2506.24119)
*Bo Liu, Leon Guertler, Simon Yu, Zichen Liu, Penghui Qi, Daniel Balcells, Mickel Liu, Cheston Tan, Weiyan Shi, Min Lin, Wee Sun Lee, Natasha Jaques*

**主要类别:** cs.AI

**AI概要:** 近期强化学习的进步表明，语言模型可以通过在具有可验证奖励的任务上进行训练来发展复杂的推理能力，但这些方法依赖于人工策划的问题-答案对和领域特定的奖励工程。我们引入了SPIRAL，一种自我游戏框架，模型通过与不断改进的自身版本进行多回合、零和游戏来自我学习，消除了对人类监督的需求。通过自我游戏，SPIRAL生成了一个无限的课程，包含逐渐更具挑战性的问题，因为模型必须不断适应更强的对手。为了实现大规模的自我游戏训练，我们为LLMs实现了一个完全在线、多回合、多代理的强化学习系统，并提出了角色条件优势估计（RAE）以稳定多代理训练。使用SPIRAL，仅在Kuhn Poker上的自我游戏训练就能使Qwen3-4B-Base在数学和通用推理方面分别提高8.6%和8.4%，超越了25,000个专家游戏轨迹的SFT表现。分析显示，这种迁移是通过三种认知模式实现的：系统分解、期望值计算和逐案分析。多游戏训练（井字棋、Kuhn Poker、简单谈判）进一步提升了性能，因为每个游戏都发展出独特的推理优势。将SPIRAL应用于强大的推理模型（DeepSeek-R1-Distill-Qwen-7B）仍可带来2.0%的平均提升。这些结果表明，零和游戏自然发展出可转移的推理能力，展示了自主推理发展的有前途方向。


<details>
  <summary>更多</summary>
  
**动机:** 当前的强化学习方法依赖于人工策划的问题-答案对和领域特定的奖励工程，这限制了其应用范围。研究者希望开发一种不需要人类监督的方法，让模型能够通过自我游戏不断提升自身的推理能力。

**方法:** 提出了一种名为SPIRAL的自我游戏框架，模型通过与不断改进的自身版本进行多回合、零和游戏来自我学习。为了支持这一框架，研究者实现了大规模的多代理强化学习系统，并提出了角色条件优势估计（RAE）技术以稳定训练过程。

**结果:** 在Kuhn Poker上的实验表明，仅使用SPIRAL进行自我游戏训练即可显著提升Qwen3-4B-Base在数学和通用推理任务上的表现。此外，多游戏训练进一步增强了模型的推理能力，即使在已经具备强大推理能力的模型上也能观察到性能提升。

**结论:** 零和游戏是一种有效的训练方式，可以自然地发展出可转移的推理能力，为自主推理的发展提供了一条有前途的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SPIRAL%3A+Self-Play+on+Zero-Sum+Games+Incentivizes+Reasoning+via+Multi-Agent+Multi-Turn+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.24119，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.24119&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in reinforcement learning have shown that language models can
develop sophisticated reasoning through training on tasks with verifiable
rewards, but these approaches depend on human-curated problem-answer pairs and
domain-specific reward engineering. We introduce SPIRAL, a self-play framework
where models learn by playing multi-turn, zero-sum games against continuously
improving versions of themselves, eliminating the need for human supervision.
Through self-play, SPIRAL generates an infinite curriculum of progressively
challenging problems as models must constantly adapt to stronger opponents. To
enable this self-play training at scale, We implement a fully online,
multi-turn, multi-agent reinforcement learning system for LLMs and propose
role-conditioned advantage estimation (RAE) to stabilize multi-agent training.
Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that
transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%
improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000
expert game trajectories. Analysis reveals that this transfer occurs through
three cognitive patterns: systematic decomposition, expected value calculation,
and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple
Negotiation) further enhances performance as each game develops distinct
reasoning strengths. Applying SPIRAL to a strong reasoning model
(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These
results demonstrate that zero-sum games naturally develop transferable
reasoning capabilities, highlighting a promising direction for autonomous
reasoning development.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [158] [Strategic A/B testing via Maximum Probability-driven Two-armed Bandit](https://arxiv.org/abs/2506.22536)
*Yu Zhang, Shanshan Zhao, Bokui Wan, Jinjuan Wang, Xiaodong Yan*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种基于反事实结果框架的最大概率驱动两臂老虎机（TAB）过程，通过加权均值波动统计量控制第一类错误，并结合置换方法提高稳健性和功效。所建立的战略中心极限定理（SCLT）表明该方法在零假设下分布更集中，在备择假设下分布更分散，从而显著提高了统计功效。实验结果表明，该方法在A/B测试中表现优异，有望降低实验成本同时保持高统计功效。


<details>
  <summary>更多</summary>
  
**动机:** 检测大规模应用中的微小平均处理效应是一项重大挑战，因为即使是微小的改进也可能带来显著的经济影响。传统方法由于对小差异不敏感，往往无法识别这些微小效应。

**方法:** 利用反事实结果框架，提出了一个最大概率驱动的两臂老虎机（TAB）过程，通过对均值波动统计量进行加权来控制第一类错误。同时，实施了置换方法以进一步增强稳健性和功效。此外，还建立了战略中心极限定理（SCLT）。

**结果:** 所提出的方法在零假设下产生更集中的分布，在备择假设下产生更分散的分布，从而显著提高了统计功效。实验结果表明，该方法在A/B测试中表现出显著改进，能够减少实验成本并维持高统计功效。

**结论:** 本文提出的方法能够有效检测微小平均处理效应，为实际应用中的A/B测试提供了一种高效且低成本的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Strategic+A%2FB+testing+via+Maximum+Probability-driven+Two-armed+Bandit，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22536，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22536&send_immediately=true&force_search=false)

**原文摘要:** Detecting a minor average treatment effect is a major challenge in
large-scale applications, where even minimal improvements can have a
significant economic impact. Traditional methods, reliant on normal
distribution-based or expanded statistics, often fail to identify such minor
effects because of their inability to handle small discrepancies with
sufficient sensitivity. This work leverages a counterfactual outcome framework
and proposes a maximum probability-driven two-armed bandit (TAB) process by
weighting the mean volatility statistic, which controls Type I error. The
implementation of permutation methods further enhances the robustness and
efficacy. The established strategic central limit theorem (SCLT) demonstrates
that our approach yields a more concentrated distribution under the null
hypothesis and a less concentrated one under the alternative hypothesis,
greatly improving statistical power. The experimental results indicate a
significant improvement in the A/B testing, highlighting the potential to
reduce experimental costs while maintaining high statistical power.

</details>


### [159] [Adjoint Schrödinger Bridge Sampler](https://arxiv.org/abs/2506.22565)
*Guan-Horng Liu, Jaemoo Choi, Yongxin Chen, Benjamin Kurt Miller, Ricky T. Q. Chen*

**主要类别:** stat.ML

**AI概要:** 提出了一种新的扩散采样器ASBS，通过简单的匹配目标学习过程，无需在训练期间估计目标样本，提高了采样效率并证明了其收敛性。


<details>
  <summary>更多</summary>
  
**动机:** 由于缺乏明确的目标样本，先前的扩散采样方法（diffusion samplers）通常需要重要性加权估计或复杂的學習过程，这限制了它们的实际应用。因此，需要一种更简单和可扩展的方法来解决这一问题。

**方法:** 提出了Adjoint Schrödinger Bridge Sampler (ASBS)，这是一种基于Schrödinger Bridge数学模型的新扩散采样器。该方法使用简单的、可扩展的匹配目标，避免了在训练过程中对目标样本的估计，并通过伴随匹配（Adjoint Matching）以大规模学习SB-based扩散采样器，同时证明了其收敛到全局解。此外，ASBS推广了最近的伴随采样方法（Adjoint Sampling），使其适用于任意源分布。

**结果:** 通过广泛的实验，展示了ASBS在从经典能量函数采样、摊销构象生成以及分子Boltzmann分布中的有效性。

**结论:** ASBS提供了一种简单且可扩展的解决方案，用于从仅通过非标准化能量函数已知的目标分布中进行采样，解决了现有方法的复杂性和效率问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adjoint+Schr%C3%B6dinger+Bridge+Sampler，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22565，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22565&send_immediately=true&force_search=false)

**原文摘要:** Computational methods for learning to sample from the Boltzmann distribution
-- where the target distribution is known only up to an unnormalized energy
function -- have advanced significantly recently. Due to the lack of explicit
target samples, however, prior diffusion-based methods, known as diffusion
samplers, often require importance-weighted estimation or complicated learning
processes. Both trade off scalability with extensive evaluations of the energy
and model, thereby limiting their practical usage. In this work, we propose
Adjoint Schr\"odinger Bridge Sampler (ASBS), a new diffusion sampler that
employs simple and scalable matching-based objectives yet without the need to
estimate target samples during training. ASBS is grounded on a mathematical
model -- the Schr\"odinger Bridge -- which enhances sampling efficiency via
kinetic-optimal transportation. Through a new lens of stochastic optimal
control theory, we demonstrate how SB-based diffusion samplers can be learned
at scale via Adjoint Matching and prove convergence to the global solution.
Notably, ASBS generalizes the recent Adjoint Sampling (Havens et al., 2025) to
arbitrary source distributions by relaxing the so-called memoryless condition
that largely restricts the design space. Through extensive experiments, we
demonstrate the effectiveness of ASBS on sampling from classical energy
functions, amortized conformer generation, and molecular Boltzmann
distributions.

</details>


### [160] [Bayesian Invariance Modeling of Multi-Environment Data](https://arxiv.org/abs/2506.22675)
*Luhuan Wu, Mingzhang Yin, Yixin Wang, John P. Cunningham, David M. Blei*

**主要类别:** stat.ML

**AI概要:** 开发了贝叶斯不变预测（BIP）模型，通过概率方法进行不变预测，能够更准确和可扩展地识别不变特征。


<details>
  <summary>更多</summary>
  
**动机:** 以前的方法主要通过假设检验或正则化优化来解决不变预测问题，但本论文希望通过概率模型提供更高效和准确的解决方案。

**方法:** 创建了BIP模型，将不变特征的索引编码为潜在变量，并通过后验推断恢复它们。同时设计了一个有效的变分逼近VI-BIP以处理大量特征。

**结果:** 在模拟和真实数据中，BIP和VI-BIP比现有方法更准确且更具扩展性。证明了后验是一致的，并且环境异质性越大，后验收缩越快。

**结论:** BIP和VI-BIP提供了比现有方法更准确和可扩展的不变预测手段，有助于泛化到新环境和揭示因果机制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bayesian+Invariance+Modeling+of+Multi-Environment+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22675，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22675&send_immediately=true&force_search=false)

**原文摘要:** Invariant prediction [Peters et al., 2016] analyzes feature/outcome data from
multiple environments to identify invariant features - those with a stable
predictive relationship to the outcome. Such features support generalization to
new environments and help reveal causal mechanisms. Previous methods have
primarily tackled this problem through hypothesis testing or regularized
optimization. Here we develop Bayesian Invariant Prediction (BIP), a
probabilistic model for invariant prediction. BIP encodes the indices of
invariant features as a latent variable and recover them by posterior
inference. Under the assumptions of Peters et al. [2016], the BIP posterior
targets the true invariant features. We prove that the posterior is consistent
and that greater environment heterogeneity leads to faster posterior
contraction. To handle many features, we design an efficient variational
approximation called VI-BIP. In simulations and real data, we find that BIP and
VI-BIP are more accurate and scalable than existing methods for invariant
prediction.

</details>


### [161] [CN-SBM: Categorical Block Modelling For Primary and Residual Copy Number Variation](https://arxiv.org/abs/2506.22963)
*Kevin Lam, William Daniels, J Maxwell Douglas, Daniel Lai, Samuel Aparicio, Benjamin Bloem-Reddy, Yongjin Park*

**主要类别:** stat.ML

**AI概要:** 开发了一种名为CN-SBM的概率框架，用于根据离散拷贝数状态同时聚类样本和基因组区域，通过分解CNV数据为主成分和残差成分，捕捉亚群特异性模式，改进了模型拟合，并在TCGA低级别胶质瘤数据中发现了具有临床相关性的亚型。


<details>
  <summary>更多</summary>
  
**动机:** 癌症是一种可以通过追踪全基因组拷贝数变异（CNVs）来监测克隆进化的遗传疾病。然而，现有的基于高斯或泊松假设的模型未能充分尊重CNV呼叫的离散性质。

**方法:** 引入了Copy Number Stochastic Block Model (CN-SBM)，一种基于二分分类块模型的概率框架，该框架利用离散拷贝数状态联合聚类样本和基因组区域，并通过块结构捕获特定亚群的模式。采用两阶段方法将CNV数据分解为主要和残差成分，从而检测大尺度染色体改变和更精细的异常。推导出可扩展的变分推理算法以应用于大规模队列和高分辨率数据。

**结果:** 基准测试显示，在模拟和真实数据集上，CN-SBM相比现有方法具有更好的模型拟合度。应用到TCGA低级别胶质瘤数据时，CN-SBM揭示了具有临床相关性的亚型和有结构的残差变化，有助于生存分析中的患者分层。

**结论:** CN-SBM被确立为一个可解释且可扩展的CNV分析框架，直接关联肿瘤异质性和预后。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CN-SBM%3A+Categorical+Block+Modelling+For+Primary+and+Residual+Copy+Number+Variation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22963，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22963&send_immediately=true&force_search=false)

**原文摘要:** Cancer is a genetic disorder whose clonal evolution can be monitored by
tracking noisy genome-wide copy number variants. We introduce the Copy Number
Stochastic Block Model (CN-SBM), a probabilistic framework that jointly
clusters samples and genomic regions based on discrete copy number states using
a bipartite categorical block model. Unlike models relying on Gaussian or
Poisson assumptions, CN-SBM respects the discrete nature of CNV calls and
captures subpopulation-specific patterns through block-wise structure. Using a
two-stage approach, CN-SBM decomposes CNV data into primary and residual
components, enabling detection of both large-scale chromosomal alterations and
finer aberrations. We derive a scalable variational inference algorithm for
application to large cohorts and high-resolution data. Benchmarks on simulated
and real datasets show improved model fit over existing methods. Applied to
TCGA low-grade glioma data, CN-SBM reveals clinically relevant subtypes and
structured residual variation, aiding patient stratification in survival
analysis. These results establish CN-SBM as an interpretable, scalable
framework for CNV analysis with direct relevance for tumor heterogeneity and
prognosis.

</details>


### [162] [AICO: Feature Significance Tests for Supervised Learning](https://arxiv.org/abs/2506.23396)
*Kay Giesecke, Enguerrand Horel, Chartsiri Jirachotkulthorn*

**主要类别:** stat.ML

**AI概要:** 这篇论文提出了一种模型和分布无关的显著性检验方法，用于评估任何回归或分类算法中输入特征的影响。通过遮蔽特征值来评估其对模型性能的增量贡献，并构建了一个随机符号检验以生成精确的p值和置信区间。该方法在大规模、高维度设置下仍然计算高效，无需模型重新训练或辅助模型。实验验证了其统计和计算优势，并展示了实际应用的价值。


<details>
  <summary>更多</summary>
  
**动机:** 监督学习算法的不透明性阻碍了科学发现并限制了其广泛应用，特别是在高风险领域。因此，需要一种通用的方法来评估任何回归或分类算法中输入特征的重要性。

**方法:** 该方法通过遮蔽特征值来评估其对模型性能的增量贡献。在零假设下，测试集上性能差异的分布具有非正的中位数。构建了一个均匀最有力的随机符号检验，用于检测中位数，从而生成精确的p值和置信区间。这种方法避免了模型重新训练或辅助模型，且在大规模、高维度数据下保持计算高效。

**结果:** 实验表明，该方法在合成任务中具有统计和计算优势，并在真实数据应用中展示了其实用价值。

**结论:** 所提出的显著性检验方法为评估特征重要性提供了一种模型和分布无关的解决方案，具有精确性和计算效率，适用于各种实际场景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AICO%3A+Feature+Significance+Tests+for+Supervised+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23396，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23396&send_immediately=true&force_search=false)

**原文摘要:** The opacity of many supervised learning algorithms remains a key challenge,
hindering scientific discovery and limiting broader deployment -- particularly
in high-stakes domains. This paper develops model- and distribution-agnostic
significance tests to assess the influence of input features in any regression
or classification algorithm. Our method evaluates a feature's incremental
contribution to model performance by masking its values across samples. Under
the null hypothesis, the distribution of performance differences across a test
set has a non-positive median. We construct a uniformly most powerful,
randomized sign test for this median, yielding exact p-values for assessing
feature significance and confidence intervals with exact coverage for
estimating population-level feature importance. The approach requires minimal
assumptions, avoids model retraining or auxiliary models, and remains
computationally efficient even for large-scale, high-dimensional settings.
Experiments on synthetic tasks validate its statistical and computational
advantages, and applications to real-world data illustrate its practical
utility.

</details>


### [163] [DPOT: A DeepParticle method for Computation of Optimal Transport with convergence guarantee](https://arxiv.org/abs/2506.23429)
*Yingyuan Li, Aokun Wang, Zhongjian Wang*

**主要类别:** stat.ML

**AI概要:** 提出了一种基于DeepParticle方法的新机器学习方法，用于从未配对样本中计算两个连续分布之间的最优传输映射。该方法在训练过程中产生最小优化，并且不对网络结构施加任何限制。理论上建立了弱收敛性和学习映射与最优传输映射之间的定量误差界限。数值实验验证了理论结果和新方法的有效性，特别是在现实任务中的表现。


<details>
  <summary>更多</summary>
  
**动机:** 现有的计算最优传输映射的方法可能受限于网络结构或缺乏理论保证，本研究旨在提出一种新的无限制的机器学习方法以克服这些问题。

**方法:** 使用DeepParticle方法设计了一种新的机器学习方法，该方法通过从未配对样本中学习来计算连续分布间的最优传输映射，同时不施加任何网络结构限制。

**结果:** 理论分析证明了弱收敛性和提供了定量误差边界；数值实验进一步验证了理论结果和方法的有效性，尤其在实际任务中的表现。

**结论:** 所提出的方法不仅在理论上得到了支持，而且在实践应用中也表现出色，为计算最优传输映射提供了一种有前景的新途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DPOT%3A+A+DeepParticle+method+for+Computation+of+Optimal+Transport+with+convergence+guarantee，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23429，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23429&send_immediately=true&force_search=false)

**原文摘要:** In this work, we propose a novel machine learning approach to compute the
optimal transport map between two continuous distributions from their unpaired
samples, based on the DeepParticle methods. The proposed method leads to a
min-min optimization during training and does not impose any restriction on the
network structure. Theoretically we establish a weak convergence guarantee and
a quantitative error bound between the learned map and the optimal transport
map. Our numerical experiments validate the theoretical results and the
effectiveness of the new approach, particularly on real-world tasks.

</details>


### [164] [Minimax Optimal Two-Stage Algorithm For Moment Estimation Under Covariate Shift](https://arxiv.org/abs/2506.23453)
*Zhen Zhang, Xin Liu, Shaoli Wang, Jiaye Teng*

**主要类别:** stat.ML

**AI概要:** 在协变量偏移情况下，当源和目标分布已知时，估计未知函数的矩的最小最大下界。提出两阶段算法实现最优边界，并提出截断版本确保双重稳健性。


<details>
  <summary>更多</summary>
  
**动机:** 协变量偏移导致训练和测试阶段输入特征分布不同，真实场景中常见但未充分研究的问题是估计未知函数的矩。

**方法:** 1. 提出两阶段算法：先基于源分布训练最优估计器，再用似然比重加权校准矩估计器。
2. 提出截断版估计器解决源和目标分布未知及似然比不稳定问题，确保双重稳健性。

**结果:** 理论分析表明该方法达到最小最大最优边界（对数因子内），数值实验验证了理论发现并证明方法有效性。

**结论:** 提出的两阶段算法及其截断版本能有效应对协变量偏移下的矩估计问题，具有理论保证和实践价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Minimax+Optimal+Two-Stage+Algorithm+For+Moment+Estimation+Under+Covariate+Shift，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23453，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23453&send_immediately=true&force_search=false)

**原文摘要:** Covariate shift occurs when the distribution of input features differs
between the training and testing phases. In covariate shift, estimating an
unknown function's moment is a classical problem that remains under-explored,
despite its common occurrence in real-world scenarios. In this paper, we
investigate the minimax lower bound of the problem when the source and target
distributions are known. To achieve the minimax optimal bound (up to a
logarithmic factor), we propose a two-stage algorithm. Specifically, it first
trains an optimal estimator for the function under the source distribution, and
then uses a likelihood ratio reweighting procedure to calibrate the moment
estimator. In practice, the source and target distributions are typically
unknown, and estimating the likelihood ratio may be unstable. To solve this
problem, we propose a truncated version of the estimator that ensures double
robustness and provide the corresponding upper bound. Extensive numerical
studies on synthetic examples confirm our theoretical findings and further
illustrate the effectiveness of our proposed method.

</details>


### [165] [Test of partial effects for Frechet regression on Bures-Wasserstein manifolds](https://arxiv.org/abs/2506.23487)
*Haoshu Xu, Hongzhe Li*

**主要类别:** stat.ML

**AI概要:** 提出了一种新的测试方法，用于评估Bures Wasserstein流形上Frechet回归的部分效应。该方法使用样本分割策略：第一部分样本用于拟合Frechet回归模型，得到协方差矩阵及其相关的最优传输映射的估计；第二部分样本用于构建检验统计量。证明了该统计量在分布上收敛到加权卡方分量混合分布，权重为由适当RKHS核定义的积分算子的特征值。还证明了该程序达到名义上的渐近大小，并且其最差情况下的功效一致收敛到1。通过广泛的模拟和实际数据应用，展示了该测试在有限样本中的准确性和实用性。


<details>
  <summary>更多</summary>
  
**动机:** 在Bures Wasserstein流形上进行Frechet回归时，需要一种有效的方法来评估部分效应。现有的方法可能不适用于这种复杂的几何结构，因此需要开发新的测试方法以适应这一需求。

**方法:** 采用样本分割策略，将数据分为两部分：一部分用于拟合Frechet回归模型，得到协方差矩阵及其相关最优传输映射的估计；另一部分用于构建检验统计量。利用适当的RKHS核定义积分算子，确定权重，从而形成加权卡方分量混合分布。

**结果:** 证明了检验统计量在分布上收敛到加权卡方分量混合分布，权重为积分算子的特征值。该程序达到名义上的渐近大小，最差情况下的功效一致收敛到1。通过模拟和实际数据应用，验证了该测试在有限样本中的准确性和实用性。

**结论:** 所提出的测试方法能够有效地评估Bures Wasserstein流形上Frechet回归的部分效应，在理论和实践中均表现出良好的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Test+of+partial+effects+for+Frechet+regression+on+Bures-Wasserstein+manifolds，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.23487，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.23487&send_immediately=true&force_search=false)

**原文摘要:** We propose a novel test for assessing partial effects in Frechet regression
on Bures Wasserstein manifolds. Our approach employs a sample splitting
strategy: the first subsample is used to fit the Frechet regression model,
yielding estimates of the covariance matrices and their associated optimal
transport maps, while the second subsample is used to construct the test
statistic. We prove that this statistic converges in distribution to a weighted
mixture of chi squared components, where the weights correspond to the
eigenvalues of an integral operator defined by an appropriate RKHS kernel. We
establish that our procedure achieves the nominal asymptotic size and
demonstrate that its worst-case power converges uniformly to one. Through
extensive simulations and a real data application, we illustrate the test's
finite-sample accuracy and practical utility.

</details>
